number,title,labels,body,assignees,milestone,comments,created_at,author_association,address_date
16731,Fixed a couple of typos,cla: yes,Fixed a couple of typos,0,,3,2018-02-03T18:42:26Z,CONTRIBUTOR,2018-02-03T18:48:31Z
16716,Linker Tools Error encountered when use StepStats,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10.0.16299
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.5 release
- **Python version**: 
3.5.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 



### Describe the problem

Encounter link error when build the program (source code attached). Build went through well with TF 1.4 release 

Error	LNK2001	unresolved external symbol ""class tensorflow::StepStatsDefaultTypeInternal tensorflow::_StepStats_default_instance_"" (?_StepStats_default_instance_@tensorflow@@3VStepStatsDefaultTypeInternal@1@A)	ReprBug	c:\Users\xx\documents\visual studio 2015\Projects\ReprBug\ReprBug\Source.obj


### Source code / logs

```cpp 
#include ""tensorflow/cc/saved_model/tag_constants.h""
#include ""tensorflow/core/public/session_options.h""
#include ""tensorflow/core/util/stat_summarizer.h""
#include ""tensorflow/contrib/session_bundle/bundle_shim.h""

class SynchronizedStatSummarizer
{
public:
	SynchronizedStatSummarizer(const tensorflow::StatSummarizerOptions& options)
		: m_statSummarizer{ options }, m_mutex{}
	{
	}

	void AddStepStats(const tensorflow::StepStats& stepStats)
	{
		std::lock_guard<std::mutex> guard{ m_mutex };
		m_statSummarizer.ProcessStepStats(stepStats);
	}

private:
	// The TF stat summarizer.
	tensorflow::StatSummarizer m_statSummarizer;

	// Synchronizes access to m_statSummarizer.
	mutable std::mutex m_mutex;
};

int main() {

	tensorflow::SessionOptions sessionOptions;
	tensorflow::RunOptions runOptions{};
	tensorflow::ConfigProto& config = sessionOptions.config;
	
	std::unique_ptr<tensorflow::SavedModelBundle> m_bundle (new tensorflow::SavedModelBundle());

	const std::string path = ""somepath"";
	tensorflow::Status status = tensorflow::serving::LoadSessionBundleOrSavedModelBundle(
		sessionOptions, runOptions, path, { tensorflow::kSavedModelTagServe }, m_bundle.get());

	std::vector<std::pair<std::string, tensorflow::Tensor>> modifiedInputs;
	std::vector<std::string> modifiedOutputNames;
	std::vector<tensorflow::Tensor> tensorOutputs;
	tensorflow::RunMetadata runMetadata{};

	tensorflow::Status run_status = m_bundle->session->Run(
		runOptions, modifiedInputs, modifiedOutputNames, {}, &tensorOutputs, &runMetadata);

	std::unique_ptr<SynchronizedStatSummarizer> m_runTracingStats;

	if (run_status.ok())
	{
		m_runTracingStats->AddStepStats(runMetadata.step_stats());
	}

}
```",0,,1,2018-02-03T01:07:58Z,CONTRIBUTOR,2018-02-03T01:39:35Z
16707,Can't initialize an all zero SparseTensor,stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Kind of?
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.12.6 (not relevant)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.5.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem
It doesn't seem possible to initialize a `tf.SparseTensor` with all zero entries. 

A call doing this would look something like:

    tf.SparseTensor(indices=[], values=[], dense_shape=(10, 10))

However, attempting this initialization produces the error:

     ValueError: Shape (0,) must have rank 2


### Source code / logs
Current relevant section from `SparseTensor.__init__`:

    indices_shape = indices.get_shape().with_rank(2) # <--- .with_rank(2) is what causes the problem
    values_shape = values.get_shape().with_rank(1)
    dense_shape_shape = dense_shape.get_shape().with_rank(1)

    # Assert number of rows in indices match the number of elements in values.
    indices_shape[0].merge_with(values_shape[0])
    # Assert number of columns in indices matches the number of elements in
    # dense_shape.
    indices_shape[1].merge_with(dense_shape_shape[0])

Example solution:

    tf.cond(tf.equal(indices.get_shape()[0], 0),
            true_fn=lambda: None,
            false_fn=self._validate_input)

    def _validate_input(self):
        indices_shape = self._indices.get_shape().with_rank(2)
        values_shape = self._values.get_shape().with_rank(1)
        dense_shape_shape = self._dense_shape.get_shape().with_rank(1)

        # Assert number of rows in indices match the number of elements in values.
        indices_shape[0].merge_with(values_shape[0])
        # Assert number of columns in indices matches the number of elements in
        # dense_shape.
        indices_shape[1].merge_with(dense_shape_shape[0])

My only worry with the example solution is that `tf.cond` is too high level a function and there's some alternative that would be better. Is that the case? ",0,,3,2018-02-02T20:29:29Z,NONE,2018-02-02T23:12:50Z
16695,Padding algo is not working as doc says,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linus centos 7
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: 0
- **GCC/Compiler version (if compiling from source)**:0
- **CUDA/cuDNN version**:0
- **GPU model and memory**:0
- **Exact command to reproduce**:

In the following situation, TF [doc](https://www.tensorflow.org/api_guides/python/nn#Convolution) is not correct.
- Input tensor shape : [1, 5, 2, 1]
- Kernel shape:           [1, 3, 1, 1]
- Stride :                      [1, 5, 5, 1]
- Padding =                  ""SAME""

According to the formula we can compute : 
out_h = 1
out_w = 1

```
if (in_height % strides[1] == 0):
  pad_along_height = max(filter_height - strides[1], 0)
else:
  pad_along_height = max(filter_height - (in_height % strides[1]), 0)
if (in_width % strides[2] == 0):
  pad_along_width = max(filter_width - strides[2], 0)
else:
  pad_along_width = max(filter_width - (in_width % strides[2]), 0)
```
gives :
pad_along_height = 0
pad_along_width = 1

then 
```
pad_top = pad_along_height // 2
pad_bottom = pad_along_height - pad_top
pad_left = pad_along_width // 2
pad_right = pad_along_width - pad_left
```

gives:

pad_top = 0
pad_bottom = 0
pad_left = 0
pad_right = 1

How tensorflow do a convolution with a kernel of height 1 on a image of height 5 and which gives output of height 1 (stride = 5) ??? How TF do this ? The doc can't explain the method used ... 

Doing retro engineering, I saw that TF apply the filter on the middle of the input tensor (pad_top = -2 and pad_bottom=-2).
I agree with this method, but the formulas of the Convolution doc is doing max(.., 0) so padding could never be negative (according to the doc).

Could someone explain me clearly what is the formula used in tensorflow ?
Could someone update the doc ?",0,,1,2018-02-02T15:19:12Z,NONE,2018-02-02T17:38:10Z
16690,change to anchor link,"awaiting testing (then merge),cla: yes",Fixing markdown typo,0,,4,2018-02-02T10:10:12Z,CONTRIBUTOR,2018-02-02T10:12:07Z
16688,how to assign the GPU device using C++?,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,1,2018-02-02T09:14:57Z,NONE,2018-02-02T18:52:08Z
16684,The link for the  tutorial on Google's Tensorflow SyntaxNet  page  gives 404 error,"stat:awaiting tensorflower,type:docs","Go tot the page 
https://www.tensorflow.org/versions/r0.12/tutorials/syntaxnet/

and click the ""tutorial"" link. It gets a 404 error.

The target of the link is
https://github.com/tensorflow/models/tree/master/syntaxnet#installation
",1,,4,2018-02-02T03:58:02Z,NONE,2018-02-02T18:44:40Z
16680,Branch 184220615,cla: yes,,0,,1,2018-02-02T01:59:52Z,MEMBER,2018-02-02T02:02:15Z
16674,Fix sanity build,cla: yes,"- [x] Fix build error
- [x] Update test
",0,,2,2018-02-01T19:00:57Z,MEMBER,2018-02-02T00:54:42Z
16672,Updating the version to 1.6.0-rc0.,cla: yes,,0,,1,2018-02-01T18:30:59Z,MEMBER,2018-02-01T18:57:50Z
16662,"Current Bazel version is 0.10.0, expected at least 0.5.4",,"I get this error message when trying to build from source (r1.5) with the new bazel version published today.

Current Bazel version is 0.10.0, expected at least 0.5.4

I guess the version check is wrong. ",0,,9,2018-02-01T15:52:21Z,NONE,2018-02-01T16:51:44Z
16661,"Fix ""Define the model"" link.",cla: yes,"The link syntax was inverted, that is, round brackets were coming before square brackets, but Markdown doesn't like it.",0,,4,2018-02-01T15:38:23Z,CONTRIBUTOR,2018-02-01T15:39:39Z
16658,Runtime Error with Qt GUI Application,"stat:awaiting response,type:bug/performance","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from source**:
- **TensorFlow version use master**:
- **Python version 2.7**: 
- **Bazel version 0.9.0**:
- **GCC/Compiler version 5.4.0**:
- **Without CUDA/cuDNN**:
- **Without GPU**:

### Describe the problem
When I used QtCreator to build GUI Application, if include ""tensorflow/core/lib/core/refcount.h"", it will throw The program has unexpectedly finished.

.pro like
####
    SOURCES += \
        main.cpp \
        mainwindow.cpp
    HEADERS += \
         mainwindow.h
    FORMS += \
         mainwindow.ui
    
    #tensorflow
    INCLUDEPATH += /home/face/Desktop/tensorflow/bazel-genfiles`
    INCLUDEPATH += /home/face/Desktop/tensorflow`
    INCLUDEPATH += /home/face/Desktop/tensorflow/tensorflow/contrib/makefile/gen/protobuf/include`
    INCLUDEPATH += /home/face/Desktop/tensorflow/tensorflow/contrib/makefile/downloads/nsync/public`
    INCLUDEPATH += /home/face/Desktop/eigen-eigen-5a0156e40feb`
    LIBS += -L/home/face/Desktop/tensorflow/bazel-bin/tensorflow -ltensorflow_cc -ltensorflow_framework

main.cpp
####
    #include ""mainwindow.h""
    #include <QApplication>
    #include <tensorflow/core/platform/env.h>
    #include <tensorflow/core/public/session.h>

    int main(int argc, char *argv[])
    {
        QApplication a(argc, argv);
        MainWindow w;
        w.show();
        return a.exec();
    }

then if ""tensorflow/core/lib/core/refcount.h"" line 79
####
    inline RefCounted::~RefCounted() {
        DCHECK_EQ(ref_.load(), 0); 
    }
to
####
    inline RefCounted::~RefCounted() {
        //DCHECK_EQ(ref_.load(), 0); 
    }
it will work.

### Source code / logs
debug log like:
####
    1  google::protobuf::internal::Mutex::Lock()                                    0x7fffde0c3516 
    2  google::protobuf::internal::OnShutdown(void ( *)())                          0x7fffde0c3833 
    3  call_init                                                     dl-init.c  72  0x7ffff7de76ba 
    4  call_init                                                     dl-init.c  30  0x7ffff7de77cb 
    5  _dl_init                                                      dl-init.c  120 0x7ffff7de77cb 
    6  dl_open_worker                                                dl-open.c  575 0x7ffff7dec8e2 
    7  _dl_catch_error                                               dl-error.c 187 0x7ffff7de7564 
    8  _dl_open                                                      dl-open.c  660 0x7ffff7debda9 
    9  dlopen_doit                                                   dlopen.c   66  0x7ffff18f0f09 
    10 _dl_catch_error                                               dl-error.c 187 0x7ffff7de7564 
    11 _dlerror_run                                                  dlerror.c  163 0x7ffff18f1571 
    12 __dlopen                                                      dlopen.c   87  0x7ffff18f0fa1 
    13 ??                                                                           0x7ffff33100e5 
    14 ??                                                                           0x7ffff3309975 
    15 QFactoryLoader::instance(int) const                                          0x7ffff32ff07e 
    16 QPlatformThemeFactory::create(QString const&, QString const&)                0x7ffff0b30231 
    17 QGuiApplicationPrivate::createPlatformIntegration()                          0x7ffff0b3aaf8 
    18 QGuiApplicationPrivate::createEventDispatcher()                              0x7ffff0b3b4bd 
    19 QCoreApplicationPrivate::init()                                              0x7ffff331ab3b 
    20 QGuiApplicationPrivate::init()                                               0x7ffff0b3cf7b 
    21 QApplicationPrivate::init()                                                  0x7ffff392d3b9 
    22 main                                                          main.cpp   103 0x402e3e  

",0,,3,2018-02-01T13:04:29Z,NONE,2018-02-02T03:49:17Z
16657,Tensorflow on banana-pi m64,,"How to install TF on banana m64? OS: Linux bpi-iot-ros-ai 3.10.105-BPI-M64-Kernel 

When i trying install i have an error

> tensorflow-1.5.0-cp34-none-any.whl is not a supported wheel on this platform.
",0,,1,2018-02-01T12:14:54Z,NONE,2018-02-01T18:01:59Z
16654,Bazel version comparison fails with bazel 0.10.0,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, just commented 6 lines in the bzl files out
- **OS Platform and Distribution**: 16.04 on Jetson TX2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: TX2 GPU, 5GB (not sure)
- **Exact command to reproduce**: bazel build -c opt --local_resources 3072,4.0,1.0 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
I cannot build tensorflow using bazel 0.10.0. It seems like the version checks in repositories.bzl and wokspace.bzl fail. Commenting them out solves the issue, even though I know that is no persistent solution. I think it is simply that bazel thinks that 0.10.0 is smaller 0.5.4 due to its string comparison, but I am no bazel expert.

### Source code / logs
ERROR: /home/nvidia/git/tensorflow/WORKSPACE:15:1: Traceback (most recent call last):
	File ""/home/nvidia/git/tensorflow/WORKSPACE"", line 15
		closure_repositories()
	File ""/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/external/io_bazel_rules_closure/closure/repositories.bzl"", line 69, in closure_repositories
		_check_bazel_version(""Closure Rules"", ""0.4.5"")
	File ""/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/external/io_bazel_rules_closure/closure/repositories.bzl"", line 172, in _check_bazel_version
		fail((""%s requires Bazel >=%s but was...)))
Closure Rules requires Bazel >=0.4.5 but was 0.10.0- (@non-git)
ERROR: Error evaluating WORKSPACE file
ERROR: /home/nvidia/git/tensorflow/WORKSPACE:41:1: Traceback (most recent call last):
	File ""/home/nvidia/git/tensorflow/WORKSPACE"", line 41
		tf_workspace()
	File ""/home/nvidia/git/tensorflow/tensorflow/workspace.bzl"", line 48, in tf_workspace
		check_version(""0.5.4"")
	File ""/home/nvidia/git/tensorflow/tensorflow/workspace.bzl"", line 38, in check_version
		fail(""\nCurrent Bazel version is {}, ...))

Current Bazel version is 0.10.0- (@non-git), expected at least 0.5.4
ERROR: Error evaluating WORKSPACE file
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'external': Package 'external' contains errors
WARNING: Target pattern parsing failed.
ERROR: error loading package 'external': Package 'external' contains errors
INFO: Elapsed time: 3.145s
FAILED: Build did NOT complete successfully (0 packages loaded)
",0,,12,2018-02-01T10:38:27Z,NONE,2018-02-01T11:53:15Z
16653,Fix docs,cla: yes,* Fix xcode path error,0,,3,2018-02-01T10:24:43Z,CONTRIBUTOR,2018-02-01T10:31:45Z
16652,v1.3 batch_norm layer,,"I use the batch norm layer like this:
`def batch_norm_layer(x,train_phase,scope_bn):

	bn_train = batch_norm(x, decay=0.999, center=True, scale=True,
	is_training=True,
	reuse=None, # is this right?
	trainable=True,
	scope=scope_bn)
	bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,
	is_training=False,
	reuse=True, # is this right?
	trainable=True,
	scope=scope_bn)
	z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)
	return z`
I don't know in v1.3.0 is the code worked?
I saw the [issue1122](https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-232535426), someone said it would not work well.

thank you in advance.",0,,1,2018-02-01T10:02:01Z,NONE,2018-02-02T02:10:41Z
16651,Temporarily remove three linter checks for now.,cla: yes,"  # C0330 bad-continuation
  # C0301 line-too-long
  # C0326 bad-whitespace
Will fix the following 25 error and add them back:
tensorflow/contrib/session_bundle/bundle_shim.py:85: [C0301(line-too-long), ] Line too long (83/80)

tensorflow/contrib/session_bundle/bundle_shim.py:94: [C0301(line-too-long), ] Line too long (89/80)

tensorflow/contrib/session_bundle/bundle_shim.py:135: [C0301(line-too-long), ] Line too long (81/80)

tensorflow/contrib/session_bundle/bundle_shim.py:136: [C0301(line-too-long), ] Line too long (81/80)

tensorflow/contrib/kafka/python/ops/kafka_dataset_ops.py:33: [C0301(line-too-long), ] Line too long (85/80)

tensorflow/contrib/tpu/profiler/pip_package/cloud_tpu_profiler/main.py:29: [C0330(bad-continuation), ] Wrong continued indentation (remove 3 spaces).

tensorflow/contrib/tpu/profiler/pip_package/cloud_tpu_profiler/main.py:31: [C0330(bad-continuation), ] Wrong continued indentation (remove 3 spaces).

tensorflow/contrib/tpu/profiler/pip_package/cloud_tpu_profiler/main.py:35: [C0330(bad-continuation), ] Wrong continued indentation (remove 3 spaces).

tensorflow/contrib/learn/python/learn/datasets/synthetic_test.py:139: [E0102(function-redefined), SyntheticTest.test_spirals] method already defined line 92

tensorflow/contrib/layers/python/layers/layers.py:63: [C0330(bad-continuation), ] Wrong hanging indentation (remove 7 spaces).

tensorflow/contrib/layers/python/layers/layers.py:1421: [C0301(line-too-long), ] Line too long (104/80)

tensorflow/contrib/py2tf/impl/api.py:89: [C0301(line-too-long), ] Line too long (81/80)

tensorflow/contrib/rnn/python/kernel_tests/core_rnn_cell_test.py:160: [C0326(bad-whitespace), ] Exactly one space required after comma

tensorflow/contrib/ndlstm/python/lstm1d.py:91: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).

tensorflow/contrib/rnn/python/kernel_tests/rnn_cell_test.py:1639: [E0102(function-redefined), WeightNormLSTMCellTest] class already defined line 1548

tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:209: [C0301(line-too-long), ] Line too long (98/80)

tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:209: [E1124(redundant-keyword-arg), get_graph_def_from_url_tarball] Argument 'filename' passed by position and keyword in function call

tensorflow/contrib/layers/python/layers/layers_test.py:1311: [C0301(line-too-long), ] Line too long (89/80)

tensorflow/python/kernel_tests/tensordot_op_test.py:108: [C0326(bad-whitespace), ] Exactly one space required after comma

tensorflow/python/data/util/nest.py:482: [C0301(line-too-long), ] Line too long (81/80)

tensorflow/python/data/ops/dataset_ops.py:909: [C0301(line-too-long), ] Line too long (88/80)

tensorflow/python/ops/image_ops_impl.py:1694: [C0301(line-too-long), ] Line too long (87/80)

tensorflow/python/ops/image_ops_impl.py:1720: [C0301(line-too-long), ] Line too long (87/80)

tensorflow/python/ops/image_ops_impl.py:1745: [C0301(line-too-long), ] Line too long (87/80)

tensorflow/python/ops/image_ops_impl.py:1771: [C0301(line-too-long), ] Line too long (87/80)",0,,1,2018-02-01T09:55:53Z,MEMBER,2018-02-01T10:22:31Z
16646,can tf.estimator.Estimator's  parameters be modified by hand? ,stat:awaiting response,"TF's  high level API  is very convenient to defined a new model. 
However, many DNN Machine Learning task has to reuse some old model's parameter to fill a new model and then  fine-tune it in new tasks. 
I have read the tf.estimator.Estimator'API  carefully, but cann't find any API to set It's parameters. Hope  TF developer  add this function to the high level API. 
Thank very much!",1,,5,2018-02-01T06:34:16Z,NONE,2018-02-02T02:08:28Z
16645,What's the difference between Univariate prediction and Multivariate prediction?,,My understanding is that paramenters of neurals are shared in Multivariate prediction and they can learn some correlations between series. There is less training time in Multivariate prediction. I wonder if that's right. Could you please explain any basic principles of Multivariate prediction with LSTM or recommend related papers to me? Thank you.,0,,1,2018-02-01T05:56:36Z,NONE,2018-02-01T18:02:23Z
16636,Add relnote about bug in ptxas in CUDA 9 and 9.1.,cla: yes,,0,,3,2018-02-01T01:56:29Z,MEMBER,2018-02-01T01:57:15Z
16631,Allow variable_overwrites on scope level,,"This is a request for allowing to pass a dict of `variable_overwrites` to variable scopes which to be returned when `tf.get_variable` is called instead of the usual procedure, if they are provided, otherwise do the usual procedure. A simple example of this beahviour is:
```
    import numpy as np
    with tf.variable_scope(""one""):
        a = tf.ones((5, 5), tf.float32)
        with tf.variable_scope(""two""):
            x1 = tf.get_variable(""x"", initializer=np.random.randn(5, 5).astype(""float32""))
            c1 = tf.sqrt(tf.abs(x1 + a))

    variables_overwrites = {x1._shared_name: c1}
    with tf.variable_scope(""one"", reuse=tf.AUTO_REUSE, variables_overwrites=variables_overwrites):
        a = tf.ones((5, 5), tf.float32)
        with tf.variable_scope(""two""):
           // x2 here is in fact the value of c1
            x2 = tf.get_variable(""x"", initializer=np.random.randn(5, 5).astype(""float32""))
            c2 = tf.sqrt(tf.abs(x2 + a))
```
This is particularly usefull for being able easily to bootstrap neural network parameters coming from inside the layers trough a standard function interface. My specific usage is for HMC for NN parameters. This is a question on whether you guys are interested in this so that I spend more time on doing this properly.",0,,5,2018-01-31T22:24:53Z,NONE,2018-01-31T23:00:57Z
16628,Tensorflow switches to CPU when using Variable.assign,,"### System information
- **OS**:Windows 10
- **TensorFlow installed from**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.6.4
- **CUDA/cuDNN version**: 8.0 / 64
- **GPU model and memory**: GeForce GTX 1080 8 GB
- **Exact command to reproduce**: run the provided code below

### Describe the problem
I'm using a `tf.Variable` for the learning rate of a optimizer. If I change its value with `sess.run(var.assign(0.1))` the performance drops extremly and it seems tensorflow switches from GPU use to only CPU use (no workload on the GPU and more load on the CPU).

### Source code / logs

I did write a minimal working example (training a network with XOR). **To see the difference just comment the line `sess.run(learning_rate.assign(0.1))` out** and it will run much much faster using the GPU.

```
import tensorflow as tf


def XOR(x_, y_):
    Theta1 = tf.Variable(tf.random_uniform([2, 2], -1, 1), name=""Theta1"")
    Theta2 = tf.Variable(tf.random_uniform([2, 1], -1, 1), name=""Theta2"")

    Bias1 = tf.Variable(tf.zeros([2]), name=""Bias1"")
    Bias2 = tf.Variable(tf.zeros([1]), name=""Bias2"")

    with tf.name_scope(""layer2""):
        A2 = tf.sigmoid(tf.matmul(x_, Theta1) + Bias1)

    with tf.name_scope(""layer3""):
        Hypothesis = tf.sigmoid(tf.matmul(A2, Theta2) + Bias2)

    with tf.name_scope(""cost""):
        cost = tf.reduce_mean(((y_ * tf.log(Hypothesis)) +
                               ((1 - y_) * tf.log(1.0 - Hypothesis))) * -1)

    return cost


if __name__ == ""__main__"":
    x_ = tf.placeholder(dtype=tf.float32, shape=[4, 2], name='x-input')
    y_ = tf.placeholder(dtype=tf.float32, shape=[4, 1], name='y-input')
    xor_cost = XOR(x_, y_)

    learning_rate = tf.Variable(0.1, dtype=tf.float32)

    with tf.name_scope(""train""):
        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(xor_cost)

    XOR_X = [[0, 0], [0, 1], [1, 0], [1, 1]]
    XOR_Y = [[0], [1], [1], [0]]

    sess = tf.Session()
    init = tf.global_variables_initializer()
    sess.run(init)

    for i in range(40001):
        sess.run(learning_rate.assign(0.1))

        _, loss = sess.run(fetches=[train_step, xor_cost], feed_dict={x_: XOR_X, y_: XOR_Y})

        if i % 100 == 0:
            print('iteration: {0:5}, loss: {1:15.10f}'.format(i, loss))


```",0,,2,2018-01-31T18:17:24Z,NONE,2018-01-31T19:05:22Z
16626,"example script multivariate.py throws ""UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape.  This may consume a large amount of memory.""",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I am using the example script `tensorflow/contrib/timeseries/examples/multivariate.py`
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS High Sierra (darwin)
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
v1.5.0-rc1-1781-g86c10063c8 1.5.0-rc1
- **Python version**: 
3.6.4
- **Bazel version (if compiling from source)**:
0.9.0-homebrew
- **GCC/Compiler version (if compiling from source)**:
Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **CUDA/cuDNN version**:
n/a compiled without CUDA support
- **GPU model and memory**:
n/a GPU not supported on Mac with SIP
- **Exact command to reproduce**:
$ python $GOPATH/src/github.com/tensorflow/tensorflow/tensorflow/contrib/timeseries/examples/multivariate.py

### Describe the problem
The script throws warning:
`UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.`

### Source code / logs
Here is the traceback of the warning on my system (I have tensorflow installed in a virtual environment called ""tf3"".:
```
  File ""multivariate.py"", line 59, in multivariate_train_and_sample
    estimator.train(input_fn=train_input_fn, steps=training_steps)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 352, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 809, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 790, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/timeseries/python/timeseries/head.py"", line 228, in create_estimator_spec
    return self._train_ops(features)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/timeseries/python/timeseries/head.py"", line 85, in _train_ops
    learning_rate=None)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py"", line 241, in optimize_loss
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 458, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 610, in gradients
    lambda: grad_fn(op, *out_grads))
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 376, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 610, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py"", line 589, in _PadGrad
    x_grad = array_ops.slice(grad, begin, sizes)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 640, in slice
    return gen_array_ops._slice(input_, begin, size, name=name)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 4591, in _slice
    ""Slice"", input=input, begin=begin, size=size, name=name)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1036, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 97, in _IndexedSlicesToTensor
    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
  File ""~/.pyenv/versions/3.6.4/lib/python3.6/warnings.py"", line 99, in _showwarnmsg
    msg.file, msg.line)
~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
```
I see discussion about this warning on SO:
https://stackoverflow.com/questions/35892412/tensorflow-dense-gradient-explanation/35893467
But the problem seems to happen in the estimator ""train"" method, not in any custom code I have written.",0,,1,2018-01-31T16:34:00Z,NONE,2018-01-31T19:24:53Z
16625,[For Test; DO NOT MERGE] Add grpcio as a pip dependency of tensorflow,cla: yes,,0,,1,2018-01-31T15:22:47Z,CONTRIBUTOR,2018-02-01T01:41:12Z
16621,Using tf.train.SyncReplicasOptimizer with multiple optimizers,stat:awaiting tensorflower,"I am trying to run the DeepLab Resnet (https://github.com/DrSleep/tensorflow-deeplab-resnet) in a distributed setup. I opted Synchronous Data parallel training approach similar to the one demonstrated in the Inception distributed training example.(https://github.com/tensorflow/models/tree/master/research/inception).

In the Inception example, a single RMS optimizer is used to reduce the loss. The tf.train.SyncReplicasOptimizer function wraps the optimizer and becomes responsible for synchronization, aggregation and application of gradients to various workers. Also, it takes care of updating the global_step variable. In my case, the DeepLab Resnet makes use of three optimizers each handling specific portions of the network. Following snippet explains the case:

    `#Three optimizers declared with different learning rates
     opt_conv = tf.train.MomentumOptimizer(learning_rate, args.momentum)
     opt_fc_w = tf.train.MomentumOptimizer(learning_rate * 10.0, args.momentum)
     opt_fc_b = tf.train.MomentumOptimizer(learning_rate * 20.0, args.momentum)`

    #Scope for every optimizer 
    grads = tf.gradients(reduced_loss, conv_trainable + fc_w_trainable + fc_b_trainable)
    grads_conv = grads[:len(conv_trainable)]
    grads_fc_w = grads[len(conv_trainable) : (len(conv_trainable) + len(fc_w_trainable))]
    grads_fc_b = grads[(len(conv_trainable) + len(fc_w_trainable)):]

    #Gradients applied to various portions of the network
    train_op_conv = opt_conv.apply_gradients(zip(grads_conv, conv_trainable))
    train_op_fc_w = opt_fc_w.apply_gradients(zip(grads_fc_w, fc_w_trainable))
    train_op_fc_b = opt_fc_b.apply_gradients(zip(grads_fc_b, fc_b_trainable))
 
    `#tf.group to combine all three operations
    train_op = tf.group(train_op_conv, train_op_fc_w, train_op_fc_b)`
   
I don't have any clue about using the tf.train.SyncReplicasOptimizer for multiple optimizers to achieve synchronous data parallel training. Also, I don't have an idea about updating the global_step variable and using the chief_queue_runner for this case. Please help me on this.


 


",0,,2,2018-01-31T11:59:09Z,NONE,2018-01-31T18:19:06Z
16620,How to use model.summary() when using placeholder instead of Input(keras),,"Dear all, 
I follow  post in ""https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html""
The little modified code I use is:
----------------------------------------------------------------------------------------------------------------------------------
import tensorflow as tf
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.backend import categorical_crossentropy
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.python.keras.models import Model


sess = tf.Session()
img = tf.placeholder(tf.float32, shape=(None, 784))
x = Dense(128, activation='relu')(img)  # fully-connected layer with 128 units and ReLU activation
x = Dense(128, activation='relu')(x)
preds = Dense(10, activation='softmax')(x)  # output layer with 10 units and a softmax activation

labels = tf.placeholder(tf.float32, shape=(None, 10))
loss = tf.reduce_mean(categorical_crossentropy(labels, preds))
mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

init_op = tf.global_variables_initializer()
sess.run(init_op)
with sess.as_default():
    for i in range(100):
        batch = mnist_data.train.next_batch(50)
        train_step.run(feed_dict={img: batch[0],
                                  labels: batch[1]})

----------------------------------------------------------------------------------------------------------------------------------
It work fine until I use model.summary :  

model = Model(inputs=img, outputs=preds)

The error message show"" Input tensors to a Model must come from `tf.layers.Input`""
I can use tf.layers.Input to solve this problem.
But I really want to use tf.placeholder so I can feed data as I like.
Can anyone help me?  Thanks!!",1,,7,2018-01-31T10:23:42Z,NONE,2018-02-02T05:15:49Z
16612,Simplify loader_impl.py logic around main Op Tensor.,"awaiting testing (then merge),cla: yes",,0,,1,2018-01-31T03:08:35Z,CONTRIBUTOR,2018-01-31T15:25:25Z
16607,MKL: Pooling and AddN bug fixes,"awaiting testing (then merge),cla: yes",,0,,1,2018-01-31T00:18:08Z,CONTRIBUTOR,2018-01-31T00:29:27Z
16605,Description in docs of one-hot vector for mnist deep example confusing and/or wrong,stat:awaiting tensorflower,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:n/a
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.4
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**:n/a
- **GCC/Compiler version (if compiling from source)**:n/a
- **CUDA/cuDNN version**:n/a
- **GPU model and memory**:n/a
- **Exact command to reproduce**:n/a

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Documentation found at https://www.tensorflow.org/versions/r1.4/get_started/mnist/pros describes one of the two parameters passed to the training process as a 2d tensor of ""one-hot"" 10-dimensional vectors specifying the classes of the samples in the other 2d tensor parameter. But clearly the placeholders define 2d and 1d tensors, not 2d and 2d. There is no ""one-hot"" representation used at all as far as I can tell by using print() statements - if the class if a sample is class 3, then the corresponding entry is simple 3, not the one-hot representation of it. If this class is subsequently converted into a one-hot representation, it does not happen in the mnist_deep.py source file. I'm too much of a beginner to say what the documentation should say, but it seems at best confusing, and at worst completely wrong.

### Source code / logs
n/a
",0,,4,2018-01-30T22:19:50Z,NONE,2018-01-30T23:11:10Z
16604,Branch 183881907,cla: yes,,1,,2,2018-01-30T22:12:40Z,MEMBER,2018-01-30T22:57:24Z
16595,Branch 183846994,cla: yes,,0,,3,2018-01-30T18:37:36Z,MEMBER,2018-01-30T18:41:07Z
16591,Fix FutureWarning on issubdtype from float to np.floating,"awaiting testing (then merge),cla: yes","This is try to fix [#16587](https://github.com/tensorflow/tensorflow/issues/16587). 
```
FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated
```

Before fix:
```
>>> np.issubdtype(np.integer, np.float)
__main__:1: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
False
```
After fix:
```
>>> np.issubdtype(np.integer, np.floating)
False
>>>
```",0,,1,2018-01-30T17:34:37Z,CONTRIBUTOR,2018-01-31T13:19:05Z
16587,Feature deprecated in h5py is used in TF1.5,stat:contributions welcome,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux, OS X
- **TensorFlow installed from (source or binary)**: source and binary
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: 9.0 - 7.0
- **GPU model and memory**: GTX1060, GTX 1050Ti 
- **Exact command to reproduce**:
`sudo pip3 install h5py`
run python3, from there, type:
`import tensorflow as tf`


### Describe the problem
A feature of h5py used in TF 1.5 is deprecated, in particular: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated.

### Source code / logs
Warning message: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
",0,,2,2018-01-30T15:48:47Z,NONE,2018-01-30T17:35:55Z
16585,TensorFlow with CUDA or Python might rebuilds more than necessary instead of re-using bazel cache,stat:contributions welcome,"Context: for DeepSpeech, we perform tensorflow builds and then keep the cache in a tar (capturing the whole of the home directory of the build user). We then untar it and the deepspeech build through `bazel build` picks the proper cached items so it does not rebuild anything.

Recently, we started to have increased (2.5x) build time on CUDA-enabled builds. Debugging with Bazel showed that it was rebuilding because the `actionKey` computed for `stream_executor_impl` was different. Instrumenting Bazel to get more informations, I could get down to the reason of the different actionKey: the ordering of the CUDA includes was different. The list itself contained the exact same content, just a different ordering.

Those includes are symlinks, and they are generated from a genrule. This is all taken care of by https://github.com/tensorflow/tensorflow/blob/ba64f5334d4bba31d22c30e09a96f806ea0e2f7e/third_party/gpus/cuda_configure.bzl#L915-L1035 which generated shell script for the genrules, that actually do perform the symlinks. Checking those shell scripts revealed the exact same and different ordering.

Checking more carefully, one will see that the headers are discovered by `_read_dir` function: https://github.com/tensorflow/tensorflow/blob/ba64f5334d4bba31d22c30e09a96f806ea0e2f7e/third_party/gpus/cuda_configure.bzl#L891-L894, it does directly get the output of `find`. This is dependant on the ordering provided by `readdir` syscall.

In our case, the ordering on the filesystem before making the tar archive, and after untarring it would be different.

One simple fix for that is to force ordering the list of headers, this way we are sure the order is always the same and we are not dependant on what `readdir` is going to get us.

In the past, Bazel would force the ordering of the elements considered to compute the actionKey. This was removed with 0.3.0 but it might have make the issue hidden https://github.com/bazelbuild/bazel/commit/9dc32111d5b6c1c7c5eaf39efad5fef75327ee75",0,,2,2018-01-30T14:16:49Z,CONTRIBUTOR,2018-01-30T18:31:46Z
16584,TensorFlow op to copy weights of Keras model,,"I am doing a distributed calibration of an LSTM model (keras 2.0 + TensorFlow 1.0)

    with tf.device(tf.train.replica_device_setter(...):
          model = ##create model by keras
          clone_model = ## create the same model by keras but now a stateful one

after calibration, I want my chief worker to use the clone_model, copy the weights the calibration reached in model, and make predictions on some test set, but simply calling

     clone_model.set_weights(model.get_weights())

does not work.
I understand I need to define this weight copy as an op and then call session(run) of that op

Can you please help with a TensorFlow op copying weights of a keras model to another (identical architecture) Keras model?

",1,,3,2018-01-30T13:00:44Z,NONE,2018-01-30T23:47:49Z
16570,Fix typos.,cla: yes,,0,,3,2018-01-30T03:59:02Z,CONTRIBUTOR,2018-01-30T04:06:40Z
16557,MKL: Fix for mkl_input conversion for MKL DNN. ,"awaiting testing (then merge),cla: yes",Fix also enables elementwise operations in MKL,0,,2,2018-01-29T23:07:55Z,CONTRIBUTOR,2018-01-31T00:30:06Z
16556,tf.argmax appears to be functioning incorrectly on occasion,"stat:awaiting response,type:bug/performance","**EDIT**

[Link to script and input/label data as pickle files to reproduce the error.](https://drive.google.com/open?id=13Kice6p3IQvRVOKIlW0DSvD9wwJDL-YH)

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I have written my own code. My code base, data set and batch generating algorithm are quite large, so I am attempting to illustrate this as best as possible. If no mistake of mine can be seen in this post and the examples I have given, then I will provide further code/data. I have asked on StackOverflow, but have got not replies. If  I am missing something simple, then I'm sure it would have been pointed out on StackOverflow by now.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  
Manjaro 17.1.3 Kernel 4.14

- **TensorFlow installed from (source or binary)**:
python pip

- **TensorFlow version (use command below)**:
tensorflow-gpu 1.5.0

- **Python version**:
3.6.4

- **CUDA/cuDNN version**:
CUDA 9.0
cuDNN 7.0

- **GPU model and memory**: 
Nvidia GeForce GTX 1050 8GB

### Describe the problem
tf.argmax seems to be occasionally producing incorrect results when used on the last axis of a 3-dimensional tensor.

### Source code / logs
To debug this, I have printed out the following operations:

```
print(self.session.run(
    tf.equal(tf.argmax(self.predictions, axis=-1),
             tf.argmax(self.labelsUnrolled, axis=-1)),
    self.batchDict))
print("""")
print(self.session.run(self.predictions, self.batchDict))
print("""")
print(self.session.run(self.labelsUnrolled, self.batchDict))
print(""\n********\n"")
```

Which on two consecutive iterations output the following:

```
[[ True  True  True]
 [ True  True  True]]

[array([[0.06275553, 0.44493628, 0.42474008, 0.06756803],
        [0.06320112, 0.49631155, 0.4021484 , 0.03833894],
        [0.04378054, 0.59403986, 0.3236889 , 0.03849069]], dtype=float32), 
array([[8.1677590e-06, 9.9997127e-01, 2.0200867e-05, 3.6184446e-07],
        [4.3686719e-06, 9.9992716e-01, 6.6905603e-05, 1.5286902e-06],
        [1.3270236e-05, 9.9986196e-01, 1.1622251e-04, 8.5579613e-06]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32),
 array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]

********

[[False False  True]
 [ True  True  True]]

 [array([[0.0466171 , 0.53605616, 0.37778312, 0.03954368],
         [0.05007472, 0.4603508 , 0.44400516, 0.0455693 ],
         [0.06134444, 0.38073638, 0.504286  , 0.05363319]], dtype=float32),
 array([[9.6363285e-05, 9.9861979e-01, 1.2741127e-03, 9.7381862e-06],
         [1.6185455e-05, 9.9977034e-01, 2.0742408e-04, 6.0521238e-06],
         [2.9893983e-05, 9.9954152e-01, 4.2436572e-04, 4.2021661e-06]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.]], dtype=float32), 
 array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]

********
```

Isn't the very first `True` in the first iteration incorrect? There are many more examples being executed where these are wrong (although it is right the majority of the time). Am I going wrong somewhere with the `tf.argmax` function?

Printing out the same operations, but not inside the session gives the following shapes:

```
Tensor(""Equal_175:0"", shape=(2, ?), dtype=bool)

[<tf.Tensor 'unstack_2:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'unstack_2:1' shape=(?, 4) dtype=float32>]

[<tf.Tensor 'unstack_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'unstack_1:1' shape=(?, 4) dtype=float32>]
```

Is it a problem that the number associated with the tensor name ""Equal_XXX:0"" is incrementing each iteration?

I have also tried changing the axis argument in both argmax functions to `axis=2`, giving the ""Equal"" tensor a shape of (2, 3) again, but there are still similar errors.

Here is an example:

```
[[False False  True]
 [ True  True False]]

[array([[0.09075877, 0.41096467, 0.4460272 , 0.05224944],
        [0.04962843, 0.43777955, 0.46654516, 0.04604685],
        [0.07901238, 0.40768984, 0.46641603, 0.04688181]], dtype=float32),
 array([[0.04444276, 0.49195835, 0.42141557, 0.04218334],
        [0.02372498, 0.47147286, 0.4679979 , 0.03680426],
        [0.03707527, 0.435518  , 0.48937747, 0.03802926]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [1., 0., 0., 0.],
        [0., 0., 1., 0.]], dtype=float32),
 array([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.]], dtype=float32)]
```

I would expect this to be:

```
[[ True False  True]
 [ True False False]]
```

I thought this may have been a problem with parallel computations on the GPU, but I tried the same execution on just my CPU and got the following, similar miscalculation, too:

```
[[False  True False]
 [False False False]]

[array([[0.07774187, 0.40993363, 0.47022063, 0.04210386],
        [0.04910654, 0.44086066, 0.46013904, 0.04989377],
        [0.06700655, 0.37128285, 0.51324743, 0.04846317]], dtype=float32), 
array([[0.07584244, 0.3863555 , 0.5090046 , 0.02879751],
        [0.06959026, 0.3221606 , 0.5715027 , 0.03674646],
        [0.09042579, 0.32515866, 0.5385905 , 0.04582503]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.]], dtype=float32),
array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]
```

",0,,6,2018-01-29T22:34:01Z,NONE,2018-01-30T02:51:17Z
16550,Add options to enable new features for cloud-tpu-profiler.,"awaiting review,cla: yes","Add options for the user to manually include dataset ops in trace collection, and to automatically recapture the traces when no trace event is collected.
Also change tf.flags to absl.flags since the former is going to be deprecated. ",0,,1,2018-01-29T17:28:23Z,CONTRIBUTOR,2018-01-29T18:07:02Z
16548,AttributeError: module 'tensorflow.python.layers.layers' has no attribute 'conv_2d',stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.13.2
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 3.6.3
- **GPU model and memory**: no GPU
- **Exact command to reproduce**:

```python
class my_RNNCell(tf.nn.rnn_cell.RNNCell):
    def __init__(self):
        super(my_RNNCell, self).__init__()
        self._output_size = 2
        self._state_size = 2

    def __call__(self, tensor_in, state):

        output = tf.layers.conv_2d(tensor_in, 1, [1, 10])

        return output, output
    
    @property
    def output_size(self):
        return self._output_size
    @property
    def state_size(self):
        return self._state_size


tf.nn.dynamic_rnn(my_RNNCell(), inputs=tf.placeholder(shape=[None,2,100,3], dtype=tf.float32), initial_state= tf.placeholder(shape=[None,2], dtype=tf.float32))

>>>
AttributeError: module 'tensorflow.python.layers.layers' has no attribute 'conv_2d'
```
",0,,8,2018-01-29T17:01:12Z,NONE,2018-01-30T02:42:37Z
16544,"deconv_output_length(input_length, filter_size, padding, stride)",,"deconv_output_length(input_length, filter_size, padding, stride) is defined [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/utils.py#L159)

When padding is valid, input_length += max(filter_size - stride, 0), why to use `max` function?
See more details on #2118

For conv2d:
```
output = (input - filter + stride) // stride  # VALID
output = (input + stride - 1) // stride  # SAME
```

For conv2d_transpose:
```
output = input * stride + filter - stride  # VALID
output = input * stride - stride + 1  # SAME 
```

Even when filter_size is less than stride, I think output is also `input * stride + filter - stride` rather `input * stride`, so why to use `max`?

 
",0,,1,2018-01-29T15:12:08Z,NONE,2018-01-30T02:37:56Z
16537,added audio_ops.cc to tf_op_files.txt to fix the Op type not registered DecodeWav error ,cla: yes,- https://github.com/tensorflow/tensorflow/issues/15921,0,,5,2018-01-29T07:38:00Z,CONTRIBUTOR,2018-01-29T07:39:22Z
16524,MKL: Reverting the switch to max_pool_v2 in python,cla: yes,"A prior commit https://github.com/tensorflow/tensorflow/pull/14983 changed python interface to call max_pool_v2 causing failure in MKL build. Currently MKL doesn't support max_pool_v2. Reverting  the commit  for now, will change it back when MKL implementation is complete.",1,,1,2018-01-28T20:54:41Z,CONTRIBUTOR,2018-01-29T14:44:24Z
16517,ou must feed a value for placeholder tensor 'import/Placeholder when i test my frozen model,,"Hello , I trying create mobile app for object recognition for my own created model. I fallow this tutorial https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/#2
But when i even get a testing model from step 3 i get error 

```
InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'import/Placeholder' with dtype float
	 [[Node: import/Placeholder = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```

I'm aware that is wrong node problem, but i trying with other and always i get failure 

My code for model creation 
```

x = tf.placeholder(tf.float32,
                   shape=[None, cons.IMAGE_SIZE, cons.IMAGE_SIZE, 3], name=""x"")
y_ = tf.placeholder(tf.float32, shape=[None, cons.LABELS_NUMB], name=""labels"")

K = 4
L = 8
M = 12
N = 200

x_image = tf.reshape(x, [-1, cons.IMAGE_SIZE, cons.IMAGE_SIZE, 3])
tf.summary.image('input', x_image, 3)
print(""X image "")
print(tf.shape(x_image))

################## first ##############

W_conv1 = weight_variable([5, 5, 3, 32], ""weight1"")
b_conv1 = bias_variable([32], ""bias1"")

h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1)
print(""W_conv1 "")
print(tf.shape(W_conv1))
print(""b_conv1 "")
print(tf.shape(b_conv1))
print(""h_conv1 "")
print(tf.shape(h_conv1))
print(""h_pool1 "")
print(tf.shape(h_pool1))

################## second ##############

W_conv2 = weight_variable([5, 5, 32, 64], ""weight2"")
b_conv2 = bias_variable([64], ""bias2"")

h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
tf.summary.histogram(""activations"", h_conv2)


h_pool2 = max_pool_2x2(h_conv2)

print(""W_conv2 "")
print(tf.shape(W_conv2))
print(""b_conv2 "")
print(tf.shape(b_conv2))
print(""h_conv2 "")
print(tf.shape(h_conv2))
print(""h_pool2 "")
print(tf.shape(h_pool2))

################## fully connected 3 ##############

W_fc1 = weight_variable([8 * 8 * 64, 1024], ""Weight3"")
b_fc1 = bias_variable([1024], ""bias3"")

h_pool2_flat = tf.reshape(h_pool2, [-1, 8 * 8 * 64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

print(""W_fc1 "")
print(tf.shape(W_fc1))
print(""b_fc1 "")
print(tf.shape(b_fc1))
print(""h_pool2_flat "")
print(tf.shape(h_pool2_flat))
print(""h_fc1 "")
print(tf.shape(h_fc1))

################ dropout  4 #################

keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

################## fully connected 5 ##############

W_fc2 = weight_variable([1024, cons.LABELS_NUMB], ""weight5"")
b_fc2 = bias_variable([cons.LABELS_NUMB], ""bias5"")

Y = tf.matmul(h_fc1_drop, W_fc2) + b_fc2
tf.summary.histogram(""final"", Y)


print(""W_fc2 "")
print(tf.shape(W_fc2))
print(""b_fc2 "")
print(tf.shape(b_fc2))
print(""Y "")
print(tf.shape(Y))

with tf.name_scope(""cross_entropy""):
    cross_entropy = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=Y))
    tf.summary.scalar(""xent"", cross_entropy)

with tf.name_scope(""train_step""):
    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)

with tf.name_scope(""Acuracy""):
    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    tf.summary.scalar(""accuracy"", accuracy)

summ = tf.summary.merge_all()
saver = tf.train.Saver()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    writer = tf.summary.FileWriter(""/home/damian/api/mnist_demo/10"")
    writer.add_graph(sess.graph)
    for i in range(1000):
        img, lb = fileCreation.next_batch(100, images32, labels)
        if i % 5 == 0:
            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 1.0})
        writer.add_summary(s, i)
        if i % 100 == 0:
            saver.save(sess, '/home/damian/api/checkpoint/my_test_model', global_step=i)
            train_accuracy = accuracy.eval(feed_dict={
                x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 1.0})
            print('step %d, training accuracy %g' % (i, train_accuracy))
        train_step.run(feed_dict={x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 0.5})

    print('test accuracy %g' % accuracy.eval(feed_dict={x: test_images32,
                                                        y_: fileCreation.dense_to_one_hot(test_labels,
                                                                                          cons.LABELS_NUMB),
                                                        keep_prob: 0.1}))
```


I'm freeze this model with and i point output_node to 'final'
next i call script from tutorial


```
python -m scripts.label_image \
  --graph=tf_files/frozen_model3.pb  \
  --input_layer=x \
  --output_layer=final \
  --image=tf_files/stop.png  \
  --input_height=32 \
  --input_width=32 \
  --input_mean=16  \
  --input_std=16  
```









",0,,1,2018-01-28T16:15:22Z,NONE,2018-01-30T19:10:44Z
16513,TF1.5.0 not working with CUDA 8.0,,"After upgrading to TF 1.5.0, when I import tensorflow, it raises:

```
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory
```

- System: Ubuntu 14.04.5 LTS (64 bit)
- Python: 2.7.6
- TensorFlow: tensorflow-gpu-1.5.0
- GPU: GeForce GTX TITAN
- CUDA: 8.0",0,,3,2018-01-28T10:24:08Z,NONE,2018-01-29T19:34:12Z
16512,how to install ffmpeg in tensorflow 1.4 binary,,"hi
i want to install ffmpeg in tensorflow 1.4 binary , python 3.5 on ubuntu 16.04 , please help me how do i do ? 
the output type python -c ""from tensorflow.contrib import ffmpeg"" is ok dont have anly error , but i dont know why : 
from tensorflow.contrib import ffmpeg

i get error , 
>>>  from tensorflow.contrib import ffmpeg
  File ""<stdin>"", line 1
    from tensorflow.contrib import ffmpeg
    ^
IndentationError: unexpected indent
",0,,2,2018-01-28T07:56:25Z,NONE,2018-01-28T14:16:58Z
16507,"ResourceExhaustedError, when running UNET",,"My computer has a gpu GeForce 940MX installed. It has the Memory bandwidth 16.02 GB/s. I'm trying to train LUNA dataset using UNET model using following code.

	from __future__ import print_function

	import numpy as np
	from keras.models import Model
	from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D
	from keras.layers import concatenate
	from keras.optimizers import Adam
	from keras.optimizers import SGD
	from keras.callbacks import ModelCheckpoint, LearningRateScheduler
	from keras import backend as K


	K.set_image_dim_ordering('th')  # Theano dimension ordering in this code

	img_rows = 512
	img_cols = 512

	smooth = 1.


	def dice_coef(y_true, y_pred):
		y_true_f = K.flatten(y_true)
		y_pred_f = K.flatten(y_pred)
		intersection = K.sum(y_true_f * y_pred_f)
		return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

	def dice_coef_np(y_true,y_pred):
		y_true_f = y_true.flatten()
		y_pred_f = y_pred.flatten()
		intersection = np.sum(y_true_f * y_pred_f)
		return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)

	def dice_coef_loss(y_true, y_pred):
		return -dice_coef(y_true, y_pred)


	def get_unet():
		inputs = Input((1,img_rows, img_cols))
		conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
		conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)
		pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

		conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)
		conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)
		pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

		conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)
		conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)
		pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

		conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)
		conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)
		pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

		conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)
		conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)

		#up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)
		up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=1)
		conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)
		conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)

		#up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)
		up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=1)
		conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)
		conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)

		#up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)
		up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=1)
		conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)
		conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)

		#up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)
		up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=1)
		conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)
		conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)

		conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)

		model = Model(inputs=inputs, outputs=conv10)

		model.compile(optimizer=Adam(lr=1.0e-5), loss=dice_coef_loss, metrics=[dice_coef])

		return model


	def train_and_predict(use_existing):
		print('-'*30)
		print('Loading and preprocessing train data...')
		print('-'*30)
		imgs_train = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""trainImages.npy"").astype(np.float32)
		imgs_mask_train = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""trainMasks.npy"").astype(np.float32)

		imgs_test = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""testImages.npy"").astype(np.float32)
		imgs_mask_test_true = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""testMasks.npy"").astype(np.float32)
		
		mean = np.mean(imgs_train)  # mean for data centering
		std = np.std(imgs_train)  # std for data normalization

		imgs_train -= mean  # images should already be standardized, but just in case
		imgs_train /= std

		print('-'*30)
		print('Creating and compiling model...')
		print('-'*30)
		model = get_unet()
		# Saving weights to unet.hdf5 at checkpoints
		model_checkpoint = ModelCheckpoint('unet.hdf5', monitor='loss', save_best_only=True)
		#
		# Should we load existing weights? 
		# Set argument for call to train_and_predict to true at end of script
		if use_existing:
			model.load_weights('./unet.hdf5')
			
		# 
		# The final results for this tutorial were produced using a multi-GPU
		# machine using TitanX's.
		# For a home GPU computation benchmark, on my home set up with a GTX970 
		# I was able to run 20 epochs with a training set size of 320 and 
		# batch size of 2 in about an hour. I started getting reseasonable masks 
		# after about 3 hours of training. 
		#
		print('-'*30)
		print('Fitting model...')
		print('-'*30)
		model.fit(imgs_train, imgs_mask_train, batch_size=50, epochs=10, verbose=1, shuffle=True,
				  callbacks=[model_checkpoint])

		# loading best weights from training session
		print('-'*30)
		print('Loading saved weights...')
		print('-'*30)
		model.load_weights('./unet.hdf5')

		print('-'*30)
		print('Predicting masks on test data...')
		print('-'*30)
		num_test = len(imgs_test)
		imgs_mask_test = np.ndarray([num_test,1,512,512],dtype=np.float32)
		for i in range(num_test):
			imgs_mask_test[i] = model.predict([imgs_test[i:i+1]], verbose=0)[0]
		np.save('masksTestPredicted.npy', imgs_mask_test)
		mean = 0.0
		for i in range(num_test):
			mean+=dice_coef_np(imgs_mask_test_true[i,0], imgs_mask_test[i,0])
		mean/=num_test
		print(""Mean Dice Coeff : "",mean)

	if __name__ == '__main__':
		train_and_predict(False)
		
But when running it using GPU I'm getting the following error.

	Warning (from warnings module):
	  File ""C:\Research\Python_installation\lib\site-packages\h5py\__init__.py"", line 36
		from ._conv import register_converters as _register_converters
	FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
	Using TensorFlow backend.
	------------------------------
	Loading and preprocessing train data...
	------------------------------
	------------------------------
	Creating and compiling model...
	------------------------------
	------------------------------
	Fitting model...
	------------------------------
	Epoch 1/10
	Traceback (most recent call last):
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1327, in _do_call
		return fn(*args)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1306, in _run_fn
		status, run_metadata)
	  File ""C:\Research\Python_installation\lib\contextlib.py"", line 66, in __exit__
		next(self.gen)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
		pywrap_tensorflow.TF_GetCode(status))
	tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]
		 [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]
		 [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3022_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

	During handling of the above exception, another exception occurred:

	Traceback (most recent call last):
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 150, in <module>
		train_and_predict(False)
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 127, in train_and_predict
		callbacks=[model_checkpoint])
	  File ""C:\Research\Python_installation\lib\site-packages\keras\engine\training.py"", line 1657, in fit
		validation_steps=validation_steps)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\engine\training.py"", line 1213, in _fit_loop
		outs = f(ins_batch)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\backend\tensorflow_backend.py"", line 2357, in __call__
		**self.session_kwargs)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 895, in run
		run_metadata_ptr)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1124, in _run
		feed_dict_tensor, options, run_metadata)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1321, in _do_run
		options, run_metadata)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1340, in _do_call
		raise type(e)(node_def, op, message)
	tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]
		 [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]
		 [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3022_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

	Caused by op 'conv2d_1/convolution', defined at:
	  File ""<string>"", line 1, in <module>
	  File ""C:\Research\Python_installation\lib\idlelib\run.py"", line 124, in main
		ret = method(*args, **kwargs)
	  File ""C:\Research\Python_installation\lib\idlelib\run.py"", line 351, in runcode
		exec(code, self.locals)
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 150, in <module>
		train_and_predict(False)
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 106, in train_and_predict
		model = get_unet()
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 39, in get_unet
		conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\engine\topology.py"", line 603, in __call__
		output = self.call(inputs, **kwargs)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\layers\convolutional.py"", line 164, in call
		dilation_rate=self.dilation_rate)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\backend\tensorflow_backend.py"", line 3195, in conv2d
		data_format=tf_data_format)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 672, in convolution
		op=op)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 338, in with_space_to_batch
		return op(input, num_spatial_dims, padding)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 664, in op
		name=name)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 131, in _non_atrous_convolution
		name=name)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 397, in conv2d
		data_format=data_format, name=name)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
		op_def=op_def)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
		original_op=self._default_original_op, op_def=op_def)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
		self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

	ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[50,32,512,512]
		 [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]
		 [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3022_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]


Can someone please kindly explain me the reason behind this error, ResourceExhaustedError. Is it because that the memory of GPU is not enough to load the dataset. This worked fine without GPU. But took around 6 hours to finish one epoch",0,,2,2018-01-28T01:43:47Z,NONE,2018-01-29T19:47:54Z
16506,Feature request: Have Estimator display Loss and Metrics for Every Epoch and not Every Step,,"Most of the papers Ive read measure the time it takes to train a model with every epoch and not every step. If it isnt possible to display the loss only for every epoch, I think it would be nice to print when an epoch has passed.",0,,1,2018-01-28T00:03:46Z,CONTRIBUTOR,2018-01-30T03:31:05Z
16500,contrib/learn: Typo in variable name x_exrta --> x_extra,"awaiting testing (then merge),cla: yes","flake8 testing of https://github.com/tensorflow/tensorflow

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./tensorflow/contrib/learn/python/learn/datasets/synthetic.py:156:32: F821 undefined name 'x_extra'
    spir_x = np.append(spir_x, x_extra)
                               ^
```",1,,4,2018-01-27T17:58:50Z,CONTRIBUTOR,2018-01-27T23:04:23Z
16496,"Feature Suggestion: ""Float-bit-strings""",type:feature,"### System information (Not really relevant ...)
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 8 (?)
- **GPU model and memory**: GTX 1070
- **Exact command to reproduce**: NA

### Summary

This proposes the use of what I call ""float-bit-strings"" or ""float-bits"" instead of one-hot-encoded arrays so as to greatly reduce the memory and computational usage e.g. in language models.

I don't think this preliminary discussion belongs on StackOverflow so I hope it is OK to post it here. It is a new feature that could be added to TensorFlow. There's quite likely somebody on the TensorFlow dev-team or in the community who has already thought of this. But I have searched the internet and cannot find any mentioning of a similar idea.


### Background

I have started looking at language-models using e.g. LSTM and encoder-decoder architectures. There are some aspects that seem to be incredibly wasteful and limiting. Let me briefly describe this and please forgive me if I am ignorant, I have only spent a week or two on studying LSTM and language models so far :-)

For example in Machine Translation we typically have the text-data for the source- and target-languages as lists of integer-tokens, where each integer maps to a word in the vocabulary. There may be e.g. 100k different words so these integer-tokens can take on values between zero and 100k. This data cannot be input directly to a Neural Network so we use an embedding layer to convert these integers to n-dimensional vectors with values between zero and one, according to a mapping-function that may either be loaded from disk or trained along with the rest of the Neural Network; if I understand correctly.

For the decoder in a language model, we have a similar problem where we must somehow convert integer-tokens to data that the neural network can work on. A typical way of doing this seems to be a one-hot encoding; if I understand correctly. (This could also be done for the encoder-part, but it doesn't seem to be necessary).

I can't figure out what the max-size of one-hot encodings are in TensorFlow and whether it can even handle 100k one-hot encoded tensors. But it is obviously an extremely wasteful data-mapping. For example, for a vocab of 100k words we only need 17-bits (log2(100k)) to represent each integer-token - but for a one-hot encoding using 32-bit floats we need 32 x 100k bits!

I can't figure out what people normally do, but it seems like the common practice is to limit the vocab to a smaller number of words, e.g. 1k or 10k. It appears that Google Translate runs on multiple GPU's and maybe that's why they can handle extremely large vocabs with one-hot encoded tensors?


### Float-bit-strings

I thought it might be possible to use a bit-string-like representation inside a TensorFlow model. I have searched the internet and cannot find anyone who has proposed a similar idea.

The idea is to convert each integer-token to what I call a ""float-bit-string"" or ""float-bits"". For example, the number 123 has the bit-string 01111011. We can then make a corresponding tensor with floats [0., 1., 1., 1., 1., 0., 1., 1.] and input this to the TensorFlow model.

In a language model we would then have to input and output these ""float-bits"" instead of one-hot encoded arrays. This would dramatically reduce the memory and computational requirements of the models.


### Test

I have hacked together a little test using numpy and Keras / TensorFlow. The idea is to see if we can learn to map integers x with values between 0 and 10k to y = 123 * x using these ""float-bit"" encodings. And it works as you can see by running the code further below! That is perhaps not a surprise as neural networks are general function approximators, but it's not always that they work according to theory :-)

However, the network cannot learn the arithmetic mapping of e.g. y = 123 * x when x and y are ""float-bits"". This means it cannot generalize to data it hasn't seen during training in the arithmetic manner we might expect. But I don't think that is necessary for use in e.g. language models where we merely want to be able to map some tensor from e.g. an LSTM to an integer-token from the vocabulary.


### Loss Functions

I have tested this with both MSE and binary cross-entropy in Keras, which unfortunately isn't documented so I'm not completely sure what it does. But in both cases it works and the model trains to get the bit-wise mapping correct.

There might be cases where you are more concerned about the MSE between the actual integer-values instead of their ""float-bit-string"" representations, in which case we would need a TensorFlow method to convert ""float-bits"" to integers and then take the MSE of the resulting integer and the true integer from the data-set. This is not relevant for language models, because the proximity of integer-keys do not correspond to words that are necessarily similar in meaning. But it could be useful in other applications.


### TensorFlow Implementation

In order to make this work in TensorFlow it seems that we just need a couple of TensorFlow-methods for converting between integers and ""float-bit-strings"". I have hacked this together using numpy but I'm sure somebody on the dev-team can make a super-fast native TensorFlow implementation. Then we just need a wrapper in Keras and that might be enough to do e.g. language models with gigantic vocabs.


### Test-Code

    import numpy as np
    from tensorflow.python.keras.models import Sequential
    from tensorflow.python.keras.layers import InputLayer
    from tensorflow.python.keras.layers import Dense
    from tensorflow.python.keras.optimizers import RMSprop
    
    
    # Number of bits to use in our ""float-bit-strings"".
    num_bits = 32
    
    def int_to_floatbits(value):
        """"""
        Convert a single integer value to an array of 0.0 and 1.0 floats
        corresponding to the bit-string.
    
        Example: value==123 gives [0.  ... 0.  1.  1.  1.  1.  0.  1.  1.]
        """"""
    
        # Convert the integer value to a bit-string.
        # NOTE: This has been fixed to 32-bit length.
        bitstr = ""{0:032b}"".format(value)
    
        # Convert the bit-string to an array of equivalent float-values.
        floatbits = np.array([1.0 if bit == '1' else 0.0 for bit in bitstr])
    
        return floatbits
    
    
    def floatbits_to_strbits(floatbits):
        """"""
        Convert an array of floats to a bit-string.
        A float value greater than 0.5 results in 1.0
        and a float value less or equal to 0.5 results in 0.0
    
        Example: [0.1, 0.49, 0.51, 0.9, 1.1, -2.3] gives ""001110""
        """"""
    
        # Convert the float-array to a list of bit-characters '0' or '1'.
        charbits = ['1' if floatbit > 0.5 else '0' for floatbit in floatbits]
    
        # Convert the bit-characters to a string.
        strbits = """".join(charbits)
    
        return strbits
    
    def floatbits_to_int(floatbits):
        """"""
        Convert a float-array to an integer, assuming each element
        of the float-array corresponds to a bit.
        
        Example: [0.1, 0.49, 0.51, 0.9, 1.1, -2.3] corresponds to
        the bit-string ""001110"" which is the integer 14.
        """"""
    
        # Convert the float-array to a bit-string.
        strbits = floatbits_to_strbits(floatbits=floatbits)
    
        # Convert the bit-string to an integer value.
        value = int(strbits, base=2)
    
        return value
    
    
    # Various tests of the above functions.
    if True:
        foo = int_to_floatbits(123)
        print(foo)
        print(floatbits_to_strbits(foo))
        print(floatbits_to_int(foo))
    
        bar = [0.3,  0.9,  0.8,  0.51,  0.501,  0.4999,  0.999,  1.1]
        print(floatbits_to_strbits(bar))
        print(floatbits_to_int(bar))
    
        baz = [0.1, 0.49, 0.51, 0.9, 1.1, -2.3]
        print(floatbits_to_strbits(baz))
        print(floatbits_to_int(baz))
    
    # quit()
    
    # We will now train a TensorFlow / Keras model
    # that maps integers between 0 and 10000 to
    # the same numbers multiplied by 123.
    # If we were to use one-hot encoding then we would
    # need 10000 inputs to the Neural Network and
    # 1230000 outputs if using the full output range.
    # Using ""bit-strings"" encoded as floats, we only need
    # 14 bits for the input and 21 bits for the output.
    # We round it up to 32-bits.
    
    # The dataset as integers,
    # we want the Neural Network to map from x to y.
    x_int = np.arange(10000, dtype=int)
    y_int = 123 * x_int
    
    # Convert the dataset to ""float-bit-strings"" (aka. float-bits).
    x = np.array(list(map(int_to_floatbits, x_int)))
    y_true = np.array(list(map(int_to_floatbits, y_int)))
    
    # Check the mapping is correct. E.g. if the number of required bits
    # exceeds num_bits then these may not create numpy matrices correctly.
    if False:
        print(x.shape)
        print(y_true.shape)
        print(x[0:10])
        print(y_true[0:10])
    
    # Start construction of the Keras Sequential model.
    model = Sequential()
    
    # Add an input layer to the model.
    model.add(InputLayer(input_shape=(num_bits,)))
    
    # Fully-connected / dense layers with ReLU-activation.
    model.add(Dense(512, activation='relu'))
    model.add(Dense(512, activation='relu'))
    
    # Last fully-connected / dense layer with sigmoid-activation
    # so the output is between 0.0 and 1.0
    model.add(Dense(num_bits, activation='sigmoid'))
    
    optimizer = RMSprop(lr=1e-3)
    
    if True:
        # Loss is MSE.
        model.compile(optimizer=optimizer,
                      loss='mean_squared_error')
    else:
        # Loss is Binary Crossentropy, but also report MSE.
        model.compile(optimizer=optimizer,
                      loss='binary_crossentropy',
                      metrics=['mse'])
    
    epochs = 50
    
    if True:
        # Fit the model using the entire data-set.
        model.fit(x, y_true, epochs=epochs)
    else:
        # Fit the model using the data-set split into training and validation.
        # You will see that the validation-error is high so the model
        # has not learned the arithmetic function of the data-set.
        model.fit(x, y_true, epochs=epochs, validation_split=0.2)
    
    # Use the model to predict the output for a part of the data-set.
    y_pred = model.predict(x[0:10])
    
    # The true output for this part of the data-set.
    y_true_subset = y_true[0:10]
    
    # Map the ""float-bit-strings"" to integers.
    y_pred_int = list(map(floatbits_to_int, y_pred))
    y_true_int = list(map(floatbits_to_int, y_true_subset))
    
    # Print the predicted and true integers.
    print(*zip(y_pred_int, y_true_int))
    
    # Round the float-bit-strings to 2 decimals for pretty printing.
    def rounded(numbers):
        return np.array([[""{:.2f}"".format(x) for x in row] for row in numbers])
    y_pred_rounded = rounded(y_pred)
    y_true_rounded = rounded(y_true_subset)
    
    # Print the predicted and true float-bit-strings.
    # (I know it is bad to reuse the same variable-names here ...)
    for y_pred_int, y_true_int, y_pred_rounded, y_true_rounded \
        in zip(y_pred_int, y_true_int, y_pred_rounded, y_true_rounded):
    
        print(y_true_int, ""\t"", y_true_rounded)
        print(y_pred_int, ""\t"", y_pred_rounded)
        print()


### Output

True integer and true ""float-bit-string"" (note that the numbers are all exactly 0.00 or 1.00):

	738 	 ['0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '1.00' '0.00' '1.00' '1.00' '1.00' '0.00' '0.00' '0.00' '1.00' '0.00']

Predicted integer and predicted ""float-bit-string"" (note that the numbers a **not** all exactly 0.00 or 1.00):

	738 	 ['0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.03' '0.00' '0.00' '0.00' '0.01' '0.00' '0.00' '0.00' '0.00' '0.99' '0.00' '1.00' '1.00' '1.00' '0.00' '0.00' '0.00' '1.00' '0.00']
",0,,1,2018-01-27T15:23:29Z,CONTRIBUTOR,2018-01-30T03:22:21Z
16486,Change RELEASE.md to specify CUDA 9.0,cla: yes,PR for https://github.com/tensorflow/tensorflow/issues/16348 (tinyest PR ever?),0,,3,2018-01-27T09:17:33Z,CONTRIBUTOR,2018-01-27T09:19:21Z
16481,Container localhost does not exist.,"stat:awaiting tensorflower,type:bug/performance","Hi,

I upgraded from 1.5.0-rc1 to the current master branch and I started receiving the following error:

```
2018-01-27 02:48:38.928667: W tensorflow/core/framework/op_kernel.cc:1201] OP_REQUIRES failed at lookup_table_op.cc:656 : Not found: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
2018-01-27 02:48:38.928786: W tensorflow/core/framework/op_kernel.cc:1201] OP_REQUIRES failed at iterator_ops.cc:855 : Not found: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
	 [[Node: Lookup_1/LookupTableFind = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT64](lookup_1_placeholder, input_1, lookup_1_placeholder_1)]]
Exception in thread ""main"" org.platanios.tensorflow.jni.NotFoundException: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
	 [[Node: Lookup_1/LookupTableFind = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT64](lookup_1_placeholder, input_1, lookup_1_placeholder_1)]]
	 [[Node: Model/Model/Iterator/Next = IteratorGetNext[output_shapes=[[?,?], [?], [?,?], [?,?], [?]], output_types=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Model/Model/Iterator)]]
```

It's hard to reproduce this error but a summary of the context is that I have a lookup table op inside a dataset map operator and I get this error when I try to execute the corresponding iterator ""GetNext"" op. I'm looking for information in how to parse and debug this error. I never explicitly set any containers for my variables or lookup tables (i.e., leave them to the default value; an empty string). Were there any changes introduced recently that could result in this error? Note that this happens with my Scala API but not with the Python API and so it may be that I haven't updated something in my code. I just don't really know where to look for this.

Thanks!",1,,11,2018-01-27T07:58:18Z,CONTRIBUTOR,2018-01-27T15:54:41Z
16479,Does 1.5.0 not suppurt CUDA 9.1? It worked with CUDA 9.0 but not 9.1,,"I installed 1.5.0, and tying to import tensorflow, but it said that 'cannot find cudart64_90.dll'. 
Then I installed the CUDA 9.0, and then everything works fine. 
So I want to make sure than does 1.5.0 not support CUDA 9.1 or I have something installed wrong? 
",0,,6,2018-01-27T02:22:43Z,NONE,2018-01-27T03:21:39Z
16478,Failed install on Windows,type:bug/performance,"Python 3.6.4

There is a strange error when I install tensorflow 1.5.

```
Collecting tensorflow
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/34/96/11f048eca7b4d6da3084ca49c636b9e720e9dd1483c0c4e9ba3cf5037564/tensorflow-1.5.0-cp36-cp36m-win_amd64.whl
Requirement already up-to-date: wheel>=0.26 in d:\python\python36\lib\site-packages (from tensorflow)
Requirement already up-to-date: numpy>=1.12.1 in d:\python\python36\lib\site-packages (from tensorflow)
Collecting absl-py>=0.1.6 (from tensorflow)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/42/3c/1985d86a44bfe44fd060c02807336f840a509bfaa2d340860fba7d22da39/absl-py-0.1.9.tar.gz
Requirement already up-to-date: protobuf>=3.4.0 in d:\python\python36\lib\site-packages (from tensorflow)
Requirement already up-to-date: six>=1.10.0 in d:\python\python36\lib\site-packages (from tensorflow)
Collecting tensorflow-tensorboard<1.6.0,>=1.5.0 (from tensorflow)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/43/69/82e2a368076c94edbba3cd15804103bf1f31486d69e11551b71fa1d1f384/tensorflow_tensorboard-1.5.0-py3-none-any.whl
Requirement already up-to-date: setuptools in d:\python\python36\lib\site-packages (from protobuf>=3.4.0->tensorflow)
Requirement already up-to-date: bleach==1.5.0 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Requirement already up-to-date: markdown>=2.6.8 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Requirement already up-to-date: werkzeug>=0.11.10 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Requirement already up-to-date: html5lib==0.9999999 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Collecting futures>=3.1.1 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/1f/9e/7b2ff7e965fc654592269f2906ade1c7d705f1bf25b7d469fa153f7d19eb/futures-3.2.0.tar.gz
Unknown requires Python '>=2.6, <3' but the running Python is 3.6.4
```

Why the dependency is *futures*? It doesn't have a verion of Python 3.6.4.",0,,15,2018-01-27T02:08:44Z,NONE,2018-01-27T03:28:37Z
16474,MKL: Making MKL-DNN default,"awaiting testing (then merge),cla: yes",Make Tensroflow use MKL DNN by default if --config=mkl is used when building,1,,10,2018-01-26T22:08:58Z,CONTRIBUTOR,2018-01-26T22:14:09Z
16473,Fixing hard_sigmoid's documentation to match impl,cla: no,,1,,3,2018-01-26T21:25:42Z,NONE,2018-01-26T21:34:42Z
16464,AssignAddVariableOp has no output,"stat:awaiting tensorflower,type:docs","
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0-rc1
- **Python version**: NA (Using Go bindings)
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 7.2.1
- **CUDA/cuDNN version**: 9.1 / 7.0
- **GPU model and memory**: GTX 1060 6GB
- **Exact command to reproduce**: See below


### Describe the problem
According to the docs, AssignAddVariableOp ""Outputs the incremented value, which can be used to totally order the increments to this variable."". Without this feature, I get non deterministic behavior when reading the value of the variable at the same time as I update it. However, at least in the Go bindings, it returns an operation which has no outputs. I can work around this problem by using two calls to `sess.Run()`, but this is inelegant.

### Source code / logs
```
package main

import (
	""fmt""

	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func main() {
	s := op.NewScope()
	value1 := op.Const(s.SubScope(""zero""), float32(0))
	value2 := op.Const(s, float32(3.1415))
	handle := op.VarHandleOp(s, tf.Float, tf.ScalarShape())
	init := op.AssignVariableOp(s, handle, value1)
	update := op.AssignAddVariableOp(s, handle, value2)
	fmt.Println(""NumOutputs:"", update.NumOutputs())
	graph, err := s.Finalize()
	if err != nil {
		panic(err)
	}
	sess, err := tf.NewSession(graph, nil)
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{init})
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, []tf.Output{update.Output(0)}, nil)
	if err != nil {
		panic(err)
	}
}
```
```
$ go run assign_demo.go 
NumOutputs: 0
panic: Tried to fetch data for 'AssignAddVariableOp:0', which produces no output.  To run to a node but not fetch any data, pass 'AssignAddVariableOp:0' as an argument to the 'target_node_names' argument of the Session::Run API.

goroutine 1 [running]:
main.main()
	/home/isaac/go/src/github.com/is8ac/gotf/assign_demo.go:32 +0x448
exit status 2
```",1,,4,2018-01-26T17:33:42Z,NONE,2018-01-27T05:35:56Z
16463,Improve profiler error message when graph_path is not available.,"awaiting testing (then merge),cla: yes,kokoro:run","This fix tries to address the issue raised in #16451 to provide a better error message when graph_path is not available for profiler.

Previously if graph_path is not available, the process will crash
with not very imformative message and a core dump:
```
2018-01-26 01:43:29.458032: F tensorflow/core/profiler/profiler.cc:206] Non-OK-status: ReadProtoFile(Env::Default(), FLAGS_graph_path, graph.get(), false) status: Not found: ; No such file or directory
Aborted (core dumped)
```

With this fix, the error message is improved to:
```
Failed to read graph_path: Invalid argument: Cannot parse proto file.
```
and the process exit with 1.

This fix fixes #16451.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,1,2018-01-26T16:55:04Z,MEMBER,2018-01-26T17:13:08Z
16462,How to create a model checkpoint based on each step rather than time interval.? using TensorFlow-Slim api.,"stat:awaiting tensorflower,type:support","slim.learning.train(
    train_op,
    logdir,
    number_of_steps=1000,
    **save_summaries_secs=300**,
    **save_interval_secs=600**):

The above api only supports to capture model checpoints periodically, but I need to checkpoint based on each step. How do achieve this using TesorFlow-Slim API.?

I am looking for parameters like this:

save_summaries_steps = 10,
save_interval_steps=10
    where the value 10 is the number of steps and that should be configurable.",0,,3,2018-01-26T16:48:53Z,NONE,2018-01-27T01:03:49Z
16458,How to parse multivalve feature using tf.feature_column and tf.data API ??,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,2,2018-01-26T15:58:52Z,NONE,2018-01-27T01:02:00Z
16454,ValueError: Dimensions 1069539296 and 13528529576648672 are not compatible,stat:awaiting response,"Traceback (most recent call last):
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 558, in merge_with
    new_dims.append(dim.merge_with(other[i]))
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 133, in merge_with
    self.assert_is_compatible_with(other)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 106, in assert_is_compatible_with
    other))
ValueError: Dimensions 1069539296 and 13528529576648672 are not compatible

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""Train.py"", line 117, in <module>
    train()
  File ""Train.py"", line 54, in train
    train_op=Evaluation.trainning(loss=loss1, learning_rate=0.0001)
  File ""/home/lihua/Documents/Projects/Project2018/trafficSignClassification/Evaluation.py"", line 36, in trainning
    train_op = optimizer.minimize(loss, global_step= global_step)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 315, in minimize
    grad_loss=grad_loss)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 386, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 560, in gradients
    in_grad.set_shape(t_in.get_shape())
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 443, in set_shape
    self._shape = self._shape.merge_with(shape)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 561, in merge_with
    raise ValueError(""Shapes %s and %s are not compatible"" % (self, other))
ValueError: Shapes (128, 4, 4, 1069539296) and (128, 4, 4, 13528529576648672) are not compatible


ubuntu16.04
tensorflow 1.4


",0,,1,2018-01-26T13:37:31Z,NONE,2018-01-27T02:52:08Z
16451,Parameter parsing error messages,,"Parameter parsing error messages probably can be improved, e.g. 

`bazel-bin/tensorflow/core/profiler/profiler --profile_path=/tmp/for_tfprof/profile_20`

runs ok, but if cd to bazel-bin/tensorflow/core/profiler/ and

`./profiler --profile_path /tmp/for_tfprof/profile_20`

results in 

> ./profiler
> --profile_path
> /tmp/for_tfprof/profile_20
> Reading Files...
> Try to use a single --profile_path instead of graph_path,op_log_path,run_meta_path
> 2018-01-26 01:43:29.458032: F tensorflow/core/profiler/profiler.cc:206] Non-OK-status: ReadProtoFile(Env::Default(), FLAGS_graph_path, graph.get(), false) status: Not found: ; No such file or directory
> Aborted (core dumped)

",0,,1,2018-01-26T10:01:37Z,CONTRIBUTOR,2018-01-26T16:55:34Z
16450,InvalidArgumentError (see above for traceback): sequence_length(0) <= 80 thrown by ctc_loss,stat:awaiting response,"I am encountering this error thrown by `ctc_loss` and I have no idea what it means nor how to resolve it.

```
InvalidArgumentError (see above for traceback): sequence_length(0) <= 80
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](transpose_1/_455, Where/_445, GatherNd, reshape_1/_457)]]
	 [[Node: CTCLoss/_459 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_7988_CTCLoss"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

```",0,,1,2018-01-26T09:49:04Z,CONTRIBUTOR,2018-01-27T03:31:47Z
16448,embedding lookup table in tensorflow serving,,"Hi, I am trying to serve a NLP model in tensorflow serving. I am wondering how embedding matrix is being stored in tensorflow serving. If I deploy model to two servers, will the embedding matrix be a distributed table with sharding for looking up?",0,,2,2018-01-26T08:36:01Z,NONE,2018-01-27T03:32:39Z
16446,use tflite bilinear op to resize input of label_image,"awaiting testing (then merge),cla: yes,comp:lite",replace previous naive `downsize()` function with a `resize()` using TF Lite RESIZE_BILINEAR operator,1,,1,2018-01-26T07:44:36Z,CONTRIBUTOR,2018-01-30T18:02:50Z
16444,Documentation update,,"The mean squared error is described as following

mean_squared_error(
    labels,
    predictions,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)

The default reduction method is MEAN in v1.4",0,,6,2018-01-26T06:42:29Z,NONE,2018-01-26T07:05:20Z
16443,Disable AWS S3 virtual addressing,"awaiting testing (then merge),cla: yes","The fix disables the virtual addressing of AWS S3, as was suggested in the comment https://github.com/tensorflow/tensorflow/issues/16397#issuecomment-360654674

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
",1,,4,2018-01-26T05:58:11Z,MEMBER,2018-01-26T18:52:02Z
16438,xrange() was removed in Python 3,"awaiting testing (then merge),cla: yes",Each of these files contains at least one call to the Python 2-only builtin function __xrange()__ which was removed in Python 3 in favor of __range()__.  To each of these files we add the line [__from six.moves import xrange__](https://pythonhosted.org/six/#module-six.moves) for compatibility with both Python 2 and Python 3.,1,,1,2018-01-26T03:43:34Z,CONTRIBUTOR,2018-01-26T06:41:59Z
16434, Imported lstm1d and lstm2d in ndlstm __init__.py.,"awaiting testing (then merge),cla: yes",Makes importing ndlstm modules easier.,1,,1,2018-01-26T02:58:34Z,CONTRIBUTOR,2018-01-26T05:38:52Z
16433,Fix an imperfect implementation of tf.losses.mean_pairwise_squared_error,"awaiting testing (then merge),cla: yes","Here is a fix for the issue [Imperfect implementation of tf.losses.mean_pairwise_squared_error (#15968)](https://github.com/tensorflow/tensorflow/issues/15968)

RELNOTES: Fixed wrong normalization in tf.losses.mean_pairwise_squared_error to conform to the math and documentation. Numerical results will be different.",1,,5,2018-01-26T02:42:02Z,CONTRIBUTOR,2018-02-01T01:27:39Z
16427,Fix build errors in contrib/mpi introduced by commit 6042b5d267f,"awaiting testing (then merge),cla: yes","The commit https://github.com/tensorflow/tensorflow/commit/6042b5d267f42d004087b44c29525951700579f9#diff-7c00d4a3caee74eedf5bb638bce23e5a 
* Introduced code to `tensorflow/contrib/mpi/mpi_rendezvous_mgr.h` to use the type `RecentRequestIds` without including the header `tensorflow/core/distributed_runtime/recent_request_ids.h`.
```
ERROR: /opt/tensorflow/tensorflow/contrib/mpi/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr' failed (Exit 1)
In file included from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:0:
./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:182:3: error: 'RecentRequestIds' does not name a type
   RecentRequestIds recv_tensor_recent_request_ids_;
   ^
```
* Probably a typo in `tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc : MPIRendezvousMgr::AddRequest()`. The variable `req` was probably meant to be `request` as per the commit message.
```
ERROR: /opt/tensorflow/tensorflow/contrib/mpi/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr' failed (Exit 1)
In file included from ./tensorflow/core/framework/variant.h:29:0,
                 from ./tensorflow/core/framework/allocator.h:26,
                 from ./tensorflow/core/framework/tensor.h:20,
                 from ./tensorflow/core/framework/device_base.h:23,
                 from ./tensorflow/core/framework/rendezvous.h:22,
                 from ./tensorflow/core/distributed_runtime/rendezvous_mgr_interface.h:22,
                 from ./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:22,
                 from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:35,
                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc: In member function 'void tensorflow::MPIRendezvousMgr::AddRequest(tensorflow::RecvTensorRequest, int)':
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:155:7: error: 'req' was not declared in this scope
       req.request_id(), ""RecvTensor (MPIRendezvousMgr)"", req));
       ^
```

I compiled with following commands:
```
echo ""deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8"" > /etc/apt/sources.list.d/bazel.list
curl https://bazel.build/bazel-release.pub.gpg | apt-key add -
git clone https://github.com/tensorflow/tensorflow .
export PYTHON_BIN_PATH=/path/to/python ## python 2.7.14
export USE_DEFAULT_PYTHON_LIB_PATH=1
export TF_NEED_JEMALLOC=1
export TF_NEED_GCP=0
export TF_NEED_HDFS=1
export TF_ENABLE_XLA=1
export TF_NEED_OPENCL=0
export TF_NEED_S3=0
export TF_NEED_GDR=0
export TF_NEED_VERBS=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=8.0
export CUDA_TOOLKIT_PATH=/path/to/cuda
export TF_CUDNN_VERSION=7
export CUDNN_INSTALL_PATH=/path/to/cudnn
export TF_CUDA_COMPUTE_CAPABILITIES=""3.5,5.2,6.0,6.1""
export TF_CUDA_CLANG=0
export GCC_HOST_COMPILER_PATH=/path/to/gcc
export TF_NEED_MPI=1
export MPI_HOME=/path/to/openmpi
export CC_OPT_FLAGS=""-march=native""
export TF_SET_ANDROID_WORKSPACE=0
./configure
bazel build --config=mkl --config=opt --config=cuda \
          //tensorflow/tools/pip_package:build_pip_package && \
bazel-bin/tensorflow/tools/pip_package/build_pip_package ./tensorflow_pkg
pip install -v ./tensorflow_pkg/tensorflow-*.whl
```",1,,6,2018-01-26T01:43:01Z,CONTRIBUTOR,2018-01-26T02:07:18Z
16424,Dependency on old version of bleach (1.5),type:build/install,"Bleach 1.5 came out Nov 4th 2016 and this is old enough to cause dependency issues for projects that stayed up to date with Bleach.

In particular, this causes issues for Jupyter users.",1,,11,2018-01-25T23:20:59Z,NONE,2018-01-27T06:01:01Z
16423,Windows 10 Cmake GPU nvcc.exe error,type:build/install,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
r1.5
- **Python version**: 
3.6
- **GCC/Compiler version (if compiling from source)**:
Visual Studio 2017
- **CUDA/cuDNN version**:
9.1
- **GPU model and memory**:
1080Ti
- **Exact command to reproduce**:

**Cmake Command:**
```
cmake -G ""Visual Studio 15 2017 Win64"" -T host=x64 -DCMAKE_BUILD_TYPE=""
Release"" -DSWIG_EXECUTABLE='C:\ProgramData\Chocolatey\bin\swig.exe' -Dtensorflow_ENABLE_GPU=ON -Dtensorflow_CUDA_VERSION
=9.1 -Dtensorflow_CUDNN_VERSION=7 -Dtensorflow_WIN_CPU_SIMD_OPTIONS=""/arch:AVX2"" -DCUDA_CUDART_LIBRARY=D:\NVIDIA\CUDA\v9
.1 -DCUDNN_HOME='D:\NVIDIA\CUDA\v9.1' ..
```

**Build command**
```
""C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\Bin\amd64\MSBuild.exe"" /m:4 /p:Configuration=Release .\tf_core_gpu_kernels.vcxproj
```
### Describe the problem
Cmake creates a bad command to send to nvcc.exe

In tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.Release.cmake:202L
there is an error with the resulting command.

There is issues with spacing, "";"" in between arguments and others.

**Command Ran**
`C:/NVIDIA/CUDA/v9.1/bin/nvcc.exe -M -D__CUDACC__ D:/tensorflow/tensorflow/core/kernels/adjust_contrast_op_gpu.cu.cc -o D:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.NVCC-depend -ccbin;C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin -m64;-DSQLITE_OMIT_LOAD_EXTENSION;-DEIGEN_AVOID_STL_ARRAY;-DNOMINMAX;-D_WIN32_WINNT=0x0A00;-DLANG_CXX11;-DCOMPILER_MSVC;-DWIN32;-DOS_WIN;-D_MBCS;-DWIN64;-DWIN32_LEAN_AND_MEAN;-DNOGDI;-DPLATFORM_WINDOWS;-DTENSORFLOW_USE_EIGEN_THREADPOOL;-DEIGEN_HAS_C99_MATH;-DTF_COMPILE_LIBRARY;-DGRPC_ARES=0;-DTF_USE_SNAPPY;-DGOOGLE_CUDA=1;-DTF_EXTRA_CUDA_CAPABILITIES=6.1 -Xcompiler;,""/DWIN32"",""/D_WINDOWS"",""/W3"",""/GR"",""/EHsc"",""/MP"",""/arch:AVX2"",""/MD"",""/O2"",""/Ob2"",""/DNDEBUG"",""/D_ITERATOR_DEBUG_LEVEL=0""  -gencode;arch=compute_61,code=""sm_61,compute_61"";--include-path;D:/tensorflow/tensorflow/contrib/cmake/build/Release;--expt-relaxed-constexpr;-ftz=true;  -DNVCC -IC:/NVIDIA/CUDA/v9.1/include ;-ID:/tensorflow ;-ID:/tensorflow/tensorflow/contrib/cmake/build ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/zlib_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/gif_archive/giflib-5.1.4 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/png_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/jpeg_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/lmdb ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/eigen_archive ;-ID:/tensorflow/third_party/eigen3 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/gemmlowp/src/gemmlowp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/src/jsoncpp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive/util ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/highwayhash ;-ID:/tensorflow/tensorflow/contrib/cmake/build/cub/src/cub ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/nsync/public ;-ID:/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src ;-ID:/tensorflow/tensorflow/contrib/cmake/build/re2/install/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/sqlite ;-ID:/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/snappy/src/snappy ;-IC:/NVIDIA/CUDA/v9.1 ;-IC:/NVIDIA/CUDA/v9.1/extras/CUPTI/include ;-ID:/tensorflow/third_party/gpus`

Multable invalid cmake varables

```
${CCBIN} = -ccbin;C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin 
${nvcc_flags} = -m64;-DSQLITE_OMIT_LOAD_EXTENSION;-DEIGEN_AVOID_STL_ARRAY;-DNOMINMAX;-D_WIN32_WINNT=0x0A00;-DLANG_CXX11;-DCOMPILER_MSVC;-DWIN32;-DOS_WIN;-D_MBCS;-DWIN64;-DWIN32_LEAN_AND_MEAN;-DNOGDI;-DPLATFORM_WINDOWS;-DTENSORFLOW_USE_EIGEN_THREADPOOL;-DEIGEN_HAS_C99_MATH;-DTF_COMPILE_LIBRARY;-DGRPC_ARES=0;-DTF_USE_SNAPPY;-DGOOGLE_CUDA=1;-DTF_EXTRA_CUDA_CAPABILITIES=6.1
${nvcc_host_compiler_flags} = -Xcompiler;,""/DWIN32"",""/D_WINDOWS"",""/W3"",""/GR"",""/EHsc"",""/MP"",""/arch:AVX2"",""/MD"",""/O2"",""/Ob2"",""/DNDEBUG"",""/D_ITERATOR_DEBUG_LEVEL=0""  
${depends_CUDA_NVCC_FLAGS} = -gencode;arch=compute_61,code=""sm_61,compute_61"";--include-path;D:/tensorflow/tensorflow/contrib/cmake/build/Release;--expt-relaxed-constexpr;-ftz=true
${CUDA_NVCC_INCLUDE_ARGS} = -IC:/NVIDIA/CUDA/v9.1/include ;-ID:/tensorflow ;-ID:/tensorflow/tensorflow/contrib/cmake/build ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/zlib_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/gif_archive/giflib-5.1.4 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/png_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/jpeg_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/lmdb ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/eigen_archive ;-ID:/tensorflow/third_party/eigen3 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/gemmlowp/src/gemmlowp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/src/jsoncpp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive/util ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/highwayhash ;-ID:/tensorflow/tensorflow/contrib/cmake/build/cub/src/cub ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/nsync/public ;-ID:/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src ;-ID:/tensorflow/tensorflow/contrib/cmake/build/re2/install/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/sqlite ;-ID:/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/snappy/src/snappy ;-IC:/NVIDIA/CUDA/v9.1 ;-IC:/NVIDIA/CUDA/v9.1/extras/CUPTI/include ;-ID:/tensorflow/third_party/gpus
```


Should be
```
${CCBIN} = -ccbin ""C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin""
${nvcc_flags} = -m64 -DSQLITE_OMIT_LOAD_EXTENSION -DEIGEN_AVOID_STL_ARRAY -DNOMINMAX -D_WIN32_WINNT=0x0A00 -DLANG_CXX11 -DCOMPILER_MSVC -DWIN32 -DOS_WIN -D_MBCS -DWIN64 -DWIN32_LEAN_AND_MEAN -DNOGDI -DPLATFORM_WINDOWS -DTENSORFLOW_USE_EIGEN_THREADPOOL -DEIGEN_HAS_C99_MATH -DTF_COMPILE_LIBRARY -DGRPC_ARES=0 -DTF_USE_SNAPPY -DGOOGLE_CUDA=1 -DTF_EXTRA_CUDA_CAPABILITIES=6.1 
${nvcc_host_compiler_flags} = -Xcompiler ""/DWIN32,/D_WINDOWS,/W3,/GR,/EHsc,/MP,/arch:AVX2,/MD,/O2,/Ob2,/DNDEBUG,/D_ITERATOR_DEBUG_LEVEL=0"" 
${depends_CUDA_NVCC_FLAGS} = -gencode arch=compute_61,code=\""sm_61,compute_61\"" --include-path D:/tensorflow/tensorflow/contrib/cmake/build/Release --expt-relaxed-constexpr -ftz=true
${CUDA_NVCC_INCLUDE_ARGS} = -IC:/NVIDIA/CUDA/v9.1/include -ID:/tensorflow -ID:/tensorflow/tensorflow/contrib/cmake/build -ID:/tensorflow/tensorflow/contrib/cmake/build/external/zlib_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/gif_archive/giflib-5.1.4 -ID:/tensorflow/tensorflow/contrib/cmake/build/external/png_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/jpeg_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/lmdb -ID:/tensorflow/tensorflow/contrib/cmake/build/external/eigen_archive -ID:/tensorflow/third_party/eigen3 -ID:/tensorflow/tensorflow/contrib/cmake/build/gemmlowp/src/gemmlowp -ID:/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/src/jsoncpp -ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive/util -ID:/tensorflow/tensorflow/contrib/cmake/build/external/highwayhash -ID:/tensorflow/tensorflow/contrib/cmake/build/cub/src/cub -ID:/tensorflow/tensorflow/contrib/cmake/build/external/nsync/public -ID:/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src -ID:/tensorflow/tensorflow/contrib/cmake/build/re2/install/include -ID:/tensorflow/tensorflow/contrib/cmake/build/external/sqlite -ID:/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/include -ID:/tensorflow/tensorflow/contrib/cmake/build/snappy/src/snappy -IC:/NVIDIA/CUDA/v9.1 -IC:/NVIDIA/CUDA/v9.1/extras/CUPTI/include -ID:/tensorflow/third_party/gpus

```",0,,1,2018-01-25T21:55:09Z,NONE,2018-01-26T01:19:15Z
16419,R1.4,cla: no,want to test ,0,,2,2018-01-25T20:09:27Z,NONE,2018-01-25T21:33:32Z
16417,Add missing library in Dockerfile,cla: yes,The local Dockerfile does not have all the dependencies for running the exercise notebooks in udacity assignments.,1,,5,2018-01-25T19:42:22Z,CONTRIBUTOR,2018-01-25T19:44:00Z
16416,Making global constant.py file for ops,cla: no,Created a separate common `constants.py` which can be used globally.,0,,2,2018-01-25T19:39:27Z,CONTRIBUTOR,2018-01-25T19:39:59Z
16413,Separate constant file for global variables,cla: no,"Created a separate common `constants.py` which can be used globally under `ops.`

**P.S:** I created the same pull request [#16401](https://github.com/tensorflow/tensorflow/pull/16401) as after pushing my latest changes I was facing CLA issues.",0,,4,2018-01-25T18:55:02Z,CONTRIBUTOR,2018-01-25T19:04:36Z
16412,Documentation on build from source is unclear,type:support,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh


python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.4.1-7-gaa03bfc', '1.4.1')
built and installed from source with
git checkout r1.4
bazel build -c opt --copt=-march=""haswell"" --config=cuda --verbose_failures --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package >pip_package_build2.log 2>&1
note: incompatible path flag is required with R1.4 at this time per https://github.com/tensorflow/tensorflow/issues/15492
ubuntu 16.04
Cuda 9.1, cudnn 7.0.4
gcc --version
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
uname -r
4.4.0-104-generic
Bazel 0.9

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
It is unclear how to build and install the entire package purely from source
I will attempt to log what I have done so far
clone and build TF R1.4 for cuda
install wheel into local directory  (sudo pip install /tmp/tensorflow-pkg/tensorflow*.whl -t ~/mytf_r1.4_c9.1
export PYTHONPATH=~/mytf_r1.4_c9.1
move tensorflow directory

install common_voice files to ~/Common_voice

per native client build from source instructions: https://github.com/mozilla/DeepSpeech/blob/master/native_client/README.md
git clone tensorflow
cd tensorflow
git checkout r1.4
ln -s ../DeepSpeech/native_client ./
./configure
edit native_client/BUILD
comment out the following:
#    tfcompile_flags = select({
#        ""//tensorflow:rpi3"": str('--target_triple=""armv6-linux-gnueabihf"" --target_cpu=""cortex-a53"" --target_features=""+neon-fp-armv8""'),
#        ""//conditions:default"": str('')
#    }),
bazel build -c opt --copt=-O3 --incompatible_load_argument_is_label=false //tensorflow:libtensorflow_cc.so //tensorflow:libtensorflow_framework.so //native_client:deepspeech //native_client:deepspeech_utils //native_client:libctc_decoder_with_kenlm.so //native_client:generate_trie
at this point all the native client binaries are in
~/tensorflow/bazel-bin/native_client
levinth@zt-gpu-lin:~/DeepSpeech/native_client$ ls ~/tensorflow/bazel-bin/native_client/
generate_trie
generate_trie-2.params
generate_trie.runfiles
generate_trie.runfiles_manifest
libctc_decoder_with_kenlm.so
libctc_decoder_with_kenlm.so-2.params
libctc_decoder_with_kenlm.so.runfiles
libctc_decoder_with_kenlm.so.runfiles_manifest
libdeepspeech.a
libdeepspeech.a-2.params
libdeepspeech.pic.a
libdeepspeech.pic.a-2.params
libdeepspeech.so
libdeepspeech.so-2.params
libdeepspeech_utils.a
libdeepspeech_utils.a-2.params
libdeepspeech_utils.pic.a
libdeepspeech_utils.pic.a-2.params
libdeepspeech_utils.so
libdeepspeech_utils.so-2.params
_objs

cd ../Deepspeech/native_client
export TFDIR ~/tensorflow
make deepspeech


at this point however the native client shared objects are still in bazel-bin/native client and have not been installed. the invocation of Deepspeech.py fails as it cannot find the shared objects
python DeepSpeech.py --train_files ../Common_voice/cv-valid-train.csv,../Common_voice/cv-other-train.csv --dev_files ../Common_voice/cv-valid-dev.csv --test_files ../Common_voice/cv-valid-test.csv >deepspeech_1.log 2>&1
tensorflow.python.framework.errors_impl.NotFoundError: native_client/libctc_decoder_with_kenlm.so: cannot open shared object file: No such file or directory

the native_client/Makefile has sections for bindings and install..so try
sudo make install
and this still generates the error as install does not put
~/tensorflow/bazel-bin/native_client/libctc_decoder_with_kenlm.so
into /usr/local/lib 
though deepspeech.so and deepspeech_utils.so are installed there.

manually copy /tensorflow/bazel-bin/native_client/libctc_decoder_with_kenlm.so to ~/DeepSpeech/native_client and set permissions
at this point the invocation now starts running but complains about
------------------------------------------------------------------------
WARNING: libdeepspeech failed to load, resorting to deprecated code
         Refer to README.md for instructions on installing libdeepspeech
------------------------------------------------------------------------
even though /usr/local/lib is in the $LD_LIBRARY_PATH

invoking
python DeepSpeech.py --train_files ../Common_voice/cv-valid-train.csv,../Common_voice/cv-other-train.csv --dev_files ../Common_voice/cv-valid-dev.csv --test_files ../Common_voice/cv-valid-test.csv --display_step 1 --validation_step 10

------------------------------------------------------------------------
WARNING: libdeepspeech failed to load, resorting to deprecated code
         Refer to README.md for instructions on installing libdeepspeech
------------------------------------------------------------------------
I STARTING Optimization
Loading the LM will be faster if you build a binary file.
Reading data/lm/lm.binary
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
terminate called after throwing an instance of 'lm::FormatLoadException'
  what():  native_client/kenlm/lm/read_arpa.cc:65 in void lm::ReadARPACounts(util::FilePiece&, std::vector<long unsigned int>&) threw FormatLoadException.
first non-empty line was ""version https://git-lfs.github.com/spec/v1"" not \data\. Byte: 43

I clearly have not figured this out
:-)
 
",0,,3,2018-01-25T17:58:48Z,NONE,2018-01-25T19:45:03Z
16409,Build libjpeg-turbo ALTIVEC SIMD,"awaiting review,cla: yes","The libjpeg-turbo package has ALTIVEC SIMD and this updates the
third_party build to build the ALTIVEC SIMD on the appropriate
platform.",1,,1,2018-01-25T16:12:29Z,CONTRIBUTOR,2018-01-25T16:59:47Z
16407,Creating placeholder with `np.uint32` dtype fails,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes custom snippet below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 7
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
not applicable
- **GCC/Compiler version (if compiling from source)**:
not applicable
- **CUDA/cuDNN version**:
CUDA 8.5
- **GPU model and memory**:
Titan X
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
np.uint32 dtype is not supported while creating placeholder

### Source code / logs
```python
import tensorflow as tf
import numpy as np
print(tf.placeholder(np.int32, [None], 'ph1'))
print(tf.placeholder(np.uint32, [None], 'ph2'))
```
Line 3 works, line 4 fails.

Console Output:
```txt
Tensor(""ph1:0"", shape=(?,), dtype=int32)
Traceback (most recent call last):
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 126, in make_type
    v = dtypes.as_dtype(v).base_dtype
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py"", line 595, in as_dtype
    ""Cannot convert value %r to a TensorFlow DType."" % type_value)
TypeError: Cannot convert value <class 'numpy.uint32'> to a TensorFlow DType.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:/scaffold-ext/scaffold_ext/analysis/ann/bug.py"", line 4, in <module>
    print(tf.placeholder(np.uint32, [None], 'ph2'))
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1599, in placeholder
    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 3083, in _placeholder
    dtype = _execute.make_type(dtype, ""dtype"")
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 129, in make_type
    (arg_name, repr(v)))
TypeError: Expected DataType for argument 'dtype' not <class 'numpy.uint32'>.
```",0,,2,2018-01-25T14:50:18Z,NONE,2018-01-25T22:14:40Z
16405,tf.contrib.framework.sort failing with <<...has no attribute 'sort'>> in TF windows ,,"### [Problem] : Can't use tf.contrib.framework.sort in my tensorflow code as it's failing to find the 'sort' attribute

Source code:
```
import tensorflow as tf

x = tf.placeholder(tf.float32, shape=(10, 10))
y = tf.contrib.framework.sort(x)

with tf.Session() as sess:
    rand_array = np.random.rand(10, 10)
    print(sess.run(y, feed_dict={x: rand_array})) 
```

Error log:
```
 <module>
    y = tf.contrib.framework.sort(x)
AttributeError: module 'tensorflow.contrib.framework' has no attribute 'sort'
```

### System information
- I've been trying to use the tf.contrib.framework.sort  within a custom loss function but issue being reproduced with a simple call to the tf.contrib.framework.sort
- Windows 10 64 bit
- TF installed with native pip3
- TF version: 1.4.0
- Python version: 3.6.2 
- TF with CPU support only
",0,,1,2018-01-25T14:17:01Z,NONE,2018-01-25T23:57:30Z
16404,remove SRU num_units == x.shape[-1] restriction,"awaiting testing (then merge),cla: yes","Based on the [author's response](https://github.com/taolei87/sru/issues/12), the restriction is unnecessary. Simply add a linear transform to the input will solve the issue

#13094 ",1,,7,2018-01-25T13:06:58Z,CONTRIBUTOR,2018-01-25T16:55:25Z
16403,1D Convolution in Tensorflow Serving,stat:awaiting response,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: tensorflow binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.6
- **CUDA/cuDNN version**: 9.0, 7.0
- **GPU model and memory**: GTX 1050

### Describe the problem
The Problem is a little bit hard to reproduce, I guess because so many steps are involved.
So, the basic scenario is, that I am using keras to train a model in python. Here is the model I am using:

`
           input = Input(shape=(200, 8))
            x = Conv1D(filters=128, kernel_size=7, activation=""relu"", padding=""same"")(input)
            x = Conv1D(filters=128, kernel_size=7, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=128, kernel_size=3, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=128, kernel_size=3, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=128, kernel_size=3, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=2, kernel_size=1, activation=""softmax"")(x)

`

Now, I extract the graph and I am saving graph and weights with the ModelBundleBuilder:

`
session = K.get_session()

        signature = tf.saved_model.signature_def_utils.build_signature_def(
            inputs={'input': tf.saved_model.utils.build_tensor_info(self._get_model().inputs[0])},
            outputs={'output': tf.saved_model.utils.build_tensor_info(self._get_model().outputs[0])},
            method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME
        )

        b = tf.saved_model.builder.SavedModelBuilder(filename)
        legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')
        b.add_meta_graph_and_variables(session,
                                       [tf.saved_model.tag_constants.SERVING],
                                       signature_def_map={
                                           tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature},
                                       legacy_init_op=legacy_init_op)
        b.save()
`

If I am loading the model via python, everything works as expected.

Now I am deploying the model into TF serving and using protobuf / gRPC to make the prediction via Java. I am converting a 3D float array to a TensorProto like this:

`
TensorShapeProto.Dim dim1 = TensorShapeProto.Dim.newBuilder()
                .setSize(data.length).build();

        TensorShapeProto.Dim dim2 = TensorShapeProto.Dim.newBuilder()
                .setSize(data[0].length).build();

        TensorShapeProto.Dim dim3 = TensorShapeProto.Dim.newBuilder()
                .setSize(data[0][0].length).build();

        TensorShapeProto shape = TensorShapeProto.newBuilder()
                .addDim(dim1).addDim(dim2).addDim(dim3).build();

        TensorProto.Builder builder = TensorProto.newBuilder()
                .setDtype(DataType.DT_FLOAT)
                .setTensorShape(shape);

        for(int i = 0; i < data.length; i++) {
            for(int j = 0; j < data[0].length; j++) {
                for(int k = 0; k < data[0][0].length; k++) {
                    builder.addFloatVal(data[k][j][i]);
                }
            }
        }

        return builder.build();
`

And do the predicition like this:

`
public class ModelClientImpl implements ModelClient {

    private String host;
    private Integer port;
    private ManagedChannel channel;
    private PredictionServiceGrpc.PredictionServiceBlockingStub stub;

    public void init() {
        channel = ManagedChannelBuilder
                .forAddress(getHost(), getPort())
                .usePlaintext(true)
                .build();

        stub = PredictionServiceGrpc.newBlockingStub(channel);
    }

    @Override
    public Map<String, TensorProto> predict(final String signatureName, Map<String, TensorProto> inputs) {
        final Predict.PredictResponse response = stub.predict(createRequest(signatureName, inputs));

        return response.getOutputsMap();
    }

    protected Predict.PredictRequest createRequest(final String signatureName, final Map<String, TensorProto> inputs) {
        final Model.ModelSpec modelSpec = Model.ModelSpec.newBuilder()
                .setName(signatureName)
                .setSignatureName(""serving_default"").build();

        final Predict.PredictRequest.Builder builder = Predict.PredictRequest.newBuilder()
                .setModelSpec(modelSpec)
                .putAllInputs(inputs);

        return builder.build();
    }

    public String getHost() {
        return host;
    }

    public void setHost(String host) {
        this.host = host;
    }

    public Integer getPort() {
        return port;
    }

    public void setPort(Integer port) {
        this.port = port;
    }

    @Override
    public void close() throws Exception {
        channel.shutdown().awaitTermination(5, TimeUnit.DAYS);
    }
}

`

But the prediction is totally different from python. Does anybody know if this is a bug or is something wromg with 1dconv?
",0,,2,2018-01-25T12:50:42Z,NONE,2018-01-26T01:32:27Z
16402,Modified Implementation of ndlstm_base_dynamic.,"awaiting review,cla: yes",It now uses a `BasicLSTMCell` that has `state_is_tuple=True` to address the deprecation thrown by having `state_is_tuple=False`.,1,,1,2018-01-25T12:13:47Z,CONTRIBUTOR,2018-01-25T16:40:02Z
16401,Separate constant file for global variables,cla: no,Created a separate common `constants.py` which can be used globally under `ops`.,1,,3,2018-01-25T11:36:25Z,CONTRIBUTOR,2018-01-25T16:52:38Z
16400,"[doc] link to ""How to Use t-SNE Effectively"" from embeddings is broken",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)  NO (since Web page problem)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7**:
- **TensorFlow installed from (source or binary) binary**:
- **TensorFlow version (use command below) 1.5.0rc0 **:
- **Python version  3.5.1**: 
- **Bazel version (if compiling from source) NOT USED**:
- **GCC/Compiler version (if compiling from source) NOT USED**:
- **CUDA/cuDNN version NOT USED**:
- **GPU model and memory NOT USED **:
- **Exact command to reproduce DOC Problem. Just look https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/**:

### Describe the problem
- Link to to ""How to Use t-SNE Effectively"" is broken.
- The page link is follows (before junmping)
  - https://www.tensorflow.org/programmers_guide/embedding
  - 404 page is following URL 
     - https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/
- Original page should be follows. (the URL in embedding.md should rewrite to follows)
  -   https://distill.pub/2016/misread-tsne/

### Source code / logs
- The problem code is follows.
  - https://github.com/tensorflow/tensorflow/blame/v1.5.0-rc1/tensorflow/docs_src/programmers_guide/embedding.md#L123

",0,,1,2018-01-25T10:17:07Z,NONE,2018-01-25T16:06:00Z
16399,Does TensorFlow 1.5 support CUDA 9.1?,,"My notebook has an MX150 display adapter, someone said that it's available with CUDA 9.1",0,,3,2018-01-25T08:28:58Z,NONE,2018-01-25T10:52:35Z
16398,Compare_and_bitpack function for bool for big endian,"cla: yes,stat:awaiting response","Added condition for endianness check and related conversion for Big Endian.
Removed the note from file: 
`// NOTE(ebrevdo): This assumes memory is little-endian.`
Please let me know your feedback.",1,,1,2018-01-25T08:22:19Z,CONTRIBUTOR,2018-01-25T16:51:11Z
16394,cmake gpu build improvement,"awaiting review,cla: no","cmake build pass with gpu enabled

python binding option can change to off now",1,,4,2018-01-25T06:23:27Z,CONTRIBUTOR,2018-01-26T16:49:11Z
16389,cherrypick bfloat16 changes,cla: yes,,1,,2,2018-01-25T04:12:26Z,NONE,2018-01-25T18:13:12Z
16386,Using keras layers within an Estimator either causes training where it shouldn't or corrupts weights,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 3.16.36
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9', '1.4.1')
- **Python version**: 2.7.9
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See gist

### Describe the problem

Hey there,

So I have a `tf.keras` model that for use on Amazon SageMaker I'm trying to convert into an Estimator. I know there's `tf.keras.estimator.model_to_estimator` but, I'm having separate [issues with that](https://github.com/tensorflow/tensorflow/issues/16385).

In the [easily run reproduction here](https://gist.github.com/zmjjmz/667d0d7e6b6c97c49b3aaf4d67b03d2c) I have (as a demonstration) a `tf.keras.layers.Embedding` which is initialized with all zeros and has `trainable=False`. Followed by that is a `Dense` layer with `use_bias=False` because I couldn't figure out how to get predictions out of an Estimator without training it first (and I can't train nothing apparently). Since all of the embeddings are zero however and can't be trained, the `Dense` layer should always produce a zero, even after training it. Instead, it produces garbage!

In fact, I've taken a few steps to ensure that no training takes place, although ideally I'd be able to just run the estimator without training:
1) I've set the loss to be 0 initially (l2_norm of what should start out as 0)
2) Optimize with SGD using a learning rate of 0
3) One training example that should have zero loss...

The output I actually get is very much non-zero. If I inspect the `embed/embeddings:0` tensor in `tfbdg`, I see this:
```
array([[ 0.14387012,  0.83495581,  0.44025695,  0.25154734,  0.7214781 ,  0.40229702,  0.82108581,  0.12210274,  0.43861651,  0.39615464],                
       [ 0.81636655,  0.48157215,  0.48987687,  0.48775947,  0.62187696,  0.25421095,  0.64555049,  0.97305572,  0.53352964,  0.34286666],                
       [ 0.82881641,  0.80365777,  0.4596678 ,  0.21614265,  0.22256434,  0.07986271,  0.92880177,  0.64946997,  0.89239001,  0.13793337],                
       [ 0.98491704,  0.15281868,  0.77106941,  0.30048406,  0.86042607,  0.88010466,  0.64362776,  0.70185173,  0.49912012,  0.61521161]], dtype=float32)
```

Even though it shouldn't have budged from all zeroes! 

So, something about how I'm doing this is fundamentally broken. I suspect that the issue is in line `61` where I start a new Session -- however this appears to be necessary, since I need to ensure that the `keras` backend is using the same `Graph` as `tensorflow.get_default_graph()` due to the peculiarities of how the Estimator calls the `model_fn`.

Notably those values hold between:
1) Runs of the estimator
2) Successive runs with the same `tf.set_random_seed` value

The latter makes me think that somehow the `Embedding` layer is receiving *a* gradient despite my best efforts, although it's hard to test this versus some sort of memory corruption. 

If you need me to provide any more information let me know -- I'm sure I've left something out.

",0,,5,2018-01-25T03:06:57Z,NONE,2018-01-25T09:48:41Z
16384,fix typos,cla: yes,fix typos,1,,1,2018-01-25T01:28:59Z,CONTRIBUTOR,2018-01-25T18:52:17Z
16382,Disable bfloat16 for sparse_matmul for 1.5.0,cla: yes,"I'm using this PR to test out simple workarounds for the sparse_matmul problem.
It doesn't need a reviewer yet.

Note: it looks like we're going to try to release 1.5.0 with the fix anyway. I will keep this PR available until that's finished.",1,,3,2018-01-25T00:00:18Z,MEMBER,2018-01-25T01:29:34Z
16379,No module named 'tensorflow'. Anaconda+windows10+tensorflow-gpu+cuda8+cudnn6,type:build/install,"I had anaconda on my windows 10.
I installed CUDA 8.0 with cuDNN 6 and then followed [http://blog.nitishmutha.com/tensorflow/2017/01/22/TensorFlow-with-gpu-for-windows.html](url) to activate tensorflow-gpu environment. Now when I import tensorflow in the console, it works but with jupyter notebook opened right in this environment it throws the error. I even upgraded setuptools as mentioned in a previous issue.
![capture5](https://user-images.githubusercontent.com/10391022/35360420-a5cd8f98-0183-11e8-919a-53da56df5ee8.JPG)
![capture6](https://user-images.githubusercontent.com/10391022/35360450-c27e44e8-0183-11e8-9c0a-5aaaa05000c4.JPG)
![capture7](https://user-images.githubusercontent.com/10391022/35360455-c7e24a42-0183-11e8-964b-7186fb233078.JPG)
![capture8](https://user-images.githubusercontent.com/10391022/35360549-174292c2-0184-11e8-9d6a-93ba6139a275.JPG)


",0,,3,2018-01-24T22:27:17Z,NONE,2018-01-24T23:43:04Z
16377,R1.4,cla: no,,0,,2,2018-01-24T21:03:18Z,NONE,2018-01-24T22:17:26Z
16375,Branch 183115307,cla: yes,,0,,4,2018-01-24T20:39:34Z,CONTRIBUTOR,2018-01-24T21:21:17Z
16371,Tegra Nvidia Jetson TX2 build python 2.7 new CUDA and CUDNN,"stat:awaiting response,type:build/install","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux4Tegra 28.2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5-rc1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.9
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: Denver2 8GB
- **Exact command to reproduce**: import tensorflow

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Clean installation with the new CUDA 9 and cudnn 7 from nvidia
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
Collecting system information...
Traceback (most recent call last):
  File ""/tmp/check_tf.py"", line 1, in <module>
    import tensorflow as tf;
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws4Time9LocalTimeEP2tml


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Wrote environment to tf_env.txt. You can review the contents of that file.
and use it to populate the fields in the github issue template.

cat tf_env.txt

```
",0,,6,2018-01-24T17:58:38Z,NONE,2018-01-26T02:09:31Z
16369,Unittesting Models with Tensorflow - How to clear the existing graph ?,stat:awaiting response,"Hello dear tensorflowers,

I have already asked the question of [StackOverflow](https://stackoverflow.com/questions/48421308/tensorflow-and-unittests-layer-already-defined), however it seams like nobody can answer my question.

So I hope you will forgive me about reposting it here:

I am developing unittests for a product I implemented with TF.

Each part of the model is tested separately then all together in different conditions.

Let's take the example of a simple GAN, I have the following tests:

 - **GeneratorTest** Class: With all tests concerning G inside
 - **DiscriminatorTest** Class: With all tests concerning D inside
 - **GAN_Train_Test** Class: G and D connected all together: 1 training step is tested.
 - **GAN_Inference_Test** Class: G and D connecteed all together: 1 inference run is tested.

------------

When the files are executed independently, everything is working nicely and fine. Tests are all fine.

Problems start occuring when I try to create one file to launch them all from one master file.

**master_test_launcher.py:**

```python
import unittest
import time
    
import tensorflow as tf
    
from tests.test_generator import GeneratorTest
from tests.test_discriminator import DiscriminatorTest
from tests.test_anovae_model import GAN_Train_Test
from tests.test_inference import GAN_Inference_Test
    
runner = unittest.TextTestRunner(verbosity=2)
    
if __name__ == '__main__':
   tf.logging.set_verbosity(tf.logging.DEBUG)
    
    for test in [GeneratorTest, DiscriminatorTest, GAN_Train_Test, GAN_Inference_Test]:
        tf.logging.debug(""Running tests for: %s ..."" % test.__str__())
    
        tf.reset_default_graph()
    
        time.sleep(2)
    
        test_suite = unittest.TestSuite()
        test_suite.addTest(unittest.makeSuite(test))
    
        runner.run(test_suite)
```

I repeatedly obtain the same error when I run the tests related to G and D connected together: 

```python
Exception: Layer 'encoder/input' already exists, please choice other 'name' or reuse this layer
Hint : Use different name for different 'Layer' (The name is used to control parameter sharing)
```

The error is quite simple to understand, each test file is independant and thus create its own session and graph. While testing only G or D, there is no problem because they have different name_scope/variable_scope. However, when testing the whole model *Layers* already have been defined by previous tests and thus leading to issue.

I would like to find a way to completely drop the graph and reset the whole TF state as **brand new and clean**. However, everything I try seem to  fail.

I would like to avoid creating a new graph for each test, leaving the old one in memory (could lead to very high amount of memory waste after a few tests).


So my question is easy: ** How can I reset the whole TF state and internal vars as ""clean"" as if you relaunch a new python shell ? By some black-magic I can't find any way doing it (after looking for it for hours).

For information here are the things I tried and which failed:
- tf.reset_default_graph()
- cleaning everything in graph collections
- creating a new graph + new session before executing each Test File: A graph is still built somewhere containing my Layers and I can't manage to find it.
- reading the TF code and trying to find any __exit__ or close function which I didn't find

Thanks a lot,

Jonathan D.",0,,1,2018-01-24T15:28:36Z,CONTRIBUTOR,2018-01-25T07:03:13Z
16366,I have an issue with http://projector.tensorflow.org/ always getting stuck,,"Can anyone help me with this issue? I start up the _**Visualizing High-Dimensional Space**_ webstie and it loads until it says something about metadata and never does anything from there. Plese, help. I'm so confused?",0,,1,2018-01-24T14:35:43Z,NONE,2018-01-24T17:41:16Z
16362,`tf.foldl` should have more robust input handling (like `tf.scan`),"stat:contributions welcome,type:feature","### System information
- Windows 10 x64
- Installed from binary
- TensorFlow 1.4.0 (Cpu version)
- Python 3.6.1

### Bug Description
`tf.foldl` (and `tf.foldr`) are conceptually very very similar to `tf.scan`. Therefore the implementations are also very similar. However, `tf.scan` accepts initializer lists or tuples with varying type arguments, while `tf.foldl` does not. I think this is a simple oversight, and it seems that cutting and pasting some code from `tf.scan` to `tf.foldl` fixes this problem. Specifically. the master `tf.foldl `code is (after removing the docstring):

```
def foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,
          swap_memory=False, name=None):
  if not callable(fn):
    raise TypeError(""fn must be callable."")

  with ops.name_scope(name, ""foldl"", [elems]):
    # Any get_variable calls in fn will cache the first call locally
    # and not issue repeated network I/O requests for each iteration.
    varscope = vs.get_variable_scope()
    varscope_caching_device_was_none = False
    if varscope.caching_device is None:
      # TODO(ebrevdo): Change to using colocate_with here and in other methods.
      varscope.set_caching_device(lambda op: op.device)
      varscope_caching_device_was_none = True

    # Convert elems to tensor array.
    elems = ops.convert_to_tensor(elems, name=""elems"")
    n = array_ops.shape(elems)[0]
    elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,
                                            dynamic_size=False,
                                            infer_shape=True)
    elems_ta = elems_ta.unstack(elems)

    if initializer is None:
      a = elems_ta.read(0)
      i = constant_op.constant(1)
    else:
      a = ops.convert_to_tensor(initializer)
      i = constant_op.constant(0)

    def compute(i, a):
      a = fn(a, elems_ta.read(i))
      return [i + 1, a]
    _, r_a = control_flow_ops.while_loop(
        lambda i, a: i < n, compute, [i, a],
        parallel_iterations=parallel_iterations,
        back_prop=back_prop,
        swap_memory=swap_memory)

    if varscope_caching_device_was_none:
      varscope.set_caching_device(None)
    return r_a

```

Modifying the code in the following manner seems to allow non-homgoenous initializer lists (tuples do not work for some reason). Note that you can toggle the mofidication with the ""useModifications"" flag:

```
def foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,
          swap_memory=False, name=None):
    if not callable(fn):
        raise TypeError(""fn must be callable."")

    with ops.name_scope(name, ""foldl"", [elems]):
        # Any get_variable calls in fn will cache the first call locally
        # and not issue repeated network I/O requests for each iteration.
        varscope = vs.get_variable_scope()
        varscope_caching_device_was_none = False
        if varscope.caching_device is None:
            # TODO(ebrevdo): Change to using colocate_with here and in other methods.
            varscope.set_caching_device(lambda op: op.device)
            varscope_caching_device_was_none = True

        # Convert elems to tensor array.
        elems = ops.convert_to_tensor(elems, name=""elems"")
        n = array_ops.shape(elems)[0]
        elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,
                                                dynamic_size=False,
                                                infer_shape=True)
        elems_ta = elems_ta.unstack(elems)

        if initializer is None:
            a = elems_ta.read(0)
            i = constant_op.constant(1)
        else:
            useModifications = True
            if useModifications:
                output_is_sequence = nest.is_sequence(initializer)
                output_flatten = lambda x: nest.flatten(x) if output_is_sequence else [x]
                initializer_flat = output_flatten(initializer)
                a = [ops.convert_to_tensor(init) for init in initializer_flat]
            else:
                a = ops.convert_to_tensor(initializer)

            i = constant_op.constant(0)

        def compute(i, a):
            a = fn(a, elems_ta.read(i))
            return [i + 1, a]

        _, r_a = control_flow_ops.while_loop(
            lambda i, a: i < n, compute, (i, a),
            parallel_iterations=parallel_iterations,
            back_prop=back_prop,
            swap_memory=swap_memory)

        if varscope_caching_device_was_none:
            varscope.set_caching_device(None)
        return r_a
```

Here is a MWE:

```
import tensorflow as tf

a = tf.constant( 1, dtype = tf.float32 )
b = tf.constant( 2, dtype = tf.int64   )

useTuple = False

def body( ab, i ):
    a = ab[0]
    b = ab[1]
    if useTuple:
        return (a,b)
    else:
        return [a,b]

N = 3
with tf.Session() as sess:
    if useTuple:
        ab = (a,b)
    else:
        ab = [a,b]
    print( ""new foldl :"", sess.run(   foldl(  body, tf.range(N), ab ) ) )  
    print( ""tf.scan   :"", sess.run( tf.scan(  body, tf.range(N), ab ) ) )
    print( ""tf.foldl  :"", sess.run( tf.foldl( body, tf.range(N), ab ) ) )
```

with useTuple = False, this returns 

```
new foldl : [1.0, 2]
tf.scan   : [array([1., 1., 1.], dtype=float32), array([2, 2, 2], dtype=int64)]
# Crash for tf.foldl with error: 
TypeError: Cannot convert a list containing a tensor of dtype <dtype: 'int64'> to <dtype: 'float32'> (Tensor is: <tf.Tensor 'Const_5:0' shape=() dtype=int64>)

```",0,,1,2018-01-24T12:58:43Z,NONE,2018-01-25T00:37:32Z
16361,Fix typo,"awaiting testing (then merge),cla: yes",fix typo,1,,1,2018-01-24T12:25:33Z,CONTRIBUTOR,2018-01-24T17:26:08Z
16360,"Python: Make an alias for ""tf.variable"" (with a lower ""v"") so the naming of it is consistent with ""tf.placeholder""/""tf.constant""",type:support,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**:  N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
Many developers learn the naming standards of the software so they can write code faster. It does not make any sense to have to things ""tf.placeholder"" and ""tf.Variable"" named using different schema. Constant, Placeholder and Variable are similar entities and can be used interchangeably. They should be named in same style even if tf.Variable is a class. 

https://www.tensorflow.org/api_docs/python/tf/placeholder
https://www.tensorflow.org/api_docs/python/tf/Variable
https://www.tensorflow.org/api_docs/python/tf/constant

### Source code / logs
N/A
",0,,1,2018-01-24T12:20:59Z,NONE,2018-01-25T01:01:53Z
16359,Return type annotation,"awaiting testing (then merge),cla: yes","Added type annotations in the docstring to the return types of dataset functions.
Presented like this, they can automatically be read by tools (I have tested that this works in PyCharm) to improve auto-completion when coding.

This is really useful in case of datasets, because they often result in long chained calls (something like `Dataset.generate...(...).map(...).repeat(...).batch(...)`). With this patch, code completion works after every `.` (again, I have only tested PyCharm).",1,,1,2018-01-24T11:42:09Z,CONTRIBUTOR,2018-01-24T17:25:35Z
16358,Request for updating keras/datasets files to r1.5,type:bug/performance,"### System information
- **executes Keras sample code imdb_fasttext.py https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py**:
- **Windows 7**:
- **TensorFlow installed from binary**:
- **TensorFlow version 1.5.0rc0**:
- **Python version 3.5.1**: 

### Describe the problem
Keras sample program does not work.
 There is a bug for numpy arange method wrong usage.
   (Need to fix from arrange to arange) 
This issue is already solved on master branch. (not in 1.5.0rc1)
Would you update these source codes?

### Source code / logs
Error messages are follows
===
C:\Users\sakaia\work\tensorflow\keras>python imdb_fasttext.py
Loading data...
Traceback (most recent call last):
  File ""imdb_fasttext.py"", line 75, in <module>
    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features
)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\keras\_imp
l\keras\datasets\imdb.py"", line 77, in load_data
    indices = np.arrange(len(x_train))
AttributeError: module 'numpy' has no attribute 'arrange'
===

Following are just checking np.arrange (not np.arange)
>git branch r1.5
>grep -rn np.arrange *
tensorflow/python/keras/_impl/keras/datasets/boston_housing.py:51:  indices = np.arrange(len(x))
tensorflow/python/keras/_impl/keras/datasets/reuters.py:76:  indices = np.arrange(len(xs))
tensorflow/python/keras/_impl/keras/datasets/imdb.py:77:  indices = np.arrange(len(x_train))
tensorflow/python/keras/_impl/keras/datasets/imdb.py:82:  indices = np.arrange(len(x_test))
>git branch -
>grep -rn np.arrange *
(This line is intentionally blank)",0,,5,2018-01-24T10:39:52Z,NONE,2018-01-24T19:56:43Z
16357,Increase tolerance in `losses_impl_test.py`. fixes #16238,cla: no,,1,,2,2018-01-24T07:53:22Z,CONTRIBUTOR,2018-01-24T17:26:57Z
16355,minor spelling tweaks for eager execution docs,"awaiting testing (then merge),cla: yes",,1,,1,2018-01-24T06:35:52Z,CONTRIBUTOR,2018-01-24T17:24:04Z
16351,TfLiteCameraDemo failed to work with NNAPI after commit e6ff665dbe4888aa5fdff8f34c44405acca2ddd1,comp:lite,"I am testing NNAPI by forcing TfLiteCameraDemo to invoking libneuralnetworks.so. It worked correctly though slower. But since commit e6ff665dbe4888aa5fdff8f34c44405acca2ddd1, TfLiteCameraDemo crashes with error message like,

	01-24 03:39:36.393 19136 19153 E AndroidRuntime: FATAL EXCEPTION: CameraBackground
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: Process: com.example.android.tflitecamerademo, PID: 19136
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: java.lang.IllegalArgumentException: Failed to run on the given Interpreter: NNAPI was requested, but dependent sized tensors being used.
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:95)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:123)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.run(Interpreter.java:104)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:114)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:663)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment.access$900(Camera2BasicFragment.java:69)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment$5.run(Camera2BasicFragment.java:558)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Handler.handleCallback(Handler.java:790)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Handler.dispatchMessage(Handler.java:99)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Looper.loop(Looper.java:164)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.HandlerThread.run(HandlerThread.java:65)
	01-24 03:39:36.396   626   871 W ActivityManager:   Force finishing activity com.example.android.tflitecamerademo/.CameraActivity

Here is my patch

	 index e44c5ae..1ed88eb 100644
	 ---a/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java
	 +++b/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java
	 @@ -91,7 +91,7 @@ public class ImageClassifier {
		
	   /** Initializes an {@code ImageClassifier}. */
	   ImageClassifier(Activity activity) throws IOException {
	-    tflite = new Interpreter(loadModelFile(activity));
	+    tflite = new Interpreter(loadModelFile(activity), true);
	     labelList = loadLabelList(activity);
	     imgData =
	         ByteBuffer.allocateDirect(
	diff --git a/tensorflow/contrib/lite/java/demo/build.gradle b/tensorflow/contrib/lite/java/demo/build.gradle
	index dd883d6..9361c71 100644
	--- a/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java
	+++ b/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java
	@@ -66,6 +66,13 @@ public final class Interpreter implements AutoCloseable {
	     }
	     wrapper = new NativeInterpreterWrapper(modelFile.getAbsolutePath());
	   }
	+  public Interpreter(@NotNull File modelFile, boolean nn) {
	+    if (modelFile == null) {
	+      return;
	+    }
	+    wrapper = new NativeInterpreterWrapper(modelFile.getAbsolutePath());
	+    wrapper.setUseNNAPI(nn);
	+  }
	
	   /**
	    * Initializes a {@code Interpreter} with a {@code MappedByteBuffer} to the model file.
	@@ -76,6 +83,10 @@ public final class Interpreter implements AutoCloseable {
	   public Interpreter(@NotNull MappedByteBuffer mappedByteBuffer) {
	     wrapper = new NativeInterpreterWrapper(mappedByteBuffer);
	   }
	+  public Interpreter(@NotNull MappedByteBuffer mappedByteBuffer,  boolean nn) {
	+    wrapper = new NativeInterpreterWrapper(mappedByteBuffer);
	+    wrapper.setUseNNAPI(nn);
	+  }
	 
	   /**
	    * Runs model inference if the model takes only one input, and provides only one output.
	   /**
	    * Runs model inference if the model takes only one input, and provides only one output.
",0,,4,2018-01-24T03:55:51Z,NONE,2018-01-24T12:14:24Z
16347,Golang API to serialize data into tf.Example protos(tfrecords),stat:awaiting response,"The Golang api [WriteContentsTo](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go#Tensor.WriteContentsTo) can be used to writes the serialized contents of a tensor to io.Writer, where the tensor is built from golang scalars, slices, and arrays. 

Yet there's not a Golang API to serialize data into tf.Example protos(tfrecords).

For example, when i want serialize a libsvm into tf.Example protos, i can do this by:
```
def libsvm2tfrecords(data_source, target_dir, delimiter='\t'):
    """"""
    a single file should not contain lines more than 1000,000, or we should use libsvm2proto_par
    :param data_source: libsvm file path
    :param target_dir: dir to storage the serialize proto file
    :param delimiter: delimiter for csv reader
    :return:
    """"""
    if not os.path.isfile(data_source):
        raise ValueError('data file passed do not exist or not a file')

    file_name = os.path.join(target_dir, os.path.splitext(
        os.path.basename(data_source))[0] + '.tfrecords')
    writer = tf.python_io.TFRecordWriter(file_name)
    start = datetime.now()
    line_c = 0
    with open(data_source, 'rb') as rf:
        f_reader = csv.reader(rf, delimiter=delimiter, quotechar='|')
        for row in f_reader:
            line_c += 1
            feature = dict()
            indexes = []
            values = []
            feature.update({'label': _float_feature([float(row[0])])})
            for e in row[1:]:
                index, value = e.split(':')
                indexes.append(int(index))
                values.append(float(value))
                feature.update({'index': _int64_feature(indexes)})
                feature.update({'value': _float_feature(values)})

            example = tf.train.Example(features=tf.train.Features(feature=feature))
            writer.write(example.SerializeToString())

        writer.close()
        end = datetime.now()

        print(""- consumed time: %ds for %s"" % ((end-start).seconds, data_source))
```
BUT Golang api does not seem to be able to achieve this. ",0,,2,2018-01-24T02:53:13Z,NONE,2018-01-24T19:57:06Z
16341,"[Bazel/Windows] Don't use -Wl, -lpthread and -lm on Windows","awaiting testing (then merge),cla: yes",,1,,1,2018-01-24T00:48:57Z,CONTRIBUTOR,2018-01-24T17:29:07Z
16339,Remove path_to_str from the public API,"awaiting testing (then merge),cla: yes",@martinwicke fyi,0,,3,2018-01-23T23:48:50Z,OWNER,2018-01-24T17:20:42Z
16337,Fix a bug that capture_tpu_profile only takes absolute logdir path.,"awaiting testing (then merge),cla: yes",Also removed package dependancy on tensorflow for better compatibility.,1,,1,2018-01-23T21:23:02Z,CONTRIBUTOR,2018-01-23T22:32:09Z
16336,Apply final cherry-picks for 1.5.0 release.,"awaiting review,cla: yes",,1,,5,2018-01-23T21:10:12Z,MEMBER,2018-01-23T22:59:59Z
16332,Fixes #16314,"awaiting testing (then merge),cla: yes",Fixes #16314.,1,,3,2018-01-23T18:47:02Z,CONTRIBUTOR,2018-01-23T19:26:44Z
16331,Compilation failure with gcc-6.4 (gcc-7.2 and clang-4) in ubuntu 17.10,stat:awaiting response,"When compiling master in ubuntu 17.10, compilation fails due to 'too perfect forwarding' in variant_op_registry
`
std::unordered_map<std::tuple<VariantXOp, StringPiece, StringPiece>, 
                     VariantXOpFn, TupleHash>
`
A workaround, by replacing tuple with a struct, provided in PR #16309 for your review.

Thanks,
Sami
",0,,2,2018-01-23T17:17:44Z,CONTRIBUTOR,2018-01-24T13:08:45Z
16328,Gradient computation across multi-GPU,,"
------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7.6
- **CUDA/cuDNN version**: 8.0/6.0

I am trying to compute global mean and global variance for batch normalization layer across GPUs, both forward and backward should be considered. With `\sigma^2 = mean(x^2) - mean(x)^2`, the gradient w.r.t. each `x` can be computed independently in the GPU that `x` is attached to. 

However, when computing the gradients, I met a problem: without specifying GPU device, `tf.gradient` will use the `\gpu:0`. I cannot specify each operation of gradient computation because the gradients are computed automatically by the `optimizer` and only gradients of parameters are computed.

My question is that if a node is explicitly attached to a GPU device, why the gradient can not be attached to the same GPU device?

I tried this code and get two timeline files [timelines.zip](https://github.com/tensorflow/tensorflow/files/1649923/timelines.zip) and two snapshots bellow.

    import tensorflow as tf
    import numpy as np
    from tensorflow.python.client import timeline
    
    N_SAMPLES = 100000000
    
    
    def all_reduce(gpu_num):
        means = []
        x2s = []
        axs = []
        for i in range(gpu_num):
            with tf.device('/cpu:0'):
                x = tf.placeholder(dtype=tf.float32, shape=[N_SAMPLES], name='local_input_%d' % i)
            with tf.device('/gpu:%d'%i):
                ax = tf.multiply(10.0, x, name='local_multiply_%d'%i)
                mean = tf.reduce_mean(ax, name='local_mean_%d'%i)
                x2 = tf.square(ax, name='local_square_%d'%i)
                axs.append(ax)
                means.append(mean)
                x2s.append(x2)
    
        with tf.device('/gpu:0'):
            global_mean = tf.reduce_mean(means, name='global_mean')
            global_var = tf.subtract(tf.reduce_mean(x2s, name='global_x2'),
                                     tf.square(global_mean, name='global_mean_square'),
                                     name='global_sub')
            print global_var.get_shape()
    
        gs = []
        # manually
        # for i in range(gpu_num):
        #     with tf.device('/gpu:%d'%i):
        #         gradient_wrt_mean = tf.gradients(global_mean, axs[i])
        #         gradient_wrt_var = tf.gradients(global_var, axs[i])
        #         gs.append(gradient_wrt_mean)
        #         gs.append(gradient_wrt_var)
    
        # auto by tf
        gradient_wrt_mean = tf.gradients(global_mean, axs)
        gradient_wrt_var = tf.gradients(global_var, axs)
        gs.append(gradient_wrt_var)
        gs.append(gradient_wrt_mean)
    
        for n in tf.get_default_graph().as_graph_def().node:
            print [n.name, n.device]
    
        return global_mean, global_var, axs, gs
    
    
    def main(_):
        gpu_num = 2
        mean_op, var_op, xs, gs = all_reduce(gpu_num)
        x = np.random.randn(N_SAMPLES*gpu_num)
        print np.mean(x), np.var(x)
        feed_dict = dict()
        for i in range(gpu_num):
            feed_dict[xs[i]] = x[i*N_SAMPLES:(i+1)*N_SAMPLES]
    
        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
        run_metadata = tf.RunMetadata()
        gpu_options = tf.GPUOptions(allow_growth=False)
        config = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options)
        sess = tf.Session(config=config)
    
        # mean, var, g = sess.run([
        #     mean_op, var_op, gs
        # ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)
        # print mean, var
    
        g = sess.run([
            gs
        ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)
    
        # Create the Timeline object, and write it to a json
        tl = timeline.Timeline(run_metadata.step_stats)
        ctf = tl.generate_chrome_trace_format()
        with open('timeline.json', 'w') as f:
            f.write(ctf)
    
    
    if __name__ == '__main__':
        tf.app.run()

Two figures:
auto, without specifying GPU device.
![image](https://user-images.githubusercontent.com/13829174/35196526-76c1fa20-fed3-11e7-995f-6c63813acc83.png)

manually specifying GPU device.
![image](https://user-images.githubusercontent.com/13829174/35196537-93757eee-fed3-11e7-8df5-986a90a8c0f8.png)

If using `tf.gradient` without specifying GPU devices, only a `tf.reduce_mean` operation is done in `/gpu:1`. So is there some easy way that the operations of gradient computation can be assigned automatically to the corresponded GPU device?


",0,,2,2018-01-23T15:30:55Z,NONE,2018-01-23T19:17:33Z
16327,iOS app does not output predictions using resnet50,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.1
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**: 0.9.0-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A 


### Steps I followed:

- I have trained resnet_v2_50 using slim.

- I created a script just to run inference, so the input image is a placeholder with name ""input_1"" and the output is the softmax with name ""softmax"". 

- I exported the .pb graph, then I ran `python python/tools/freeze_graph.py --input_graph=resnet_v2_50.pb --input_checkpoint=model.ckpt-1 --output_graph=frozen_resnet_v2_50.pb --input_binary=True --output_node_names=""softmax""` to freeze my graph using my checkpoint.

- I ran `bazel-bin/tensorflow/tools/graph_transforms/transform_graph  --inputs=input_1 --in_graph=frozen_resnet_v2_50.pb --outputs=softmax --out_graph=quantized_resnet_v2_50.pb --transforms='add_default_attributes strip_unused_nodes(type=float, shape=""1,180,180,3"") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) quantize_weights strip_unused_nodes sort_by_execution_order'`
 to quantize it. Here it's worth mentioning that when I add fold_batch_norms and fold_old_batch_norms at transforms I am getting this error: `E tensorflow/tools/graph_transforms/transform_graph.cc:210] Beta input to batch norm has bad shape: [64]`

- I imported the graph and also my labels file into `tensorflow/examples/ios/camera/data` and I ran the model on my iphone 5s (ios 10.3) using xcode 9.2. 

### Problem:
The app is running on my iphone but I am not getting any labels and probabilities while the `tensorflow/examples/label_image/label_image.py` script gives me results. If I use my model inside `tensorflow/examples/ios/simple` with iphone 8 simulator, the model is loaded but the predictions are empty. The result is the same if I use the frozen version before quantization. The result is also the same if I don't use slim.batch_norm as normalizer_fn in slim.conv2d and add fold_batch_norms and fold_old_batch_norms at transforms. Is there any bug? The message running on the simple example is the following (no predictions):
`I/Users/christos/tensorflow/tensorflow/examples/ios/my_simple/RunModelViewController.mm:246] Predictions: ` 

EDIT: The above process is working for a dummy small network that I made with the following nodes and ops:
input_1=>Placeholder
v/tower_0/trivial/Flatten/flatten/Reshape/shape/_0__cf__0=>Const
v/tower_0/trivial/Flatten/flatten/Reshape=>Reshape
v/trivial/fc3/weights_quantized_max=>Const
v/trivial/fc3/weights_quantized_min=>Const
v/trivial/fc3/weights_quantized_const=>Const
v/trivial/fc3/weights=>Dequantize
v/tower_0/trivial/fc3/MatMul=>MatMul
v/trivial/fc3/biases_quantized_max=>Const
v/trivial/fc3/biases_quantized_min=>Const
v/trivial/fc3/biases_quantized_const=>Const
v/trivial/fc3/biases=>Dequantize
v/tower_0/trivial/fc3/BiasAdd=>BiasAdd
v/tower_0/trivial/fc3/Relu=>Relu
v/trivial/fc4/weights_quantized_max=>Const
v/trivial/fc4/weights_quantized_min=>Const
v/trivial/fc4/weights_quantized_const=>Const
v/trivial/fc4/weights=>Dequantize
v/tower_0/trivial/fc4/MatMul=>MatMul
v/trivial/fc4/biases_quantized_max=>Const
v/trivial/fc4/biases_quantized_min=>Const
v/trivial/fc4/biases_quantized_const=>Const
v/trivial/fc4/biases=>Dequantize
v/tower_0/trivial/fc4/BiasAdd=>BiasAdd
softmax=>Softmax


You can see the node names and the operations of resnet 50 below (here the output node is v/tower_0/resnet_v2_50/predictions/Reshape_1):

input_1=>Placeholder
v/tower_0/Reshape/shape=>Const
v/tower_0/Reshape=>Reshape
v/tower_0/split/split_dim=>Const
v/tower_0/split=>Split
v/tower_0/sub/y=>Const
v/tower_0/sub=>Sub
v/tower_0/sub_1/y=>Const
v/tower_0/sub_1=>Sub
v/tower_0/sub_2/y=>Const
v/tower_0/sub_2=>Sub
v/tower_0/concat/axis=>Const
v/tower_0/concat=>ConcatV2
v/tower_0/Reshape_1/shape=>Const
v/tower_0/Reshape_1=>Reshape
v/tower_0/resnet_v2_50/Pad/paddings=>Const
v/tower_0/resnet_v2_50/Pad=>Pad
v/resnet_v2_50/conv1/weights_quantized_max=>Const
v/resnet_v2_50/conv1/weights_quantized_min=>Const
v/resnet_v2_50/conv1/weights_quantized_const=>Const
v/resnet_v2_50/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/conv1/Conv2D=>Conv2D
v/resnet_v2_50/conv1/biases=>Const
v/tower_0/resnet_v2_50/conv1/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/pool1/MaxPool=>MaxPool
v/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/biases=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/BiasAdd=>BiasAdd
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/add=>Add
v/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/add=>Add
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/shortcut/MaxPool=>MaxPool
v/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/Relu=>Relu
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/Pad/paddings=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/Pad=>Pad
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/add=>Add
v/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/biases=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/BiasAdd=>BiasAdd
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/add=>Add
v/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/add=>Add
v/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/add=>Add
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/shortcut/MaxPool=>MaxPool
v/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/Relu=>Relu
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/Pad/paddings=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/Pad=>Pad
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/add=>Add
v/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/BiasAdd=>BiasAdd
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/add=>Add
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/add=>Add
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/add=>Add
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/add=>Add
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/add=>Add
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/shortcut/MaxPool=>MaxPool
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/Relu=>Relu
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/Pad/paddings=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/Pad=>Pad
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/add=>Add
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/BiasAdd=>BiasAdd
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/add=>Add
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/add=>Add
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/add=>Add
v/resnet_v2_50/postnorm/gamma_quantized_max=>Const
v/resnet_v2_50/postnorm/gamma_quantized_min=>Const
v/resnet_v2_50/postnorm/gamma_quantized_const=>Const
v/resnet_v2_50/postnorm/gamma=>Dequantize
v/resnet_v2_50/postnorm/beta_quantized_max=>Const
v/resnet_v2_50/postnorm/beta_quantized_min=>Const
v/resnet_v2_50/postnorm/beta_quantized_const=>Const
v/resnet_v2_50/postnorm/beta=>Dequantize
v/tower_0/resnet_v2_50/postnorm/Const=>Const
v/tower_0/resnet_v2_50/postnorm/Const_1=>Const
v/tower_0/resnet_v2_50/postnorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/postnorm/Relu=>Relu
v/tower_0/resnet_v2_50/pool5/reduction_indices=>Const
v/tower_0/resnet_v2_50/pool5=>Mean
v/resnet_v2_50/logits/weights_quantized_max=>Const
v/resnet_v2_50/logits/weights_quantized_min=>Const
v/resnet_v2_50/logits/weights_quantized_const=>Const
v/resnet_v2_50/logits/weights=>Dequantize
v/tower_0/resnet_v2_50/logits/Conv2D=>Conv2D
v/resnet_v2_50/logits/biases_quantized_max=>Const
v/resnet_v2_50/logits/biases_quantized_min=>Const
v/resnet_v2_50/logits/biases_quantized_const=>Const
v/resnet_v2_50/logits/biases=>Dequantize
v/tower_0/resnet_v2_50/logits/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/SpatialSqueeze=>Squeeze
v/tower_0/resnet_v2_50/predictions/Reshape/shape=>Const
v/tower_0/resnet_v2_50/predictions/Reshape=>Reshape
v/tower_0/resnet_v2_50/predictions/Softmax=>Softmax
v/tower_0/resnet_v2_50/predictions/Shape=>Const
v/tower_0/resnet_v2_50/predictions/Reshape_1=>Reshape",0,,2,2018-01-23T15:26:45Z,NONE,2018-01-25T07:13:12Z
16326,Making string values in constant,"awaiting testing (then merge),cla: yes",We already have a `constant.py` in session_bundle since adding these string values as a constant.,1,,3,2018-01-23T15:10:05Z,CONTRIBUTOR,2018-01-24T17:45:49Z
16324,Using math_ops instead of defining separate mulop function,"awaiting review,cla: yes",,1,,1,2018-01-23T14:31:41Z,CONTRIBUTOR,2018-01-23T18:10:31Z
16321,tpu contrib fix,"awaiting testing (then merge),cla: yes",fix the issue in https://github.com/tensorflow/tensorflow/issues/16262,1,,3,2018-01-23T10:33:16Z,CONTRIBUTOR,2018-01-23T10:37:03Z
16318,import tensorflow as tf,cla: yes,These five files do not explicitly `import tensorflow as tf` yet they use they use __tf.__ methods or functions which drives linters like pylint and flake8 crazy unless special directives are put in place.,1,,4,2018-01-23T09:51:42Z,CONTRIBUTOR,2018-01-25T17:11:48Z
16316,Lack of clarity in tf.while_loop documentation,type:docs,"I believe that the documentation for tf.while_loop is lacking usage clarity, and actually provides contradictory statements. 

Specifically, it seems that many people are using the tf.while_loop as a ""for loop"" ([see stackoverflow](https://stackoverflow.com/questions/35330117/how-can-i-run-a-loop-with-a-tensor-as-its-range-in-tensorflow)). However, the [tf.while_loop](https://www.tensorflow.org/versions/r0.12/api_docs/python/control_flow_ops/control_flow_operations#while_loop) docs state:

> For correct programs, while_loop should return the same result for any parallel_iterations > 0.

A loop counter inside of the ""while loop"" body, seems to violate this constraint despite the fact that this is given as an example usage in the docs:

> python i = tf.constant(0) c = lambda i: tf.less(i, 10) b = lambda i: tf.add(i, 1) r = tf.while_loop(c, b, [i])

So it seems that there are two bad outcomes here:

1. If this is indeed the canonical way of creating a ""for loop"", then the example explicitly creates a dependency between iterations, meaning that the ""while loop"" iterations cannot be run in parallel. 

1. The example is incorrect? 

It seems like the while_loop docs should have an example which better illustrates how to use it as a ""for loop"", if such usage is indeed intended, or a warning on the implications of the provided example.  
",0,,3,2018-01-23T08:07:11Z,NONE,2018-01-24T01:26:10Z
16315,Remove Variables from a TF Server (e.g.),,"I have a cluster of long-lived TensorFlow servers  (//tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server).
My problem is how to reset variables on these server. 

There is a behavior in distributed TensorFlow in which a variable defined on a worker (e.g. PS) outlives the session which defines it. I understand this behavior is intentional to support between graph model-replica.

However, In my use case this behavior causes unexpected problem. I have not found a mechanism to override this. It there is, I believe it is helpful to better reflect it in the documentation, if there is not, I hope I can make a case to motivate its existence.

In my use case different training jobs are ran _sequentially_ (i.e. one training job at a time) on this cluster, each using one client (which connects to only one master). 

The problem I have is if a variable is defined in two training job with a same name but different shape sizes, the latter client gets the following error on ""Session"" creation:
```
InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [100] rhs shape= [200]%0A%09 [[Node: a/Assign = Assign[T=DT_FLOAT, _class=[""loc:@a""], use_locking=true, validate_shape=true, _device=""/job:worker/replica:0/task:0/device:GPU:0""](a, a/Initializer/random_uniform)]]
```

`tf.reset_default_graph` does not help. The solution to this problem could be a mechanism similar `tf.reset_default_graph` that resets variables in all the workers.

To replicate this problem let say we have two workers: (one PS, and on Worker)

Worker 1:
```bash
#worker 1
./bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server --cluster_spec=""ps|localhost:2222,worker|localhost:2223"" --job_name=worker --task_id=0 &
#worker 2
./bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server --cluster_spec=""ps|localhost:2222,worker|localhost:2223"" --job_name=worker --task_id=0
```
(Same result with `tf.train.Server` workers.)

Then run the simple code:
```python
import tensorflow as tf
var = tf.get_variable(""A"", shape=(100,))
with tf.train.MonitoredTrainingSession(master=""localhost:2223"") as sess:
   pass
```
It should work just fine.
Then when this code (which is identical except the variable shape) is ran:
```python
import tensorflow as tf
var = tf.get_variable(""A"", shape=(5000,))
with tf.train.MonitoredTrainingSession(master=""localhost:2223"") as sess:
   pass
```
This example fails.
",0,,2,2018-01-23T07:33:05Z,CONTRIBUTOR,2018-01-23T07:33:40Z
16314,Published libtensorflow_framework.so binaries ABI Problem,,"The distributed `libtensorflow_framework.so` included in the JAR files published to Maven are built using the C++ 11 ABI, in contrast to the main TF build. I think that affects all continuous integration builds of the shared objects. I believe the `-D_GLIBCXX_USE_CXX11_ABI=0` compiler flag should be used for the CI builds as is done for the main build. An example of its use is shown in commit 550df413158b32645ca5df4dcaabc67f1a48964d. This causes some trouble when using these shared objects and developing custom ops, as those are required to be built using that compiler flag. It would be great if the use of the flag was consistent and all binaries were built using the same ABI.

Thanks! ",0,,1,2018-01-23T06:57:28Z,CONTRIBUTOR,2018-01-23T18:47:30Z
16313,Bug of tf.data.TFRecordDataset? Couldn't use tf.reshape after the operations of tf.data.TFRecordDataset,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: Nvidia GeForce GTX TITAN X 12GB
- **Exact command to reproduce**:


### Describe the problem
I want to use  function **tf.profiler.ProfileOptionBuilder.float_operation** to show the flops of the model. But it need a certain input shape while the the output shape of **tf.data.TFrecordDataset** is like (?, 32,32,3). When I want to use tf.reshape to reshape the output of **tf.data.TFrecordDataset**, it generates an error ""Input to reshape is a tensor with 64512 values, but the requested shape has 98304"".  

### Source code 

  def dataset_input(self, dataset_type):
    with tf.variable_scope(""batch_"" + dataset_type):
        def parser(record):
            features = tf.parse_single_example(
                record,
                features={
                    'image': tf.FixedLenFeature([], tf.string),
                    'label': tf.FixedLenFeature([], tf.int64)
                })
            image, label = features['image'], features['label']
            height, width, channels = self.input_size, self.input_size, self.input_dim
            image = tf.decode_raw(image, tf.uint8)
            image = tf.reshape(image, [height, width, channels])
            return image, label
        dataset = tf.data.TFRecordDataset([self.dataset_dir[dataset_type]])
        dataset = dataset.map(parser)
        dataset = dataset.shuffle(buffer_size=50000)
        dataset = dataset.batch(self.batch_size)
        dataset = dataset.repeat()
        iterator = dataset.make_one_shot_iterator()
        features, labels = iterator.get_next()
        features = tf.reshape(features, [self.batch_size, self.input_size, self.input_size, self.input_dim])
        return features, labels
  ",0,,4,2018-01-23T06:26:49Z,NONE,2018-01-23T22:45:02Z
16312,Allow step callback for scipy SLSQP,"awaiting testing (then merge),cla: yes,stat:awaiting response",This simple fix allows `SLSQP` method of scipy optimizer to use step callback as reported in issue [#16294](https://github.com/tensorflow/tensorflow/issues/16294). ,1,,2,2018-01-23T03:56:13Z,CONTRIBUTOR,2018-01-28T20:23:04Z
16309,Workaround 'too perfect forwarding' issue in variant_op_registry,"awaiting testing (then merge),cla: yes","In variant_op_registry.h 
```
std::unordered_map<std::tuple<VariantXOp, StringPiece, StringPiece>, 
                     VariantXOpFn, TupleHash>
```
seems to be falling victim to 'too perfect forwarding' issue ( [SO link](https://stackoverflow.com/questions/44475317/variadic-template-issue), [Andrzej's blog](https://akrzemi1.wordpress.com/2013/10/10/too-perfect-forwarding/)) with gcc-6.4, gcc-7.2 and clang-4 in ubuntu-17.10 (and possibly others). This PR works around the issue by replacing std::tuple with a simple struct.",1,,3,2018-01-23T03:08:35Z,CONTRIBUTOR,2018-01-23T17:46:51Z
16307,Fix Conv3DTranspose in tf.keras,"awaiting testing (then merge),cla: yes",,1,,6,2018-01-23T02:53:57Z,CONTRIBUTOR,2018-01-23T02:59:05Z
16303,Don't load libcupti.so from regular path on Android,"awaiting testing (then merge),cla: yes,kokoro:run","Open to alternatives, but as other methods in this file work similarly this doesn't seem too bad.",1,,11,2018-01-23T00:52:20Z,MEMBER,2018-01-23T21:44:33Z
16298,Bug of tf.data.TFRecordDataset? or my codes wrong?,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Windows 10
- **TensorFlow installed from (source or binary)**:  binary
- **TensorFlow version (use command below)**:  1.4
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: Nvidia Quadro K4000
- **Exact command to reproduce**: 


I tested to write dynamic numbers of variables into tfrecord. But when I use the tf.data.TFRecordDataset to read VarLenFeature, the program crashes. However, if I do not use dataset, but just tf.python_io.tf_record_iterator. The program works without problem. I wonder whether this is a bug of tf.data.TFRecordDataset, or there is something wrong in my codes?

My writing codes are 

    def test_write():
      writer = tf.python_io.TFRecordWriter('test.tfrecord')

      for i in range(3):
        val_list = []
        for j in range(i+1):
          val_list.append(i+j)
        feature_dict = {
          'val': tf.train.Feature(int64_list=tf.train.Int64List(value=val_list)),
        }
    
        example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
        writer.write(example.SerializeToString())

      writer.close()

The reading codes using tf.data.TFRecordDataset and causing error are

	def parse_test(example):
	  features = {
		'val': tf.VarLenFeature(dtype=tf.int64)
	  }
	  parsed_features = tf.parse_single_example(example, features)

	  return parsed_features

	def test_read():
	  dataset = tf.data.TFRecordDataset(['test.tfrecord'])
	  dataset = dataset.map(parse_test)
	  dataset = dataset.batch(1)

	  iterator = dataset.make_one_shot_iterator()
	  feature_dict =  iterator.get_next()

	  with tf.Session() as sess:
		for _ in range(3):
		  curr_dict = sess.run(feature_dict)
		  print([curr_dict['val']])

The error message is:

	TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(""ParseSingleExample/Slice_Indices_val:0"", shape=(?, 1), dtype=int64), values=Tensor(""ParseSingleExample/ParseExample/ParseExample:1"", shape=(?,), dtype=int64), dense_shape=Tensor(""ParseSingleExample/Squeeze_Shape_val:0"", shape=(1,), dtype=int64)). Consider casting elements to a supported type.


 The successful reading codes without using tf.data.TFRecordDataset are as below

	def test_read2():
	  with tf.Session() as sess:
		for serialized_example in tf.python_io.tf_record_iterator('test.tfrecord'):
		  features = tf.parse_single_example(serialized_example,
			features={
			  'val': tf.VarLenFeature(dtype=tf.int64),
			}
		  )

		  temp = features['val']

		  values = sess.run(temp)
		  print(values)

This code successfully print out

	SparseTensorValue(indices=array([[0]], dtype=int64), values=array([0], dtype=int64), dense_shape=array([1], dtype=int64))
	SparseTensorValue(indices=array([[0],
		   [1]], dtype=int64), values=array([1, 2], dtype=int64), dense_shape=array([2], dtype=int64))
	SparseTensorValue(indices=array([[0],
		   [1],
		   [2]], dtype=int64), values=array([2, 3, 4], dtype=int64), dense_shape=array([3], dtype=int64))

However, I am still hoping to use the dataset structure to deal with the VarLenFeature. Is there anything wrong with my reading codes or there is a bug in tf.data.TFRecordDataset? Thank you.

",0,,1,2018-01-22T19:48:46Z,NONE,2018-01-23T22:48:02Z
16294,ScipyOptimizer SLSQP supporting callback,"stat:contributions welcome,type:bug/performance","The callback is deprecated when `SLSQP` method in scipy optimizer is selected (see [here](https://github.com/tensorflow/tensorflow/blob/04b5c75aae4bdbdac7c713714a369f9b360daf70/tensorflow/contrib/opt/python/training/external_optimizer.py#L400)). Actually, `SLSQP` does support callback, so 
```python 
if method == 'SLSQP':
  # SLSQP doesn't support step callbacks. Obviate associated warning
  # message.
  del minimize_kwargs['callback']
```
in the above linked file could be removed. 

The following example shows that `SLSQP` do support callback. 

```python 
from scipy.optimize import minimize, rosen, rosen_der

def callback(xk, step=[0]):
  print step[0], xk[0]
  step[0] += 1
  
x0 = [1.3, 0.7, 0.8, 1.9, 1.2]
res = minimize(rosen, x0, callback=callback, method='SLSQP',
    options={'ftol': 1e-6, 'disp': True})
 
print res.x[0]
```






",0,,3,2018-01-22T18:49:22Z,CONTRIBUTOR,2018-01-23T02:15:44Z
16288,Tensorflow works in command prompt but not in Spyder,type:build/install,"Hello.
I'm new to Python so maybe I've missed something but anyway, here is my problem.
I've installed tensorflow in Anaconda prompt by using 
```
C:\WINDOWS\system32> conda create -n tensorflow python=3.6
C:\WINDOWS\system32> activate tensorflow
(tensorflow)C:\WINDOWS\system32> pip install --ignore-installed --upgrade tensorflow
```
Instalation was succesful, then I opened python and tried to import tensorflow to verify installation
```
(base) C:\WINDOWS\system32>python
Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow'
```

After activating tensorflow, it works.
```
(tensorflow) C:\WINDOWS\system32>python
Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
>>>
```
But I cannot find a way to make it work in Spyder IDE. I always get error:
```ModuleNotFoundError: No module named 'tensorflow'```",0,,5,2018-01-22T14:47:27Z,NONE,2018-01-22T17:36:48Z
16287,[Bug] LuongMonotonicAttention in contrib/seq2seq/python/ops/attention_wrapper.py,stat:awaiting tensorflower,"`LuongMonotonicAttention.__init__(...)` calls its parent `_BaseAttentionMechanism` with `query_layer` as follows:
```
        query_layer=layers_core.Dense(
            num_units, name=""query_layer"", use_bias=False),
```
But, it doesn't apply it on query in `LuongMonotonicAttention.__call__(...)`.
```
  def __call__(self, query, previous_alignments):
    """"""...
    """"""
    with variable_scope.variable_scope(None, ""luong_monotonic_attention"",
                                       [query]):
      score = _luong_score(query, self._keys, self._scale)
      score_bias = variable_scope.get_variable(
          ""attention_score_bias"", dtype=query.dtype,
          initializer=self._score_bias_init)
      score += score_bias
    alignments = self._probability_fn(score, previous_alignments)
    return alignments
```
Guessing from the way `LuongAttention` works, there should be `query_layer=None` in `LuongMonotonicAttention.__init__(...)`.",0,,3,2018-01-22T13:50:55Z,NONE,2018-01-26T22:10:26Z
16286,Removes redundant variable assignment,"awaiting testing (then merge),cla: yes","Addresses alert raised by lgtm.com:
https://lgtm.com/projects/g/tensorflow/tensorflow/snapshot/e6183fbeecf069148371be83988e8e5db2b14185/files/tensorflow/python/framework/constant_op.py#xb77a2f6647d782be:1

It doesn't seem like assigning `attr_tshape = attr_tshape` does anything, so there's no need to keep it in.",1,,5,2018-01-22T13:26:57Z,CONTRIBUTOR,2018-01-22T13:28:34Z
16281,"After Working with Tensorflow cpu version for 2 days, it gave me an error on installation today","stat:awaiting response,type:build/install","I had installed and used Tensorflow successfully but today when I opened my computer it gave me this error message:
Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/asus/PycharmProjects/untitled3/CNNCIFARTFNEW.py"", line 2, in <module>
    import tensorflow as tf
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.


Please help. This is urgent.",0,,2,2018-01-22T07:38:03Z,NONE,2018-01-22T19:38:09Z
16280,Repair compilation error of tensorflow built with MKL-DNN,"cla: yes,stat:awaiting response","When we compile tensorflow with Intel **MKL-DNN**, it will meet a failure:

`bazel build --copt -O3 --copt=-DINTEL_MKL_DNN --config=mkl -c opt //tensorflow/tools/pip_package:build_pip_package`

**error: 'mkldnn::algorithm' is not a namespace
  using mkldnn::algorithm::lrn_across_channels;**

 Removing the 'algorithm' field in _tensorflow/core/kernels/mkl_lrn_op.cc_ can solve this problem and lead to successful compilation.",1,,7,2018-01-22T07:27:10Z,CONTRIBUTOR,2018-01-22T07:56:56Z
16278,__init__() got multiple values for argument 'strides',type:support,"    model = Sequential()
    model.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height, 3)))
    print(model.output_shape)
    model.add(Convolution2D(64, 3, 3, strides=(
        1, 1), activation='relu', name='conv1_1'))
above is my code, I got error:
__init__() got multiple values for argument 'strides'
If i don't use 'strides', it's fine. but the stride is 3. How should I set strides?",0,,2,2018-01-22T06:57:46Z,NONE,2018-01-25T01:13:49Z
16277,Control dependency does not ensure write observed by read,,"TF version 1.3.0

```python
def sleep(t):
    '''TF sleep'''
    import time
    def f(t):
        time.sleep(t)
        return np.array([], dtype=np.float32)
    return tf.py_func(f, [t], [tf.float32])[0]

with tf.device('gpu'):
    x = tf.Variable(0.)
with tf.control_dependencies([tf.identity(sleep(0.1))]):
    with tf.device('gpu'):
        mod = tf.assign(x, 100.)
with tf.device('cpu'):
    a = x+1.
    with tf.control_dependencies([tf.identity(mod)]):
        b = x+2.
        with tf.device('gpu'):
            c = x+3.

x.initializer.run()
sess.run([a, b, c])
# [1.0, 2.0, 103.0]
```

When a variable is read on another device, TF seems to copy once regardless of dependencies. I understand this is how TF works, but I think it would be nice to have dependencies ensure memory access order.",0,,2,2018-01-22T06:22:40Z,CONTRIBUTOR,2018-01-23T19:21:05Z
16274,R1.4,cla: no,,0,,2,2018-01-22T01:06:46Z,NONE,2018-01-23T01:03:04Z
16272,Add a rnn example on mnist dataset using tf library,"awaiting review,cla: yes,stat:awaiting tensorflower","A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.
This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)
Links:
    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)
    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).

Training and evaluation log:
Extracting /tmp/mnist_data/train-images-idx3-ubyte.gz
Extracting /tmp/mnist_data/train-labels-idx1-ubyte.gz
Extracting /tmp/mnist_data/t10k-images-idx3-ubyte.gz
Extracting /tmp/mnist_data/t10k-labels-idx1-ubyte.gz
Step 1, Minibatch Loss= 2.5586, Training Accuracy= 0.258
Step 200, Minibatch Loss= 0.2419, Training Accuracy= 0.930
Step 400, Minibatch Loss= 0.1863, Training Accuracy= 0.938
Step 600, Minibatch Loss= 0.1000, Training Accuracy= 0.969
Step 800, Minibatch Loss= 0.0935, Training Accuracy= 0.977
Step 1000, Minibatch Loss= 0.0773, Training Accuracy= 0.969
Step 1200, Minibatch Loss= 0.0500, Training Accuracy= 0.984
Step 1400, Minibatch Loss= 0.0550, Training Accuracy= 0.977
Step 1600, Minibatch Loss= 0.0615, Training Accuracy= 0.984
Step 1800, Minibatch Loss= 0.0635, Training Accuracy= 0.977
Step 2000, Minibatch Loss= 0.0550, Training Accuracy= 0.992
Training Finished!
Testing Accuracy: 0.992188",1,,8,2018-01-21T18:23:28Z,CONTRIBUTOR,2018-01-22T18:23:04Z
16271,"tf.pow(x, y) edge case with negative x (Bug)",type:support,"I am using tf.pow for my project, but my losses are 'nan', so I setup the test cases as shown below.
I found that whenever x is negative, tf.pow seems to output nan instead of the correct answer.

>>> r = tf.pow(0.4,0.4)
>>> r2 = tf.pow(-0.4,-0.4)
>>> r3 = tf.pow(0.4,-0.4)
>>> r4 = tf.pow(-0.4,0.4)
>>> sess.run(r)
0.69314486
>>> sess.run(r2)
nan
>>> sess.run(r3)
1.4426999
>>> sess.run(r4)
nan

I appreciate for anyone of the community who can address this issue.

Respectfully,",0,,3,2018-01-21T18:03:16Z,NONE,2018-01-22T03:44:57Z
16269,ContentTooShortError: <urlopen error retrieval incomplete: got only 246506328 out of 247336696 bytes>,,"I have started UDACITY deep learning course.
I was copying the assignment 1 codes then I got the this error during downloading the notMNIST_large.tar.gz
Here are some screenshots of the code.
I am doing this on jupyter python3 notebook in ubuntu.

![1](https://user-images.githubusercontent.com/25321783/35194207-ca56cdc6-fed5-11e7-9fe7-3232c3741689.png)
![2](https://user-images.githubusercontent.com/25321783/35194208-cacbbc6c-fed5-11e7-9ff0-600527e4990a.png)
![error](https://user-images.githubusercontent.com/25321783/35194209-cb3fbd6a-fed5-11e7-8162-9b999b6240bb.png)
",0,,1,2018-01-21T12:36:47Z,NONE,2018-01-22T04:12:24Z
16265,ImportError: cannot import name tf,"stat:awaiting response,type:support","### System information
- **Os version**: Linux Ubuntu 14.04
- **TensorFlow installed from**: binary (sudo pip  install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.1-cp27-none-linux_x86_64.whl
)
- **TensorFlow version** :1.4.1
- **Python version**: 2.7

### Describe the problem
After Install when i run the following command it throws error;
`from tensorflow import tf`

It throws
`Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name tf
`
",0,,2,2018-01-21T06:37:52Z,NONE,2018-01-22T03:46:25Z
16262,"Cannot opened include file ""tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h"": no such file or directory",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: source 
- **TensorFlow version (use command below)**: current git master branch, should be v1.4.1 or v1.5.0rc1?
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: MSVC2015
- **CUDA/cuDNN version**: CPU build only, gpu function is off
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Trying a minimal build with cmake, with only snappy support and optimize for native arch turned on

### Describe the problem
Build failing due to missing header files ""tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h"".

Everything build succesful except for tpu project. I don't really know how to generate the pb.h file from protoc manually. I trying to fix the problem by chaning .cmake files, but not sure which one is for tpu.

### Source code / logs
133>D:\MSVC-source\tensorflow\tensorflow\contrib\tpu\ops\tpu_embedding_ops.cc(16): fatal error C1083: Cannot open include file: 'tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h': No such file or directory

",0,,6,2018-01-21T02:28:38Z,CONTRIBUTOR,2018-01-22T00:27:42Z
16255,tf.scatter_update Error ,,"Hi, 

I use tf.scatter_update to update non-trainable variables AS and AO in a code. As I found, when one uses scatter_update, the gradient misses, so that there is no gradient. 
Because of that I set both AL and AO as non-trainable variables (actually they are non-trainable), called the optimizer: tf.train.AdamOptimizer(config.actor_lr0,0.9,0.999,1e-8).minimize(actor_loss), and I thought everything should be fine. 
However, I am getting error:
LookupError: No gradient defined for operation 'actor/encoder/beer_game_flow_8/next_scat_j_2' (op type: ScatterUpdate). 
Here are the lines of the code that I update AO and AS that gives the error:

self.players[k-1].AS = tf.scatter_update(self.players[k-1].AS, 
                    self.curTime + leadTimeIn, 
                    tf.add(self.players[k-1].AS[self.curTime + leadTimeIn], possible_shipment), name='next_scat_j' )                   

self.players[k+1].AO = tf.scatter_update(self.players[k+1].AO, 
                    self.curTime + leadTime, tf.add(self.players[k+1].AO[self.curTime + leadTime], 
                    self.players[k].actionValue(self.curTime, self.playType))
                    , name='handle_scat_j')

Since both AS and AO are non-trainable, I do not need their gradient, and AS and AO are the only variable in this op. So, I was wondering why TensorFlow want to obtain the gradient, since there is no trainable variable here? 
Is it something that you can fix it, or is there any reason behind this behavior? 

BTW, I use python 2.7 with tf 1.4.0 on Debian 8.7 with a K80 with 12GB of memory. 

Thanks, 
Afshin
",0,,3,2018-01-19T23:39:34Z,NONE,2018-01-24T13:08:14Z
16254,adding placeholder_with_default in order to feed both via dataset and placeholders produces error on GPU,,"
Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:1.4.0
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:9.1.85
- **GPU model and memory**:Titan V, 12G
- **Exact command to reproduce**: See below

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The issue is when you want to train a classifier that takes both placeholder and dataset via queue to feed input. The reason one may want to do that is to run inference via placeholders. 

I added the following line under define the model section of train_image_classifier.py of slim library

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


_images = tf.placeholder_with_default(image, shape=[...], name='input)
 I get an error of the following kind when running on GPU:
Cannot assign a device for operation 'input': Could not satisfy explicit device specification '/device:GPU:0' 


",1,,4,2018-01-19T23:37:05Z,NONE,2018-01-22T00:02:36Z
16249,Branch 182554969,cla: yes,,0,,1,2018-01-19T19:00:38Z,MEMBER,2018-01-19T19:01:11Z
16246,Failed to build error: mismatched argument pack lenghts...,,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 4.14.13-1-ARCH
- **TensorFlow installed from (source or binary)**: git
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9.0-1
- **GCC/Compiler version (if compiling from source)**: 6.4.1
- **CUDA/cuDNN version**:  9.1.85-1 / 7.0.5-2
- **Exact command to reproduce**:
./configure
bazel build --config=opt --config=cuda --jobs 12 //tensorflow/tools/pip_package:build_pip_package


### Describe the problem
failed to build

### Source code / logs

> /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'
>        return __and_<is_convertible<_UElements&&, _Elements>...>::value;
>                                                                  ^~~~~
> /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>}; bool <anonymous> = true; _Elements = {tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece}]' not a return-statement
>      }
>  ^
> ERROR: /home/user/dev/git/tensorflow/tensorflow/core/kernels/BUILD:1884:1: output 'tensorflow/core/kernels/_objs/list_kernels_gpu/tensorflow/core/kernels/list_kernels.cu.pic.o' was not created
> ERROR: /home/user/dev/git/tensorflow/tensorflow/core/kernels/BUILD:1884:1: not all outputs were created or valid
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 29.727s, Critical Path: 28.35s
> FAILED: Build did NOT complete successfully
> ",1,,9,2018-01-19T17:10:53Z,NONE,2018-01-30T18:18:56Z
16245,Branch 182511847,cla: yes,,0,,2,2018-01-19T16:35:22Z,MEMBER,2018-01-19T17:55:52Z
16244,Benchmarking GPU ops in Tensorflow Graphs,,"I tried using the tool for benchmarking Tensorflow Graphs at
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark

Seems it only gives the memory profile for RAM per ops. How can I get the GPU memory and utilization profile per ops using this tool?

",0,,1,2018-01-19T16:03:21Z,NONE,2018-01-19T21:20:25Z
16240,graph_metrics.py does not work well,,"I want to use function in graph_metrics.py, I run corresponding test file graph_metrics_test.py, but I get the following assertion error for ""weight_parameters"" metric
```

line 32, in testGraphMetrics
    self.assertEqual(expected[statistic_type], current_stats.value)
AssertionError: 100 != None
```
has anyone encounter this problem?",0,,2,2018-01-19T12:35:24Z,NONE,2018-01-19T21:11:21Z
16238,//tensorflow/contrib/gan:losses_impl_test fails with AssertionError ,type:bug/performance,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04  s390x
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: No GPU
- **GPU model and memory**: NA
- **Exact command to reproduce**: bazel test -c opt //tensorflow/contrib/gan:losses_impl_test

### Describe the problem
One of the sub-test `test_stable_global_norm_unchanged` fails on s390x with 
`AssertionError: 110.709068 != 110.709084 +/- 0.000010`

Seems like a minor difference, so I tried changing the tolerance slightly as below:
```
-        self.assertNear(gnorm_np, precond_gnorm_np, 1e-5)
+        self.assertNear(gnorm_np, precond_gnorm_np, 2e-5)
```
with this the test is passing.

Is it ok to create a PR with this change? Could you please share your thoughts on this.

### Source code / logs
```
.......................F..................................................................................
======================================================================
FAIL: test_stable_global_norm_unchanged (__main__.CombineAdversarialLossTest)
Test that preconditioning doesn't change global norm value.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/gan/losses_impl_test.runfiles/org_tensorflow/tensorflow/contrib/gan/python/losses/python/losses_impl_test.py"", line 602, in test_stable_global_norm_unchanged
    self.assertNear(gnorm_np, precond_gnorm_np, 1e-5)
  File ""/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/gan/losses_impl_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 879, in assertNear
    if msg is not None else """"))
AssertionError: 110.709068 != 110.709084 +/- 0.000010

----------------------------------------------------------------------
Ran 106 tests in 9.119s

FAILED (failures=1)
```

",2,,2,2018-01-19T10:25:54Z,CONTRIBUTOR,2018-01-19T21:00:09Z
16237,support preconditioner for `conjugated_gradient()` in `linear_equations.py`,"awaiting testing (then merge),cla: yes","1. support preconditioner for `conjugated_gradient()` in `tensorflow\tensorflow\contrib\solvers\python\ops\linear_equations.py`
2. add identity_operator() in `util.py` as default preconditioner
3. edit unit test files(`util_test.py`, `linear_equations_test.py`) to validate preconditioner",1,,4,2018-01-19T07:12:01Z,CONTRIBUTOR,2018-01-19T07:13:03Z
16236,Branch 182474037,cla: yes,,0,,3,2018-01-19T06:47:58Z,MEMBER,2018-01-19T07:29:26Z
16235,Feature Request: Make NDLSTM use state_is_tuple=True,type:feature,"I have successfully used [NDLSTM](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ndlstm) (specifically lstm2d.separable_lstm) in my own project but whenever I use it, I enocunter this warning: `""Using a concatenated state is slower and will soon be deprecated. use state_is_tuple=true.""`

The warning is caused by  `ndlstm_base_dynamic` in lstm1d.py. Specifically, this line: `lstm_cell = rnn_cell.BasicLSTMCell(noutput, state_is_tuple=False)`

I modified the code such that the deprecation warning won't appear:

```
with variable_scope.variable_scope(scope, ""SeqLstm"", [inputs]):
    lstm_cell = rnn_cell.BasicLSTMCell(noutput)
    if reverse:
      inputs = array_ops.reverse_v2(inputs, [0])
    outputs, _ = rnn.dynamic_rnn(
        lstm_cell, inputs, time_major=True, dtype=inputs.dtype)
    if reverse:
      outputs = array_ops.reverse_v2(outputs, [0])
    return outputs
```

Before I make any pull requests, is there a reason why the `state_is_tuple` argument is set to `False` in the code?",0,,5,2018-01-19T02:36:34Z,CONTRIBUTOR,2018-01-19T02:45:25Z
16234,The recognized result is not correct when converting the frozen graph to tflite for android device use ,comp:lite,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: mac High Sierra
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.5.0
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:c++/4.2.1
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

The detailed system information, you can check the url:
https://drive.google.com/file/d/19oKikJ0PcGHx9daauub28IYb8J3hA-rw/view?usp=sharing

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Hi, I covert the frozen graph:mobilenet_v1_224 to tflite, and put it in the tflitecamerademo app, but the regonization result is not correct. If I use the tflite file which download from https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip

The regonization result is correct, I don't know what steps is not correct when I covert the frozeon graph to tflite file, could you help me to review what steps is the wrong?

I put the frozen graph, coverting tflite file and the regonized picture in the https://drive.google.com/drive/folders/12h9O2AtcnDuQZXogAdmexRSjxIQVc1Ej?usp=sharing

The correct result should be ""malamute"", but I use the my coverting tflite file, the result is ""shower curtain""

I use the command to do the covert
bazel run --config=opt //tensorflow/contrib/lite/toco:toco -- '--input_file=/tmp/mobilenet_frozen_graph.pb' '--output_file=/tmp/mobilenet_quant_20180117.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=QUANTIZED_UINT8' '--inference_input_type=QUANTIZED_UINT8' '--input_shapes=1,224,224,3' '--input_arrays=input' '--output_arrays=MobilenetV1/Predictions/Reshape_1' '--mean_values=128' '--std_values=128' '--default_ranges_min=0' '--default_ranges_max=6'

Thanks

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,4,2018-01-19T02:26:39Z,NONE,2018-01-31T20:13:48Z
16232,Singleton S3Client,"awaiting testing (then merge),cla: yes","Fixes #16230 .

This drastically speeds up performance of interactions with S3, and eliminates a lot of spurious log warnings when interacting with S3 files.

The filesystem unit tests went from taking ~40 seconds to taking ~4 seconds with this change, a 10X performance improvement.

Some items of note:
- I updated the delete test to work on a bucket that had tests run previously. Without this change, a manual wipe of the file in quest was required after each run.
- I moved the request timeout to a central location, instead of being local to the `Sync` operation.
- I eliminated the increased connection timeout for `Sync`, which shouldn't be needed.
- Configuration is no longer a static variable protected by a mutex, but instead created as-needed. This should be non-functional, given that config is only created once during normal operation now.",1,,7,2018-01-18T23:54:26Z,CONTRIBUTOR,2018-01-22T21:26:47Z
16231,x86_64 compilation failed,"stat:awaiting tensorflower,type:build/install","### System information

- **MacOS High Sierra 10.13.2**:
- **Python 3.6.3**:
- **TensorFlow Latest Pull from 1/17/18**:

### Describe the problem
I am following Pete Warden's TensorFlow for Mobile Poets guide and seem to have a found an error. When I run ""tensorflow/contrib/makefile/build_all_ios.sh"" after about 20 minutes it returns an error. 

I have tried running lipo -info /Users/ryan/Downloads/tensorflow2/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a

and this returns: 

Architectures in the fat file:
/Users/ryan/Downloads/tensorflow2/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a are: i386 

I have the entire error script here:
https://drive.google.com/file/d/1JovTMGBJKbqzRPBzXy3cIQ-hbz76n0ab/view?usp=sharing

### Source code / logs
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see 
invocation)
make: *** [/Users/ryan/Desktop/tensorflow-
master/tensorflow/contrib/makefile/gen/bin/ios_X86_64/benchmark] Error 1
+ '[' 2 -ne 0 ']'
+ echo 'x86_64 compilation failed.'
x86_64 compilation failed.
+ exit 1
",1,,2,2018-01-18T23:51:42Z,NONE,2018-01-22T04:02:19Z
16230,A new S3Client is created with all file operations.,,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS X 10.12.6
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: commit 4595f1cff635ce024e875f0f3d480172731b0b22
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.5.4-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A

### Describe the problem

The [S3 filesystem](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc) creates a new `Aws::S3::S3Client` object with all interactions with S3. This is a heavyweight object, and takes relatively large amount of time to create and destroy.

This should be a singleton associated with the filesystem object.

Fix shortly.",0,,4,2018-01-18T23:39:21Z,CONTRIBUTOR,2018-01-19T07:03:18Z
16225,maxout lose the number of features in the shape of its output,,"In tf.contrib.layers.maxout(), when the shape of ""inputs"" is not completely specified, the shape of its output will be completely unknown, such as [None, None, None] in the 3d case.
Since ""num_units"" has specified the final number of features in the maxout axis, the output should set its shape accordingly:
https://github.com/tensorflow/tensorflow/pull/16114",1,,3,2018-01-18T19:19:05Z,NONE,2018-01-19T07:03:04Z
16224,Suppress AWS curl init warning,"awaiting testing (then merge),cla: yes","This shows up each time we run the TensorBoard command, even if we're
not using anything AWS related.

```sh
jart@compy:~/tmp/aws-sdk-cpp-1.3.15$ grep -R ""Initializing Curl library"" .
./aws-cpp-sdk-core/source/http/curl/CurlHttpClient.cpp:        AWS_LOGSTREAM_INFO(CURL_HTTP_CLIENT_TAG, ""Initializing Curl library"");
```",1,,1,2018-01-18T18:56:07Z,MEMBER,2018-01-18T19:44:23Z
16221,Meaning of report_tensor_allocations_upon_oom output,"stat:awaiting response,type:support","Python: 3.6.2
OS: Ubuntu 16.04
Tensorflow: 1.5.0rc1

When running a session with `tf.RunOptions` and `report_tensor_allocations_upon_oom=True` I get the following output at the end of my log.

1. I am wondering why some entries occur multiple times? How can a single node have multiple allocations? Why are they not summed?
2. Does `Remaining 1252 nodes with 98.80MiB` mean that all 1252 nodes together use 98.80MiB or each single one uses that amount?
3. When summing up all values I get `10.607822265625GiB` but my free GPU space when starting my program is `11.92GiB` so shouldn't there still be enough space??

```
Current usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc
  250.78MiB from network/convolutions/conv2d_5/Conv2D
  217.34MiB from network/convolutions/conv2d_5/Conv2D
  203.75MiB from network/convolutions/conv2d_5/Conv2D
  192.91MiB from network/convolutions/conv2d_11/Conv2D
  168.05MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_11/Conv2D
  163.84MiB from network/convolutions/conv2d_7/Conv2D
  160.50MiB from network/convolutions/conv2d_7/Conv2D
  140.99MiB from network/convolutions/conv2d_12/Conv2D
  133.75MiB from network/convolutions/conv2d_6/Conv2D
  133.75MiB from network/convolutions/conv2d_7/Conv2D
  133.75MiB from network/convolutions/conv2d_11/Conv2D
  133.75MiB from network/convolutions/conv2d_11/Conv2D
  133.75MiB from network/convolutions/conv2d_11/Conv2D
  133.75MiB from network/convolutions/conv2d_12/Conv2D
  103.66MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  Remaining 1252 nodes with 98.80MiB
```",0,,2,2018-01-18T14:39:30Z,CONTRIBUTOR,2018-01-18T17:01:36Z
16220,Fix result shape of tf.tensordot unknown when axes is an integer number,"awaiting testing (then merge),cla: yes","#8452 add the function ""Tensordot partial shape inference"", solves the problem #6682.
However, the shape of result of `tensordot` is still `<unknown>` when `axes` is an integer N, which is in common use.
For example, 
```
a = tf.placeholder('float32', shape=[None, 100])
b = tf.placeholder('float32', shape=[100, 300])
```
set `axes=1`,
```
result_tensordot = tf.tensordot(a, b, axes=1)
result_tensordot.get_shape()  # TensorShape(None)
result_tensordot.get_shape().as_list()  # Error
```
The equivalent `axes=[[1], [0]]` behaves correctly,
```
result_tensordot = tf.tensordot(a, b, axes=[[1], [0]])
result_tensordot.get_shape()  # TensorShape([Dimension(None), Dimension(300)])
result_tensordot.get_shape().as_list()  # [None, 300]
```
The simplified is more common and the partial shape should be inferred correctly.
This PR solves the problem.

",1,,5,2018-01-18T12:10:26Z,CONTRIBUTOR,2018-01-23T02:11:01Z
16217,Windows: Add missing dependencies in lib_proto_parsing,cla: yes,"Fix http://ci.tensorflow.org/job/tf-master-win-bzl/2275/console
Culprit: https://github.com/tensorflow/tensorflow/commit/ccbd14b741e6efbe51769f0f1b9cb3719c42c23b
@gunan @panyx0718 ",0,,2,2018-01-18T09:11:23Z,MEMBER,2018-01-18T09:11:42Z
16215,Tensorflow doesn't delete previous checkpoints,"stat:awaiting response,type:support","### System information
- Linux Ubuntu 16.04:
- Tensorflow version 1.4.1*:
- Python 3.5.2: 

### Describe the problem

A brief summary is that, if I run multiple times my training script tensorflow doesn't delete the checkpoints created in previous runs of the script.

I am preparing a automatic script that every X days runs and train with the new data collected. But I am facing a problem, even that I have configured the saver to keep the 2 last checkpoints, it doesn't work as I expected. 

Example:
I configure to run 100.000 iterations and each 10.000 to save the checkpoint. The system works and starts saving 10.000, 20.000, ... And when get to 30.000 starts deleting the firsts checkpoints. When the script ends I have the 2 last checkpoints(90.000 and 100.000). 

Then when I train again the system starts from the last checkpoint, in this example the 100.000, and do the same as the previous, 110.000, 120.000,.. and when gets to the 130.000 starts to delete the 100.000 and so on. But the 2 checkpoints from the previous run(90.000 and 100.000) remain there even that in the checkpoint txt are not listed there.

This will be repeated in every run of the script, creating files that I don't need anymore and growing during the time.

This is an intended behavior(expecting to the user to delete or manage manually) or it is really a problem?
It exist any workaround?

Thank you for your time and amazing work. 
 ",0,,2,2018-01-18T08:40:20Z,NONE,2018-01-18T19:04:23Z
16208,using string_input_producer with train dataset and validate dataset,stat:awaiting response,"I have two datasets(files), for train and validate respectively. I can successfully load training set thru tf.train.string_input_producer, set num_epochs=5. Then I can iteratively get batch of data to optimize my model.
But, I got stuck when trying to load my validation set by the same way, the program keeps saying ""OutOfRange Error"" even I didn't set num_epochs in string_input_producer.
Can you supply an example that using string_input_producer  with two or more dataset?
same as the question on stackoverflow: [here](https://stackoverflow.com/questions/37068324/read-big-train-validation-test-datasets-in-tensorflow)
Please help me solve the problem. Thank you very much.
",0,,2,2018-01-18T03:21:26Z,NONE,2018-01-18T13:27:03Z
16206,make label_image for tflite build again,"awaiting testing (then merge),cla: yes","1. add namespace to label_image.h to make label_image for tflite build again
2. add --config monolithic and mention NDK settings in label_image.md
3. fix a typo in display_usage()",1,,2,2018-01-18T02:33:26Z,CONTRIBUTOR,2018-01-25T22:24:20Z
16200,Accepts `PathLike` objects for `model_dir`,"awaiting testing (then merge),cla: yes","* Retrieves the file system path representation if `PathLike` object is passed to `Estimator` or `RunConfig` for `model_dir`, instead of `str`.
* Closes #15784",0,,6,2018-01-17T23:27:52Z,CONTRIBUTOR,2018-01-17T23:37:01Z
16198,Unable to build Tensorflow Benchmark model for Android,,"I've been trying to benchmark the model on mobile but I'm not able to build the model for Android. For desktop, I have been able to build and run the benchmark model.

The machine I'm using is a MacBook pro 15 inch with High Sierra and tensorflow v.1.4

I've been following the directions given at the following links:

(https://www.tensorflow.org/mobile/optimizing#how_to_profile_your_model
https://github.com/tensorflow/tensorflow/tree/r1.4/tensorflow/tools/benchmark)

Edit: Updated answer to the issue template

Have I written custom code

- No custom code was written

OS Platform and Distribution

- Mac OS High Sierra

TensorFlow installed from

- Tensorflow installed from source

TensorFlow version

- 1.4

Bazel version

- Bazel version 0.9.0

CUDA/cuDNN version

- N/A

GPU model and memory

- N/A

Exact command to reproduce

```
bazel build -c opt --cpu=armeabi-v7a \
  --crosstool_top=//external:android/crosstool \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  tensorflow/tools/benchmark:benchmark_model
```

Error message received

```
ERROR: /private/var/tmp/_bazel_abhisheksehgal/486c675b3107dc770b2281b905f670fe/external/highwayhash/BUILD:8:1: C++ compilation of rule '@highwayhash//:sip_hash' failed (Exit 1)
In file included from external/highwayhash/highwayhash/sip_hash.cc:15:
In file included from external/highwayhash/highwayhash/sip_hash.h:25:
external/highwayhash/highwayhash/state_helpers.h:76:3: error: use of undeclared identifier 'static_assert'; did you mean 'static_cast'?
  static_assert((kPacketSize & (kPacketSize - 1)) == 0, ""Size must be 2^i."");
  ^
In file included from external/highwayhash/highwayhash/sip_hash.cc:15:
external/highwayhash/highwayhash/sip_hash.h:33:15: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
  using Key = HH_U64[2];
              ^
external/highwayhash/highwayhash/sip_hash.h:104:22: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using SipHashState = SipHashStateT<2, 4>;
                     ^
external/highwayhash/highwayhash/sip_hash.h:105:24: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using SipHash13State = SipHashStateT<1, 3>;
                       ^
external/highwayhash/highwayhash/sip_hash.cc:20:13: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using Key = highwayhash::SipHashState::Key;
            ^
external/highwayhash/highwayhash/sip_hash.cc:21:15: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using Key13 = highwayhash::SipHash13State::Key;
              ^
5 warnings and 1 error generated.
Target //tensorflow/tools/benchmark:benchmark_model failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2.060s, Critical Path: 1.60s
FAILED: Build did NOT complete successfully
```",0,,5,2018-01-17T22:45:21Z,NONE,2018-01-18T07:03:03Z
16196,Fix issue of branch switching not working with bazel,"awaiting review,cla: yes","This fix tries to address the issue raised in #15957 where bazel stops working after switching git branch, and reconfigure with `./configure` will not work as well.

This fix adds a quick fix as was suggested, by having `export TF_CONFIG_TIME=""$(date)"" `in configure.py and add it to the environ list in git_configure.bzl.

This fix fixes #15957.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,2,2018-01-17T19:00:24Z,MEMBER,2018-01-19T18:27:46Z
16193,Performance issues with TF1.5 on CPU,type:bug/performance,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0-rc1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:

Hello,
I'm facing performance issues with the last releases of TF using a CPU.
I'm using the [benchmark tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/benchmark_model.cc) to calculate mean inference time of a model.

For example, in order to evaluate mobilenet (trained on a custom dataset), I'm using this command :
`bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=""path to mobilenet graph"" --input_layer=""input"" --input_layer_shape=""1,224,224,3"" --input_layer_type=""float"" --output_layer=""MobilenetV1/Predictions/Reshape_1""`
After setting CUDA_VISIBLE_DEVICES to """" in order to run on CPU.

With TF 1.4.1, I obtain a mean inference time equals to 26ms (13ms if I compile with optimization flags).
Using tf 1.5.*, I obtain a mean inference time equals to 51ms (45ms if I compile with optimization flags).

The loss is very important, so I'm wondering if it's a known issue and how I can improve this.

I tried with tags/v1.5.0-rc0, tags/v1.5.0-rc1 and master, and the problem is the same.

Thank you",1,,8,2018-01-17T13:39:19Z,NONE,2018-01-17T21:15:04Z
16192,how to set ignore_label in tensorflow?,,"when i set the label=-1, there is an error: Received a label value of -1 which is outside the valid range of [0, 8)",0,,4,2018-01-17T13:31:38Z,NONE,2018-01-18T01:24:34Z
16191,Fix docstring typo of losses_impl.py,cla: yes,"Add missing ""`"" to the docstring.",0,,3,2018-01-17T12:58:14Z,CONTRIBUTOR,2018-01-17T13:02:57Z
16188,benchmark_model tool not build successfully for android version,,"Hello,

I try to build the benchmark_model for the android, but I encounter some errors.
Please help, is any setting not correct?

The configuration of the SDK and NDK in the WORKSPACE is
android_sdk_repository(
    name = ""androidsdk"",
    api_level = 23,
    build_tools_version = ""26.0.1"",
    path = ""/home/kk/android_sdk/android-sdk-linux"",
)

android_ndk_repository(
    name=""androidndk"",
    path=""/home/kk/android_sdk/ndk/android-ndk-r14"",
    api_level=14)

Use the command to build:
bazel build --cxxopt='--std=c++11' -c opt 
--crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain tensorflow/tools/benchmark:benchmark_model

There are three errors
1. external/gif_archive/lib/openbsd-reallocarray.c:33:19: error: use of undeclared identifier 'SIZE_MAX'
2. tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:172:53: error: no member named 'nanf' in namespace 'std'; did you mean simply 'nanf'?
3. external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: cannot find -lpthread

Thanks

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04.4 LTS
- **TensorFlow installed from (source or binary)**:use the pip install
- **TensorFlow version (use command below)**:1.4.0
- **Python version**: Python 2.7.6
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:(Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
- **CUDA/cuDNN version**:NA
- **GPU model and memory**:NA
- **Exact command to reproduce**:NA

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,4,2018-01-17T09:46:35Z,NONE,2018-01-18T16:15:45Z
16186,A bug when applying MultiRNNCell?,,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: This code is very similar to an official example
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip install tensorflow
- **TensorFlow version (use command below)**: b'unknown' 1.4.0
- **Python version**: Python 3.5.2 :: Anaconda 4.2.0 (64-bit)
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
tf.nn.MultiRNNCell sometimes doesn't work.

It raises an issue like this:
ValueError: Dimensions must be equal, but are 64 and 96 for 'lstm/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [128,64], [96,128].

### Source code / logs
  import tensorflow as tf
  import numpy as np

  hidden_layer_size = 32
  embed = tf.zeros((128, 6, 64), dtype=tf.float32)

  num_LSTM_layers = 2
  with tf.variable_scope(""lstm""):
    
    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size, forget_bias=1.0)
    cell = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell]*num_LSTM_layers, state_is_tuple=True)
    outputs, states = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)
   
Error:
ValueError: Dimensions must be equal, but are 64 and 96 for 'lstm/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [128,64], [96,128].

",0,,3,2018-01-17T09:23:15Z,NONE,2018-01-27T15:17:54Z
16183,`AssignVariableOp` supports `DT_BFLOAT16`,cla: yes,Fix #16103,0,,2,2018-01-17T08:06:26Z,CONTRIBUTOR,2018-01-20T03:15:37Z
16180,Tensorboard is down after upgrading the tensorflow?,stat:awaiting response,"Hello everyone:

I meet a issue about tensorboard after upgrading the tensorflow. It runs nicely before, but I want maintain some Python2.7 codes in Python3.4. That is why I install tensorflow .whl file of Python 3.4 and modify some grammer from Python2.7 to Python3.4. Then codes still run fine, but tensorboard is donw. The error message as following:

![image](https://user-images.githubusercontent.com/12611573/35029640-d1981942-fb96-11e7-9b89-c3c14ffaa54b.png)

OS Platform: Ubuntu 14.04
TensorFlow installed from: pip instll .whl file
TensorFlow version: tensorflow 1.2.1 for Python2, but can not check the version for Python 3

What should I do for this issue? degrade tensorflow or upgrade CUDA?
Can anybody give me any help? Thank you!
",0,,15,2018-01-17T07:00:41Z,NONE,2018-01-17T19:12:04Z
16178,Crash in TF lite demo android app when using preprocessing layer,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: v1.4.0-rc0-21-g1e25994 1.4.0-rc1
- **Python version**: Python 3.6.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8/6
- **GPU model and memory**: Titan X (Pascal), 12 GB
- **Exact command to reproduce**:


### Describe the problem
I have a problem adding preprocessing layers to MobileNetV1 model that is quantized afterward. As preprocessing method I would like to use inception preprocessing, but TF lite does not support several operations (sub, div, broadcasting, ...), so I modified following preprocessing

```python
images = tf.divide(images, tf.constant(255.0))
images = tf.subtract(images, tf.constant(0.5))
images = tf.multiply(images, tf.constant(2.0))
```

to

```python
shape = images.get_shape()
c1 = tf.constant(1.0/255.0, shape=shape)
c1 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)
c2 = tf.constant(-0.5, shape=shape)
c2 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)
c3 = tf.constant(2.0, shape=shape)
c3 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)

images = tf.multiply(images, c1)
images = tf.fake_quant_with_min_max_args(images, min=0, max=1)
images = tf.add(images, c2)
images = tf.fake_quant_with_min_max_args(images, min=-0.5, max=0.5)
images = tf.multiply(images, c3)
images = tf.fake_quant_with_min_max_args(images, min=-1.0, max=1.0)
```

Quantization is performed with
```python
fold_batch_norms.FoldBatchNorms(graph)
quantize.Quantize(graph, is_training=is_training)
```
 and can be trained and evaluated.

Further, graph is frozen.
```bash
bazel-bin/tensorflow/python/tools/freeze_graph \
  --input_graph=MobileNetV1-4.pbtxt \
  --input_checkpoint=MobileNetV1-4.ckpt \
  --output_node_names=output/softmax \
  --output_graph=MobileNetV1-4-frozen.pb
```

Finally, frozen graph is converted to TF lite model using command.
```bash
bazel-bin/tensorflow/contrib/lite/toco/toco \
 --input_file=MobileNetV1-4-frozen.pb \
 --input_format=TENSORFLOW_GRAPHDEF \
 --output_format=TFLITE \
 --output_file=model.tflite \
 --inference_type=QUANTIZED_UINT8 \
 --inference_input_type=QUANTIZED_UINT8 \
 --input_array=input/image \
 --output_array=output/softmax \
 --input_shape=1,224,224,3
```

During conversion no error occurs.
```
2018-01-17 11:25:52.905034: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 604 operators, 896 arrays (0 quantized)
2018-01-17 11:25:53.301108: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 66 operators, 127 arrays (1 quantized)
2018-01-17 11:25:53.302502: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 66 operators, 127 arrays (1 quantized)
2018-01-17 11:25:53.303020: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 35 operators, 96 arrays (1 quantized)
2018-01-17 11:25:53.303601: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 35 operators, 96 arrays (1 quantized)
2018-01-17 11:25:53.326761: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 1: 34 operators, 95 arrays (94 quantized)
2018-01-17 11:25:53.327269: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 2: 34 operators, 95 arrays (94 quantized)
2018-01-17 11:25:53.327854: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 1756160 bytes, theoretical optimal value: 1204224 bytes.
2018-01-17 11:25:53.328080: I tensorflow/contrib/lite/toco/toco_tooling.cc:269] Estimated count of arithmetic ops: 1.14175 billion (note that a multiply-add is counted as 2 ops).
```

When I upload generated model to TF lite demo application, app crashes logcat prints this error.
```
01-17 11:56:52.190 10923-10923/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
01-17 11:56:52.190 10923-10923/? A/DEBUG: Build fingerprint: 'samsung/dreamlteks/dreamlteks:7.0/NRD90M/G950NKSU1AQL3:user/release-keys'
01-17 11:56:52.191 10923-10923/? A/DEBUG: Revision: '11'
01-17 11:56:52.191 10923-10923/? A/DEBUG: ABI: 'arm64'
01-17 11:56:52.191 10923-10923/? A/DEBUG: pid: 10865, tid: 10881, name: CameraBackgroun  >>> android.example.com.tflitecamerademo <<<
01-17 11:56:52.191 10923-10923/? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x0   0000000000000000  x1   0000000000002a81  x2   0000000000000006  x3   0000000000000008
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x4   0000007e81515040  x5   0000007e815166c0  x6   0000ffffffffffff  x7   ffffffffffffffff
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x8   0000000000000083  x9   ffffffffffffffdf  x10  0000000000000000  x11  ffffffffffffffff
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x12  0000000000000000  x13  ffffffffffff0000  x14  00000000000002e0  x15  000000000000044d
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x16  0000007e9533aed0  x17  0000007e952e29f4  x18  0000000000000001  x19  0000007e817524f8
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x20  0000000000000006  x21  0000007e81752450  x22  000000000000000b  x23  0000007e858340f0
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x24  0000007e817524e8  x25  0000000000000000  x26  0000000000000080  x27  0000007e81516740
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x28  0000000000000001  x29  0000007e81750730  x30  0000007e952dfd14
01-17 11:56:52.191 10923-10923/? A/DEBUG:     sp   0000007e81750710  pc   0000007e952e29fc  pstate 0000000060000000
01-17 11:56:52.198 10923-10923/? A/DEBUG: backtrace:
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #00 pc 000000000006f9fc  /system/lib64/libc.so (tgkill+8)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #01 pc 000000000006cd10  /system/lib64/libc.so (pthread_kill+64)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #02 pc 0000000000025078  /system/lib64/libc.so (raise+24)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #03 pc 000000000001cc04  /system/lib64/libc.so (abort+52)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #04 pc 00000000000881a0  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #05 pc 0000000000071ce4  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #06 pc 00000000000707fc  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #07 pc 000000000007f99c  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #08 pc 0000000000011c5c  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+1628)
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #09 pc 0000000000384abc  /data/app/android.example.com.tflitecamerademo-1/oat/arm64/base.odex (offset 0x329000)
```

This error doesn't seem to be related to added preprocessing layer, but without adding preprocessing layer, no error occurs and app can run.
",0,,2,2018-01-17T05:09:08Z,NONE,2018-01-17T06:53:47Z
16177,"RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6   return f(*args, **kwds)",stat:awaiting response,"### System information
- **OS Platform and Distribution :**  Linux Ubuntu 17.10
- **TensorFlow installed from :**  Anaconda [followed this tutorial](https://www.tensorflow.org/install/install_linux#InstallingAnaconda)
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: Python 3.6.4 :: Anaconda, Inc.
- **CUDA/cuDNN version**: not using GPU version
- **GPU model and memory**: 2GB GT720

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**result :** 
/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
v1.4.0-19-ga52c8d9 1.4.1

### Describe the problem
Followed Official tensorflow documentation to install tensorflow on Ubuntu 17.10, python3 (python 3.6) and with CPU support. Used conda environment. Followed [this](https://www.tensorflow.org/install/install_linux#InstallingAnaconda) and in the 4th step this is the command I used `pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.1-cp36-cp36m-linux_x86_64.whl`
it installed successfully. But when I try to import tensorflow in python I'm getting this error.

```
/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)

```

### Source code / logs
Activate Conda environment
`source activate tensorflow`

**command :** `python`
**log :** 
```
Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) 
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
```

**command :** `import tensorflow`
**log :**
```
/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
```


Why can't I use the tensorflow for python 3.6 in python 3.6 . How to fix this ?",0,,2,2018-01-17T05:05:21Z,NONE,2018-01-17T12:59:46Z
16176,4d55397500 patch 1,"awaiting testing (then merge),cla: yes","Provide a practical meaning for the `pos_weights` parameter.

The current  weighted_cross_entropy_with_logits docs don't explain practically the relationship of
`pos_weights > 1`,  `pos_weights < 1` to precision, recall, and class imbalance.
",1,,2,2018-01-17T04:52:42Z,CONTRIBUTOR,2018-01-22T21:30:49Z
16173,Add C++ toolchain for portable Linux builds,"awaiting review,cla: yes,kokoro:run",See #15777,1,,12,2018-01-17T02:10:01Z,MEMBER,2018-01-17T22:35:41Z
16172,Merge changes from r1.5 into master,"awaiting testing (then merge),cla: yes","This change picks up the commits exclusive to the r1.5 branch and puts them back into master.

There were a bunch of merge conflicts here. I favored master in most cases except those to do with obvious versioning differences.

I'm not sure if I did the merge correctly, considering there are a great many CLs presented here.",0,,3,2018-01-17T00:46:21Z,MEMBER,2018-01-18T01:00:26Z
16167,Documentation Method Templates Improvement,type:docs,"### System information
N/A

### Describe the problem
The method/class templates in documentation should include a full, functioning path to the method instead of just truncating to the method's name.

I.e. this is what we have at present (bad): 
<img width=""399"" alt=""screen shot 2018-01-16 at 2 23 08 pm"" src=""https://user-images.githubusercontent.com/9597721/35007940-0511d55c-fac9-11e7-9d0c-4be2db021533.png"">

This is a more practical and copy/paste-friendly version:
<img width=""426"" alt=""screen shot 2018-01-16 at 2 22 49 pm"" src=""https://user-images.githubusercontent.com/9597721/35007976-2970cdc2-fac9-11e7-80b8-0ec1e2334734.png"">

I'm constantly just grabbing method templates, pasting to my text editor and then coming back to docs to copy/paste the package path which is now the header of the page; which is an awful workflow.

### Source code / logs
N/A",1,,5,2018-01-16T19:28:06Z,NONE,2018-01-16T22:44:48Z
16165,Error when building from source Fedora 27 CUDA 9.1,type:build/install,"### System information
- OS Platform and Distribution: Fedora 27
- TensorFlow installed from (source or binary): binary
- TensorFlow version: r1.4
- Python version: 3.6.3
- Bazel version: 0.8.1
- GCC/Compiler version: 7.2.1
- CUDA/cuDNN version: CUDA 9.1 cuDNN 7.0.5
- **GPU model and memory**: NVidia Geforce GTX 960 4GB
- Exact command to reproduce: bazel build -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
So I'm attempting to build tensorflow from source on fedora with the version of CUDA and cuDNN I already had installed to avoid have to also install an older version. The build however errors with the following message:

```
ERROR: .cache/bazel/_bazel_xd009642/f9f5dea1a139b69420e1045d339dda45/external/nccl_archive/BUILD:33:1: error while parsing .d file: /home/xd009642/.cache/bazel/_bazel_xd009642/f9f5dea1a139b69420e1045d339dda45/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/external/nccl_archive/_objs/nccl/external/nccl_archive/src/all_reduce.cu.pic.d (No such file or directory)
<command-line>:0:15: warning: ISO C++11 requires whitespace after the macro name
<command-line>:0:1: error: macro names must be identifiers
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 0.392s, Critical Path: 0.12s
FAILED: Build did NOT complete successfully
```
I also tried the command `bazel build --config=opt --config=cuda --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package` from [here](http://www.python36.com/install-tensorflow141-gpu/) with the same end result.

Any guidance is appreciated as this is my first time using bazel (and also trying to compile tensorflow).",0,,11,2018-01-16T18:21:58Z,NONE,2018-01-16T22:49:02Z
16164,"Accidentally cancelled inceptionV3 during install, now can't install at all",,"Hello,
i was setting up tensorflow for image classification, and after i ran : 

python -m scripts.retrain \
  --bottleneck_dir=tf_files/bottlenecks \
  --model_dir=tf_files/models/""${ARCHITECTURE}"" \
  --summaries_dir=tf_files/training_summaries/""${ARCHITECTURE}"" \
  --output_graph=tf_files/retrained_graph.pb \
  --output_labels=tf_files/retrained_labels.txt \
  --architecture=""${ARCHITECTURE}"" \
  --image_dir=tf_files/flower_photos

It automatically started installing inception, i realized that i needed to change some options so i cancelled the install of inception.
Now i believe that i have a half install that doesn't let me install the full package or use the half package.

I may be wrong, but any suggestions would be appreciated.
FYI: i've run :
pip install inception, to which i receive a ""python setup.py egg_info"" failed with error code 1 in {my local/temp dir}

I also just tried running the scripts.retrain again, to which i receive a ""EOFError: compressed file ended before the end-of-stream marker was reached""

Running on Windows 7",1,,5,2018-01-16T17:28:24Z,NONE,2018-01-17T01:01:27Z
16161,tf.case raising IllegalArgumentError 'None of the conditions evaluated as True' when used with Dataset,"stat:awaiting tensorflower,type:bug/performance","### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 64bit
- **TensorFlow installed from (source or binary)**:
via pip
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
3.5

When I use tf.case within a tf.data.Dataset, I get an IllegalArgumentError 'None of the conditions evaluated as True'. However, when I use the same code without the Dataset, it works fine. Furthermore, if I understand the error message correctly, it already tells me that one condition evaluated to true (see the end of the first line):

```
InvalidArgumentError (see above for traceback): assertion failed: [None of the conditions evaluated as True. Conditions: (Equal_3:0, Equal_4:0, Equal_5:0), Values:] [1 0 0]
	 [[Node: case/If_0/Assert_1/AssertGuard/Assert = Assert[T=[DT_STRING, DT_BOOL], summarize=3](case/If_0/Assert_1/AssertGuard/Assert/Switch, case/If_0/Assert_1/AssertGuard/Assert/data_0, case/If_0/Assert_1/AssertGuard/Assert/Switch_1)]]
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](OneShotIterator)]]
```

The following code reproduces the issue:
```
import tensorflow as tf


def random_map(i):
	random_int = tf.random_uniform([], minval=0, maxval=3, dtype=tf.int32)
	random_int = tf.Print(random_int, [random_int, tf.equal(random_int, 0), tf.equal(random_int, 1), tf.equal(random_int, 2)], 'random_int')

	result = tf.case([
		(tf.equal(random_int, 0), lambda: i * 10000),
		(tf.equal(random_int, 1), lambda: i * 20000),
		(tf.equal(random_int, 2), lambda: i * 30000)
	], exclusive=True)

	return result


print('working =========================================================================')
with tf.Session() as sess:
	input_pl = tf.placeholder(dtype=tf.int32)
	result = random_map(input_pl)
	for i in range(5):
		result_value = sess.run(result, feed_dict={input_pl: i})
		print(result_value)

print('not working =====================================================================')
with tf.Session() as sess:
	dataset = tf.data.Dataset.from_tensor_slices(tf.range(5))
	dataset = dataset.map(random_map)
	iterator = dataset.make_one_shot_iterator()
	next_result = iterator.get_next()

	for i in range(5):
		result_value = sess.run(next_result)
		print(result_value)
```

I also found [this question]( http://www.programfaqs.com/faq/tensorflow-case-error-invalid-argument-assertion-failed-none-of-the-conditions-evaluated-as-true/), which seems to be the same problem.",0,,3,2018-01-16T14:01:26Z,CONTRIBUTOR,2018-01-16T22:56:02Z
16160,tf.contrib.lookup.HashTable(kv_initializer) does not work in eager mode.,"comp:eager,stat:awaiting tensorflower,type:bug/performance","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------
```
== cat /etc/issue ===============================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64
Mac OS X 10.11.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 7.3.0 (clang-703.0.31)
Target: x86_64-apple-darwin15.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.11.2)
protobuf (3.4.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named tensorflow

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib:

== nvidia-smi ===================================================
/Users/xiaoyun/tf14py3/bin/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

== cat /etc/issue ===============================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64
Mac OS X 10.11.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 7.3.0 (clang-703.0.31)
Target: x86_64-apple-darwin15.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.11.2)
protobuf (3.4.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.5.0-rc1
tf.GIT_VERSION = v1.5.0-rc0-9-gf9472619f6
tf.COMPILER_VERSION = v1.5.0-rc0-9-gf9472619f6
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib:

== nvidia-smi ===================================================
/Users/xiaoyun/tf14py3/bin/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.5.0-rc0-9-gf9472619f6 1.5.0-rc1

### Describe the problem
Can not use hashtable in eager mode.

### Source code / logs
```
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

keyfile = ""./key_dict""
kv_initializer = tf.contrib.lookup.TextFileInitializer(
    keyfile, tf.string, 0, tf.int64, 1, delimiter=""\t"")
table = tf.contrib.lookup.HashTable(kv_initializer, 0)
table.init.run()

filenames = [""./data1""]
dataset = tf.data.TextLineDataset(filenames)
#dataset = dataset.map(lambda tkns:table.lookup(tkns))
for x in tfe.Iterator(dataset):
    print(x)

Traceback (most recent call last):
  File ""torch/textline.py"", line 13, in <module>
    table = tf.contrib.lookup.HashTable(kv_initializer, 0)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py"", line 282, in __init__
    super(HashTable, self).__init__(table_ref, default_value, initializer)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py"", line 168, in __init__
    self._init = initializer.initialize(self)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py"", line 531, in initialize
    if constant_op.is_constant(filename):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 224, in is_constant
    op = tensor_or_op.op
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 825, in op
    raise AttributeError(""op not supported for Eager Tensors."")
AttributeError: op not supported for Eager Tensors.
```",1,,5,2018-01-16T13:11:57Z,NONE,2018-01-16T18:06:37Z
16158,GAN model: move generated and real operations under discriminator namespace,"awaiting testing (then merge),cla: yes","Hi everybody,

`gan_model` runs the discriminator on both the generated and real data. This PR changes/fixes the namespace of the generated graph.

* Current: The network variables and operations on generated data are in the `Discriminator` namespace but the operations on real data are in the `Discriminator_1` namespace.
* PR: The network variables stay in the `Discriminator` namespace. Operations on generated data are in `Discriminator/generated` and operations on real data are in `Discriminator/real`.

`gan_model` only searches the `Discriminator` namespace for regularization. Presumably, if you were running activity regularization in your discriminator, only the part on generated data would be picked up. Plus, the graph looks much better visually this way and you can tell which discriminator is which.

Cheers",1,,2,2018-01-16T11:36:00Z,CONTRIBUTOR,2018-01-16T11:40:07Z
16157,Update rules_closure to fix bazel version check,cla: yes,Related https://github.com/bazelbuild/bazel/issues/4425#issuecomment-357681237,0,,2,2018-01-16T11:31:14Z,MEMBER,2018-01-16T11:43:16Z
16153,New features: tf.alphas and tf.alphas_like - Related to #16128,"API review,awaiting review,cla: yes","This PR is related to the issue: #16128

#### I send my work here for peer reviewing and discussion. Please do not merge now.

### A few interrogations before merging

1. Are the names I have chosen fine with everyone or you would like it to be changed to something else ?
2. Do my implementations seem fine ?
3. What kind of tests should I implement ? Where shall I put them ?
4. Is it a good idea to replace the function body of tf.ones/tf.zeros and tf.ones_like/tf.zeros_like by a function call to tf.alphas and tf.alphas_like ? Not doing it would lead to code duplication, however I would understand that you might be reluctant, these functions are at the core of the library.

### Why I created these functions ?

I oftenly need to create similar tensors with a non-zero/one value. A simple example would be cost functions in GANs with *label smoothing* applied. 

As stated by @facaiy in #16128, I could use : 
```python
b1 = tf.ones_like(a, dtype=tf.float32) * 0.9 # Tensor full of 0.9
b2 = tf.ones_like(a, dtype=tf.int32) * 2 # Tensor full of 2
b4 = tf.ones_like(a, dtype=tf.bool) # Tensor full of True
```
 However, as shown in my later comments in the issue, the method implemented in this PR is almost twice as fast.

In a wider view, I think that using a single function more *generic* is always a good thing whenever it is possible.

### How does the function API work ?

In a very similar manner than the existing ones: 

```python
import tensorflow as tf

a = tf.constant([
    [
        [4, 5, 6],
        [1, 2, 3]
    ],
    [
        [4, 5, 6],
        [1, 2, 3]
    ]
])

b1 = tf.alphas_like(a, 0.5431)
b2 = tf.alphas_like(a, 5)
b3 = tf.alphas_like(a, -5)
b4 = tf.alphas_like(a, True)

with tf.Session() as sess:
    _b1, _b2, _b3, _b4 = sess.run([b1, b2, b3, b4])
    
print(""b1:"", _b1)
print(""b2:"", _b2)
print(""b3:"", _b3)
print(""b4:"", _b4)

############### OUTPUTS ###############

>>> b1: [
  [
    [ 0.5431  0.5431  0.5431]
    [ 0.5431  0.5431  0.5431]
  ]
  [
    [ 0.5431  0.5431  0.5431]
    [ 0.5431  0.5431  0.5431]
  ]
]

>>> b2: [
  [
    [5 5 5]
    [5 5 5]
  ]
  [
    [5 5 5]
    [5 5 5]
  ]
]

>>> b3: [
  [
    [-5 -5 -5]
    [-5 -5 -5]
  ]
  [
    [-5 -5 -5]
    [-5 -5 -5]
  ]
]

>>> b4: [
  [
    [ True  True  True]
    [ True  True  True]
  ]
  [
    [ True  True  True]
    [ True  True  True]
  ]
]
```

---------------------------

I'm of course free for discussion over video-calls. It's the first time I try to make a change at the core of TF, and I'm quite afraid of breaking everything ;) Thanks for your help btw.

All the best,

Jonathan DEKHTIAR",1,,6,2018-01-16T10:12:45Z,CONTRIBUTOR,2018-01-22T09:52:58Z
16151,ValueError: Labels are incompatible with given information. ,,"Have I written custom code: yes
OS: Windows 8.1
Tensorflow installed from: conda
Tensorflow version: 1.4

I am having problems in adding validation monitors to `Estimator.fit`. With this code I have:

```
def main(_):
    image_paths, labels = dataset_utils.read_dataset_list('../test/dummy_labels_file.txt')
    data_dir = ""../test/dummy_data/""
    images = dataset_utils.read_images(data_dir=data_dir, image_paths=image_paths, image_extension='png')
    print('Done reading images')
    images = dataset_utils.resize(images, (1596, 48))
    images = dataset_utils.transpose(images)
    labels = dataset_utils.encode(labels)
    x_train, x_test, y_train, y_test = dataset_utils.split(features=images, test_size=0.5, labels=labels)
    print(x_test)
    x_train_seq_lens = dataset_utils.get_seq_lens(x_train)
    x_test_seq_lens = dataset_utils.get_seq_lens(x_test)

    train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""x"": np.array(x_train),
           ""seq_lens"": np.array(x_train_seq_lens)},
        y=np.array(y_train),
        num_epochs=1,
        shuffle=True,
        batch_size=1
    )

    validation_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""x"": np.array(x_test),
           ""seq_lens"": np.array(x_test_seq_lens)},
        y=np.array(y_test),
        shuffle=True
    )

    validation_monitor = learn.monitors.ValidationMonitor(
        input_fn=validation_input_fn,
        every_n_steps=1
    )

    model = GridRNNModelFn(num_time_steps=1596, num_features=48, num_hidden_units=128, num_classes=80,
                           learning_rate=0.001, optimizer=Optimizers.MOMENTUM)

    classifier = learn.Estimator(model_fn=model.model_fn, params=model.params, model_dir=""/tmp/grid_rnn_ocr_model"")
    classifier.fit(input_fn=train_input_fn, monitors=[validation_monitor])


if __name__ == '__main__':
    tf.app.run(main=main)
```

It throws this error:

`ValueError: Labels are incompatible with given information. Given labels: Tensor(""random_shuffle_queue_DequeueUpTo:3"", shape=(?, 37), dtype=int32), required signatures: TensorSignature(dtype=tf.int32, shape=TensorShape([Dimension(None), Dimension(33)]), is_sparse=False).`

Which leads me to think that the dynamic label lengths are not accepted. To reproduce this, simply clone this [repository](https://github.com/selcouthlyBlue/simplified_bi_lstm_ocr) and run the script specified in the readme.",0,,1,2018-01-16T08:43:19Z,CONTRIBUTOR,2018-01-16T09:31:29Z
16148,non_max_suppression is on CPU?,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
     Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
      Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
      binary(By pip)
- **TensorFlow version (use command below)**:
      1.4.1
- **Python version**: 
      3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
    8.0.61/6.0.21
- **GPU model and memory**:
    GTX 1080 Ti, 11172MiB
- **Exact command to reproduce**:
     python main.py

### Describe the problem
    
I train my RFCN by tensorflow. My project need very high speed. So I use the profile and I find that non_max_suppression is on CPU? Is there a GPU version?I think if you calculate all pairs of boxes IOU first, then just for-loop once will ultimately boost speed, there have some trick in it, just see the source code in [https://github.com/rbgirshick/py-faster-rcnn/tree/master/lib/nms](https://github.com/rbgirshick/py-faster-rcnn/tree/master/lib/nms). I think cuda version of NMS is faster than CPU version.
",0,,2,2018-01-16T08:21:02Z,NONE,2018-01-16T13:30:51Z
16145,Decoding contents of BMP file on big endian,"awaiting testing (then merge),cla: yes","As the BMP file contents are encoded in little endian format, added byte swapping for reading the various header components correctly on big endian.",1,,2,2018-01-16T06:29:25Z,CONTRIBUTOR,2018-01-25T19:04:02Z
16144,Is it possible to train CNN model by using tensorflow JAVA API?,,"Hello, TF.
I have plane to train my CNN model by using tensorflow JAVA API.
I got success on simple model( with a simple matmul operation between weights and bias)
BUT I failed to train CNN model.
",0,,2,2018-01-16T06:15:29Z,NONE,2018-01-16T07:20:46Z
16143,"Undefined symbol ""_ZN3Aws8Security14SecureMemClearEPhj""",stat:awaiting response,"compiled tensorflow r.15 from source , when import tensorflow in python got following error:
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/_pywrap_tensorflow_internal.so: Undefined symbol ""_ZN3Aws8Security14SecureMemClearEPhj""


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace

thanks in advance !!!",0,,2,2018-01-16T06:04:50Z,NONE,2018-01-16T19:01:24Z
16142,fix typo,"awaiting testing (then merge),cla: yes",,2,,4,2018-01-16T05:36:10Z,CONTRIBUTOR,2018-01-16T05:41:07Z
16135,Distributed Tensorflow  using MPI,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

I have tried stackflow and Google group discussion forum but could  get any reply or comment

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Redhat 7.4

- **TensorFlow installed from (source or binary)**:
from source with MPI
- **TensorFlow version (use command below)**:
1.41
- **Python version**: 
2.7.14
- **Bazel version (if compiling from source)**:

- **GCC/Compiler version (if compiling from source)**:
GCC 6.0
- **CUDA/cuDNN version**:
8.0/6.5
- **GPU model and memory**:
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K20Xm         Off  | 00000000:08:00.0 Off |                    0 |
| N/A   34C    P0    61W / 235W |      0MiB /  5699MiB |     72%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I am using the following  script to launch distributed computing.


#! /bin/bash

module load openmpi/3.0.0-gnu

host=$(hostname -s)
if [[ $host == ""node06"" ]]; then
        echo ""statring Node 6""
        python tf_dis_2.py --job_name=""ps"" --task_index=0
elif [[ $host == ""node07"" ]]; then
        echo ""starting Node 7 as worker""
        python tf_dis_2.py --job_name=""worker"" --task_index=0
elif [[ $host == ""node08"" ]]; then
        echo ""starting Node 8 as worker""
        python tf_dis_2.py --job_name=""worker"" --task_index=1
fi

-----

I am running it on slurm  with three nodes.

srun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh

I am using MPI instead of GPRC.

I am getting the following message:

---------------------------------------------------
srun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh
statring Node 6
starting Node 8 as worker
starting Node 7 as worker
2018-01-15 11:34:59.961617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732
pciBusID: 0000:08:00.0
totalMemory: 5.57GiB freeMemory: 5.49GiB
2018-01-15 11:34:59.961674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
E0115 11:35:00.020327488   36133 ev_epoll1_linux.c:1051]     grpc epoll fd: 22
2018-01-15 11:35:00.026716: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> node06:2222}
2018-01-15 11:35:00.026760: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> node07:2223, 1 -> localhost:2224}
2018-01-15 11:35:00.029261: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2224
2018-01-15 11:35:00.439045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732
pciBusID: 0000:08:00.0
totalMemory: 5.57GiB freeMemory: 5.49GiB
2018-01-15 11:35:00.439124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
E0115 11:35:00.497022377   13701 ev_epoll1_linux.c:1051]     grpc epoll fd: 22
2018-01-15 11:35:00.503585: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}
2018-01-15 11:35:00.503622: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> node07:2223, 1 -> node08:2224}
2018-01-15 11:35:00.505803: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222
2018-01-15 11:33:39.681311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732
pciBusID: 0000:08:00.0
totalMemory: 5.57GiB freeMemory: 5.49GiB
2018-01-15 11:33:39.681375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
E0115 11:33:39.739196190   46236 ev_epoll1_linux.c:1051]     grpc epoll fd: 22
2018-01-15 11:33:39.745655: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> node06:2222}
2018-01-15 11:33:39.745697: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> node08:2224}
2018-01-15 11:33:39.747692: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223
Abid Malik
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
Variables initialized ...
Traceback (most recent call last):
  File ""tf_dis_2.py"", line 102, in <module>
    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=""/tmp/train_logs"",global_step=global_step,init_op=init_op)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 336, in __init__
    self._verify_setup()
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 885, in _verify_setup
    ""their device set: %s"" % op)
ValueError: When using replicas, all Variables must have their device set: name: ""weights/Variable""
op: ""VariableV2""
attr {
  key: ""container""
  value {
    s: """"
  }
}
attr {
  key: ""dtype""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""shape""
  value {
    shape {
      dim {
        size: 784
      }
      dim {
        size: 100
      }
    }
  }
}
attr {
  key: ""shared_name""
  value {
    s: """"
  }
}

2018-01-15 11:33:41.719083: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: Endpoint read failed
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
Variables initialized ...
Traceback (most recent call last):
  File ""tf_dis_2.py"", line 114, in <module>
    with sv.prepare_or_wait_for_session(server.target) as sess:
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 708, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 273, in prepare_session
    config=config)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 205, in _restore_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1666, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnavailableError: Endpoint read failed
srun: error: node08: task 2: Exited with exit code 1
srun: error: node07: task 1: Exited with exit code 1
---------------------------------------------------------------------------------

Why is it crashing? I have been trying to solve this for the last three weeks by putting it on different forums and groups. However, could not get any reply. I would be grateful if someone can guide me. I apologize in advance if this is not the right forum.





### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

``
from __future__ import print_function

import tensorflow as tf
import sys
import time


print(""Abid Malik"")


parameter_servers = [""node06:2222""]
workers = [""node07:2223"",""node08:2224""]
cluster = tf.train.ClusterSpec({""ps"":parameter_servers, ""worker"":workers})



tf.app.flags.DEFINE_string(""job_name"", """", ""Either 'ps' or 'worker'"")
tf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")
FLAGS = tf.app.flags.FLAGS





server = tf.train.Server(
    cluster,
    job_name=FLAGS.job_name,
    task_index=FLAGS.task_index)


batch_size = 100
learning_rate = 0.0005
training_epochs = 20
logs_path = ""/tmp/mnist/1""


from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

if FLAGS.job_name == ""ps"":
    server.join()
elif FLAGS.job_name == ""worker"":

        with tf.device(tf.train.replica_device_setter(worker_device=""/job:worker/task:%d"" % FLAGS.task_index,cluster=cluster)):
              
                global_step = tf.get_variable('global_step',[],initializer = tf.constant_initializer(0), trainable = False)

              
        with tf.name_scope('input'):
              
                  x = tf.placeholder(tf.float32, shape=[None, 784], name=""x-input"")
               
                  y_ = tf.placeholder(tf.float32, shape=[None, 10], name=""y-input"")

                
        tf.set_random_seed(1)
        with tf.name_scope(""weights""):
                        W1 = tf.Variable(tf.random_normal([784, 100]))
                        W2 = tf.Variable(tf.random_normal([100, 10]))

               
        with tf.name_scope(""biases""):
                        b1 = tf.Variable(tf.zeros([100]))
                        b2 = tf.Variable(tf.zeros([10]))

               
        with tf.name_scope(""softmax""):
                        # y is our prediction
                        z2 = tf.add(tf.matmul(x,W1),b1)
                        a2 = tf.nn.sigmoid(z2)
                        z3 = tf.add(tf.matmul(a2,W2),b2)
                        y  = tf.nn.softmax(z3)

               
        with tf.name_scope('cross_entropy'):
                        # this is our cost
                        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))

             
        with tf.name_scope('train'):
                       
                                                                                                                                                                                                                                                                

grad_op = tf.train.GradientDescentOptimizer(learning_rate)
                        train_op = grad_op.minimize(cross_entropy, global_step=global_step)


        with tf.name_scope('Accuracy'):
                        # accuracy
                        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
                        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

   
        tf.summary.scalar(""cost"", cross_entropy)
        tf.summary.scalar(""accuracy"", accuracy)

        saver = tf.train.Saver()
       
        summary_op = tf.summary.merge_all()
        init_op = tf.global_variables_initializer()
        print(""Variables initialized ..."")

        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=""/tmp/train_logs"",global_step=global_step,init_op=init_op)


        begin_time = time.time()
        frequency = 100
        with sv.prepare_or_wait_for_session(server.target) as sess:
                # create log writer object (this will log on every machine)
                writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())

                # perform training cycles
                start_time = time.time()
                for epoch in range(training_epochs):

                        # number of batches in one epoch
                        batch_count = int(mnist.train.num_examples/batch_size)

                        count = 0
                        for i in range(batch_count):
                                batch_x, batch_y = mnist.train.next_batch(batch_size)

                                # perform the operations we defined earlier on batch
                                _, cost, summary, step = sess.run([train_op, cross_entropy, summary_op, global_step], feed_dict={x: batch_x, y_: batch_y})
                                writer.add_summary(summary, step)

                                count += 1
                                if count % frequency == 0 or i+1 == batch_count:
                                        elapsed_time = time.time() - start_time
                                        start_time = time.time()
                                        print(""Step: %d,"" % (step+1),
                                                                "" Epoch: %2d,"" % (epoch+1),
                                                                "" Batch: %3d of %3d,"" % (i+1, batch_count),
                                                                "" Cost: %.4f,"" % cost,
                                                                "" AvgTime: %3.2fms"" % float(elapsed_time*1000/frequency))
                                        count = 0


                print(""Test-Accuracy: %2.2f"" % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
                print(""Total Time: %3.2fs"" % float(time.time() - begin_time))
                print(""Final Cost: %.4f"" % cost)

        sv.stop()
        print(""done"")
                                                                                                                                                                                                                                                                 

``",0,,4,2018-01-15T18:05:52Z,NONE,2018-01-21T00:55:39Z
16132,Bug while printing parameters and gradients,type:support,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary (anaconda)
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: using CPU
- **GPU model and memory**: using CPU
- **Exact command to reproduce**: see below

### Describe the problem
The model is very simple, I do digits classification with MNIST. There is only one parameter matrix W, no bias and no non-linearities. The model show convergence since the loss is decreasing. I checked predictions and accuracy but I do not copy paste useless code here. If I print the parameters before and after training they are the same, however, it shouldn't be the case. Moreover, the gradient of the loss w.r.t. parameters are zero but again it shouldn't be the case since the model converges so there should be a non-zero gradient. I cannot explain why and my implementation seems correct, that's why I am posting my code here.

### Source code / logs
```
import numpy as np
import tensorflow as tf

tf.set_random_seed(42)

from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets('data/', one_hot=True)

x = tf.placeholder(tf.float32, shape=(None, 784))
y = tf.placeholder(tf.float32, shape=(None, 10))

W = tf.get_variable('W0', (784, 10))
pred = tf.matmul(x, W)
loss = tf.reduce_sum((y - pred) ** 2)
grads = tf.gradients(loss, W)
train_step = tf.train.AdamOptimizer().minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

print(sess.run(W))

>>> [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605
  -0.01042821]
 [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129
  -0.0796528 ]
 [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736
  -0.04312544]
 ...
 [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174
   0.06578781]
 [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339
  -0.02337921]
 [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324
  -0.02436799]]

for _ in range(1000):
    x_mb, y_mb = mnist.train.next_batch(32)
    loss_, _ = sess.run([loss, train_step], {x: x_mb, y: y_mb})
    print('loss: {:2.5}'.format(loss_))

>>> I won't print uselss log here but the loss is decreasing

print(sess.run(W))

>>> [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605
  -0.01042821]
 [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129
  -0.0796528 ]
 [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736
  -0.04312544]
 ...
 [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174
   0.06578781]
 [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339
  -0.02337921]
 [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324
  -0.02436799]]

print(sess.run(grads, {x: x_mb, y: y_mb}))

>>> [array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)]

```
",0,,1,2018-01-15T16:08:51Z,NONE,2018-01-19T22:26:56Z
16131,Update contrib/HVX readme,"awaiting testing (then merge),cla: yes","I updated the README because of some imprecisions and to add clarifications of what I think will guide the users more appropriately.

First, the very simple ""quick start guide"" doesn't work, there's no ""-X"" option (at least publicly) and so you always need to have the SDK installed manually.

Apart from that, some clarifications and rewording were done to help the users understand what's happening.

/cc @satok16 ",1,,2,2018-01-15T15:59:50Z,CONTRIBUTOR,2018-01-23T18:04:35Z
16130,Fix broken python3 build,"awaiting testing (then merge),cla: yes","Currently building tensorflow master branch with python3 fails with following error message.
```
ERROR: ${BAZEL_CACHE}/external/astor_archive/BUILD:8:1: Converting to Python 3: external/astor_archive/astor/code_gen.py failed (Exit 1).
```
It seems that the 3 newly added `third_party/*.BUILD` scripts from https://github.com/tensorflow/tensorflow/pull/15955/commits/4080654c8f03ec34f2822c14db5fd8b75f63d569 are missing `srcs_version = ""PY2AND3""` part, which all the other py_library modules have.

I'm using bazel 0.5.4 on linux ubuntu 16.04 to build the current master branch.",0,,2,2018-01-15T11:00:22Z,CONTRIBUTOR,2018-01-15T16:09:15Z
16127,"fix default parameters for TimeFreqLSTMCell, fixes #16100",cla: yes,"Resolve #16100 

The default parameters for TimeFreqLSTMCell lead to a division by `None`, which throws an exception.",0,,5,2018-01-15T10:24:06Z,CONTRIBUTOR,2018-01-15T10:26:15Z
16125,Disable stacktrace_handler_test becase stack trace isn't generated on Windows,cla: yes,Fix http://ci.tensorflow.org/job/tf-master-win-bzl/2259/console,0,,3,2018-01-15T08:16:44Z,MEMBER,2018-01-15T08:18:41Z
16124,How can I batch images of arbitrary sizes in tensorflow?,stat:awaiting response,I want to realize arbitrary inputs that I can batch them in one batch.,0,,4,2018-01-15T07:02:24Z,NONE,2018-01-15T18:59:25Z
16119,Created dense_to_sparse in contrib.layers,"awaiting testing (then merge),cla: yes",Added `dense_to_sparse`. This does the conversion of dense labels into sparse ones to be passed into the core ctc_loss function. Addresses feature request https://github.com/tensorflow/tensorflow/issues/15985,1,,7,2018-01-15T04:41:50Z,CONTRIBUTOR,2018-01-23T00:14:50Z
16118,Minor improvements to TFRecord format docs,"awaiting testing (then merge),cla: yes","The TFRecord format documentation mentions that hashes are computed using a CRC32, but doesn't mention the polynomial used. I added that detail, so the documentation is now sufficient for a developer trying to write a parser / writer for (uncompressed) TFRecord files.",1,,3,2018-01-15T01:26:53Z,CONTRIBUTOR,2018-01-22T23:12:57Z
16114,Update maxout.py,"awaiting testing (then merge),cla: yes,stat:awaiting response",Specify the final number of features in the maxout axis,1,,7,2018-01-14T22:37:39Z,NONE,2018-01-29T21:51:45Z
16113,Propagate the error string of GIF processing for decode_gif,"cla: yes,stat:awaiting response","This fix tries to improve the error thrown by `decode_gif` to include the error string generated by GIF processing.

Previously, the error was not very indicative as the error string
returned by GIF processing was hidden:
```
..........
InvalidArgumentError (see above for traceback): Invalid GIF data, size 2091369
	 [[Node: DecodeGif = DecodeGif[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile)]]
```

This fix propagate the error string (`can't process optimized gif`) to be part of the `InvalidArgumentError`:
```
InvalidArgumentError (see above for traceback): Invalid GIF data (size 2091369), can't process optimized gif
         [[Node: DecodeGif = DecodeGif[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile)]]
```

This fix fixes #15838.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,1,2018-01-14T18:39:01Z,MEMBER,2018-01-23T00:20:48Z
16108,No tf.metrics.true_negatives,,"**TensorFlow version**: 1.4.1

Is there any particular reason for why there is no `tf.metrics.true_negatives` method? I know it's simple to calculate from other confusion metrics that are available, but I was wondering why the developers chose to let this one method out.",1,,5,2018-01-14T09:47:59Z,NONE,2018-01-14T18:55:05Z
16106,Eager: Invalid placement of vars/consts depending on their types and not the tf.device,,"Hi,
I'm currently testing eager execution on TF 1.5.0-rc1 (built it with XLA and CUDA enabled) and observe strange behavior: variables and constants get created either on GPU or CPU depending on their types, and not `with tf.device(...):` block. Moreover, on creation of int32 variable it fails completely. For example, when I run the following code

```
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

print('TensorFlow version:', tf.__version__)

with tf.device('/gpu:0'):
    A = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)
    print('Const A is placed on:', A.device)

    B = tf.constant([1, 2, 3], dtype=tf.int32)
    print('Const B is placed on:', B.device)

    C = tfe.Variable([1.0, 2.0, 3.0], dtype=tf.float32)
    print('Variable C is placed on:', C.device)

    D = tfe.Variable([1, 2, 3], dtype=tf.int32)
    print('Variable D is placed on:', D.device)
```

I get the following output:

```
TensorFlow version: 1.5.0-rc1
2018-01-14 01:16:06.385929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-14 01:16:06.386198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.91GiB freeMemory: 363.06MiB
2018-01-14 01:16:06.386223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Const A is placed on: /job:localhost/replica:0/task:0/device:GPU:0
Const B is placed on: CPU:0
Variable C is placed on: /job:localhost/replica:0/task:0/device:GPU:0
Traceback (most recent call last):
  File ""tf_bug.py"", line 18, in <module>
    D = tfe.Variable([1, 2, 3], dtype=tf.int32)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 277, in __init__
    constraint=constraint)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 422, in _init_from_args
    graph_mode=self._in_graph_mode)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 53, in _eager_safe_variable_handle
    container=container)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py"", line 396, in var_handle_op
    attrs=_attrs, ctx=_ctx, name=name)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'VarHandleOp' OpKernel for GPU devices compatible with node VarHandleOp = VarHandleOp[container=""eager-execution-2/"", dtype=DT_INT32, shape=[3], shared_name=""11""]()
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; dtype in [DT_VARIANT]
  device='GPU'; dtype in [DT_COMPLEX128]
  device='GPU'; dtype in [DT_COMPLEX64]
  device='GPU'; dtype in [DT_BOOL]
  device='GPU'; dtype in [DT_DOUBLE]
  device='GPU'; dtype in [DT_FLOAT]
  device='GPU'; dtype in [DT_HALF]
  device='CPU'
  device='XLA_GPU'
  device='XLA_CPU'
 [Op:VarHandleOp] name: Variable/
```

As you can see, the constants and variables get placed either on GPU:0 or CPU:0 despite all of them gathered inside the same `tf.device('/gpu:0')` block.",0,,14,2018-01-13T20:28:29Z,NONE,2018-01-14T06:54:09Z
16103,No OpKernel was registered to support Op 'AssignVariableOp' with DT_BFLOAT16,"stat:awaiting response,type:support","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch linux
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.5.0-rc1
- **Python version**: NA (go bindings)
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 7.2.1
- **CUDA/cuDNN version**: na (CPU)
- **GPU model and memory**: na
- **Exact command to reproduce**: See below

### Describe the problem
`AssignVariableOp` does not appear to appear to have a kernel for `DT_BFLOAT16`.


### Source code / logs
```
package main

import (
	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func main() {
	s := op.NewScope()
	bfloat := op.Cast(s, op.Const(s, float32(0.1234)), tf.Bfloat16)
	variable := op.VarHandleOp(s, tf.Bfloat16, tf.ScalarShape())
	init := op.AssignVariableOp(s, variable, bfloat)

	graph, err := s.Finalize()
	if err != nil {
		panic(err)
	}
	sess, err := tf.NewSession(graph, nil)
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{init})
	if err != nil {
		panic(err)
	}
}
```
```
go run bfloat_demo.go 
panic: No OpKernel was registered to support Op 'AssignVariableOp' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; dtype in [DT_VARIANT]
  device='CPU'; dtype in [DT_QINT32]
  device='CPU'; dtype in [DT_QUINT8]
  device='CPU'; dtype in [DT_QINT8]
  device='CPU'; dtype in [DT_RESOURCE]
  device='CPU'; dtype in [DT_STRING]
  device='CPU'; dtype in [DT_BOOL]
  device='CPU'; dtype in [DT_COMPLEX128]
  device='CPU'; dtype in [DT_COMPLEX64]
  device='CPU'; dtype in [DT_DOUBLE]
  device='CPU'; dtype in [DT_FLOAT]
  device='CPU'; dtype in [DT_HALF]
  device='CPU'; dtype in [DT_INT8]
  device='CPU'; dtype in [DT_UINT8]
  device='CPU'; dtype in [DT_INT16]
  device='CPU'; dtype in [DT_UINT16]
  device='CPU'; dtype in [DT_INT32]
  device='CPU'; dtype in [DT_INT64]

	 [[Node: AssignVariableOp = AssignVariableOp[dtype=DT_BFLOAT16](VarHandleOp, Cast)]]

goroutine 1 [running]:
main.main()
	/home/isaac/go/src/github.com/is8ac/gotf/bfloat_demo.go:24 +0x250
exit status 2
```",0,,2,2018-01-13T17:45:32Z,NONE,2018-01-19T22:34:16Z
16101,Add stream selection support for `tf.contrib.ffmpeg.decode_audio`,"awaiting review,cla: yes,stat:awaiting tensorflower","This fix tries to address the issue raised in #16073 where it was not possible to selectively decode a perticular stream with `tf.contrib.ffmpeg.decode_audio`.
This fix adds an additional attribute `stream` which could be used to specify the stream to decode (e.g., `stream=""1""`). By default `stream=""""` which leaves the decision to ffmpeg.

This fix fixes #16073.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,11,2018-01-13T15:50:52Z,MEMBER,2018-01-13T16:34:12Z
16100,Exception when not providing optional parameter frequency_skip in TimeFreqLSTMCell,"stat:contributions welcome,type:bug/performance","### System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below
- OS Platform and Distribution: 
- TensorFlow installed from: `pip3 install --user tensorflow-gpu`
- TensorFlow version: 1.4.1
- Python version: 3.5.2
- CUDA: 8.0
- GPU: NVidia Titan X

### Describe the problem

Using a `TimeFreqLSTMCell` in a `dynamic_rnn` or `static_rcnn` without providing the optional parameter `frequency_skip` results in an exception:

```
TypeError: unsupported operand type(s) for /: 'int' and 'NoneType'
```

The line which throws this exception is https://github.com/tensorflow/tensorflow/blob/8b78c23c161c9d0bec462d5f4c73f0fca413bc8b/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L474-L475
`frequency_skip` has it's default value `None` here.

Maybe the default should be changed to `1`?

### Source code / logs

Sadly I am not allowed to share my full source code. However, this is how I create the RNN layers:

```
lstmcell = tf.contrib.rnn.TimeFreqLSTMCell(lstm_input.shape.as_list()[2], forget_bias = self.lstm_forget_bias, feature_size = lstm_input_rev.shape.as_list()[2])
                
stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstmcell] * self.layers_lstm)
                
lstm_output, lstm_state = tf.nn.dynamic_rnn(stacked_lstm, lstm_input_rev, dtype=""float32"", time_major=True)
```",1,,1,2018-01-13T12:20:41Z,CONTRIBUTOR,2018-01-13T15:02:35Z
16099,Make srcd in variable,"awaiting testing (then merge),cla: yes",,0,,3,2018-01-13T10:00:11Z,CONTRIBUTOR,2018-01-16T21:06:32Z
16096,Address bad merge in Java install instructions,cla: yes,,0,,2,2018-01-13T07:25:31Z,MEMBER,2018-01-13T08:07:42Z
16094,Shape must be rank 1 but is rank 0 for 'CTCLoss' (op: 'CTCLoss'),stat:awaiting response,"Have I written custom code: yes
OS: Windows 8.1
Tensorflow installed from: conda
Tensorflow version: 1.4


I've successfully converted a Tensor into a SparseTensor with this code:

```
def dense_to_sparse(dense_tensor, out_type):
    indices = tf.where(tf.not_equal(dense_tensor, tf.constant(0, dense_tensor.dtype)
    values = tf.gather_nd(dense_tensor, indices)
    shape = tf.shape(dense_tensor, out_type=out_type)
    return tf.SparseTensor(indices, values, shape)
```

I want to try out using a `SparseTensor` converted from a dense one: 

```
input_layer = tf.placeholder(tf.float32, [None, 1596, 48])
dense_labels = tf.placeholder(tf.int32)
sparse_from_dense = dense_to_sparse(dense_lables, out_type=tf.int64)
cell_fw = grid_rnn.Grid2LSTMCell(num_units=128)
cell_bw = grid_rnn.Grid2LSTMCell(num_units=128)
bidirectional_grid_rnn = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, input_layer, dtype=tf.float32)
outputs = tf.reshape(bidirectional_grid_rnn[0], [-1, 256])

W = tf.Variable(tf.truncated_normal([256, 80], stddev=0.1, dtype=tf.float32), name='W')
b = tf.Variable(tf.constant(0., dtype=tf.float32, shape=[80], name='b'))

logits = tf.matmul(outputs, W) + b
logits = tf.reshape(logits, [tf.shape(input_layer)[0], -1, 80])
logits = tf.transpose(logits, (1, 0, 2))

loss = tf.nn.ctc_loss(inputs=logits, labels=sparse, sequence_length=320)
```

Unfortunately, when I do this, I encounter this error:

`Shape must be rank 1 but is rank 0 for 'CTCLoss' (op: 'CTCLoss') with input shapes: [?,?,80], [?,1], [?], [].`

",0,,3,2018-01-13T04:41:46Z,CONTRIBUTOR,2018-01-13T12:53:28Z
16093,when will we have multi gpu support under eager mode? Pytorch has it.,type:feature,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,3,2018-01-13T03:22:07Z,NONE,2018-01-17T18:50:12Z
16090,MKL-DNN:  fix batchnorm unit test failures,"awaiting testing (then merge),cla: yes","Fix failures of all 9 fuse batchnorm test cases.

handle corner case (empty input tensors)
handle inference case properly - bwd bug related to fwd primitive creation (as a hint)
refactor - moving output tensor allocation to separate methods - to avoid duplicated code",0,,2,2018-01-13T01:02:12Z,CONTRIBUTOR,2018-01-16T16:50:19Z
16088,Disable keras data_utils_test as it's flaky.,"awaiting review,cla: yes",,1,,5,2018-01-12T23:44:10Z,MEMBER,2018-01-12T23:49:57Z
16086,[Intel MKL-DNN] fixes for several MKLDNN unit tests.,"awaiting testing (then merge),cla: yes",Current MKLDNN element wise (add) results in several unit test failure. A temporary workaround is provided by comment out the MKLDNN element wise (add) optimization. ,0,,2,2018-01-12T22:49:26Z,CONTRIBUTOR,2018-01-16T16:48:04Z
16084,Update download_dependencies.sh to prevent crash from 403,"awaiting testing (then merge),cla: yes","The eigen bitbucket seems to have changed causing the scrip to crash with a unrecognized archive error.
Changing to grep -v mirror.bazel seems to fix this because otherwise we get a 403 forbidden error.",1,,2,2018-01-12T22:20:03Z,CONTRIBUTOR,2018-01-23T18:28:46Z
16081,MKL-DNN: fix concat issue related to negative input concat_dim,"awaiting testing (then merge),cla: yes","For a negative concat_dim input, the actual concat_dim should be N + concat_dim with N
being the dims of input tensors.

This PR fixes an issue of setting N properly.",0,,2,2018-01-12T20:40:28Z,CONTRIBUTOR,2018-01-16T16:51:11Z
16079,Branch 181765083,cla: yes,,0,,2,2018-01-12T19:16:43Z,MEMBER,2018-01-12T19:17:35Z
16075,optimize_for_inference_lib.fold_batch_norms() preserves data_format,"awaiting testing (then merge),cla: yes","`fold_batch_norms()` currently breaks graphs containing convolutions using NCHW data format. The function replaces a BiasAdd operation with a new one, while not preserving the data format of the original operation. As a result, the new operation always has NHWC data format, and the execution of the resulting graph fails because of mismatching dimensions.

The proposed resolution is to copy the `data_format` property from the original operation.

The patch fixes https://github.com/tensorflow/tensorflow/issues/15034.",1,,10,2018-01-12T17:07:04Z,CONTRIBUTOR,2018-01-14T12:10:31Z
16073,Feature request: (optionally) return all audio streams in tf.contrib.ffmpeg.decode_audio,type:feature,"I'm trying to read [musdb18](https://sigsep.github.io/musdb.html) with `tf.data` and it comes in the form of mp4 files with multiple audio streams so `ffmpeg -map` is needed.

`tf.contrib.ffmpeg.decode_audio` cannot be configured to choose the audio stream. I wonder if we could have a `tf.contrib.ffmpeg.decode_audios` that returns every audio stream in the file, or if we could have a new argument in `tf.contrib.ffmpeg.decode_audio` for choosing streams.

Being able to choose the audio stream is also important for getting the right language in a movie file's audio, and similarly `tf.contrib.ffmpeg.decode_video` could need the same extension (though multiple video streams is not as common AFAIK).",0,,6,2018-01-12T16:28:55Z,CONTRIBUTOR,2018-01-12T16:30:59Z
16071,fix comments and code matches,"awaiting review,cla: yes",,0,,2,2018-01-12T13:39:17Z,CONTRIBUTOR,2018-01-14T02:23:16Z
16069,Key generator/encoder_8/conv/filter not found in checkpoint,type:support,"I'm using python 3.6.3 win 10 64bit and tensorflow 1.2.1 and now I'm working on https://github.com/datitran/face2face-demo project and **part 4.export model** I'm taking this error:NotFoundError (see above for traceback): Key generator/encoder_8/conv/filter not found in checkpoint 

how can I solve this problem ?

what I run
`C:\Users\hajum>python C:\Users\hajum\Desktop\face2face-demo-master\reduce_model.py --model-input C:\Users\hajum\Desktop\face2face-model --model-output C:\Users\hajum\Desktop\face2face-reduced-model`

same folder names with project but I have my own models

what it shows
```
2018-01-12 13:24:05.337267: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.337407: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.338476: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.338779: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339070: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339369: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339659: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339962: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.779044: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_8/conv/filter not found in checkpoint
2018-01-12 13:24:05.779482: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_2/deconv/filter not found in checkpoint
2018-01-12 13:24:05.779562: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_1/deconv/filter not found in checkpoint
2018-01-12 13:24:05.780339: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_3/deconv/filter not found in checkpoint
2018-01-12 13:24:05.780354: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_3/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.781063: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_2/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.781066: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_3/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.781015: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_2/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.784971: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_8/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.785703: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_4/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.785849: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_4/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.785928: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_4/deconv/filter not found in checkpoint
2018-01-12 13:24:05.787052: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_5/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.787160: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_6/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.787346: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_5/deconv/filter not found in checkpoint
2018-01-12 13:24:05.787687: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_5/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.791739: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_8/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.793255: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_6/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.793508: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_6/deconv/filter not found in checkpoint
2018-01-12 13:24:05.794303: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_7/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.795123: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_7/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.795823: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_8/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.796067: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_7/deconv/filter not found in checkpoint
2018-01-12 13:24:05.797352: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_8/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.798112: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_7/conv/filter not found in checkpoint
2018-01-12 13:24:05.800556: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_1/conv/filter not found in checkpoint
2018-01-12 13:24:05.801703: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_2/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.801868: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_8/deconv/filter not found in checkpoint
2018-01-12 13:24:05.801974: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_2/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.801977: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_2/conv/filter not found in checkpoint
2018-01-12 13:24:05.804154: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_3/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.805983: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_3/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.806160: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_7/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.807834: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_4/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.808628: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_3/conv/filter not found in checkpoint
2018-01-12 13:24:05.808808: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_5/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.809721: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_4/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.809836: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_4/conv/filter not found in checkpoint
2018-01-12 13:24:05.810842: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_5/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.811998: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_5/conv/filter not found in checkpoint
2018-01-12 13:24:05.812062: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_7/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.812846: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_6/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.812889: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_6/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.813195: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_6/conv/filter not found in checkpoint
Traceback (most recent call last):
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1139, in _do_call
    return fn(*args)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1121, in _run_fn
    status, run_metadata)
  File ""C:\Users\hajum\Anaconda3\lib\contextlib.py"", line 88, in __exit__
    next(self.gen)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Key generator/encoder_8/conv/filter not found in checkpoint
         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\hajum\Desktop\face2face-demo-master\reduce_model.py"", line 215, in <module>
    saver.restore(sess, checkpoint)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1548, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 789, in run
    run_metadata_ptr)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 997, in _run
    feed_dict_string, options, run_metadata)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1132, in _do_run
    target_list, options, run_metadata)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key generator/encoder_8/conv/filter not found in checkpoint
         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]

Caused by op 'save/RestoreV2_43', defined at:
  File ""C:\Users\hajum\Desktop\face2face-demo-master\reduce_model.py"", line 213, in <module>
    saver = tf.train.Saver()
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1139, in __init__
    self.build()
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1170, in build
    restore_sequentially=self._restore_sequentially)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 691, in build
    restore_sequentially, reshape)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 407, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 247, in restore_op
    [spec.tensor.dtype])[0])
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 640, in restore_v2
    dtypes=dtypes, name=name)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

NotFoundError (see above for traceback): Key generator/encoder_8/conv/filter not found in checkpoint
         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]
```",0,,4,2018-01-12T11:30:28Z,NONE,2018-01-16T23:01:49Z
16067,[XLA] Separate out the dynamic slice wrapping tests,"awaiting testing (then merge),cla: yes","This is a set of changes to allow disabling of bfloat16 tests, for backends which don't support bfloat16.  Originally it was a change to the same set of tests to allow the wrapping behaviour tests to be disabled.  That change was made obsolete by some parallel work.

---
The original text was:

The XLA documentation says that the behaviour of dynamic slice and dynamic update slice is undefined when the indices wrap.

This separates out the tests which check for wrapping behaviour, so that they can be ignored for backends which don't exhibit the test's expected results.
",0,,5,2018-01-12T09:26:37Z,CONTRIBUTOR,2018-01-15T13:50:52Z
16066,the loss is nan,,"when i training the facenet(build by myself) the loss is normal on the first iteration, but on the second and following iteration ,the loss became nan, i don't know what happened, please help me, Thanks!!!",0,,2,2018-01-12T05:50:20Z,NONE,2018-01-13T03:50:46Z
16060,Branch 181679271,cla: yes,Merging internal changes,0,,1,2018-01-12T01:17:21Z,CONTRIBUTOR,2018-01-12T03:18:38Z
16059,[Intel MKL] Fixes for various MKLDNN unit test failures,"awaiting testing (then merge),cla: yes","1. MklLayout pass changes

   Making workspace type uint8 for MaxPool; Handling duplicate control edge insertion

   1) Handles case of inserting duplicate control edge (fixing Mkl layout graph
   pass unit test)
   2) Enables uint8 as workspace tensor type (makes consistent with LRN workspace
   handling)

   Workspace tensor type change is also performed in MaxPool and MaxPoolGrad
   operators.

2. Handling MklReshape failing case

   MklReshape was failing on a unit test when Mkl layout and Tensorflow layout for
   input tensors were same, but shape of input tensor and output tensor was
   different. No reorder is required in such case, but reshape is needed. Before
   this fix, we were asserting that reorder is performed.

3. Adding support for empty input/filter tensors in Convolution backprop operators",0,,2,2018-01-12T00:47:26Z,CONTRIBUTOR,2018-01-12T05:09:32Z
16058,How to initialize embeddings layer within Estimator API?,"stat:awaiting response,type:bug/performance","I'm trying to use existing embeddings within tensorflow model, the size of embedding is greater than 2Gb and this makes my original try of doing this unsuccessful:

```
embedding_var = tf.get_variable(
        ""embeddings"", 
        shape=GLOVE_MATRIX.shape, 
        initializer=tf.constant_initializer(np.array(GLOVE_MATRIX))
)
```
Which gave me this error:

` Cannot create a tensor proto whose content is larger than 2GB.`

I'm using AWS SageMaker, which based on the Estimator API, and the actual running of the graph in session happens behind the scene, so I'm not sure how to initialize some placeholders for embedding given that. Would be helpful if someone will be able to share the way how to do such initialization in term of EstimatorAPI.

--------------------------

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,3,2018-01-12T00:22:23Z,CONTRIBUTOR,2018-01-12T21:24:07Z
16057,Make platform a proper module,cla: yes,"This fixes an issue where the nice error about importing tensorflow from the TF source directory is not displayed in Python 2.7

Fixes #16019 ",1,,6,2018-01-11T22:58:56Z,NONE,2018-01-22T23:41:35Z
16056,Apply 1.5-rc1 cherry-picks.,cla: yes,,0,,4,2018-01-11T22:49:23Z,MEMBER,2018-01-11T22:58:56Z
16055,MKL: Fixed 3 bugs picked up by the unit tests,cla: yes,"- There were 2 kinds of registrations for MatMul - with and without the 'eigen' label. Re-added the registrations with the 'eigen' label when MKL is used. 
- Removed the ifdef that removed the check for the label when MKL was used. The eigen op should be called when the eigen label is used.
- In the selective registration header test, unicode strings aren't handled correctly, so there's a ""u"" before the kernel class string that is compared to the hardcoded string. This has been fixed.
```
- [('BiasAdd', 'BiasOp<CPUDevice, float>'), 
+ [('BiasAdd', u'BiasOp<CPUDevice, float>'), 
```",0,,2,2018-01-11T22:10:37Z,CONTRIBUTOR,2018-01-12T05:07:01Z
16047,Branch 181629980,cla: yes,,0,,2,2018-01-11T19:10:21Z,MEMBER,2018-01-11T19:10:49Z
16046,Feature Request: clarify supported environments for official binaries.,,"As it stands now, binary release of TensorFlow 1.5 is set to drop compatibility with Ubuntu 14.04 ( https://github.com/tensorflow/tensorflow/issues/15777), and compatibility with Debian Linux distros, such as Amazon Linux AMI (`ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found`).

To avoid surprise, TensorFlow should either:
1. Follow other open-source projects like Ray/PyTorch and provide official binaries for these systems
or
2. Document that support is dropped, to encourage other players (ie, AWS) to take over the job of providing these binaries

@martinwicke",0,,11,2018-01-11T18:13:57Z,CONTRIBUTOR,2018-01-11T18:58:25Z
16041,Code documentation for `confusion_matrix.py` misleading,stat:awaiting tensorflower,"### Describe the problem

The documentation for `confusion_matrix.py` says:

```
  Args:
    labels: 1-D `Tensor` of real labels for the classification task.
    predictions: 1-D `Tensor` of predictions for a given classification.
```

, however I found that those two arguments are simply python arrays and not Tensors. The following trial test code demonstrates this. As a TF/Python newbie, I'm wondering if this is actually a real issue, and if so I'll create a PR to correct it to prevent confusion to future programmers.

### Source code / logs

```
import tensorflow as tf

y_ = [0, 2, 2, 2]
y = [2, 1, 2, 2]

with tf.Session() as sess:
    confusion_matrix = tf.confusion_matrix(labels=y_, predictions=y, num_classes=4)
    confusion_matrix_to_Print = sess.run(confusion_matrix)
    print(confusion_matrix_to_Print)

```",0,,2,2018-01-11T14:25:22Z,NONE,2018-01-12T02:04:25Z
16039,How TF-Detect draw a rectangular?,,"How TF-Detect draw a rectangular?
I can't find the corresponding code?
Is it calling OpenGL to draw a rectangular?",0,,4,2018-01-11T13:12:20Z,NONE,2018-01-12T01:03:25Z
16036,"raise PiCameraMMALError(status, prefix) picamera.exc.PiCameraMMALError: Failed to enable connection: Out of resources",stat:awaiting response,,0,,4,2018-01-11T11:14:55Z,NONE,2018-01-11T19:01:23Z
16031,tf.data.Dataset.padded_batch() doesn't work with dataset.map using tf.py_func,stat:awaiting response,"
------------------------

### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: CentOS Linux release 7.2.1511
- **TensorFlow installed from**: pip
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.5

### Describe the problem
It's quite common for NLP tasks to read variable-length sentences from text files, to map them and to padd them. But Dataset.padded_batch() doesn't work with tf.dataset which uses map (tf.py_func)

### Source code
```
import tensorflow as tf                                                                           
import numpy as np                                                                                
                                                                                                  
def convert(line):                                                                                
    tokens = line.split()                                                                 
    return np.array(tokens, dtype=np.int32)

# Simulating reading variable-length sentences from a file. Using TextLineDataset will have the same problem
dataset = tf.data.Dataset.from_tensor_slices([""1 2 3"", ""4 5""])         
                           
# Tokenize each sentence and convert it to list of int
dataset = dataset.map(lambda line: tf.py_func(convert, [line], [tf.int32]))     
                  
dataset = dataset.padded_batch(1, [None]) # This line doesn't work whatever the batch_size is
# dataset = dataset.batch(1) # This line works well                                              
 
iterator = dataset.make_one_shot_iterator()                                                       
batch_data = iterator.get_next()                                                                  
                                                                                                  
with tf.Session() as sess:                                                                        
    print sess.run(batch_data)                                                                                                 
```

### Log
```
    dataset = dataset.padded_batch(1, [None])
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 695, in padded_batch
    return PaddedBatchDataset(self, batch_size, padded_shapes, padding_values)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1292, in __init__
    input_dataset.output_shapes, _partial_shape_to_tensor, padded_shapes)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/util/nest.py"", line 512, in map_structure_up_to
    assert_shallow_structure(shallow_tree, input_tree)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/util/nest.py"", line 356, in assert_shallow_structure
    ""Input has type: %s."" % type(input_tree))
TypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: <type 'list'>.
```",0,,3,2018-01-11T05:44:19Z,NONE,2018-01-11T12:57:18Z
16027,py2tf: add py2tf_internal BUILD rule to pip package,cla: yes,* to make pip tests pass,0,,1,2018-01-11T03:11:27Z,CONTRIBUTOR,2018-01-11T03:38:01Z
16024,R1.4,cla: no,,0,,3,2018-01-11T00:17:30Z,NONE,2018-01-11T00:18:57Z
16021,Update version strings.,cla: yes,,0,,1,2018-01-10T22:01:39Z,MEMBER,2018-01-10T22:02:37Z
16019,tf-nightly and master - cannot import tensorflow,stat:contributions welcome,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: tf-nightly-gpu-1.6.0.dev20180110
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: V100 16GB
- **Exact command to reproduce**: ```import tensorflow```

### Describe the problem
On the current tf-nightly-gpu I cannot import tensorflow, the following error is produced. I am also seeing the same behavior on tf-nightly and also a build from source of master (SHA: 82b1e8eee8847730026379e3a5762c0e09d6fd36):
``` python
In [1]: import tensorflow
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

/home/ubuntu/tensorflow/tensorflow/__init__.py in <module>()
     22
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26

/home/ubuntu/tensorflow/tensorflow/python/__init__.py in <module>()
     47 import numpy as np
     48
---> 49 from tensorflow.python import pywrap_tensorflow
     50
     51 # Protocol buffers

/home/ubuntu/tensorflow/tensorflow/python/pywrap_tensorflow.py in <module>()
     23 import traceback
     24
---> 25 from tensorflow.python.platform import self_check
     26
     27

ImportError: No module named platform
```

### Source code / logs
N/A",0,,3,2018-01-10T20:50:09Z,NONE,2018-01-11T01:44:18Z
16018,Branch 181499300,cla: yes,,0,,5,2018-01-10T20:38:55Z,MEMBER,2018-01-10T20:39:22Z
16015,Modify `_parse_bazel_version` to return a tuple of ints,cla: yes,"Bazel is updating its version to 0.10.0, and this will break the version check. Applying suggested fix in https://github.com/bazelbuild/bazel/issues/4425.",0,,1,2018-01-10T19:17:41Z,MEMBER,2018-01-10T19:17:59Z
16013,Disabling the interleave_op_test for now.,cla: yes,,0,,2,2018-01-10T18:43:39Z,MEMBER,2018-01-10T20:02:05Z
16012,Fix a bug in ResolveConstantConcat,"awaiting testing (then merge),cla: yes",Changes to fix a bug in ResolveConstantConcat whereby shared tensors are removed without checking if they are used in other operators in the graph.,1,,14,2018-01-10T18:10:58Z,CONTRIBUTOR,2018-01-10T23:23:08Z
16011,Tensorboard issue with the official docker image - 1.5.0-rc0-gpu-py3,"stat:awaiting tensorflower,type:build/install","Hello everyone,

I have the exact same issue as stated here: https://github.com/tensorflow/tensorflow/issues/14855
And on the official tensorboard repository: https://github.com/tensorflow/tensorboard/issues/812

The fact that I am using the official Docker Image and I didn't build anything from scratch.

`tensorflow/tensorflow   1.5.0-rc0-gpu-py3   9e770d59b136        6 days ago          2.85GB`

What I get when I try to launch Tensorboard as usual:

```
# tensorboard --logdir=log_directory
/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""/usr/local/bin/tensorboard"", line 7, in <module>
    from tensorboard.main import run_main
ImportError: cannot import name 'run_main'
```

**Resolution Idea:**
I noticed that by simply running the command: `pip install tensorboard` inside the container the problem is solved and I am able to normally launch Tensorboard.

Thanks a lot for the help,

All the best,

Jonathan
",1,,17,2018-01-10T16:29:58Z,CONTRIBUTOR,2018-01-10T17:51:10Z
16010,lib_package does not bundle MKL-DNN,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.8.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: 

### Describe the problem
I wan't to build the tensorflow C-API from source with MKL-DNN support in order to use it in another project. The easiest solution (if not the only convenient one) I found for building the C-API is using the lib_package tool:

```bash
bazel build --config=mkl -c opt //tensorflow/tools/lib_package:libtensorflow
```
The build succeeds. However, the packaged library does not contain `libmklml_intel.so` and `libiomp5.so`. 

```bash
$ ldd libtensorflow_framework.so
	linux-vdso.so.1 =>  (0x00007ffec0f8a000)
	libmklml_intel.so => not found
	libiomp5.so => not found
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007feb07b2f000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007feb07826000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007feb07609000)
	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007feb07287000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007feb07071000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007feb06ca7000)

```
Is there a way to fix the Bazel build such that it outputs all necessary libs?
",0,,3,2018-01-10T14:47:46Z,NONE,2018-01-11T06:15:12Z
16009,"bazel build ask for ANDROID_NDK_HOME, ANDROID_SDK_HOME -- no way to disable it",type:build/install,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
('v1.5.0-rc0-1-g793280a', '1.5.0-rc0')
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
```
Build label: 0.9.0
Build target: bazel-out/k8-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Dec 19 09:31:58 2017 (1513675918)
Build timestamp: 1513675918
Build timestamp as int: 1513675918
```
- **GCC/Compiler version (if compiling from source)**:
g++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
- **CUDA/cuDNN version**:
toolkit_9.0 and cudnn 7.0.5_for_9.0
- **GPU model and memory**:
different machines (irrelevant)
- **Exact command to reproduce**:
see [this gist](https://gist.github.com/PatWie/aef90e72dbeaf2f79fbcaa031d74baad) which is mainly

```bash
export TF_NEED_GCP=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=""$($CUDA_TOOLKIT_PATH/bin/nvcc --version | sed -n 's/^.*release \(.*\),.*/\1/p')""
export TF_CUDA_COMPUTE_CAPABILITIES=6.1,5.2,3.5
export TF_NEED_HDFS=0
export TF_NEED_OPENCL=0
export TF_NEED_JEMALLOC=1
export TF_ENABLE_XLA=0
export TF_NEED_VERBS=0
export TF_CUDA_CLANG=0
export TF_CUDNN_VERSION=7
export TF_NEED_MKL=0
export TF_DOWNLOAD_MKL=0
export TF_NEED_MPI=0
export TF_NEED_GDR=0
export TF_NEED_S3=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_COMPUTECPP=0
export GCC_HOST_COMPILER_PATH=$(which gcc)
export CC_OPT_FLAGS=""-march=native""

./configure

bazel build --config=opt --copt=-mfpmath=both --copt=-msse4.2 --copt=-O3 --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=1 ....
```
### Describe the problem
In the past, using exactly this scripted worked. However, there are now a few issues:
The build uses `AVX2` even I haven't specified it as `--copt` (which worked in the past)

### Source code / logs
depending on the machine it gives

```
Python 2.7.12 (default, Nov 20 2017, 18:23:56) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
2018-01-10 15:06:19.070740: F tensorflow/core/platform/cpu_feature_guard.cc:36] The TensorFlow library was compiled to use AVX2 instructions, but these aren't available on your machine.
zsh: abort      python
```
or
```
Python 2.7.12 (default, Nov 20 2017, 18:23:56) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
zsh: illegal hardware instruction  python
```

On machines with AVX2 everything is fine. Further, there is no way to skip to setup ANDROID_NDK_HOME, ANDROID_SDK_HOME (I manually uncommented this in `configure.py`).

*edit*
I am willing to provide a pull-request for `configure.py`, adding something like `TF_NEED_ANDROID`.",0,,5,2018-01-10T14:10:14Z,NONE,2018-01-10T21:49:22Z
16008,"Java/JNI , Object Detection: Not big Difference with GPU or CPU? (Insignificant difference) ~300ms with and without GPU",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: 

**binary** by instructions https://www.tensorflow.org/versions/master/install/install_java
> 
> Install on Linux
> 
> Take the following steps to install TensorFlow for Java on Linux or macOS:
> 
> 1 Download libtensorflow.jar, which is the TensorFlow Java Archive (JAR).
> 2 Decide whether you will run TensorFlow for Java on CPU(s) only or with the help of GPU(s). To help you decide, read the section entitled ""Determine which TensorFlow to install"" in one of the following guides:
>  - Installing TensorFlow on Linux
>
> 3 Download and extract the appropriate Java Native Interface (JNI) file for your operating system and processor support by running the following shell commands:
> 
>  TF_TYPE=""gpu""
>  OS=$(uname -s | tr '[:upper:]' '[:lower:]')
>  mkdir -p ./jni
>  curl -L \
>    ""https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-${TF_TYPE}-${OS}-x86_64-1.4.0.tar.gz"" |
>    tar -xz -C ./jni

- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: n/a, not used here (Java instead)
- **Bazel version (if compiling from source)**: n/a, not used here
- **GCC/Compiler version (if compiling from source)**: n/a, not used here
- **CUDA/cuDNN version**: Cuda compilation tools, release 8.0, V8.0.61, cuDNN 6
- **GPU model and memory**: GeForce 940MX

### Source code / logs
```
Checking to see if TensorFlow native methods are already loaded
TensorFlow native methods not found, attempting to load via tensorflow_inference
Successfully loaded TensorFlow native methods (RunStats error may be ignored)
2018-01-10 15:51:41.115224: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-01-10 15:51:41.254497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-10 15:51:41.255183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 1.2415
pciBusID: 0000:01:00.0
totalMemory: 1,96GiB freeMemory: 1,51GiB
2018-01-10 15:51:41.255217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)
Model load took 313ms, TensorFlow version: 1.4.0
```
",0,,5,2018-01-10T14:06:41Z,NONE,2018-01-11T06:11:54Z
16007,Fix inline if/else statement in CMAKE_CACHE_ARGS,"awaiting testing (then merge),cla: yes","An if/else statement was given inline as an argument to CMAKE_CACHE_ARGS for some CMake external projects as discussed in #15209. This resulted in the following init cache entries on some systems:

```
set(CMAKE_POSITION_INDEPENDENT_CODE ""ON;if(;tensorflow_ENABLE_POSITION_INDEPENDENT_CODE;);else;(;)""CACHE BOOL ""Initial cache"" FORCE)
set(CMAKE_POSITION_INDEPENDENT_CODE ""OFF;endif;(;)"" CACHEBOOL ""Initial cache"" FORCE)
```
This commit changes the inline if/else arguments to -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=${tensorflow_ENABLE_POSITION_INDEPENDENT_CODE} which is functionality equivalent.",0,,5,2018-01-10T13:05:09Z,CONTRIBUTOR,2018-01-11T02:33:39Z
16006,Add property to get cell wrapped by DropoutWrapper ,"awaiting testing (then merge),cla: yes",Adding wrapped cell property as discussed in #15810.,1,,7,2018-01-10T12:20:19Z,CONTRIBUTOR,2018-01-14T03:25:19Z
16005,Verbs w 0 copies,"awaiting testing (then merge),cla: yes","## Verbs implementation to use direct tensor writes (0 copies)

### Motivation:

Following HKUST research on the use of GPU direct, and their [GDR implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gdr/README.md), we wish to adopt the 0 copies approach and apply it to the current verbs implementation, while keeping the current implementation advantages, such as configurability and the use of RDMA for control messages.

### Performance:

Compared with the current GRPC, verbs and GDR implementation, the result implementation gave the best performance for every model, with any number of nodes. For VGG16 on 8 nodes with 4 P100 GPUs each, the prototype beat the second place by over 15%.

### Implementation requirements:

1. Tensor writes need to be done directly from the source Tensor to the destination Tensor, with no memory copies in between. This should be done for all DMAble tensors which are located either on CPU or on a RDMA compatible GPU device (GPU direct). 
2. Non DMAble tensors (CanMemCopy == false) will be serialized to proto on the sender side, RDMA written to a registered buffer on the receiver side, and then deserialized by the receiver.
3. Tensors which are located on a non-RDMA-compatible GPU, will be RDMA written to a registered CPU proxy buffer on the receiver side, and then copied to GPU by the receiver.

### Implementation constrains:

For best stability and proof of correctness, we will divide the implementation to two stages:
1. At first stage we will keep changes to the current implementation to the minimum possible. The expense will be that we may have unused or unnecessary code leftovers, which may also affect performance. 
2. At second stage, we will re-iterate over the code and remove irrelevant code parts.
The design of the solution aims that we will achieve both stages with relative ease. 

### Design guidelines:

1. Since we do not want to do any unnecessary memory copying, we will no longer allocate a fixed CPU buffer as the destination for the RDMA write. Instead we will do the writing directly to the result tensor, or if the result tensor is on a device which does not support RDMA, we will do the writing to a proxy CPU tensor and then copy its content to the result tensor.
2. The address of the destination Tensor needs to be sent to the sender side for writing, meaning that the result/proxy tensor should be pre-allocated on the receiver side, prior to sending the tensor request. In order to do that, we need to know its meta-data, i.e. shape and data-type for DMAble tensors, and proto-size for serialized tensors. Unfortunately, this information is only available on the sender side which complicates manners. In order to avoid sending extra messages for querying the meta-data on each step, we store a local meta-data cache per tensor. Based on the assumption that the meta-data of a tensor rarely changes between steps, we expect that on most times the cache will only be updated once. When the sender receives a request for a tensor, if it is the first time this tensor is requested, or in the rare case that the meta-data did change, the sender will first send a meta-data response, on which the receiver will update the local cache, and reallocate the result/proxy tensors if required. When the receiver sends the tensor request, it will contain also the meta-data currently stored in its local cache, so the sender can compare it to see if there was a change.
3. When the sender writes the tensor content to the result tensor, no additional data is being written with it. That means we need to reside on ibverbs immediate (uint32_t) to indicate which request we are responding to (in order to trigger the receive callback). The easiest and most elegant way is to key the recv callback with a unique request_index (uint32_t), instead of the current key_with_step_id (string). 
4. Since the sender no longer writes the tensor from/to fixed buffers, we no longer need to schedule the writes using the local/remote status. In addition we no longer rely on the RmdaTensorBuffer members as the source/destination addresses and rkey/lkey. Instead, each RdmaTensorBuffer will hold multiple ""Response"" objects (one per step-id), from which we derive destination address and rkey. The source address and lkey are always the ones of the source Tensor.
5. With the addition of tensor pre-allocation, we noticed there is a large code similarity between sending the first tensor request and re-sending the request in case of meta-data changes. After implementing a common method for tensor pre-allocation, it turned out that implementation becomes much simpler by encapsulating the process of request sending/re-sending, meta-data response callback and content response callback, all in a single ""Request"" class. The request class holds all the relevant request information, which reduces excessive parameter passing and lambda capturing. This decision is purely for elegance and code simplicity, and we decided to implement it in first stage because it makes the implementation much easier.
6. At phase 2, we adopt that approach for the sender side as well, by encapsulate all the send and resend logic in the ""Response"" class (and remove the RdmaTensorBuffer class completely). This should make our design easier to understand, and also hold common notions with the rest of the distributed implementations.

### New types/classes:

* **enum RdmaImmDataType** - Immediate types to distinguish between different RDMA writes on the remote side. Ack writes and control-message writes have a fixed immediate value. The rest of the writes are tensor writes and the immediate value is the relevant request index.
* **enum  RdmaWriteIDType**    - Types to distinguish between different RDMA write-complete events: Ack, control message, tensor DMA write and tensor proto write.
* **class RdmaWriteID**        - Context for RDMA write complete events. Holds the RdmaWriteIDType and additional data.
* **class RemoteAddressContext** - Remote address information (address + mr). Will be passed as write context for tensor proto writes.
* **class RdmaTensorMetaData** - Meta-data for a tensor (type, shape, is_dead, proto_size).
* **class RdmaMemoryMgr**      - Manages the meta-data cache, and the registered memory regions.
* **class RdmaTensorRequest**    - Holds and manages information for a single tensor request throughout the entire receive cycle. API:
	* **Start()**                - Start the request sequence.
		* Allocate the result tensor (and proxy tensor if required).
		* Send RDMA_MESSAGE_TENSOR_REQUEST to the remote side.
	* **RecvTensorMetaData()**   - Receive meta-data from the remote side.
		* Update the local meta-data cache.
		* Reallocate the result tensor (and proxy tensor if required).
		* Re-send the request to the remote side.
	* **RecvTensorContent()**    - Receive tensor content from the remote side (RDMA write was completed).
		* Decode proto if required and/or move to GPU if the content was not written to it directly (GPU direct is not avaliable).
		* Invoke the done callback.
* **class RdmaTensorResponse**   - Holds and manages information for a single tensor response throughout the entire send cycle. API:
	* **Start()**                - Start the response sequence. 
		* Find the tensor in the local tag-match table.
		* Compare the tensor's meta-data to the meta-data in the message (taken from the requester's local cache). 
			* If meta-data changed:
				* Clone the tensor to be sent later.
				* Send a meta-data update message and wait for re-request.
			* Else:
				* Send the tensor's content (using direct RDMA write).
	* **Resume()**               - Resume the response sequence after a re-request. Send the tensor's content that was cloned earlier.
	* **Destroy()**              - Destroy the response's resources and remove it form the pending list.

### Protocol changes:

The protocol messages themselves will remain mostly unchanged at the first stage, but will be used differently, as described below. The current messages structures already have most of the required fields for the new implementation. The only change is the ""buffer_size"" field which is no longer used since we are no longer sending additional information with the tensor, and thus it is now always equal to the ""tensor_bytes"" field. Instead, we use that field to pass the ""request_index"".

### Message structure:

| type | name_size | name | step_id | request_index | remote_addr | rkey | is_dead | data_type | tensor_shape | tensor_bytes |
|------|---------- |------|---------|---------------|-------------|------|---------|-----------|--------------|--------------|
|  1B  |    2B     | 512  |  8B     |      8B       |         8B  |   4B |      1B |     XB    |    XB        |    8B        |

* **RDMA_MESSAGE_TENSOR_REQUEST**  - (receiver ==> sender) The original tensor request. 
	* type - The message type.
	* name (name_size) - Name of the requested tensor.
	* step_id - Step ID.
	* request_index - Request index.
	* remote_addr/rkey - Address/rkey of the result/proxy tensor. Irrelevant for first-time request.
	* is_dead/data_type/tensor_shape/tensor_bytes - The current meta-data as stored in the receiver local cache. The sender will use that information to know if the receiver's cache requires updating.
* **RDMA_MESSAGE_BUFFER_REQUEST**  - (sender ==> receiver) The meta-data update message in case meta-data had changed (or if it is the first time the tensor is requested).
	* type - The message type.
	* request_index - Request index.
	* is_dead/data_type/tensor_shape/tensor_bytes - The up-to-date meta-data.

  **Note:** At phase 2 this message is renamed to - **RDMA_MESSAGE_META_DATA_UPDATE**.
* **RDMA_MESSAGE_BUFFER_RESPONSE** - (receiver ==> sender) Tensor re-requset after meta-data update and reallocation of result/proxy tensors.
	* type - The message type.
	* name (name_size) - Name of the requested tensor.
	* step_id - Step ID.
	* request_index - Request index.
	* remote_addr/rkey - Address/rkey of the reallocated result/proxy tensor.
	* is_dead/data_type/tensor_shape/tensor_bytes - The new meta-data. Will be removed in the next phase.

  **Note:** At phase 2 this message is renamed to - **RDMA_MESSAGE_TENSOR_RE_REQUEST**.
* **RDMA_MESSAGE_TENSOR_WRITE**    - (sender ==> receiver) No longer sent. There is only a direct write of the tensor content to the result/proxy tensor. Request index passed as the immediate value of the write.
* **RDMA_MESSAGE_TENSOR_IDLE**     - (receiver ==> sender) No longer sent.

### Phase 1:
![alt text](https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies_phase1_protocol.jpg ""Phase 1 transport protocol"")

### Phase 2:
![alt text](https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies.png ""Phase 2 transport protocol"")

### Second stage optimizations:
1. Remove unused code leftovers - Done.
2. Remove the ACK buffer completely, since we can rely completely on its immediate value - Done.

### Future optimizations:
1. Map the tensor names to indexes, to significantly reduce the request message size.
2. Understand the purpose of empty tensors and if we can skip remote fetching for them.
3. Consider concatenating multiple requests and/or using multiple message buffers.
4. Consider a no-request architecture.
  
 ",0,,8,2018-01-10T12:15:07Z,CONTRIBUTOR,2018-01-10T12:18:11Z
16003,Adding meta_graph_be.pb testdata for big endian for framework_meta_graph_test,cla: yes,,1,,9,2018-01-10T09:11:05Z,CONTRIBUTOR,2018-01-16T06:34:03Z
16002,fix a_ to allocator_,cla: yes,"```bash
[07:05:58]	[Step 1/1] In file included from tensorflow/core/common_runtime/threadpool_device.cc:32:0:
[07:05:58]	[Step 1/1] ./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'virtual void tensorflow::MklCPUAllocator::ClearStats()':
[07:05:58]	[Step 1/1] ./tensorflow/core/common_runtime/mkl_cpu_allocator.h:120:32: error: 'a_' was not declared in this scope
[07:05:58]	[Step 1/1]    void ClearStats() override { a_->ClearStats(); }
[07:05:58]	[Step 1/1]                                 ^
```

Please review this PR ASAP... @yuefengz ",0,,5,2018-01-10T07:55:45Z,NONE,2018-01-11T05:18:01Z
15998,tensorflow input/output tensor reshape c++,stat:awaiting response,"currently , I am working on loading and testing a tensorflow model on android using c++, and the trained model is a full convolutional model, so the input need to be dynamically reshaped according to input image size.

I can make this done easily using python. but when turn to c++ , I can hardly find much examples and experience on this.

the trained model is converted to *.pb file , and the input and output tensor shape has been specified before conversion in python. and now I want to reshape the input and output in c++ before using the model.",0,,2,2018-01-10T05:53:51Z,NONE,2018-01-10T13:17:17Z
15995,/home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/astor_archive/BUILD:8:1: Converting to Python 3: external/astor_archive/astor/source_repr.py failed (Exit 1). ,type:support,"

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:clone from git
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:0.5.4
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**:GTX1070ti  8GB
- **Exact command to reproduce**:


### Describe the problem
build error.when I finished ./configure and run bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package, met an error.

### Source code / logsExtracting Bazel installation...
..............
WARNING: /home/hp/Downloads/tensorflow/tensorflow/core/BUILD:1825:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/hp/Downloads/tensorflow/tensorflow/tensorflow.bzl:1152:30.
WARNING: /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/grpc/WORKSPACE:1: Workspace name in /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/grpc/WORKSPACE (@com_github_grpc_grpc) does not match the name given in the repository's definition (@grpc); this will cause a build error in future versions.
WARNING: /home/hp/Downloads/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/hp/Downloads/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Found 1 target...
ERROR: /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/astor_archive/BUILD:8:1: Converting to Python 3: external/astor_archive/astor/source_repr.py failed (Exit 1).

  ",0,,1,2018-01-10T03:41:02Z,NONE,2018-01-11T00:22:55Z
15993,Fix typos,"awaiting testing (then merge),cla: yes","This PR fixes some typos: `refered`, `ouptuts`, `from from`, `suport`, `whithin`, `posibility`, and `then then`.",1,,2,2018-01-10T02:59:48Z,CONTRIBUTOR,2018-01-11T02:15:41Z
15991,Hide MSVC workaround from Clang on Windows,"awaiting testing (then merge),cla: yes",#15990,0,,2,2018-01-10T01:51:56Z,CONTRIBUTOR,2018-01-11T20:19:47Z
15989,Fix freeze_graph command line argument error.,"awaiting review,cla: yes",Fix TypeError: main() missing 1 required positional argument: 'unused_args' when using freeze_graph command line tool (pip console script entry point),1,,7,2018-01-10T00:12:56Z,NONE,2018-01-29T19:53:17Z
15988,Add internal release notes that were previously missing.,cla: yes,"I wasn't sure about some of the Important/Other changes, so please double-check
that I haven't missed anything actually critical.",0,,1,2018-01-09T23:45:17Z,MEMBER,2018-01-10T00:28:45Z
15987,"Documentation for placeholder does not explain when shape is (), [] or [None]",type:docs,"### System information

Not necessary.

### Describe the problem

The documentation for `placeholder` does not explain the case when its shape is `()`, `[]` or `[None]`.

### Possible solution

Add the explanations in [this SO  answer](https://stackoverflow.com/a/46941087/3924118) to the documentation of `placeholder`, including the example !!

  ",0,,2,2018-01-09T23:28:05Z,NONE,2018-01-10T01:01:55Z
15986,Add new internal release notes that were missed in the previous iteration.,cla: no,,0,,2,2018-01-09T23:22:48Z,MEMBER,2018-01-09T23:23:48Z
15985,Feature Request: Dense to Sparse and Dense to Sparse Tensor Ops,"stat:contributions welcome,type:feature","I think it would be helpful if there is a dense_to_sparse op in Tensorflow for ops like `ctc_loss` that requires sparse labels. I'm not really sure where else it can be used aside from that but in case only `ctc_loss` uses it, I think it would help if dense labels can be passed into `ctc_loss` and do the conversion within.",0,,24,2018-01-09T23:19:33Z,CONTRIBUTOR,2018-01-10T00:56:54Z
15983,Feature request: Reduce learning rate on plateau,"stat:awaiting tensorflower,type:feature","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes. But applies to stock examples as well.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04.
- **TensorFlow installed from (source or binary)**:
Binary.
- **TensorFlow version (use command below)**:
v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 
Python 3.5.4
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
CUDA: V8.0.61
cuDNN: 6.0.21
- **GPU model and memory**:
GTX 1080Ti 11GB running driver version 384.98
- **Exact command to reproduce**:
N/A. However, I am using the Experiment, Estimator and Dataset APIs in order to do training.

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I would like to reduce the learning rate during training. However, I do not want to treat this as another tunable hyperparameter, so I would like this to be based on performance plateauing. In Keras, it is easy to implement learning rate reduction by monitoring the validation loss using the ReduceLROnPlateau callback function, but in TensorFlow this does not seem to be the case/easy. I certainly haven't found any implementation of this, so I propose this as a feature request.

Feel free to close this if this is not the correct forum.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,4,2018-01-09T22:49:36Z,NONE,2018-01-11T00:48:57Z
15981,tf.Estimator creates loss and loss_1 for eval/train,type:support,"- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.12
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4
- **Python version**: 
2.7

### Describe the problem

When using the tf.Estimator, the summary files save out summaries for the loss variable evaluated every checkpoint.  The summary for the training, is saved as 'loss_1' .  I got this tensorboard by running the ciphar10_estimator code located: https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator/

This makes it difficult to compare the eval/train loss on the same graph in tensorboard.  What causes this naming issue and what can be done to fix it?

Thanks!

![screen shot 2018-01-09 at 12 02 53 pm](https://user-images.githubusercontent.com/22623388/34740703-311a2fc6-f535-11e7-9f88-ab16f65052ee.png)

  ",0,,4,2018-01-09T20:04:17Z,NONE,2018-01-10T00:07:50Z
15980,While loop randomly doesn't evaluate tensors,stat:awaiting tensorflower,"Hello!
I believe to have found a bug in Tensorflow when running the code below. I am currently trying to build a neural transducer, and have stumbled across TF sometimes not returning any values for a tensor. I have not had the chance yet to test this out on another machine (no GPU, TF 1.4.1, Ubuntu 17.10). The code is redacted a bit to highlight only the parts that fail. [I've also posted to StackOverflow](https://stackoverflow.com/questions/48081063/tensorflow-non-deterministic-behaviour-with-large-model-using-while-loop) but didn't get any response there.

Notes:

- I believe the bug occurs around line 160, in the body of the while loop in the function run_full_transducer
- The session is returning [encoder_outputs, transducer_outputs]
- I do not use random functions
- As far as I can tell, if I remove the Print OP in line 164, the output is always 0

Example of a correct return value (more or less):
```
array([[[ 0.00811536, -0.00200322, -0.01177037,  0.03676344, -0.01909475,
             -0.03157664,  0.026092  ,  0.02367685, -0.01894805,  0.02832799,
              0.0377345 , -0.02583589, -0.02908566,  0.0299024 ,  0.00518877,
             -0.00064737,  0.01431572, -0.01053502, -0.01783628, -0.00382657,
              0.00076749, -0.02705991,  0.00112415, -0.0193013 ,  0.02346764,
              0.03014467,  0.02663364,  0.02503882,  0.03362656, -0.01877708,
              0.01859642,  0.02460729, -0.01395229, -0.03033791,  0.01177907,
             -0.03049169, -0.00389978,  0.02221515, -0.00073605,  0.01248251,
              0.00424051,  0.01070387,  0.02818898,  0.0321721 , -0.02462685,
              0.03495178, -0.02408989, -0.02742486,  0.00331823, -0.02311424,
             -0.01327039,  0.01095297,  0.02584363,  0.02083527, -0.01588045,
              0.02837921,  0.02100117,  0.00918638,  0.00109535, -0.02965789,
              0.01040822, -0.03240473,  0.00453057, -0.00603903]],
    
           [[ 0.01053647, -0.00457577, -0.01939731,  0.06317309, -0.03113565,
             -0.05525927,  0.04647589,  0.04213476, -0.03498235,  0.04962765,
              0.05989208, -0.04340284, -0.04777668,  0.05346756,  0.00395604,
             -0.0005207 ,  0.02079381, -0.01424338, -0.02584206, -0.00530154,
             -0.00031365, -0.04966826, -0.00091683, -0.03025239,  0.04526306,
              0.0595435 ,  0.0463665 ,  0.04578522,  0.05916505, -0.031725  ,
              0.03164144,  0.04257958, -0.02865831, -0.04795898,  0.01856991,
             -0.05512668, -0.00730711,  0.03953242,  0.00017992,  0.01710426,
              0.00754557,  0.01975578,  0.0469296 ,  0.05237873, -0.04435374,
              0.05924731, -0.04474678, -0.04605344,  0.00947831, -0.04284734,
             -0.01979787,  0.02003288,  0.04196753,  0.03900779, -0.02887472,
              0.05130195,  0.03419674,  0.0105699 ,  0.001114  , -0.0524303 ,
              0.01738651, -0.06084244,  0.01364262, -0.01153531]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```
Incorrect:
```
 [array([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],
    
           [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```

Code:
``` python
 import tensorflow as tf
    from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple
    from tensorflow.python.layers import core as layers_core
    import numpy as np
    # NOTE: Time major
    
    # Constants
    input_dimensions = 1
    vocab_size = 3
    input_embedding_size = 20
    encoder_hidden_units = 64
    inputs_embedded = True
    transducer_hidden_units = 64
    batch_size = 1
    GO_SYMBOL = vocab_size - 1  # TODO: Make these constants correct
    END_SYMBOL = vocab_size
    input_block_size = 2
    log_prob_init_value = 0
    
    
    # ---------------- Helper classes -----------------------
    
    
    # ----------------- Model -------------------------------
    embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)
    
    
    class Model(object):
        def __init__(self):
            self.encoder_inputs, self.encoder_inputs_length, self.encoder_hidden_state, \
            self.encoder_outputs, self.encoder_hidden_state_new = self.build_encoder_model()
            self.encoder_raw_outputs, self.trans_hidden_state, self.transducer_amount_outputs, \
            self.transducer_hidden_state_new, self.logits, self.decoder_prediction = self.build_transducer_model()
    
        def build_encoder_model(self):
            encoder_inputs = tf.Variable(tf.zeros(shape=(input_block_size, batch_size, input_dimensions)),
                                         dtype=tf.float32, name='encoder_inputs', trainable=False)
            encoder_inputs_length = tf.Variable([tf.shape(encoder_inputs)[0]], dtype=tf.int32,
                                                name='encoder_inputs_length', trainable=False)
            encoder_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, encoder_hidden_units)), dtype=tf.float32,
                                               name='encoder_hidden_state')  # Save the state as one tensor
    
            if inputs_embedded is True:
                encoder_inputs_embedded = encoder_inputs
            else:
                encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)
    
            # Build model
            encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)
    
            # Build previous state
            encoder_hidden_c, encoder_hidden_h = tf.split(encoder_hidden_state, num_or_size_splits=2, axis=0)
            encoder_hidden_c = tf.reshape(encoder_hidden_c, shape=[-1, encoder_hidden_units])
            encoder_hidden_h = tf.reshape(encoder_hidden_h, shape=[-1, encoder_hidden_units])
            encoder_hidden_state_t = LSTMStateTuple(encoder_hidden_c, encoder_hidden_h)
    
            #   encoder_outputs: [max_time, batch_size, num_units]
            encoder_outputs, encoder_hidden_state_new = tf.nn.dynamic_rnn(
                encoder_cell, encoder_inputs_embedded,
                sequence_length=encoder_inputs_length, time_major=True,
                dtype=tf.float32, initial_state=encoder_hidden_state_t)
    
            # Modify output of encoder_hidden_state_new so that it can be fed back in again without problems.
            encoder_hidden_state_new = tf.concat([encoder_hidden_state_new.c, encoder_hidden_state_new.h], axis=0)
            encoder_hidden_state_new = tf.reshape(encoder_hidden_state_new, shape=[2, -1, encoder_hidden_units])
    
            return encoder_inputs, encoder_inputs_length, encoder_hidden_state, encoder_outputs, encoder_hidden_state_new
    
        def build_transducer_model(self):
            encoder_raw_outputs = tf.Variable(tf.zeros(shape=(input_block_size, 1, encoder_hidden_units)),
                                              dtype=tf.float32,
                                              name='encoder_raw_outputs')
            trans_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, transducer_hidden_units)),
                                             dtype=tf.float32,
                                             name='trans_hidden_state')  # Save the state as one tensor
            transducer_amount_outputs = tf.Variable(0, dtype=tf.int32, name='transducer_amount_outputs',
                                                    trainable=False)
    
            # Model building
            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
                embedding=embeddings,
                start_tokens=tf.tile([GO_SYMBOL], [batch_size]),
                end_token=END_SYMBOL)
    
            attention_states = tf.transpose(encoder_raw_outputs,
                                            [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]
    
            attention_mechanism = tf.contrib.seq2seq.LuongAttention(
                encoder_hidden_units, attention_states)
    
            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
                tf.contrib.rnn.LSTMCell(transducer_hidden_units),
                attention_mechanism,
                attention_layer_size=transducer_hidden_units)
    
            projection_layer = layers_core.Dense(vocab_size, use_bias=False)
    
            # Build previous state
            trans_hidden_c, trans_hidden_h = tf.split(trans_hidden_state, num_or_size_splits=2, axis=0)
            trans_hidden_c = tf.reshape(trans_hidden_c, shape=[-1, transducer_hidden_units])
            trans_hidden_h = tf.reshape(trans_hidden_h, shape=[-1, transducer_hidden_units])
            trans_hidden_state_t = LSTMStateTuple(trans_hidden_c, trans_hidden_h)
    
            decoder = tf.contrib.seq2seq.BasicDecoder(
                decoder_cell, helper,
                decoder_cell.zero_state(1, tf.float32).clone(cell_state=trans_hidden_state_t),
                output_layer=projection_layer)
    
            outputs, transducer_hidden_state_new, _ = tf.contrib.seq2seq.dynamic_decode(decoder,
                                                                                        output_time_major=True,
                                                                                        maximum_iterations=transducer_amount_outputs)
            logits = outputs.rnn_output  # logits of shape [max_time,batch_size,vocab_size]
            decoder_prediction = outputs.sample_id  # For debugging
    
            # Modify output of transducer_hidden_state_new so that it can be fed back in again without problems.
            transducer_hidden_state_new = tf.concat(
                [transducer_hidden_state_new[0].c, transducer_hidden_state_new[0].h],
                axis=0)
            transducer_hidden_state_new = tf.reshape(transducer_hidden_state_new,
                                                     shape=[2, -1, transducer_hidden_units])
    
            return encoder_raw_outputs, trans_hidden_state, transducer_amount_outputs, transducer_hidden_state_new, \
                   logits, decoder_prediction
    
    
    model = Model()
    
    
    # ----------------- Alignment -------------------------
    
    # ----------------- Training --------------------------
    
    def run_full_transducer():
        # Inputs
        max_blocks = tf.placeholder(dtype=tf.int32, name='max_blocks')
        inputs_full_raw = tf.placeholder(shape=(None, batch_size, input_dimensions), dtype=tf.float32,
                                         name='inputs_full_raw')
        transducer_list_outputs = tf.placeholder(shape=(None,), dtype=tf.int32,
                                                 name='transducer_list_outputs')  # amount to output per block
    
        # Turn inputs into tensor which is easily readable
        inputs_full = tf.reshape(inputs_full_raw, shape=[max_blocks, input_block_size, batch_size, input_dimensions])
    
        # Outputs
        outputs_ta = tf.TensorArray(dtype=tf.float32, size=max_blocks)
    
        # Hidden states
        # TODO: make these correct
        encoder_hidden_init = tf.ones(shape=(2, 1, encoder_hidden_units))
        trans_hidden_init = tf.ones(shape=(2, 1, transducer_hidden_units))
    
        init_state = (0, outputs_ta, encoder_hidden_init, trans_hidden_init)
    
        def cond(current_block, outputs_int, encoder_hidden, trans_hidden):
            return current_block < max_blocks
    
        def body(current_block, outputs_int, encoder_hidden, trans_hidden):
            # Process encoder
            model.encoder_inputs = model.encoder_inputs.assign(inputs_full[current_block])
            model.encoder_inputs_length = model.encoder_inputs_length.assign([tf.shape(model.encoder_inputs)[0]])
            model.encoder_hidden_state = model.encoder_hidden_state.assign(encoder_hidden)
    
            # TODO: Error is SOMETIMES gone when using tf.Print
            current_block = tf.Print(current_block, [model.encoder_inputs], message='Enc in: ')
            #current_block = tf.Print(current_block, [model.encoder_outputs], message='Enc out: ')
    
            # Flow data from encoder to transducer
            model.encoder_raw_outputs = model.encoder_raw_outputs.assign(model.encoder_outputs)
            model.trans_hidden_state = model.trans_hidden_state.assign(trans_hidden)
            model.transducer_amount_outputs = model.transducer_amount_outputs.assign(transducer_list_outputs[current_block])
    
            # Note the outputs
            outputs_int = outputs_int.write(current_block, model.logits)
    
            return current_block + 1, outputs_int, model.encoder_hidden_state_new, model.transducer_hidden_state_new
    
        _, outputs_final, _, _ = tf.while_loop(cond, body, init_state)
    
        # Process outputs
        outputs = outputs_final.stack()  # Now the outputs are of shape [block, amount_of_trans_out, batch_size, vocab]
        outputs = tf.reshape(outputs, shape=(-1, 1, vocab_size))  # And now its [amount_outputs, batch_size, vocab]
    
        model.encoder_outputs = tf.Print(model.encoder_outputs, [model.encoder_outputs], message='Current block enc out: ')
    
        return max_blocks, inputs_full_raw, transducer_list_outputs, outputs, model.encoder_outputs
    
    # ---------------------- Testing -----------------------------
    
    
    # ---------------------- Management -----------------------------
    
    init = tf.global_variables_initializer()
    
    with tf.Session() as sess:
        sess.run(init)
    
        inp_max_blocks, inp_inputs_full_raw, inp_trans_list_out, out_outputs, enc_out = run_full_transducer()
    
        print sess.run([enc_out, out_outputs], feed_dict={
            inp_max_blocks: 3,
            inp_inputs_full_raw: np.ones(shape=(3 * input_block_size, 1, input_dimensions)),
            inp_trans_list_out: [1, 3, 2]
        })
```
System information:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10 (Artful Aardvark)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  1.4.1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Execute the code block as a python file a few times

Thanks!
Nikita
  
  
  ",0,,6,2018-01-09T19:58:14Z,NONE,2018-01-11T03:02:39Z
15979,Branch 181341793,cla: yes,,0,,2,2018-01-09T19:16:46Z,MEMBER,2018-01-09T19:16:57Z
15975,MKL: Fix for a compilation error caused by a previous commit,"awaiting testing (then merge),cla: yes",,1,,5,2018-01-09T13:05:01Z,CONTRIBUTOR,2018-01-11T03:11:25Z
15974,Estimator.predict always loads model checkpoint preventing partially loading checkpoints,stat:awaiting tensorflower,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
pip
- **TensorFlow version (use command below)**:
1.4
- **Python version**: 
3.5

### Describe the problem
When using Tensorflow's Estimator to do predictions, the Estimator always loads the checkpoint in the model_dir. As this is done after the model_fn is called, there is no way to partially load a checkpoint for predictions. For training, I partially load the initial checkpoint in the model_fn which works fine.

I also tried not specifying the model_dir for the Estimator. As the [documentation states](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#__init__), this results in the Estimator using a temporary folder. However, as the temporary folder does not contain a checkpoint, I get the error `Could not find trained model in model_dir`.

It looks like there is no way to only partially load a checkpoint for prediction. If so, please provide a way to do this. For me, this is important, because I have a large model with several outputs. For different datasets, some of the outputs have different sizes. However, some of them are the same for all datasets. That's why I want to load them with the same code and only partially, because I don't need to load and run the whole network for this prediction. ",0,,2,2018-01-09T12:56:34Z,CONTRIBUTOR,2018-01-10T00:21:20Z
15973,"How to change the model, without any change into android APK file",,"Hello,

I want to make an android app in this way, like we can change model file anytime in future, and it will not require any change into application code, means no need to generate new APK file of application, on any change into model.
In short I want to know, is there anyway to place model file other then assets folder. So that I can refer updated model file anytime from app.

Thanks,
Sumeet Guha.",0,,4,2018-01-09T10:46:45Z,NONE,2018-01-09T19:01:48Z
15972,Maven Version of tensorflow Java API jar wrongly updated in Documentation,,"### System information
Have I written custom code : N/A
OS Platform and Distribution : N/A
TensorFlow installed from : N/A
TensorFlow version : N/A
Bazel version : N/A
CUDA/cuDNN version : N/A
GPU model and memory : N/A
Exact command to reproduce : N/A

### Describe the problem
https://www.tensorflow.org/install/install_java shows maven version as 1.4.1 

```
<dependency>
  <groupId>org.tensorflow</groupId>
  <artifactId>tensorflow</artifactId>
  <version>1.4.1</version>
</dependency>
```
However, this version is not available in public maven Repositories.
https://mvnrepository.com/artifact/org.tensorflow/tensorflow
Only versions  1.3.0 , 1.4.0, 1.4.0-rc0 and 1.5.0-rc0 are available.
Please correct documentation or release 1.4.1 Versions.

### Source code / logs
N/A
  ",1,,5,2018-01-09T09:44:50Z,NONE,2018-01-09T19:01:44Z
15968,Imperfect implementation of tf.losses.mean_pairwise_squared_error,stat:awaiting response,"### System information
- **TensorFlow version**: 1.4.0, 1.4.1, and 1.5.0-rc0 (checked)
- **Have I written custom code**: N/A
- **OS Platform and Distribution**: N/A
- **TensorFlow installed from**: N/A
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
The implementation of `tf.losses.mean_pairwise_squared_error` looks imperfect.
For example, as explained in [the API reference of the function](https://www.tensorflow.org/api_docs/python/tf/losses/mean_pairwise_squared_error)
> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are three pairs of differences are summed to compute the loss: loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3

let me put the following data as `labels` and `predictions`:
```
labels = tf.constant([[0., 0.5, 1.]])
predictions = tf.constant([[1., 1., 1.]])
tf.losses.mean_pairwise_squared_error(labels, predictions)
```
In this case, the result should be `[(0-0.5)^2+(0-1)^2+(0.5-1)^2]/3=0.5`, but tensorflow returns different value 0.3333333134651184.

### Suggestion to fix the source code
[tensorflow/python/ops/losses/losses_impl.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/losses/losses_impl.py)

If the loss function `mean_pairwise_squared_error` measures the differences between pairs of corresponding elements of `predictions` and `labels` as explained in [the API reference of the function](https://www.tensorflow.org/api_docs/python/tf/losses/mean_pairwise_squared_error), here is a simple patch:
> (lines 520-521 need to be changed as)
> `term1 = 2.0 * _safe_div(sum_squares_diff_per_batch, num_present_per_batch-1)`
and
> (lines 525-526 need to be changed as)
> `term2 = 2.0 * _safe_div(math_ops.square(sum_diff), math_ops.multiply(num_present_per_batch, num_present_per_batch-1))`
  ",1,,2,2018-01-09T05:51:16Z,CONTRIBUTOR,2018-01-09T13:00:03Z
15967,Make graph transform tool accessible via command line for pip install.,"awaiting review,cla: yes","RELNOTE: Make graph transform tool available from command line as
`transform_graph` for pip package.
Fix  #13287.",1,,1,2018-01-09T05:23:56Z,MEMBER,2018-01-23T21:41:49Z
15964,DownloadfileTask Failed,stat:awaiting response,"try projrct as https://www.tensorflow.org/mobile/android_build#android_sample_apps,but downloadtask failed,  then solve it ,may be you shoule change the 

> download-models.gradle  classpath 'de.undercouch:gradle-download-task:3.2.0' to 3.3.0",0,,2,2018-01-09T03:37:07Z,NONE,2018-01-09T12:59:53Z
15961,Adding cuda_config.h to the pip package.,cla: yes,,0,,2,2018-01-09T01:13:19Z,MEMBER,2018-01-11T01:24:09Z
15960,Branch 181239691,cla: yes,,0,,2,2018-01-09T01:09:42Z,MEMBER,2018-01-09T01:10:13Z
15959,Adding an install sources line for 1.5.0-rc0. Earlier we only updated,cla: yes, this for official.,0,,3,2018-01-08T22:21:15Z,MEMBER,2018-01-08T23:36:06Z
15957,Switching branch and run ./configure does not regenerate spec.json,type:build/install,"When building from source with TensorFlow and switch to another branch, error returned even if I rerun `./configure`:

```
ubuntu@ubuntu:~/tensorflow$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
..........
ubuntu@ubuntu:~/tensorflow$ git checkout -b test
ubuntu@ubuntu:~/tensorflow$ ./configure
..........
ubuntu@ubuntu:~/tensorflow$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
..........
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded).
INFO: Found 1 target...
ERROR: /home/ubuntu/tensorflow/tensorflow/core/BUILD:1671:1: Executing genrule //tensorflow/core:version_info_gen failed (Exit 1)
Traceback (most recent call last):
  File ""tensorflow/tools/git/gen_git_source.py"", line 284, in <module>
    generate(args.generate)
  File ""tensorflow/tools/git/gen_git_source.py"", line 229, in generate
    (old_branch, new_branch))
RuntimeError: Run ./configure again, branch was 'refs/heads/master' but is now 'refs/heads/test'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 9.025s, Critical Path: 0.30s
FAILED: Build did NOT complete successfully
```


I think the issue is that `spec.json` is not updated when running `./configure`


```
ubuntu@ubuntu:~/tensorflow$ cat /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/local_config_git/gen/spec.json
{
  ""path"": ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/org_tensorflow/"", 
  ""git"": true, 
  ""branch"": ""refs/heads/master""
}
ubuntu@ubuntu:~/tensorflow$ 
```

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5) 
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```sh
git checkout -b test
./configure
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

  ",1,,3,2018-01-08T21:07:43Z,MEMBER,2018-01-11T01:39:12Z
15955,Branch 181174976,cla: yes,,0,,4,2018-01-08T18:28:33Z,MEMBER,2018-01-08T18:28:49Z
15951,[Build] Source build at HEAD generating XLA erros on Mac OS,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS High Sierra
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: HEAD@a770968
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 9.0.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Build procedures on doc optimized for native arch and XLA enabled.

### Describe the problem
Building TensorFlow on Mac OS with XLA enabled and configuration given above optimized for native arch and CPU only yields the following errors:
```
ERROR: /Users/adriano/MachineLearning/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:522:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:runtime_fft' failed (Exit 1)
In file included from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:21:
./tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h:42:30: error: implicit instantiation of undefined template 'std::__1::array<long long, 3>'
  const std::array<int64, 3> fft_shape = {
                             ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__tuple:222:64: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TEMPLATE_VIS array;
                                                               ^
In file included from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:21:
./tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h:65:30: error: implicit instantiation of undefined template 'std::__1::array<long long, 3>'
  const std::array<int64, 3> fft_shape = {
                             ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__tuple:222:64: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TEMPLATE_VIS array;
                                                               ^
In file included from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:21:
./tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h:106:30: error: implicit instantiation of undefined template 'std::__1::array<long long, 3>'
  const std::array<int64, 3> fft_shape = {
                             ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__tuple:222:64: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TEMPLATE_VIS array;
                                                               ^
3 errors generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1997.286s, Critical Path: 88.18s
FAILED: Build did NOT complete successfully

```
",1,,2,2018-01-08T17:04:44Z,CONTRIBUTOR,2018-01-10T21:10:23Z
15949,Building TensorFlow on Windows: patch and rm,type:build/install,"### System information
- **Have I written custom code**: Yes, provided below.
- **OS Platform and Distribution**: Windows 10 1709, Build 16299.192
- **TensorFlow installed from (source or binary)**: Binary, Attempting source build of master
- **TensorFlow version**: 1.4.0
- **Python version**: 3.6
- **Bazel version**:  0.9.0
- **GCC/Compiler version**:  MSYS2 Shell, GCC unknown.
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A, CPU is i7-8550U, 8 GB memory
- **Exact command to reproduce**: Any `bazel build` on Windows. Please see Description.

### Describe the problem
Building TensorFlow on Windows has been a struggle with compatibility due to the fact that for many, MSYS will not run `patch` when installed from the MSYS2 shell. I have found a reliable way to resolve the issue: using Choco to install `patch`, moving patch.exe to a folder FOLDERNAME within its default directory, and then running %FOLDERNAME%/patch.exe with the flag `--binary` (to use CR LF line breaks) with a custom batch script compiled into a executable.

`bazel build` now completes `patch` commands without issue. But as it often is, another hurdle exists to the finish line. Bazel now attempts to recursively force remove a file using `rm -rf`, which obviously does not exist as a package in Choco as a bash command. MSYS will run it, but not from the command line.

Is there any way to get around the use of `rm`, or make a compatible solution for Windows using `del`?

I have ensured that #15829 has been installed. Still fails

If this is better left to the Bazel developers, please close this issue. 

### Source code

#### patch.bat
``` sh
start C:\ProgramData\chocolatey\lib\patch\tools\bin\%FOLDERNAME%\patch.exe --binary
exit
```

### Logs
``` sh
C:\tensorflow>bazel build --config=mkl --config=monolithic -c opt --copt=-march=native --copt=-mmmx --copt=-msse --copt=-msse2 --copt=-msse3 --copt=-mssse3 --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-mavx2 --copt=-maes --copt=-mfpmath=both //tensorflow/tools/pip_package:build_pip_package
#ERROR: C:/tensorflow/tensorflow/python/BUILD:4646:1: no such package '@cython//': Traceback (most recent call last):
        File ""C:/tensorflow/third_party/repo.bzl"", line 86
                _apply_delete(ctx, ctx.attr.delete)
        File ""C:/tensorflow/third_party/repo.bzl"", line 68, in _apply_delete
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/tensorflow/third_party/repo.bzl"", line 44, in _execute_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'C:\msys64\usr\bin\bash.exe -c rm -rf C:/users/eric/appdata/local/temp/_bazel_eric/x1e5egqw/external/cython/BUILD.bazel':
Stdout:
Stderr: /usr/bin/bash: rm: command not found
 and referenced by '//tensorflow/python:framework/fast_tensor_util.pyx_cython_translation'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed
INFO: Elapsed time: 61.324s
FAILED: Build did NOT complete successfully (92 packages loaded)
```

  ",1,,3,2018-01-08T15:32:59Z,NONE,2018-01-10T19:08:57Z
15947,Windows: Override /DEIGEN_STRONG_INLINE=inline for //tensorflow/core/kernels:conv_ops,"awaiting testing (then merge),cla: yes","This change reduces the Windows building time by more than 15 minutes

Fix #10521",0,,7,2018-01-08T15:22:06Z,MEMBER,2018-01-09T10:20:33Z
15946,Support for large number of classes when using tf.metrics.mean_per_class_accuracy(),"awaiting testing (then merge),cla: yes,kokoro:run","`tf.metrics.mean_per_class_accuracy()` uses a `num_classes x num_classes` matrix to keep track of accuracies for each class. This wastes a lot of memory and doesn't work well for large number of classes (e.g. matrix size for 500k classes is 500000^2*4 = 1 terabyte).

By switching to two 1-D variables of size `num_classes` instead, memory usage is reduced considerably. One variable keeps track of correct predictions for each class, while the other variable keeps track of the total number of predictions for each class.",1,,7,2018-01-08T14:49:46Z,CONTRIBUTOR,2018-01-22T23:48:42Z
15941,Import Error: No module named '_pywrap_tensorflow',type:build/install,"On running the following command: import tensorflow I get an error:

`C:\Users\Neerav>python
Python 3.5.0 (v3.5.0:374f501f4567, Sep 13 2015, 02:27:37) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ImportError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.`


I have the following system features:

windows 64 bit
python 3.5.0 64 bit
Nvidia computing toolkit/CUDA/v8.0./(the cuDNN version 6.0)
all of them are added to my path location also which is: Python\Python35\Scripts
i have tensorflow in Python\Python35\Lib\site-packages\tensorflow
I even have a _pywrap_tensorflow.so file and pywrap_tensorflow.py",0,,2,2018-01-08T10:46:51Z,NONE,2018-01-10T00:20:15Z
15939,Slim VGG losses increase gradually with default training configuration,,"Hi, when I try to train imagenet with slim vgg network with default configuration,
The loss increases gradually from ~0.1 to over 10000. 
I am not even able to debug this issue because, all tensors losses are encapsulated inside slim.
Is there any way to debug this issue? ",0,,3,2018-01-08T09:30:13Z,NONE,2018-01-08T19:02:48Z
15938,An easy problem about tensorflow tf.reduce_mean op,,"I know... there might not be a suitable palce to ask this question, but I really hope someone cloud help me.

i want to use the ""tf.reduce_mean"" to obtain the mean of an array (ignore the zeros element)
eg:
    data = [[1,2,3],[4,5,6],[0,0,0]]   
    i want to obtain mean= [2.5, 3.5, 4.5]  
    but  tf.reduce_mean op gets the mean=[1.6, 2.3, 3]

Thank you very much!
  ",0,,1,2018-01-08T08:33:27Z,NONE,2018-01-08T15:55:30Z
15930,Discontinuity at halfway point in graph output,stat:awaiting response,"- **I have written custom code (as opposed to using a stock example script provided in TensorFlow)**:
to reproduce the error:
1) convert HnH_gate.txt to HnH_gate.py
2) Edit mypath in out() method at end of file for your system.  Save 
3) in python: run HnH_gate.py
4) run out() to create csv files for the good and bad output
            i) out(""101"", new_probka_good)
            ii) out(""102"", new_probka_bad)
5) Plot data from hh_101.csv and hh_102.csv and verify the discontinuity at half way point in hh_102.csv
6) Two additional tests can be run:
          i) Edit parameter timepoints in main() to show error remaps to half way point.
          ii) My temporary correction is to create 2x points and throw half away.  this is done in p_update() setting cut_in_half = True

7) This same error was found running the code in Tensorflow 1.4 on MacOS Sierra.  My system info is:


== cat /etc/issue ===============================================
Linux PAULP-XPS15 4.4.0-43-Microsoft #1-Microsoft Wed Dec 31 14:42:53 PST 2014 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux PAULP-XPS15 4.4.0-43-Microsoft #1-Microsoft Wed Dec 31 14:42:53 PST 2014 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
numpydoc (0.7.0)
protobuf (3.4.1)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.5)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================


### Describe the problem
I am running a RNN for a Hodgkin and Huxley type gating of an ion channel protein
called HnH_gate.py.
The program takes a placeholder vmem and produces a timeseries output of the size
timepoints.
At the halfway point in the timeseries there is a discontinuity in the results
This only appears with some arrays fed to my tf.placeholder.  Others produce normal
results.  I can correct for the problem by doubling the number of timepoints requested
and throwing half away.

The array:
vmem_list_good = [[-100.0, -90.0, -80.0, -70.0, -60.0, -50.0, -41.0, -30.0, -20.0, -10.0, 0.0, 10.0, 20.0, 30.0, 40.0, 50.0],
                [-100.0, -90.0, -80.0, -70.0, -60.0, -50.0, -41.0, -30.0, -20.0, -10.0, 0.0, 10.0, 20.0, 30.0, 40.0, 50.0]]
appears to work perfectly
The array:
vmem_list_bad = [[80.0, 60.0, 40.0, 20.0, 00.0, -20.0, -41.0, -60.0, -80.0, -55.0, 0.0, 10.0, 20.0, 30.0, 40.0, 50.0],
            [70.0, 50.0, 30.0, 10.0, -10.0, -30.0, -50.0, -70.0, -90.0, -30.0, -10.0, 0.0, 10.0, 20.0, 30.0, 40.0]]

shows the error.

To see my temporary correction, edit the parameter in the HH.p_update() method
to: cut_in_half = True

I have written a short output routine to export the simulation to a csv file,
just edit the path and provide a string to make a unique filename:

out(""101"", new_probka_good)
out(""102"", new_probka_bad)

### Source code / logs
program file is: HnH_gate.py (provided as HnH_gate.txt)
HnH_gate.txt  (convert to HnH_gate.py)
[HnH_gate.txt](https://github.com/tensorflow/tensorflow/files/1610057/HnH_gate.txt)

System and Error Description: HnH_gate_bug_report.txt
[HnH_gate_bug_report.txt](https://github.com/tensorflow/tensorflow/files/1610056/HnH_gate_bug_report.txt)

Output example demonstrating problem: Artifact plotting new_probka_bad.py
[Artifact plotting new_probka_bad.pdf](https://github.com/tensorflow/tensorflow/files/1610058/Artifact.plotting.new_probka_bad.pdf)

Thanks for your help.
Paul
  
  ",0,,3,2018-01-07T18:17:27Z,NONE,2018-01-08T06:54:37Z
15929,"C API, SIGABRT abort, Non-OK-status: RegisterAlreadyLocked, Invalid name.","stat:awaiting response,type:support","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Working with public C API.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
MacOS 10.13.2 (17C88)
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
- libtensorflow 1.4.1 (from brew package)
- **Python version**: 
non
- **Bazel version (if compiling from source)**:
non
- **GCC/Compiler version (if compiling from source)**:
Apple Swift version 4.0.3 (swiftlang-900.0.74.1 clang-900.0.39.2), lldb-900.0.64, Swift-4.0
- **CUDA/cuDNN version**:
non
- **GPU model and memory**:
non
- **Exact command to reproduce**:
Using swift code as example (https://github.com/Octadero/Example).

### Describe the problem
Dear TensorFlow community, 
It is really strange issue, from time to time at the same code, I have SIGABRT crash.
```
2018-01-05 21:03:55.627002: F tensorflow/core/framework/op.cc:165] Non-OK-status: RegisterAlreadyLocked(deferred_[i]) status: Invalid argument: Invalid name: {\242	
(lldb) bt
* thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGABRT
    frame #0: 0x00007fff528f7e3e libsystem_kernel.dylib`__pthread_kill + 10
    frame #1: 0x0000000108cd41b4 libsystem_pthread.dylib`pthread_kill + 333
    frame #2: 0x00007fff52854312 libsystem_c.dylib`abort + 127
    frame #3: 0x0000000108e600c0 libtensorflow_framework.so`tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 32
    frame #4: 0x0000000108e600d0 libtensorflow_framework.so`tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 16
    frame #5: 0x0000000108d28676 libtensorflow_framework.so`tensorflow::OpRegistry::MustCallDeferred() const + 406
    frame #6: 0x0000000108d2819d libtensorflow_framework.so`tensorflow::OpRegistry::LookUp(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::OpRegistrationData const**) const + 61
    frame #7: 0x0000000108d0c875 libtensorflow_framework.so`tensorflow::FunctionLibraryDefinition::LookUp(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::OpRegistrationData const**) const + 117
    frame #8: 0x0000000108d27b0a libtensorflow_framework.so`tensorflow::OpRegistryInterface::LookUpOpDef(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::OpDef const**) const + 42
    frame #9: 0x0000000108d81b65 libtensorflow_framework.so`tensorflow::Graph::AddNode(tensorflow::NodeDef const&, tensorflow::Status*) + 69
    frame #10: 0x0000000108d8177a libtensorflow_framework.so`tensorflow::Graph::Graph(tensorflow::OpRegistryInterface const*) + 458
    frame #11: 0x00000001004f7d73 libtensorflow.so`TF_NewGraph + 51
  * frame #12: 0x00000001066b739d CAPI`newGraph() at Graph.swift:26
    frame #13: 0x00000001063021b3 TensorFlowKit`Graph.init() at Graph.swift:36
    frame #14: 0x000000010630214a TensorFlowKit`Graph.__allocating_init() at Graph.swift:0
    frame #15: 0x0000000106500f2d TensorFlowKit`static SavedModel.load(exportPath=""/Users/Volodymyr/Projects/Examples/03_Reinforcement/Resources/save/"", tags=1 value, options=0x00000001098e35d0, self=TensorFlowKit.SavedModel) at SavedModel.swift:221
    frame #16: 0x00000001000069f4 03_Reinforcement`static Network.loadGraph(self=_3_Reinforcement.Network) at Network.swift:146
    frame #17: 0x0000000100003428 03_Reinforcement`Network.init() at Network.swift:73
    frame #18: 0x0000000100002e4c 03_Reinforcement`Network.__allocating_init() at Network.swift:0
    frame #19: 0x0000000100008656 03_Reinforcement`main at main.swift:32
    frame #20: 0x00007fff527a8115 libdyld.dylib`start + 1
    frame #21: 0x00007fff527a8115 libdyld.dylib`start + 1
```
Sanitizer options can't help to resolve that issue. List of libs loaded in attached file.
[dyld_log.txt](https://github.com/tensorflow/tensorflow/files/1609977/dyld_log.txt)


### Source code / logs
Using C API I am alloc [new Graph by TF_NewGraph()](https://github.com/Octadero/TensorFlow/blob/34addfc80cb7f220a7d9afa310f8a9845dba0d36/Sources/CAPI/Graph.swift#L26)",0,,5,2018-01-07T17:15:42Z,NONE,2018-01-09T08:26:15Z
15838,Gif can't be decoded. InvalidArgumentError: Invalid GIF data,stat:awaiting response,"## System information
**Have I written custom code : yes
OS Platform and Distribution : Mac OS 10.12.3
TensorFlow installed from : pip3
TensorFlow version : 1.4
Bazel version : N/A
CUDA/cuDNN version : N/A
GPU model and memory : N/A**

## Describe the problem
![0071qvrrgy1fn3h6v55gag308w0adx6p](https://user-images.githubusercontent.com/8256827/34550343-04b28b42-f14b-11e7-9b8a-729a82ba4742.gif)

The gif can be decoded by PIL,but the error occurred when I used tf.image.decode_gif to decode.

```
def load_gif(image_path, sess):
    image = tf.read_file(image_path)
    image = tf.image.decode_gif(image)
    return sess.run(image)

load_gif('0071Qvrrgy1fn3h6v55gag308w0adx6p',sess))
```

The error is:
```
NotFoundError                             Traceback (most recent call last)
/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1322     try:
-> 1323       return fn(*args)
   1324     except errors.OpError as e:

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1301                                    feed_dict, fetch_list, target_list,
-> 1302                                    status, run_metadata)
   1303 

/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    472             compat.as_text(c_api.TF_Message(self.status.status)),
--> 473             c_api.TF_GetCode(self.status.status))
    474     # Delete the underlying status object from memory otherwise it stays alive

NotFoundError: /Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p; No such file or directory
	 [[Node: ReadFile_21 = ReadFile[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_21/filename)]]

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
<ipython-input-70-7b1d3fa2f712> in <module>()
      1 print(load_gif('/Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p',
----> 2                sess))

<ipython-input-55-fd8c43263043> in load_gif(image_path, sess)
      3     # image = tf.image.decode_png(image, channels=3)
      4     image = tf.image.decode_gif(image)
----> 5     return sess.run(image)

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    887     try:
    888       result = self._run(None, fetches, feed_dict, options_ptr,
--> 889                          run_metadata_ptr)
    890       if run_metadata:
    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1119       results = self._do_run(handle, final_targets, final_fetches,
-> 1120                              feed_dict_tensor, options, run_metadata)
   1121     else:
   1122       results = []

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1315     if handle is None:
   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1317                            options, run_metadata)
   1318     else:
   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1334         except KeyError:
   1335           pass
-> 1336       raise type(e)(node_def, op, message)
   1337 
   1338   def _extend_graph(self):

NotFoundError: /Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p; No such file or directory
	 [[Node: ReadFile_21 = ReadFile[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_21/filename)]]

Caused by op 'ReadFile_21', defined at:
  File ""/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/usr/local/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/usr/local/lib/python3.6/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/usr/local/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2698, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2808, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-70-7b1d3fa2f712>"", line 2, in <module>
    sess))
  File ""<ipython-input-55-fd8c43263043>"", line 2, in load_gif
    image = tf.read_file(image_path)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 376, in read_file
    ""ReadFile"", filename=filename, name=name)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): /Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p; No such file or directory
	 [[Node: ReadFile_21 = ReadFile[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_21/filename)]]```
  
  
  
  ",0,,4,2018-01-04T07:27:19Z,NONE,2018-01-04T19:05:38Z
15836,Add axis support for `tf.nn.crelu`,"awaiting testing (then merge),cla: yes","This fix tries to address the issue raised in #15619 where it was not possible to specify an `axis` for
`tf.nn.crelu`. By default, `axis=-1` was used for concatenation implicitly.

This fix adds the support of `axis` for `tf.nn.crelu`, and adds test cases for it.

This fix fixes #15619.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,7,2018-01-04T06:21:58Z,MEMBER,2018-01-05T15:42:22Z
15834,I think some bug in  tf.contrib.layers.l2_regularizer,,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:win10
- **TensorFlow installed from (source or binary)**:install
- **TensorFlow version (use command below)**:1.5
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
when I call the  tf.contrib.layers.l2_regularizer(0.5)(w), I was toll that ""Expected int64, got 0.5 of type 'float' instead."" But the doc says that it need a float clearly and as a weight it can't be an integer.

### Source code / logs
` l2 = 0
for w in tf.global_variables():
    l2 += tf.contrib.layers.l2_regularizer(0.5)(w)
loss = tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels,logits=result_vector)+l2`

  ",0,,1,2018-01-04T05:41:11Z,NONE,2018-01-04T06:52:26Z
15832,Feature Request: saver.save mkdir if directory not exist,,"### Describe the problem
`saver.save(sess, 'my-model') `
returns error if directory not exist.
It would be nice if saver can automatically create the missing directory.
`Traceback (most recent call last):
  File ""tf_voice_recognition.py"", line 783, in <module>
    saver.save(sess, 'my-model')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1594, in save
    raise exc
ValueError: Parent directory of my-model doesn't exist, can't save.
`",0,,4,2018-01-04T04:06:02Z,CONTRIBUTOR,2018-01-04T05:58:27Z
15830,tensorflow/contrib/lite/kernels/resize_bilinear.cc:42 NumInputs(node) != 1 (2 != 1),"comp:lite,stat:awaiting response","###System information
Have I written custom code: Yes, 
OS Platform and Distribution: Ubuntu14.04
TensorFlow installed from: source build w/ Bazel
TensorFlow version: 1.4
Python version: Anaconda 3.5.2
Bazel version: 0.9.0
GCC/Compiler version (if compiling from source): gcc version 4.8.4
CUDA/cuDNN version: Not relevant
GPU model and memory: Not  relevant
Exact command to reproduce: Not relevant


### Describe the problem

I construct a network with only bilinear_resize operation (I use the tf.image.resize_bilinear) and add operation, and  convert it to a tflite model successfully. However,  when I run the tflite mode, it comes to the errors as follows:

java.lang.NullPointerException: Can not allocate memory for the given inputs: tensorflow/contrib/lite/kernels/resize_bilinear.cc:42 NumInputs(node) != 1 (2 != 1)

I find the code line in resize_bilinear.cc:42 as follows:
TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);

Is it right to modify the code line to : 
TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2); 
   


### Source code / logs
def network():
      img = tf.placeholder(name='img', dtype=tf.float32, shape=(1,100,100,3))
      img = tf.layers.conv2d(img, 3, (3,3), padding='same', name='conv1')
      img = tf.image.resize_bilinear(img, [200,200])
      var = tf.get_variable('weights', dtype=tf.float32, shape=(1,200,200,3))
      val = img + var
      out = tf.identity(val, name='out')


  ",0,,3,2018-01-04T03:42:09Z,NONE,2018-01-04T12:59:48Z
15829,[Bazel/Windows] Wrap rm -rf in Bash for Windows (and some refactoring),"awaiting review,cla: yes","`rm -rf` is not available in Windows command prompt. Run it in Bash like `patch -pl`.

Refactor the wrapping logic into a new macro `_wrap_bash_cmd`.",1,,3,2018-01-04T03:10:47Z,CONTRIBUTOR,2018-01-04T15:56:59Z
15827,ci.tensorflow.org lacks a security certificate,type:support,"### The Problem
https://ci.tensorflow.org/ now lacks a security certificate (or it expired).
To repro, visit https://ci.tensorflow.org/. Chrome will note that the connection is not private.

This is breaking TensorBoard's tests that run on travis. The tests `pip install` nightly versions of TensorFlow from these URLs:

- https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp27-none-linux_x86_64.whl 
- https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp34-cp34m-linux_x86_64.whl

### Error Logs from a Failed Test Run

> Collecting tensorflow==1.head from https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp27-none-linux_x86_64.whl
> Exception:
> Traceback (most recent call last):
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/basecommand.py"", line 215, in main
>     status = self.run(options, args)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/commands/install.py"", line 335, in run
>     wb.build(autobuilding=True)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/wheel.py"", line 749, in build
>     self.requirement_set.prepare_files(self.finder)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/req/req_set.py"", line 380, in prepare_files
>     ignore_dependencies=self.ignore_dependencies))
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/req/req_set.py"", line 620, in _prepare_file
>     session=self.session, hashes=hashes)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 821, in unpack_url
>     hashes=hashes
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 659, in unpack_http_url
>     hashes)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 853, in _download_http_url
>     stream=True,
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 488, in get
>     return self.request('GET', url, **kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 386, in request
>     return super(PipSession, self).request(method, url, *args, **kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 475, in request
>     resp = self.send(prep, **send_kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 596, in send
>     r = adapter.send(request, **kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/cachecontrol/adapter.py"", line 47, in send
>     resp = super(CacheControlAdapter, self).send(request, **kw)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/adapters.py"", line 497, in send
>     raise SSLError(e, request=request)
> SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:661)

  ",1,,6,2018-01-04T00:12:28Z,MEMBER,2018-01-04T00:17:14Z
15826,Remove :0 in final result argument,"awaiting review,cla: no","Small change to line 393 - in my recent testing of retrain and the label_image workflow as of today (TF master @  136697e) I had to remove :0 from the label_image output_layer argument.

Thank you.",0,,4,2018-01-04T00:00:28Z,CONTRIBUTOR,2018-01-04T00:00:52Z
15825,Update advise.md,"awaiting testing (then merge),cla: yes",,0,,4,2018-01-03T21:42:09Z,CONTRIBUTOR,2018-01-03T21:43:33Z
15823,Fix a pessimizing-move warning in GetDeviceLapackInfo(),"awaiting testing (then merge),cla: yes","clang reports:
<pre>
./tensorflow/core/kernels/cuda_solvers.h:430:10: warning: moving a local object in a return statement prevents copy elision [-Wpessimizing-move]
  return std::move(new_dev_info);
         ^
./tensorflow/core/kernels/cuda_solvers.h:430:10: note: remove std::move call here
  return std::move(new_dev_info);
         ^~~~~~~~~~            ~
</pre>",1,,3,2018-01-03T20:39:18Z,CONTRIBUTOR,2018-01-03T20:49:20Z
15822,Updating error handling in normalize_tuple,"awaiting testing (then merge),cla: yes","In normalize_tuple we test to see if all values are an int (or able to be cast to an int) using `int()`

`ValueError` is thrown if `int()` is called with an input like 'asdf' - this is caught and gives a helpful error, using the 'name' param to provide more context.

**However**, when given an input other than a string or int, a `TypeError` is thrown. This is **not** caught - making error messages much more esoteric than the helpful one written out here, especially when coming from a very long stack trace.

For example, before this change I was getting an error:
```
TypeError: int() argument must be a string or a number, not 'tuple'
```

With this change, I get the more useful:
```
ValueError: The `kernel_size` argument must be a tuple of 2 integers. 
Received: ((0, 3), 50) including element (0, 3) of type <type 'tuple'>
```
  
  ",1,,2,2018-01-03T16:41:25Z,CONTRIBUTOR,2018-01-24T13:14:27Z
15821,TFGAN not compatible with eager execution mode,comp:eager,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colaboratory Google Compute Engine backend (not sure about OS here)
- **TensorFlow installed from (source or binary)**: binary (non-GPU version)
- **TensorFlow version (use command below)**: 1.5.0-dev20180102
- **Python version**: 3

### Describe the problem
When enabling eager execution mode
```python
import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()
```
and running 
```python
noise_dims = 64
gan_model = tfgan.gan_model(
    generator_fn,
    discriminator_fn,
    real_data=images,
    generator_inputs=tf.random_normal([batch_size, noise_dims]))
```
from the [TFGAN tutorial](https://github.com/tensorflow/models/blob/master/research/gan/tutorial.ipynb) by @joel-shor, I get the following error:
```bash
ValueError: When Eager Execution is enabled, variable collections are not supported.
```
because of the following lines:
https://github.com/tensorflow/tensorflow/blob/8c2d6fc2b0202304885d5d6c3cba57eb2a1b3262/tensorflow/contrib/gan/python/train.py#L119-L121
.

Are there any plans to make TFGAN compatible with eager in the short term? Is there any help wanted in this regard? I'd be happy to contribute.",1,,12,2018-01-03T16:08:13Z,NONE,2018-01-03T23:37:49Z
15820,"Revert ""Remove unneeded branch check (#13495)""","awaiting testing (then merge),cla: yes","This reverts commit 3aee5f1df97f44d9c14995505895f1877d7de8ae.

Fix build with Python3",0,,1,2018-01-03T15:36:42Z,MEMBER,2018-01-03T15:46:57Z
15815,"When data become large,parition variables can not initialized successfully",,"#15216 
i have a issue, but nobody help me to solve it ",0,,6,2018-01-03T12:42:54Z,NONE,2018-01-04T01:39:21Z
15812,"Closing input stream, runner session in TensorFlowInferenceInterface.java and fixing bit changes","cla: yes,stat:awaiting response",,1,,3,2018-01-03T11:19:08Z,CONTRIBUTOR,2018-01-03T18:43:27Z
15807,Fix unstable test case for Select op,"awaiting testing (then merge),cla: yes","Fix #14862. CF: #15764

In the test case for Select op, the condition might switch to another value when `x1` is close to `x2`.  The PR is opened to resolve the unstable condition.

Test passed for 100 times.
```bash
[facai@h1077922 tensorflow]$ bazel test --runs_per_test=100 -c opt //tensorflow/cc:gradients_math_grad_test
HEAD is now at c642574... TST: modify unstable test case
INFO: Found 1 test target...
Target //tensorflow/cc:gradients_math_grad_test up-to-date:
  bazel-bin/tensorflow/cc/gradients_math_grad_test
INFO: Elapsed time: 49.959s, Critical Path: 21.29s
//tensorflow/cc:gradients_math_grad_test                                 PASSED in 21.3s
  Stats over 100 runs: max = 21.3s, min = 0.7s, avg = 11.2s, dev = 4.4s

Executed 1 out of 1 test: 1 test passes.
```

cc @gunan @drpngx 
  ",1,,4,2018-01-03T06:12:08Z,CONTRIBUTOR,2018-01-03T20:57:59Z
15806,Can we install tensorflow with a package manager in Linux distro?,,"As we all know, Tensorflow is a major opensource deeplearning framework to the developers. 
BTW, How can we install tensorflow with a package manager such as apt-get (for *.deb), yum/zypper/dnf (for *.rpm)  in Linux distro?

* Reference - https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/builds

I could not find related scripts from the above web address. Does tensorflow support 1)  manual compilation with ./tools/ci_build/build/*.sh and 2) pre-built docker-based compilation only?

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.0 and Ubuntu 16.04.3
- **TensorFlow installed from (source or binary)**: Latest version of Tensorflow (form github)
- **TensorFlow version (use command below)**: Latest version of Tensorflow (form github)
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: with Cmake (w/o Bazel)
- **GCC/Compiler version (if compiling from source)**: GCC 5.0
- **CUDA/cuDNN version**:  None (w/ CPU only)
- **GPU model and memory**: None , DRAM 16GB
- **Exact command to reproduce**:   Nonthing.



### Describe the problem
No support

### Source code / logs
Omission. 

  ",0,,1,2018-01-03T06:03:26Z,NONE,2018-01-03T20:55:34Z
15803,Memory allocation improvement for `decode_libsvm`,"awaiting testing (then merge),cla: yes","This fix is an improvement to #14330. Previously, string split was handled through `str_util::Split`, which may incur unnecessary memory allocations. This fix uses StringPiece instead.

See comment https://github.com/tensorflow/tensorflow/pull/14330#pullrequestreview-79877956 for reference.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,1,2018-01-03T02:34:18Z,MEMBER,2018-01-03T20:26:39Z
15802,tf.stack eats memory over time,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**:  3.5
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: 1060 + 6GB
 

### Describe the problem
I use tf.stack to stack 2 images. But the memory used by this program increase over time. I use `memory_profiler` check it. it is caused by tf.stack, here is the minimal re-produce code:

```
import tensorflow as tf
import glob
import gc
from memory_profiler import profile


@profile
def function_mark():
  pass

@profile
def stack_images():
  image_file_list = glob.glob(""car_images/*.jpg"")
  sess = tf.Session()

  for _ in range(300):
    # read image
    image1 = tf.gfile.FastGFile(image_file_list[0], 'rb').read()
    image2 = tf.gfile.FastGFile(image_file_list[1], 'rb').read()
    # decode image
    image1_decode = tf.image.decode_image(image1, channels=3)
    image2_decode = tf.image.decode_image(image2, channels=3)
    # stack image
    image_stack = tf.stack([image1_decode, image2_decode])
    # run session
    r_image_stack = sess.run(image_stack)
    # mark function. so I can check the memory-usage of every loop.
    function_mark()
    # force garbage collection, so all the un-reference variable will be freed.
    del r_image_stack
    gc.collect()
```

 First, I profile it line by line, here is the result:
**you can see it very clearly that line 26 take a lot of memory. My image is 800*600 and I only stack 2 image each time, so 1.3G memory consumption is not normal.**

> 
> Line #    Mem usage    Increment   Line Contents
> ================================================
>     11    190.0 MiB    190.0 MiB   @profile
>     12                             def stack_images():
>     13    190.0 MiB      0.0 MiB     image_file_list = glob.glob(""car_images/*.jpg"")
>     14    421.6 MiB    231.7 MiB     sess = tf.Session()
>     15
>     16   1998.4 MiB      0.0 MiB     for _ in range(300):
> 
>     17                                 # read image
>     18   1992.5 MiB      1.5 MiB       image1 = tf.gfile.FastGFile(image_file_list[0], 'rb').read()
>     19   1992.5 MiB      0.0 MiB       image2 = tf.gfile.FastGFile(image_file_list[1], 'rb').read()
>     20                                 # decode image
>     21   1992.8 MiB     77.6 MiB       image1_decode = tf.image.decode_image(image1, channels=3)
>     22   1993.3 MiB    108.6 MiB       image2_decode = tf.image.decode_image(image2, channels=3)
>     23                                 # stack image
>     24   1993.3 MiB      0.7 MiB       image_stack = tf.stack([image1_decode, image2_decode])
>     25                                 # run session
>     26   1998.4 MiB   1350.4 MiB       r_image_stack = sess.run(image_stack)
>     27                                 # mark function. so I can check the memory-usage of every loop.
>     28   1998.4 MiB     29.0 MiB       function_mark()
>     29                                 # force garbage collection, so all the un-reference variable will be freed.
>     30   1998.4 MiB      0.0 MiB       del r_image_stack
>     31   1998.4 MiB      8.9 MiB       gc.collect()

Then **I profile it over time.** I use function `function_mark` to distinguish each loop. So we can see it very clearly that the memory usage of this program is increase over time.
![figure_1](https://user-images.githubusercontent.com/5325686/34505710-d25c80b0-f061-11e7-99c6-5db2e990d9aa.png)

My question is: How should I avoid this problem. because it cause a serious performance regression.
",0,,2,2018-01-03T02:15:52Z,CONTRIBUTOR,2018-01-03T06:27:07Z
15801,Remove calculation of unnecessary matrix columns in SVD gradient,"awaiting testing (then merge),cla: yes","The SVD gradient calculation when `compute_uv=False` currently uses the orthogonal ""U"" and ""V"" matrices returned by the SVD operation with `full_matrices=True`, but it really requires only the `full_matrices=False` versions.  This pull request makes the calculation use the `full_matrices=False` versions.

@rmlarsen pointed out that this change could be made in https://github.com/tensorflow/tensorflow/pull/14259#discussion_r157067512.",1,,8,2018-01-03T01:41:42Z,CONTRIBUTOR,2018-01-03T01:49:21Z
15800,[bug] tf.estimator.DNNClassifier setting n_classes has no effect,,"### Describe the problem
I was following the examples for a [tensorflow estimator](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html). I am setting n_classes but the label check in (_check_labels tensorflow/python/estimator/canned/head.py"", line 222) keeps kicking back the following error:

**ValueError: Mismatched label shape. Classifier configured with n_classes=1.  Received 4. Suggested Fix: check your n_classes argument to the estimator and/or the shape of your label.**

### Source code / logs
``` python
import tensorflow as tf
import numpy as np

trainX = np.array([1,0,2,3])
num_classes = 4
feature_names = ['f1']
feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]
classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns, 
                                            n_classes=num_classes, #setting number of classes here
                                            hidden_units=[10])

def input_fn():
    my_int_variable = tf.get_variable(""my_int_variable"", [1], dtype=tf.int32, initializer=tf.zeros_initializer)
    label = tf.one_hot(my_int_variable, num_classes) #using same number of classes
    return {'f1':trainX},label

classifier.train(input_fn=lambda: input_fn())
```

``` bash
Traceback (most recent call last):
  File ""test.py"", line 17, in <module>
    classifier.train(input_fn=lambda: input_fn())
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 334, in _model_fn
    config=config)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 203, in _dnn_model_fn
    logits=logits)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 493, in create_estimator_spec
    features=features, mode=mode, logits=logits, labels=labels)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 433, in create_loss
    label_ids = self._label_ids(_check_labels(_maybe_expand_dim(labels), 1))
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 222, in _check_labels
    (expected_labels_dimension, dim1))
ValueError: Mismatched label shape. Classifier configured with n_classes=1.  Received 4. Suggested Fix: check your n_classes argument to the estimator and/or the shape of your label.
```

------------------------

### System information
Mac OSX 10.12.6
Python 2.7
Tensorflow ('v1.4.0-19-ga52c8d9b01', '1.4.1')



",0,,3,2018-01-03T00:18:44Z,NONE,2018-01-03T10:05:09Z
15800,[bug] tf.estimator.DNNClassifier setting n_classes has no effect,,"### Describe the problem
I was following the examples for a [tensorflow estimator](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html). I am setting n_classes but the label check in (_check_labels tensorflow/python/estimator/canned/head.py"", line 222) keeps kicking back the following error:

**ValueError: Mismatched label shape. Classifier configured with n_classes=1.  Received 4. Suggested Fix: check your n_classes argument to the estimator and/or the shape of your label.**

### Source code / logs
``` python
import tensorflow as tf
import numpy as np

trainX = np.array([1,0,2,3])
num_classes = 4
feature_names = ['f1']
feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]
classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns, 
                                            n_classes=num_classes, #setting number of classes here
                                            hidden_units=[10])

def input_fn():
    my_int_variable = tf.get_variable(""my_int_variable"", [1], dtype=tf.int32, initializer=tf.zeros_initializer)
    label = tf.one_hot(my_int_variable, num_classes) #using same number of classes
    return {'f1':trainX},label

classifier.train(input_fn=lambda: input_fn())
```

``` bash
Traceback (most recent call last):
  File ""test.py"", line 17, in <module>
    classifier.train(input_fn=lambda: input_fn())
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 334, in _model_fn
    config=config)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 203, in _dnn_model_fn
    logits=logits)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 493, in create_estimator_spec
    features=features, mode=mode, logits=logits, labels=labels)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 433, in create_loss
    label_ids = self._label_ids(_check_labels(_maybe_expand_dim(labels), 1))
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 222, in _check_labels
    (expected_labels_dimension, dim1))
ValueError: Mismatched label shape. Classifier configured with n_classes=1.  Received 4. Suggested Fix: check your n_classes argument to the estimator and/or the shape of your label.
```

------------------------

### System information
Mac OSX 10.12.6
Python 2.7
Tensorflow ('v1.4.0-19-ga52c8d9b01', '1.4.1')



",0,,3,2018-01-03T00:18:44Z,NONE,2018-01-03T10:05:09Z
15797,Change dso_loader to look for libcupti.so instead of libcupti.so.8.0,"awaiting review,cla: yes","On Android, it is hard to package libcupti.so.8.0 with bazel to generate CUDA-enabled apk
The cc_library macro in bazel only looks for *.so, not *.so.*",1,,5,2018-01-02T22:58:58Z,CONTRIBUTOR,2018-01-04T17:29:49Z
15796,Fix build issues with cuda 9.1 through updating eigen.,cla: yes,,0,,2,2018-01-02T21:55:13Z,OWNER,2018-01-04T18:26:17Z
15795,configure.py environment variables,stat:awaiting response,"Have I written custom code: No
OS Platform and Distribution: Linux (Any?)
TensorFlow installed from: Source
TensorFlow version: 1.4 (Master Branch)
Bazel version: 0.6
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A


### System information
- Building from source
- Branch Master (currently d87a9fbbc5f49ec5ae8eb52c62628f0b1a0bf67f)



### Describe the problem
Line 1353 of configure.py needs to have int( ) wrapped around the function get_var, otherwise the string that is returned always flags positive and setting TF_SET_ANDROID_WORKSPACE=0 environment variable to avoid user interaction in building from source will never work.

  ",0,,2,2018-01-02T21:50:40Z,NONE,2018-01-03T07:34:35Z
15790,order quantized table by value for ease of reading,cla: yes,,0,,2,2018-01-02T15:18:42Z,CONTRIBUTOR,2018-01-02T17:40:24Z
15789,order quantized table by value for ease of reading,cla: yes,,0,,2,2018-01-02T15:18:10Z,CONTRIBUTOR,2018-01-02T17:40:42Z
15787,order quantized table by value for ease of reading,cla: yes,,0,,2,2018-01-02T15:11:26Z,CONTRIBUTOR,2018-01-02T17:40:52Z
15780,[configure] eagerly determine the truthfulness of environment variables,"awaiting testing (then merge),cla: yes","Eagerly determine the truthfulness of environment variables in `get_var` function.

This way we can skip checking, e.g. Android workspace setup if the user sets `TF_SET_ANDROID_WORKSPACE=0`

Tested manually. 
",1,,7,2018-01-02T05:59:18Z,CONTRIBUTOR,2018-01-02T06:01:07Z
15775,R1.2,cla: no,,0,,3,2018-01-01T18:44:44Z,NONE,2018-01-02T01:20:34Z
15774,Fix invalid Markdown in docstring,"awaiting testing (then merge),cla: yes","There is currently invalid markdown in the docstring, which is causing the docs site to render incorrectly.

https://www.tensorflow.org/api_docs/python/tf/contrib/gan/estimator/GANEstimator

<img width=""902"" alt=""screen shot 2018-01-01 at 10 17 59 am"" src=""https://user-images.githubusercontent.com/279498/34469987-1d2fbce6-eedd-11e7-896b-a43f65326fd0.png"">

This patch fixes the indentation on the MD code block, which should fix the issue.",0,,5,2018-01-01T18:19:03Z,CONTRIBUTOR,2018-01-01T18:19:44Z
15773,How to define multiple loss function and train_op  in tf.estimator.EstimatorSpec,stat:awaiting tensorflower,"

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0
- **Python version**: 2.7.12
- **CUDA/cuDNN version**: 6.0
- **GPU model and memory**: 1080 Ti + 1080

### Describe the problem
I'm currently implementing a pose estimation system and I defined my network with 3 loss and train_op in each of degree, yaw, pitch and roll. And I'm current using your tf.estimator API which I think is pretty convenient to monitor the system, however I found that I may only be able to define one loss and train_op using this set of API. I would like to know if there is any solution to train and monitor the system with multiple loss and train_op at the same time. Thanks.

### Source code / logs
```
    return tf.estimator.EstimatorSpec(
        mode=mode,
        predictions=predictions,
        loss=[yaw_total_loss, pitch_total_loss, roll_total_loss],  # error
        train_op=[yaw_train_op, pitch_train_op, roll_train_op],  # error
        eval_metric_ops=None)
```
",1,,3,2018-01-01T18:14:02Z,NONE,2018-01-09T23:13:30Z
15772,remove trailing semicolon at the end of line,"awaiting testing (then merge),cla: yes","removed trailing semicolon(;) in the statement

according to [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html?showone=Semicolons#Semicolons)
 _""Do not terminate your lines with semi-colons and do not use semi-colons to put two commands on the same line.""_ ",0,,5,2018-01-01T16:17:46Z,CONTRIBUTOR,2018-01-01T16:28:55Z
15768,Training broke with ResourceExausted error,,"------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.5
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: TITAN X, 12207MiB

----------------------

Most Probably everyone will be asking about this is a question for StackOverflow. Here is the link to the StackOverflow question,  But please take a look at the problem. There could be some bug in tensorflow as the error occurs after 32 epoch.
https://stackoverflow.com/questions/48007984/training-broke-with-resourceexausted-error

Here is the code of the model, https://paste.ubuntu.com/26298336/
A short description of the model would be,

- Character level Embedding Vector -> Embedding lookup -> LSTM1
- Word level Embedding Vector->Embedding lookup -> LSTM2
- [LSTM1+LSTM2] -> single layer MLP-> softmax layer/CRF layer
- [LSTM1+LSTM2] -> Single layer MLP-> WGAN discriminator

While running the code it produces the following error output at the epoch 32,

`ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[24760,100] [[Node: chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split = Split[T=DT_FLOAT, num_split=4, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients_2/Add_3/y, chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] [[Node: bi-lstm/bidirectional_rnn/bw/bw/stack/_167 = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]]`

My question is if there is any error then it should occur in the first epoch why at 32 epoch?

I am using embedding_lookup in following way,

```
_word_embeddings = tf.Variable(
                        embeddings,
                        name=""_word_embeddings"",
                        dtype=tf.float32,
                        trainable=False)
            word_embeddings = tf.nn.embedding_lookup(_word_embeddings, self.word_ids, name=""word_embeddings"")

```

Where `embeddings` is a `(61698, 100)` sized vector. which is only 24 MB. However in the error message, it showed the error with, `(24760, 100)` sized vector which is only 10MB. It also produces warning while declaring optimizers for the model. it was suggested as below,

> gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.

",0,,1,2018-01-01T11:31:01Z,NONE,2018-01-03T02:13:59Z
15766,tf.assert_equal raises incorrect traceback in Eager mode,"comp:eager,type:bug/performance","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.1 LTS
- **TensorFlow installed from (source or binary)**: pip binary
- **TensorFlow version (use command below)**: 1.5.0-dev20171227
- **Python version**: 3.5.0
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: python main.py


### Describe the problem

In eager mode, tf.assert_equal only shows `[]` in traceback message when two inputs are different. However, in graph mode, it does show different values in the message. 

### Source code / logs

```python
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

x = tf.constant([1,2,3])
y = tf.constant([3,2,1])

with tf.control_dependencies([tf.assert_equal(x, y)]):
    output = tf.reduce_sum(x)

```


Eager Mode Traceback:

```
Traceback (most recent call last):
  File ""/Users/matt/PycharmProjects/scratch/main.py"", line 9, in <module>
    with tf.control_dependencies([tf.assert_equal(x, y)]):
  File ""/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 376, in assert_equal
    summary_msg)))
tensorflow.python.framework.errors_impl.InvalidArgumentError: 
Condition x == y did not hold.
Indices of first 0 different values:
[]
Corresponding x values:
[]
Corresponding y values:
[]
```


Graph Mode Traceback:
```
Traceback (most recent call last):
  File ""/Users/matt/PycharmProjects/scratch/main.py"", line 9, in <module>
    with tf.control_dependencies([tf.assert_equal(x, y)]):
  File ""/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 391, in assert_equal
    _assert_static(condition_static, data)
  File ""/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 104, in _assert_static
    message='\n'.join(data_static))
tensorflow.python.framework.errors_impl.InvalidArgumentError: 
Condition x == y did not hold element-wise:
x (Const:0) = 
[1 2 3]
y (Const_1:0) = 
[3 2 1]
```
",1,,3,2018-01-01T09:43:43Z,NONE,2018-01-02T18:51:31Z
15763,Branch 180441903,cla: yes,,0,,1,2018-01-01T00:05:18Z,MEMBER,2018-01-01T06:30:37Z
15762,Make unused variable warning an error during TF builds.,"awaiting testing (then merge),cla: yes",We will need to eyeball build logs and see if this is really working as intended,1,,10,2017-12-31T23:44:09Z,OWNER,2017-12-31T23:56:06Z
15761,"Revert ""add c++ gradient for op: Pow (#15245)""","awaiting testing (then merge),cla: yes","This reverts commit e1ded7fa7cfacaeea43a903e738dd3fe2baabc57.

CC @facaiy ",0,,3,2017-12-31T23:20:10Z,OWNER,2018-01-01T00:19:35Z
15760,Custom gradient aggregation methods,,"I would like a way to apply some custom gradient aggregation ops. Probably the simplest thing to do is just allow `tf.gradients` (and `Optimizer.compute_gradients`) to return un-aggregated gradients so I could work with those?
Anyway, seems like an easy fix? I will do this myself in a month or so (cant now as on holiday), but if someone else wants to do it/has some thoughts, I am interested.
",0,,6,2017-12-31T20:41:42Z,NONE,2018-01-01T22:04:44Z
15759,Update license year,"awaiting testing (then merge),cla: yes","TO DO:

- [x] Wait to the next year (100% done, depends on timezone)
- [ ] Merge!

:octocat:",0,,7,2017-12-31T20:26:04Z,CONTRIBUTOR,2017-12-31T20:36:00Z
15757,Fixes #15736,cla: yes,"This will allow users to import keras submodules without typing `from tensorflow.python.keras...` but just directly from `tensorflow.keras`.
This change also does not create a duplicate of the module object but just assign it two names, one with `tensorflow.python.keras` keeping current functionality and `tensorflow.keras` allowing a much more consistent API use.

With this change the user is able to import keras objects directly, for example
```python
from tensorflow.keras.layers import Dense
```",0,,2,2017-12-31T14:05:33Z,NONE,2017-12-31T22:34:57Z
15755,Tensorflow Dataset.from_generator blocks input?,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: both win7 and CentOS 7.2.1511
- **TensorFlow installed from (source or binary)**: pip3
- **TensorFlow version (use command below)**: 'v1.4.0-rc1-11-g130a514 1.4.0' and '1.5.0-dev20180102'
- **Python version**: 3.5.3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem

I submitted a problem on stackoverflow but nobody solved it. So I open it here. Does anybody can help to solve it?

Here is the problem:
[https://stackoverflow.com/questions/47917288/tensorflow-dataset-from-generator-blocks-input](https://stackoverflow.com/questions/47917288/tensorflow-dataset-from-generator-blocks-input)

I installed tensorflow 1.4.0 via pip, without gpu support. My python version is 3.5.3
  ",0,,7,2017-12-31T12:14:53Z,NONE,2018-01-03T06:41:38Z
15752,Eager: Incompatible rnn model shapes inferred when using more than one CudnnGRU/LSTM,"comp:eager,stat:awaiting tensorflower","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.5.0dev20171230
- **Python version**: 3.6

### Describe the problem
When I use more than one CudnnGRU in eager, I got an error:
```Python
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-f618cd483215> in <module>()
     26 with tf.device(device):
     27     images = tf.constant(toy_data)
---> 28     logits = net(images)

~\Anaconda3\lib\site-packages\tensorflow\python\layers\base.py in __call__(self, inputs, *args, **kwargs)
    651 
    652         if not in_deferred_mode:
--> 653           outputs = self.call(inputs, *args, **kwargs)
    654           if outputs is None:
    655             raise ValueError('A layer\'s `call` method should return a Tensor '

<ipython-input-1-f618cd483215> in call(self, x)
     17         x = tf.transpose(x, [1, 0, 2])
     18         x, s = self.gru1(x)
---> 19         x, s = self.gru2(x)
     20         x = tf.transpose(x, [1, 0, 2])
     21         x = self.fc(x)

~\Anaconda3\lib\site-packages\tensorflow\python\layers\base.py in __call__(self, inputs, *args, **kwargs)
    651 
    652         if not in_deferred_mode:
--> 653           outputs = self.call(inputs, *args, **kwargs)
    654           if outputs is None:
    655             raise ValueError('A layer\'s `call` method should return a Tensor '

~\Anaconda3\lib\site-packages\tensorflow\contrib\cudnn_rnn\python\layers\cudnn_rnn.py in call(self, inputs, initial_state, training)
    400       c = array_ops.constant([], dtype=dtype)
    401     outputs, (output_h, output_c) = self._forward(inputs, h, c, self.kernel,
--> 402                                                   training)
    403     if self._rnn_mode == CUDNN_LSTM:
    404       return outputs, (output_h, output_c)

~\Anaconda3\lib\site-packages\tensorflow\contrib\cudnn_rnn\python\layers\cudnn_rnn.py in _forward(self, inputs, h, c, opaque_params, training)
    475         direction=self._direction,
    476         dropout=self._dropout,
--> 477         seed=self._seed)
    478     return output, (output_h, output_c)
    479 

~\Anaconda3\lib\site-packages\tensorflow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py in _cudnn_rnn(inputs, input_h, input_c, params, is_training, rnn_mode, input_mode, direction, dropout, seed, name)
    858       seed=seed,
    859       seed2=seed2,
--> 860       name=name)
    861   return (outputs, output_h, output_c)
    862 

~\Anaconda3\lib\site-packages\tensorflow\contrib\cudnn_rnn\ops\gen_cudnn_rnn_ops.py in cudnn_rnn(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name)
    120               ""seed2"", seed2, ""is_training"", is_training)
    121     _result = _execute.execute(b""CudnnRNN"", 4, inputs=_inputs_flat,
--> 122                                attrs=_attrs, ctx=_ctx, name=name)
    123   _execute.record_gradient(
    124       ""CudnnRNN"", _inputs_flat, _attrs, _result, name)

~\Anaconda3\lib\site-packages\tensorflow\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     64     else:
     65       message = e.message
---> 66     six.raise_from(core._status_to_exception(e.code, message), None)
     67   # pylint: enable=protected-access
     68   return tensors

~\Anaconda3\lib\site-packages\six.py in raise_from(value, from_value)

InvalidArgumentError: Incompatible rnn model shapes inferred: expecting [num_layers, input_size, num_units, dir_count]: [1, 28, 100, 1], getting [num_layers, input_size, num_units, dir_count]: [1, 100, 100, 1]. [Op:CudnnRNN]
```
If I use same network in graph mode, there is no problem.
reproduce error code:
```Python
import tensorflow as tf
import tensorflow.contrib.eager as tfe
import numpy as np
config = tf.ConfigProto()
config.gpu_options.allow_growth=True
tfe.enable_eager_execution(config=config)
class CudnnCrashNet(tfe.Network):
    def __init__(self, name):
        super(CudnnCrashNet, self).__init__(name)
        self.gru1 = tf.contrib.cudnn_rnn.CudnnGRU(1, 100)
        self.track_layer(self.gru1)
        self.gru2 = tf.contrib.cudnn_rnn.CudnnGRU(1, 100)
        self.track_layer(self.gru2)
        self.fc = self.track_layer(tf.layers.Dense(10))
    def call(self, x):
        x = tf.reshape(x, [-1, 28, 28])
        x = tf.transpose(x, [1, 0, 2])
        x, s = self.gru1(x)
        x, s = self.gru2(x)
        x = tf.transpose(x, [1, 0, 2])
        x = self.fc(x)
        return x
toy_data = np.ones((100, 784)).astype(np.float32)
device = ""gpu:0"" if tfe.num_gpus() else ""cpu:0""
net = CudnnCrashNet('net')
with tf.device(device):
    images = tf.constant(toy_data)
    logits = net(images)
```
normal graph-mode code:
```Python
import tensorflow as tf
import numpy as np
class CudnnNormalNet(tf.layers.Layer):
    def __init__(self, name):
        super(CudnnNormalNet, self).__init__(name)
        self.gru1 = tf.contrib.cudnn_rnn.CudnnGRU(1, 100)
        self.gru2 = tf.contrib.cudnn_rnn.CudnnGRU(1, 100)
        self.fc = tf.layers.Dense(10)
    def call(self, x):
        x = tf.reshape(x, [-1, 28, 28])
        x = tf.transpose(x, [1, 0, 2])
        x, s = self.gru1(x)
        x, s = self.gru2(x)
        x = tf.transpose(x, [1, 0, 2])
        x = self.fc(x)
        return x
config = tf.ConfigProto()
config.gpu_options.allow_growth=True
toy_data = np.ones((100, 784)).astype(np.float32)
with tf.Graph().as_default():
    net = CudnnNormalNet('net')
    x_p = tf.placeholder(tf.float32, [None, 784])
    logits = net(x_p)
    with tf.Session(config=config) as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(logits, feed_dict={x_p: toy_data})
```",1,,5,2017-12-31T10:29:19Z,NONE,2018-01-03T02:08:50Z
15750,[XLA] Fix std::array initialization take 2,"awaiting testing (then merge),cla: yes","#15511 did not fix the issue on MSVC.

The actual root cause is due to the presence of `(` and `)` around const C array. For `std::array<int, 2> a ({1, 2, 3})`, MSVC seems to see `({1, 2, 3})` as a pointer, which triggers [C2100 compile error](https://msdn.microsoft.com/en-us/library/bzf3eha6.aspx). Removing `(` and `)` fixes the issue.

#15213",0,,2,2017-12-31T09:29:10Z,CONTRIBUTOR,2017-12-31T22:31:11Z
15749,[XLA] Define TF_COMPILE_LIBRARY for two libraries,"awaiting testing (then merge),cla: yes","Both [`index_ops_kernel_argmax_float_1d.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/kernels/index_ops_kernel_argmax_float_1d.cc#L48) and [`index_ops_kernel_argmax_float_2d.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/kernels/index_ops_kernel_argmax_float_2d.cc#L50) use `TF_EXPORT` macro. We need to define `TF_COMPILE_LIBRARY` (comes from [`tf_copts`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L170)) to make sure `TF_EXPORT` is expanded into `__declspec(dllexport)`.

#15213",0,,2,2017-12-31T09:20:20Z,CONTRIBUTOR,2017-12-31T22:29:15Z
15748,[XLA] Add missing win header deps to framework_lite,"awaiting testing (then merge),cla: yes","Continue from #15579. Most Tensorflow components depend on `framework` rather than `framework_lite`, so I did not notice this until I try to build XLA/tfcompile locally again.

#15213",0,,2,2017-12-31T09:14:28Z,CONTRIBUTOR,2017-12-31T22:27:59Z
15746,CIFAR10 slows down every 100th step,"stat:community support,type:bug/performance","### System information
- **Have I written custom code**: No
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: Have tried both binary and source
- **TensorFlow version**: 1.4.0-19-ga52c8d9, 1.4.1
- **Python version**: 2.7.12
- **CUDA/cuDNN version**: 8.0.61
- **Hardware**: GPU: NVIDIA GeForce GTX 1080 Ti (11GB), RAM: 64GB, CPU: Intel i7-6850K
- **Exact command to reproduce**: python cifar10_train.py

### Describe the problem
The [CIFAR10 Tensorflow tutorial](https://www.tensorflow.org/tutorials/deep_cnn) seems to have a few odd patterns when it comes to the number of examples per second it can compute:
 - Step 0 is extremely slow 
 - Every 100th step is significantly slow 
 - The step after a really slow step is either a little slow or average
 - The next 10-30 steps after that are slightly boosted (faster than average)
 - The rest of the steps are average speed

I'm hoping for (in order of importance):
 - An explanation and fix for every 100th step being so slow
 - An explanation and instructions showing me how to make every step run at the boosted speed (the speed shortly after a slow step)
 - An explanation and fix for the slow 0th and 1st step

I can't find any additional logging or processing that happens on every 100th step. Could it be `tf.train.MonitoredSession`?

### Reproducible:
- when training on CPU rather than GPU
- independent of batch size
- on MacBook Pro (Retina, 13-inch, Mid 2014)

### Hardware utilization: 
1. Average:
- CPU: 82-84%
- GPU: 70-85%
- RAM: 3.7GB

2. Every 100th step:
- CPU: 9%
- GPU: 0%
- RAM: 3.7GB

3. Boosted after slow step: 
- GPU: 92%
- CPU: 82-84%
- RAM: 3.7GB

4. Idle:
- CPU: 1%
- GPU: 0%
- RAM: 1.6GB

*Overall CPU and RAM usage (clearly showing CPU trough every 100 steps)*:
![Overall CPU and RAM usage](https://user-images.githubusercontent.com/7654904/34998310-6b153646-fae7-11e7-8e0c-00ccf01349bf.png)

### Logs excerpt [(full logs)](https://github.com/tensorflow/tensorflow/files/1635342/terminalOutput.txt):

> step 0: (587.3 examples/sec; 6.974 sec/batch)
> step 1: (22630.6 examples/sec; 0.181 sec/batch)
> step 2: (36253.6 examples/sec; 0.113 sec/batch)
> step 3: (37966.0 examples/sec; 0.108 sec/batch)
> step 4: (38511.4 examples/sec; 0.106 sec/batch)
> step 5: (38554.6 examples/sec; 0.106 sec/batch)
> step 6: (32112.4 examples/sec; 0.128 sec/batch)
> step 7: (38912.4 examples/sec; 0.105 sec/batch)
> step 8: (39377.0 examples/sec; 0.104 sec/batch)
> step 9: (38206.2 examples/sec; 0.107 sec/batch)
> step 10: (38222.1 examples/sec; 0.107 sec/batch)
> step 11: (38757.5 examples/sec; 0.106 sec/batch)
> step 12: (38833.1 examples/sec; 0.105 sec/batch)
> step 13: (39774.8 examples/sec; 0.103 sec/batch)
> step 14: (39795.9 examples/sec; 0.103 sec/batch)
> step 15: (37850.5 examples/sec; 0.108 sec/batch)
> step 16: (38443.5 examples/sec; 0.107 sec/batch)
> step 17: (39194.6 examples/sec; 0.105 sec/batch)
> step 18: (39164.0 examples/sec; 0.105 sec/batch)
> step 19: (39057.5 examples/sec; 0.105 sec/batch)
> step 20: (33268.7 examples/sec; 0.123 sec/batch)
> step 21: (39459.7 examples/sec; 0.104 sec/batch)
> step 22: (39336.2 examples/sec; 0.104 sec/batch)
> step 23: (39207.1 examples/sec; 0.104 sec/batch)
> step 24: (39330.5 examples/sec; 0.104 sec/batch)
> step 25: (38783.9 examples/sec; 0.106 sec/batch)
> step 26: (39038.9 examples/sec; 0.105 sec/batch)
> step 27: (39214.2 examples/sec; 0.104 sec/batch)
> step 28: (39525.9 examples/sec; 0.104 sec/batch)
> step 29: (37209.0 examples/sec; 0.110 sec/batch)
> step 30: (38356.7 examples/sec; 0.107 sec/batch)
> step 31: (36077.0 examples/sec; 0.114 sec/batch)
> step 32: (37143.8 examples/sec; 0.110 sec/batch)
> step 33: (35961.1 examples/sec; 0.114 sec/batch)
> step 34: (33378.4 examples/sec; 0.123 sec/batch)
> step 35: (37830.3 examples/sec; 0.108 sec/batch)
> step 36: (36789.5 examples/sec; 0.111 sec/batch)
> step 37: (36638.2 examples/sec; 0.112 sec/batch)
> step 38: (36848.1 examples/sec; 0.111 sec/batch)
> step 39: (36041.4 examples/sec; 0.114 sec/batch)
> step 40: (36612.0 examples/sec; 0.112 sec/batch)
> step 41: (35623.9 examples/sec; 0.115 sec/batch)
> step 42: (37589.3 examples/sec; 0.109 sec/batch)
> step 43: (37462.9 examples/sec; 0.109 sec/batch)
> step 44: (35823.6 examples/sec; 0.114 sec/batch)
> step 45: (35911.8 examples/sec; 0.114 sec/batch)
> step 46: (36073.8 examples/sec; 0.114 sec/batch)
> step 47: (36930.2 examples/sec; 0.111 sec/batch)
> step 48: (36142.9 examples/sec; 0.113 sec/batch)
> ...
> step 99: (36434.8 examples/sec; 0.112 sec/batch)
> step 100: (1215.0 examples/sec; 3.371 sec/batch)
> step 101: (35952.9 examples/sec; 0.114 sec/batch)
> step 102: (38422.5 examples/sec; 0.107 sec/batch)
> step 103: (39315.8 examples/sec; 0.104 sec/batch)
> step 104: (38989.1 examples/sec; 0.105 sec/batch)
> step 105: (39091.4 examples/sec; 0.105 sec/batch)
> step 106: (39247.6 examples/sec; 0.104 sec/batch)
> step 107: (38009.7 examples/sec; 0.108 sec/batch)
> step 108: (38746.7 examples/sec; 0.106 sec/batch)
> step 109: (39505.4 examples/sec; 0.104 sec/batch)
> step 110: (39340.0 examples/sec; 0.104 sec/batch)
> step 111: (39065.0 examples/sec; 0.105 sec/batch)
> step 112: (38561.1 examples/sec; 0.106 sec/batch)
> step 113: (39109.0 examples/sec; 0.105 sec/batch)
> step 114: (39203.7 examples/sec; 0.104 sec/batch)
> step 115: (39144.4 examples/sec; 0.105 sec/batch)
> step 116: (38317.6 examples/sec; 0.107 sec/batch)
> step 117: (33757.5 examples/sec; 0.121 sec/batch)
> step 118: (34115.4 examples/sec; 0.120 sec/batch)
> step 119: (35671.4 examples/sec; 0.115 sec/batch)
> step 120: (35297.2 examples/sec; 0.116 sec/batch)
> step 121: (36152.8 examples/sec; 0.113 sec/batch)
> step 122: (35780.1 examples/sec; 0.114 sec/batch)
> step 123: (35847.1 examples/sec; 0.114 sec/batch)
> step 124: (36888.9 examples/sec; 0.111 sec/batch)
> step 125: (36946.2 examples/sec; 0.111 sec/batch)
> ...",0,,9,2017-12-31T09:02:50Z,NONE,2018-01-03T02:07:55Z
15745,Eager: variable created in @tfe.defun is invalid and raise error when print,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.5.0dev20171230
- **Python version**: 3.6

### Describe the problem
I want to use defun to speed up static rnn compute in eager:
```Python
def _eager_dynamic_rnn(cell,
                       inputs,
                       sequence_length=None,
                       initial_state=None,
                       dtype=tf.float32,
                       parallel_iterations=None,
                       swap_memory=False,
                       time_major=False,
                       scope=None):
    time_axis = 0 if time_major else 1
    input_shape = inputs.shape.as_list()
    seq_len = input_shape[time_axis]
    if time_major:
        batch_size = input_shape[1]
    else:
        batch_size = input_shape[0]
    if initial_state is None:
        initial_state = cell.zero_state(batch_size, dtype)
    inputs = tf.unstack(inputs, num=seq_len, axis=time_axis)
    outputs = []
    for inp in inputs:
        output, initial_state = cell(inp, initial_state)
        outputs.append(output)
    outputs = tf.stack(outputs, axis=time_axis)
    return outputs, initial_state


@tfe.defun
def _eager_compiled_dynamic_rnn(cell,
                                inputs,
                                sequence_length=None,
                                initial_state=None,
                                dtype=tf.float32,
                                parallel_iterations=None,
                                swap_memory=False,
                                time_major=False,
                                scope=None):
    return _eager_dynamic_rnn(cell, inputs, sequence_length, initial_state,
                              dtype, None, False, time_major, scope)
```
If I directly use `_eager_compiled_dynamic_rnn` in forward, because of `tf.layers.Layer` create variables in its first __call__, then variables created in `_eager_compiled_dynamic_rnn` is invalid, if print it, get a error:
```Python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-7-308544234cc4> in <module>()
      1 for var in net.variables:
----> 2     print(var)

~\Anaconda3\lib\site-packages\tensorflow\python\ops\variables.py in __repr__(self)
    233       return ""<tf.Variable '%s' shape=%s dtype=%s, numpy=%s>"" % (
    234           self.name, self.get_shape(), self.dtype.name,
--> 235           ops.numpy_text(self.read_value(), is_repr=True))
    236     else:
    237       return ""<tf.Variable '%s' shape=%s dtype=%s>"" % (

~\Anaconda3\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py in read_value(self)
    679       # Ensure we read the variable in the same device as the handle.
    680       with ops.device(self._handle_device):
--> 681         value = self._read_variable_op()
    682     # Return an identity so it can get placed on whatever device the context
    683     # specifies instead of the device where the variable is.

~\Anaconda3\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py in _read_variable_op(self)
    657       tape.watch_variable(self)
    658     return gen_resource_variable_ops.read_variable_op(self._handle,
--> 659                                                       self._dtype)
    660 
    661   def read_value(self):

~\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_resource_variable_ops.py in read_variable_op(resource, dtype, name)
    209     _attrs = (""dtype"", dtype)
    210     _result = _execute.execute(b""ReadVariableOp"", 1, inputs=_inputs_flat,
--> 211                                attrs=_attrs, ctx=_ctx, name=name)
    212   _execute.record_gradient(
    213       ""ReadVariableOp"", _inputs_flat, _attrs, _result, name)

~\Anaconda3\lib\site-packages\tensorflow\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,
     59                                                op_name, inputs, attrs,
---> 60                                                num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

TypeError: provided list of inputs contains objects other than 'EagerTensor'
```
To solve this problem, I must call function which isn't decorated by tfe.defun in first forward, then switch to `_eager_compiled_dynamic_rnn`:
```Python
if self._cell.built is True:
    func = _eager_compiled_dynamic_rnn
else:
    func = _eager_dynamic_rnn
outputs, state = func(
    self._cell,
    inputs,
    seq_len,
    state,
    dtype=self._rnn_dtype,
    time_major=self._time_major, )
```
locate this error cost me much time. please consider to fix it.",1,,6,2017-12-31T08:13:11Z,NONE,2017-12-31T08:32:09Z
15744,Summary op crashes when run multiple times,stat:awaiting response,"### System information
- **Windows 10 16299**:
- **pip install tensorflow-gpu**:
- **b'unknown' 1.4.0**:
- **3.5**: 
- **CUDA: V9.1.85, cuDNN version: 6**:
- **Geforce 930 MX, 2GB**:

### Describe the problem
Opening a session for the second time and trying to run a summary op causes crash for some reason. The error message is misleading as a placeholder is provided.

### Source code / logs
```
import tensorflow as tf

class MyModel:
    def __init__(self, sess, i):
        self.x = tf.placeholder(tf.float32, [2, 2])
        self.W = tf.Variable(tf.truncated_normal([2, 2]))
        self.y = tf.matmul(self.W, self.x)

        tf.summary.scalar('y',tf.reduce_sum(self.y))

        sess.run(tf.global_variables_initializer())

        self.summary = tf.summary.merge_all()
        self.writer = tf.summary.FileWriter('./logs/test' + str(i),sess.graph)

    def run(self, sess):        
        feed_dict = {self.x:[[0,0],[0,0]]}
        sess.run(self.y,feed_dict)
        print('inside1')
        s = sess.run(self.summary,feed_dict)
        print('inside2')
        self.writer.add_summary(s,0)

    def close(self):
        self.writer.close()        

# Swapping order of line 1 and 2 still causes crash
for i in range(3): # line 1
    with tf.Session() as sess: # line 2
        print('outside')
        M = MyModel(sess, i)
        M.run(sess)
        M.close()
    #sess.close() # No luck
```

OUTPUT:
```
outside
inside1
inside2
outside
inside1

Traceback (most recent call last):
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1323, in _do_call
    return fn(*args)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1302, in _run_fn
    status, run_metadata)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [2,2]
	 [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[2,2], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
	 [[Node: Sum_1/_7 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_7_Sum_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\TensorFlow\tensorflow-experiments\ravens-matrix-autoencoder\minimum.py"", line 31, in <module>
    M.run(sess)
  File ""D:\TensorFlow\tensorflow-experiments\ravens-matrix-autoencoder\minimum.py"", line 20, in run
    s = sess.run(self.summary,feed_dict)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 889, in run
    run_metadata_ptr)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [2,2]
	 [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[2,2], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
	 [[Node: Sum_1/_7 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_7_Sum_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'Placeholder', defined at:
  File ""<string>"", line 1, in <module>
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\idlelib\run.py"", line 124, in main
    ret = method(*args, **kwargs)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\idlelib\run.py"", line 351, in runcode
    exec(code, self.locals)
  File ""D:\TensorFlow\tensorflow-experiments\ravens-matrix-autoencoder\minimum.py"", line 30, in <module>
    M = MyModel(sess, i)
  File ""D:\TensorFlow\tensorflow-experiments\ravens-matrix-autoencoder\minimum.py"", line 5, in __init__
    self.x = tf.placeholder(tf.float32, [2, 2])
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1599, in placeholder
    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 3090, in _placeholder
    ""Placeholder"", dtype=dtype, shape=shape, name=name)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [2,2]
	 [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[2,2], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
	 [[Node: Sum_1/_7 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_7_Sum_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```
",0,,2,2017-12-31T04:19:35Z,NONE,2018-01-03T02:05:56Z
15743,Including common.h with NEON_2_SSE.h,"awaiting testing (then merge),cla: yes,comp:lite",Including common.h to make sure that USE_NEON is defined in case of NEON_2_SSE.h is used; otherwise USE_NEON will not be propagated to this file and `portable_tensor_utils.h` will be used,1,,25,2017-12-31T00:15:31Z,CONTRIBUTOR,2017-12-31T00:16:45Z
15742,Use Eigen version of the `scalar_pow_op` for `pow` ops,"awaiting review,cla: yes","This fix use `scalar_pow_op` in Eigen to replace customerized scalar_binary_pow_op_google, as `scalar_pow_op` seems to be in place in Eigen.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,3,2017-12-30T23:47:11Z,MEMBER,2017-12-31T05:50:49Z
15741,Python tensorflow module cannot be reloaded (bug),"stat:awaiting tensorflower,type:feature","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  confirmed on both Ubuntu 16.04 LTS in VirtualBox and OS X 10.12.6
- **TensorFlow installed from (source or binary)**:  installed via pip
- **TensorFlow version (use command below)**:  ('v1.4.0-19-ga52c8d9b01', '1.4.1')
- **Python version**: Python 2.7.13
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

> import tensorflow as tf
> reload(tf)

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Simple bug:

Trying to reload the module causes a failure.  Not a major problem, in general, but troublesome for the task, which is automated testing of the tensorflow Python API using TSTL (https://github.com/agroce/tstl).

The exact sequence is trivial:

>>> import tensorflow as tf
>>> reload(tf)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/__init__.py"", line 40, in <module>
    del python
NameError: name 'python' is not defined

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,8,2017-12-30T22:51:59Z,NONE,2017-12-31T08:29:45Z
15740,Simplify the dense_to_one_hot method,cla: yes,"Simplified the `dense_to_one_hot` method.
The updated method behaves exactly like the old one, but is more concise.",0,,3,2017-12-30T21:01:09Z,NONE,2017-12-31T05:53:40Z
15739,Fix the headers error due to recent CUDA9.1 change,"awaiting testing (then merge),cla: yes",Some headers in CUDA 9.1 has been move to cuda/include/crt directory. ,0,,10,2017-12-30T19:41:04Z,CONTRIBUTOR,2017-12-31T05:54:15Z
15738,"wrong output(extra symbol ""b"")",,"**System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows  Home
TensorFlow installed from (source or binary): binary (pip)
TensorFlow version (use command below): 1.4.0 (GPU)
Python version: 3.6**
GPU: GTX1050 Ti M (disabled Intel visualization)

Basically any output I try to write in console is with extra symbol : ""b""
for example:
with cmmand:
```
C: python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'unknown' 1.4.0 //OUTPUT
```
I downloaded GPU verison form guide from official webiste(with:Anaconda):
[https://www.tensorflow.org/install/install_windows#installing_with_anaconda](url)

i have tried the validation example and same problem (extra symbol : ""b"")
",0,,3,2017-12-30T18:37:28Z,NONE,2017-12-31T03:48:03Z
15737,"AttentionWrapper zero_state(batch_size, tf.float32).clone(cell_state=encoder_state) fails when batch size is 1",stat:awaiting tensorflower,"Hello!
I believe to have found a small bug when using the `zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)` command. When batch size is 1, the error `ValueError: The shape for decoder/while/Merge_5:0 is not an invariant for the loop. It enters the loop with shape (1, 512), but has shape (?, 512) after one iteration. Provide shape invariants using either the shape_invariants argument of tf.while_loop or set_shape() on the loop variables.` is thrown. This error does not occur when batch size is 2 or larger. The error also doesn't occur if I remove the .clone command. 
I tried investigating where the error is, but couldn't find the cause. I'm using this in context of trying to build a neural transducer, but also get the same error for basic seq2seq:
(Based on the NMT tutorial)

```python
# .... Encoder, constants etc...
# Decoder
helper = tf.contrib.seq2seq.TrainingHelper(
    decoder_inputs_embedded, decoder_full_length, time_major=True) 

attention_states = tf.transpose(encoder_outputs, [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]
attention_mechanism = tf.contrib.seq2seq.LuongAttention(
    encoder_hidden_units, attention_states,
    memory_sequence_length=encoder_inputs_length)
decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
    tf.contrib.rnn.LSTMCell(decoder_hidden_units),
    attention_mechanism,
    attention_layer_size=decoder_hidden_units)

projection_layer = layers_core.Dense(
    vocab_size, use_bias=False)


decoder = tf.contrib.seq2seq.BasicDecoder(
    decoder_cell, helper, decoder_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state),
    output_layer=projection_layer)

# ---- Training ----
outputs, last_state, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True)
logits = outputs.rnn_output
decoder_prediction = outputs.sample_id
```


TF Version: ('v1.4.0-19-ga52c8d9', '1.4.1')
System details:
```
== cat /etc/issue ===============================================
Linux nikita-coolboi 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""17.10 (Artful Aardvark)""
VERSION_ID=""17.10""
VERSION_CODENAME=artful

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 7.2.0-8ubuntu3) 7.2.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux nikita-coolboi 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.5.1)
tensorflow (1.4.1)
tensorflow-tensorboard (0.4.0rc3)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.4.1
tf.GIT_VERSION = v1.4.0-19-ga52c8d9
tf.COMPILER_VERSION = v1.4.0-19-ga52c8d9
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```
I believe this is an important issue, as often times when experimenting with new seq2seq models I start off with trying to get it to work for a batch size of 1.

Thanks!
Nikita",2,,12,2017-12-30T18:04:33Z,NONE,2017-12-31T08:36:14Z
15736,Importing submodules from tensorflow.keras fails with No module named 'tensorflow.keras',,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary (pip)
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 3.6
- **Exact command to reproduce**:
from tensorflow.keras import datasets # fails but should work
import tensorflow as tf #succeeds
tf.keras.datasets #succeeds
from tensorflow.python.keras import datasetst # succeeds

### Describe the problem
Importing submodules from tensorflow.keras fails with error: `ModuleNotFoundError: No module named 'tensorflow.keras'`. but `import tensorflow as tf` and then doing `tf.keras.datasets` works. This is a big inconsistency, also it means that every time an element from within the `tensforlow.keras` module you need to write the complete path (which is very annoying) this removes the simplicity and readability of the `keras` API. A work around is to import submodules from `tensorflow.python.keras`, which again is inconsistent. 

In my opinion, since the documentation states that `keras` is availabe at `tf.keras` that should be the access path to the submodules and not `tensorflow.python.keras`. I'll try to  make a pull request for this.

### Source code / logs

```python
# fails but should work
from tensorflow.keras import datasets
```
```python
# succeeds
import tensorflow as tf
tf.keras.datasets
```
```python
# succeeds
from tensorflow.python.keras import datasetst
```",0,,9,2017-12-30T16:44:49Z,NONE,2017-12-31T07:05:41Z
15734,fix typo,"awaiting testing (then merge),cla: yes",fix typo,0,,1,2017-12-30T14:58:24Z,CONTRIBUTOR,2017-12-30T16:17:44Z
15733,"""Failed to load the native TensorFlow runtime."" error on Raspbian stretch",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Raspbian Strecth
- **TensorFlow installed from (source or binary)**: Source (Compiled from git on raspberry pi 3)
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: gcc version 4.8.5 (Raspbian 4.8.5-4) 
- **CUDA/cuDNN version**: No (nonconfigured)
- **GPU model and memory**:No (nonconfigured)
- **Exact command to reproduce**: python3 "">>> import tensorflow""
Hi,
I have compiled tensorflow's 1.4.1 source code from git source. There was no error while compiling from source but after installition by pip3, I can't import tensorflow library in python3
When I gave ""import tensorflow"" command in the python 3 shell it gives this error:

 "">>> import tensorflow
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws11Environment6GetEnvEPKc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws11Environment6GetEnvEPKc


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems""

",0,,4,2017-12-30T14:40:22Z,NONE,2017-12-31T08:46:34Z
15728,Clarify the description of batch_norm in order to highlight the dimen,"awaiting testing (then merge),cla: yes","sion selection for normalization.

This should work now.",1,,2,2017-12-30T08:25:29Z,CONTRIBUTOR,2018-01-04T18:01:48Z
15727,using tf.layers.batch_normalization() gives erratic validation loss though implementation seems correct.,,"I am trying to use Batch Normalization using [tf.layers.batch_normalization()][1] and I have followed the documentation closely. My code looks like this:

<!-- language: python -->

    def create_conv_exp_model(fingerprint_input, model_settings, is_training):
      

      # Dropout placeholder
      if is_training:
        dropout_prob = tf.placeholder(tf.float32, name='dropout_prob')

      # Mode placeholder
      mode_placeholder = tf.placeholder(tf.bool, name=""mode_placeholder"")

      he_init = tf.contrib.layers.variance_scaling_initializer(mode=""FAN_AVG"")

      # Input Layer
      input_frequency_size = model_settings['bins']
      input_time_size = model_settings['spectrogram_length']
      net = tf.reshape(fingerprint_input,
                       [-1, input_time_size, input_frequency_size, 1],
                       name=""reshape"")
      net = tf.layers.batch_normalization(net, 
                                          training=mode_placeholder,
                                          name='bn_0')

      for i in range(1, 6):
        net = tf.layers.conv2d(inputs=net,
                               filters=8*(2**i),
                               kernel_size=[5, 5],
                               padding='same',
                               kernel_initializer=he_init,
                               name=""conv_%d""%i)
        net = tf.layers.batch_normalization(net,
                                            training=mode_placeholder,
                                            name='bn_%d'%i)
        with tf.name_scope(""relu_%d""%i):
          net = tf.nn.relu(net)
        net = tf.layers.max_pooling2d(net, [2, 2], [2, 2], 'SAME', 
                                      name=""maxpool_%d""%i)

      net_shape = net.get_shape().as_list()
      net_height = net_shape[1]
      net_width = net_shape[2]
      net = tf.layers.conv2d( inputs=net,
                              filters=1024,
                              kernel_size=[net_height, net_width],
                              strides=(net_height, net_width),
                              padding='same',
                              kernel_initializer=he_init,
                              name=""conv_f"")
      net = tf.layers.batch_normalization( net, 
                                            training=mode_placeholder,
                                            name='bn_f')
      with tf.name_scope(""relu_f""):
        net = tf.nn.relu(net)

      net = tf.layers.conv2d( inputs=net,
                              filters=model_settings['label_count'],
                              kernel_size=[1, 1],
                              padding='same',
                              kernel_initializer=he_init,
                              name=""conv_l"")

      ### Squeeze
      squeezed = tf.squeeze(net, axis=[1, 2], name=""squeezed"")

      if is_training:
        return squeezed, dropout_prob, mode_placeholder
      else:
        return squeezed, mode_placeholder

And my train step looks like this:

<!-- language: python -->

    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):
      optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_input)
      gvs = optimizer.compute_gradients(cross_entropy_mean)
      capped_gvs = [(tf.clip_by_value(grad, -2., 2.), var) for grad, var in gvs]
      train_step = optimizer.apply_gradients(gvs))

During training, I am feeding the graph with:

<!-- language: python -->

    train_summary, train_accuracy, cross_entropy_value, _, _ = sess.run(
        [
            merged_summaries, evaluation_step, cross_entropy_mean, train_step,
            increment_global_step
        ],
        feed_dict={
            fingerprint_input: train_fingerprints,
            ground_truth_input: train_ground_truth,
            learning_rate_input: learning_rate_value,
            dropout_prob: 0.5,
            mode_placeholder: True
        })

During validation, 

<!-- language: python -->

    validation_summary, validation_accuracy, conf_matrix = sess.run(
                    [merged_summaries, evaluation_step, confusion_matrix],
                    feed_dict={
                        fingerprint_input: validation_fingerprints,
                        ground_truth_input: validation_ground_truth,
                        dropout_prob: 1.0,
                        mode_placeholder: False
                    })

My loss and accuracy curves (orange is training, blue is validation):
[Plot of loss vs number of iterations][2],
[Plot of accuracy vs number of iterations][3]

The validation loss (and accuracy) seem very erratic. Is my implementation of Batch Normalization wrong? Or is this normal with Batch Normalization and I should wait for more iterations? Or maybe, moving statistics are not being saved and hence poor performance. I tried StackOverflow and found many people have the same problem and there is no definitive guide on how to resolve this.

  [1]: https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization
  [2]: https://i.stack.imgur.com/ZAqDw.png
  [3]: https://i.stack.imgur.com/CYKJX.png",0,,1,2017-12-30T07:00:14Z,NONE,2017-12-30T09:12:21Z
15725,Source Built r1.4 on GPUs with CPU optimized is always slower than 'no cpu optimization',stat:awaiting tensorflower,"### System information
- **Have I written custom code**: Yes, the model named PSIque [arXiv:1711.10644](https://arxiv.org/abs/1711.10644)
- **OS Platform and Distribution**: CentOS 7.1/CentOS 7.3
- **TensorFlow installed from**: source build w/ Bazel
- **TensorFlow version**: 1.4
- **Python version**: Anaconda 3.6.2
- **Bazel version**: 0.8.1
- **GCC/Compiler version (if compiling from source)**: gcc version 4.8.3 20140911 (Red Hat4.8.3-9) (GCC)
- **CUDA/cuDNN version**: CUDA 8.0/r375.26/cuDNN 6.0.0 & CUDA 9.0/r384.81/cuDNN 7.0.5 
- **GPU model and memory**: E5-2660v3*2 Socket, K40m 12GB, P100-PCIE-16GB
- **Exact command to reproduce**: python model.py

### Describe the problem
* I built tensorflow from source for boosting operation performance.

* 6 different distributions were built;
  * CPU only & No CPU optimization (NO EXTRA flags)
  * CPU only & CPU optimization (--config=opt)
  * GPU support & CUDA 8/9 & No CPU optimization (--config=cuda)
  * GPU support & CUDA 8/9 & CPU optimization (--config=opt --config=cuda)

* Experiments were basically done by 5 phases; experiments on CPU only are still going on,
so please focus on GPU version results.

* Results are quite frustrating me, because 'most of CPU optimized versions' gave me slow results.
![results](https://user-images.githubusercontent.com/20734988/34451067-1d8c1cf0-ed5e-11e7-9a4f-3ff21a5c9aea.png)

* Test were made on multiple machine with random order.
  * P100: 2 nodes
  * K40m: 7 nodes
  * CPU only: 8 nodes

* I am curious why CPU optimized version is slow
  * on every experiment combinations
  * even different GPU environments
  * even Dual CPU socket (E5-2660v3)

* (Extra) I believe my current model does not require high throughputs

### tf_env_collect.sh
== cat /etc/issue ===============================================
Linux <hostname> 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7 (Core)""
VERSION_ID=""7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 4.8.3 20140911 (Red Hat 4.8.3-9)
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux vis5 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.4.0)
tensorflow (1.4.0)
tensorflow-tensorboard (0.4.0rc3)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.4.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Sat Dec 30 12:39:08 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |
| N/A   28C    P0    35W / 250W |      0MiB / 16276MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P100-PCIE...  On   | 00000000:82:00.0 Off |                    0 |
| N/A   35C    P0    38W / 250W |  15661MiB / 16276MiB |     19%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    1     68169      C   python                                     15643MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-9.0/lib64/libcudart_static.a
",0,,6,2017-12-30T03:46:20Z,NONE,2017-12-31T08:41:20Z
15724,How to register all kernels to Android lib,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.0.1
- **Python version**: 3.4
- **Bazel version (if compiling from source)**: 0.4

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I am trying to compile a Android lib which can load a MetaGraph into a session as [this link](https://stackoverflow.com/questions/35508866/tensorflow-different-ways-to-export-and-run-graph-in-c/43639305#43639305) specifies. I first extracts a GraphDef from this MetaGraph and uses this GraphDef to generate `ops_to_register.h`. Then I compile the lib as 

`bazel build my_model/test:test_lib --copt=-DSELECTIVE_REGISTRATION  --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a`

However when I trying to run the test_lib, it complains some kernels are not registered. Then I brutally adds all kernels to be registered as

`#define SHOULD_REGISTER_OP_KERNEL(clz) true`

However, it still complains

```
Error creating graph: Invalid argument: No OpKernel was registered to support Op 'Const' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[Node: save/RestoreV2_8/shape_and_slices = Const[_output_shapes=[[1]], dtype=DT_STRING, value=Tensor<type: string shape: [1] values: >]()]]
```

How can I solve this by adding this Const kernel or simply registering all kernels?",0,,1,2017-12-30T03:16:38Z,NONE,2018-01-03T01:57:02Z
15723,Load region from `~/.aws/config` if possible in S3,"awaiting review,cla: yes","This fix tries to address the issue raised in https://github.com/tensorflow/tensorflow/issues/15662#issuecomment-354272697 where TensorFlow does not load region from `~/.aws/config` if exists. The reason was that AWS C++ SDK does not use the config file by default.

This fix adds the loading of config file (`~/.aws/config`) explicitly, if either AWS_REGION or S3_REGION is not available. In case none of the `AWS_REGION`, `S3_REGION`, `~/.aws/config` is available, then the default `use-east-1` is used (by AWS C++ SDK).

This fix is related to #15562.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,8,2017-12-30T01:58:15Z,MEMBER,2017-12-30T04:13:48Z
15719,Fixed a typo for CreateBody(),"awaiting testing (then merge),cla: yes",,0,,1,2017-12-29T19:44:28Z,CONTRIBUTOR,2017-12-29T23:17:06Z
15718,MKL: update mkldnn to the latest release,"awaiting testing (then merge),cla: yes","This commit will pull the latest changes from the mkl-dnn tree.

the mirror.bazel.build URL doesn't exist: ""https://mirror.bazel.build/github.com/01org/mkl-dnn/archive/e0bfcaa7fcb2b1e1558f5f0676933c1db807a729.tar.gz"" can you create it?",0,,3,2017-12-29T18:43:38Z,CONTRIBUTOR,2017-12-29T18:45:06Z
15714,Optimizing code and adding from http to https,"awaiting testing (then merge),cla: yes",,0,,3,2017-12-29T09:35:10Z,CONTRIBUTOR,2018-01-01T05:20:01Z
15711,Update estimator.md,"awaiting testing (then merge),cla: yes","The anchor ""#fit_dnnclassifier"" can not locate to a valid position. You can try the following two links:

https://www.tensorflow.org/get_started/estimator#fit_dnnclassifier

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/get_started/estimator.md#fit-dnnclassifier",0,,3,2017-12-29T07:21:34Z,CONTRIBUTOR,2017-12-29T23:20:43Z
15710,fix doc `tf.data.contrib.map_and_batch` to `tf.contrib.data.map_and_batch`,"awaiting testing (then merge),cla: yes",change from unexist `tf.data.contrib.map_and_batch` to `tf.contrib.data.map_and_batch`,0,,5,2017-12-29T05:18:09Z,CONTRIBUTOR,2017-12-29T06:56:43Z
15709,Update graphs.md,"awaiting testing (then merge),cla: yes",,0,,3,2017-12-29T04:52:40Z,CONTRIBUTOR,2017-12-29T06:55:57Z
15708,Update estimator.md,"awaiting testing (then merge),cla: yes",,0,,3,2017-12-29T04:46:32Z,CONTRIBUTOR,2017-12-29T06:55:26Z
15707,updated the latest mkldnn,cla: no,"This commit will pull the latest changes from the mkl-dnn tree.

the mirror.bazel.build URL doesn't exist: ""https://mirror.bazel.build/github.com/01org/mkl-dnn/archive/e0bfcaa7fcb2b1e1558f5f0676933c1db807a729.tar.gz"" can you create it?",0,,5,2017-12-29T03:44:35Z,CONTRIBUTOR,2017-12-29T04:10:15Z
15705,updated the latest mkldnn,cla: no,"This commit will pull the latest changes from the mkl-dnn tree.

the mirror.bazel.build URL doesn't exist: ""https://mirror.bazel.build/github.com/01org/mkl-dnn/archive/e0bfcaa7fcb2b1e1558f5f0676933c1db807a729.tar.gz"" can you create it?",0,,2,2017-12-29T03:22:37Z,CONTRIBUTOR,2017-12-29T03:37:43Z
15704,Used template version of SafeStringToNumeric to reduce code duplication,"awaiting testing (then merge),cla: yes","This fix uses template version of SafeStringToNumeric to avoid customerized ConvertHelper, for the purpose of reduce code duplication.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,3,2017-12-29T01:33:00Z,MEMBER,2017-12-29T01:33:57Z
15703,Branch 180304271,"awaiting testing (then merge),cla: yes",,0,,10,2017-12-29T01:11:30Z,MEMBER,2017-12-29T01:47:38Z
15702,Fix out of memory issue on Tegra devices,"awaiting testing (then merge),cla: yes","TF used to allocate (available memory - 300MB) or (available memory - 225MB)
for TF to use. This is fine for graphic cards, but will cause out of memory
issue on Tegra.
Modify to allocate (available memory - 1GB) for Tegra.
1GB should be enough for OS and other apps
(available memory - 1GB) should be 0.8-1.5GB which is enough for most graph for TF.",0,,5,2017-12-28T20:18:32Z,CONTRIBUTOR,2017-12-28T23:44:39Z
15699,[Android] Dex cannot parse version 52 byte code,"stat:awaiting response,type:support","When including:

`compile 'org.tensorflow:tensorflow-lite:0.1.1'`

I get the error:

> [Android] Dex cannot parse version 52 byte code

If I use AGP 3.0.0+ the problem goes away since it has support for Java 8, but the problem exists if I try to use 2.3.3. 
",0,,2,2017-12-28T16:25:40Z,NONE,2017-12-29T02:35:20Z
15698,Exception trying to import a retrained model in android classifier demo app,"stat:awaiting response,type:support","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
('v1.3.0-rc1-5733-gb43d0f3', '1.4.0')
- **Python version**: 
2.7.12
- **Bazel version (if compiling from source)**:
0.9.0
- **GCC/Compiler version (if compiling from source)**:
5.4.0
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


I have retrained a mobilenet_v1_1.0_224 frozen_graph.pb model with the following script:

export PYTHONPATH=/usr/local/lib/python2.7/dist-packages:/usr/lib/python2.7/dist-packages
export IMAGE_SIZE=224
export WIDTH_MUL=1.0 # 0.75 0.50 0.25
export ARCHITECTURE=""mobilenet_${WIDTH_MUL}_${IMAGE_SIZE}""
export BASE_DIR=""$( cd ""$( dirname ""${BASH_SOURCE[0]}"" )"" && pwd )""
export TF_FILES_DIR=""tf_files""
python -m scripts.retrain \
  --bottleneck_dir=""${TF_FILES_DIR}""/bottlenecks \
  --how_many_training_steps=4000 \
  --model_dir=""${TF_FILES_DIR}""/models/ \
  --summaries_dir=""${TF_FILES_DIR}""/training_summaries/""${ARCHITECTURE}"" \
  --output_graph=""${TF_FILES_DIR}""/retrained_graph.pb \
  --output_labels=""${TF_FILES_DIR}""/retrained_labels.txt \
  --architecture=""${ARCHITECTURE}"" \
  --image_dir=""${TF_FILES_DIR}""/flower_photos

I copied ""retrained_graph.pb"" and ""retrained_labels.txt"" in the ""assets"" directory of the android demo app in tensorflow/examples/android and modified the source code in ClassifierActivity.java as follows:

/* Original code:
  private static final String INPUT_NAME = ""input"";
  private static final String OUTPUT_NAME = ""output"";
  private static final String MODEL_FILE = ""file:///android_asset/tensorflow_inception_graph.pb"";
  private static final String LABEL_FILE = ""file:///android_asset/imagenet_comp_graph_label_strings.txt"";
*/
/* Replaced by: */
  private static final String INPUT_NAME = ""input"";
  private static final String OUTPUT_NAME = ""final_result"";
  private static final String MODEL_FILE = ""file:///android_asset/retrained_graph.pb"";
  private static final String LABEL_FILE = ""file:///android_asset/retrained_labels.txt"";
/* End */

I cleaned and rebuilt the whole project in Android Studio, successfully.

The app was then loaded on a Huawei P8 Lite (Android 7.0) and starting TF Classify, the following error occurs:

E/tensorflow: CameraActivity: Exception!
              java.lang.RuntimeException: Failed to load model from 'file:///android_asset/retrained_graph.pb'
                  at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:113)
                  at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:103)
                  at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:118)
                  at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:120)
                  at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1204)
                  at android.os.Handler.dispatchMessage(Handler.java:105)
                  at android.os.Looper.loop(Looper.java:156)
                  at android.app.ActivityThread.main(ActivityThread.java:6523)
                  at java.lang.reflect.Method.invoke(Native Method)
                  at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:942)
                  at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:832)
               **Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[""SAME"", ""VALID""]; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"", ""NCHW""]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_0/convolution = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true](input, MobilenetV1/Conv2d_0/weights/read)**. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
                  at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:535)
                  at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:105)
                  at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:103)
                  at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:118)
                  at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:120)
                  at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1204)
                  at android.os.Handler.dispatchMessage(Handler.java:105)
                  at android.os.Looper.loop(Looper.java:156)
                  at android.app.ActivityThread.main(ActivityThread.java:6523)
                  at java.lang.reflect.Method.invoke(Native Method)
                  at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:942)
                  at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:832)
Application terminated.

How to fix this error?",0,,5,2017-12-28T16:13:06Z,NONE,2017-12-29T02:11:39Z
15697,Very slow tf.transpose on CPU (compared to numpy),,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Tested on Linux Ubuntu 16.04 and Mac OS
- **TensorFlow installed from (source or binary)**: Both affected
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**:  3.6.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: 

### Describe the problem
Tensorflow transpose is 10000 slower than numpy transpose on my example.

### Source code / logs
```
import numpy as np
a = np.random.randn(*(10, 10, 10, 100, 10, 10, 10))
%timeit np.transpose(a, [3, 0, 1, 2, 4, 5, 6])
# 744 ns on my machine
b = tf.Variable(tf.random_normal((10, 10, 10, 100, 10, 10, 10))) # variable to avoid generating random numbers while measuring time
sess = tf.Session()
sess.run(tf.global_variables_initializer())
op = tf.transpose(b, [3, 0, 1, 2, 4, 5, 6]).op
%timeit sess.run(op)
# 7.94 s
```",0,,3,2017-12-28T15:33:56Z,CONTRIBUTOR,2017-12-28T15:56:01Z
15696,A fix for error in tf.layers.conv3d_transpose when inferred batch size,stat:contributions welcome,"## Context
When using the `tf.layers.conv3d_tranpose` Op with a dynamic batch size and when `use_bias=True` then there is a [well known error that occurs](https://github.com/tensorflow/tensorflow/issues/10520).
The error is due to a [tf.reshape Op that chunks together some of the axes before adding the bias for a slight performance improvement](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L1628).
E.g. in the case of `data_format == 'channels_last'`, 
```
outputs_4d = array_ops.reshape(outputs, [
            outputs_shape[0], outputs_shape[1],
            outputs_shape[2] * outputs_shape[3], outputs_shape[4]
        ])
```
gives an error when `outputs_shape[0] == None`, i.e. when batch size is inferred. 

## Simple fix
To fix this with minimal modification to other code, it would be great if someone could replace `outputs_shape[0]` with `-1` in these two lines:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L1627
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L1632",0,,8,2017-12-28T15:27:47Z,NONE,2017-12-29T01:08:27Z
15691,The problem of the tensorboard,,"Hi:
  I am the newbie of the tensorflow,  there is a problem during learning the usage of  tensorboard. 
  There is no problem with the first code compilation after start the Compiler(Anaconda spyder), but recompiling the code will go wrong, that is very strange. can anyone help me?

thank you very much!

**First compilation:**
![1](https://user-images.githubusercontent.com/31270354/34410598-60c20af0-ec0c-11e7-9acc-b3a5757b2101.PNG)
![2](https://user-images.githubusercontent.com/31270354/34410606-6776095a-ec0c-11e7-8dcb-6607336bd7d2.PNG)
![3](https://user-images.githubusercontent.com/31270354/34410611-6cab0768-ec0c-11e7-8f39-38a7cee2087f.PNG)
everything is ok during first compilation

**Second compilation:**
![4](https://user-images.githubusercontent.com/31270354/34410636-8f812b64-ec0c-11e7-91c9-22ad651245d9.PNG)
Something was wrong!!!

**the code of the tensorflow:**
```
import tensorflow as tf

x = tf.placeholder(tf.int32)
y = x + 2
                   
sess = tf.Session()

tf.summary.scalar('Accuracy' , y)
merged = tf.summary.merge_all()
writer = tf.summary.FileWriter(""logs/"", sess.graph)

for i in range(200):     
    rs = sess.run(merged , feed_dict = {x: 10})
    writer.add_summary(rs, i)
```
**Error report:**
```
runfile('C:/Users/Administrator/.spyder-py3/temp.py', wdir='C:/Users/Administrator/.spyder-py3')
Traceback (most recent call last):

  File ""<ipython-input-2-66eb2d566452>"", line 1, in <module>
    runfile('C:/Users/Administrator/.spyder-py3/temp.py', wdir='C:/Users/Administrator/.spyder-py3')

  File ""C:\Program Files\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 866, in runfile
    execfile(filename, namespace)

  File ""C:\Program Files\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/Administrator/.spyder-py3/temp.py"", line 13, in <module>
    rs = sess.run(merged , feed_dict = {x: 10})

  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 895, in run
    run_metadata_ptr)

  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)

  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1321, in _do_run
    options, run_metadata)

  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)

InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype int32
	 [[Node: Placeholder = Placeholder[dtype=DT_INT32, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op 'Placeholder', defined at:
  File ""C:\Program Files\Anaconda3\lib\site-packages\spyder\utils\ipython\start_kernel.py"", line 223, in <module>
    main()
  File ""C:\Program Files\Anaconda3\lib\site-packages\spyder\utils\ipython\start_kernel.py"", line 219, in main
    kernel.start()
  File ""C:\Program Files\Anaconda3\lib\site-packages\ipykernel\kernelapp.py"", line 474, in start
    ioloop.IOLoop.instance().start()
  File ""C:\Program Files\Anaconda3\lib\site-packages\zmq\eventloop\ioloop.py"", line 162, in start
    super(ZMQIOLoop, self).start()
  File ""C:\Program Files\Anaconda3\lib\site-packages\tornado\ioloop.py"", line 887, in start
    handler_func(fd_obj, events)
  File ""C:\Program Files\Anaconda3\lib\site-packages\tornado\stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Program Files\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""C:\Program Files\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""C:\Program Files\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""C:\Program Files\Anaconda3\lib\site-packages\tornado\stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Program Files\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 276, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""C:\Program Files\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 228, in dispatch_shell
    handler(stream, idents, msg)
  File ""C:\Program Files\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 390, in execute_request
    user_expressions, allow_stdin)
  File ""C:\Program Files\Anaconda3\lib\site-packages\ipykernel\ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""C:\Program Files\Anaconda3\lib\site-packages\ipykernel\zmqshell.py"", line 501, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""C:\Program Files\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2717, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""C:\Program Files\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2827, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\Program Files\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-66eb2d566452>"", line 1, in <module>
    runfile('C:/Users/Administrator/.spyder-py3/temp.py', wdir='C:/Users/Administrator/.spyder-py3')
  File ""C:\Program Files\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 866, in runfile
    execfile(filename, namespace)
  File ""C:\Program Files\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)
  File ""C:/Users/Administrator/.spyder-py3/temp.py"", line 3, in <module>
    x = tf.placeholder(tf.int32)
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1548, in placeholder
    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 2094, in _placeholder
    name=name)
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype int32
	 [[Node: Placeholder = Placeholder[dtype=DT_INT32, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]    
```




",0,,4,2017-12-28T12:25:23Z,NONE,2017-12-28T19:33:41Z
15689,TensorFlowInferenceInterface: readNodeFloat error,"stat:awaiting response,type:support","This is part of my Tensorflow frozen graph, I have named the input and output nodes.

    >>> g.ParseFromString(open('frozen_graph.pb','rb').read())
    >>> g
    node {
      name: ""input""
      op: ""Placeholder""
      attr {
        key: ""dtype""
        value {
          type: DT_FLOAT
        }
      }
      attr {
        key: ""shape""
        value {
          shape {
            dim {
              size: -1
            }
            dim {
              size: 68
            }
          }
        }
      }
    }
    ...
    node {
      name: ""output""
      op: ""Softmax""
      input: ""add""
      attr {
        key: ""T""
        value {
          type: DT_FLOAT
        }
      }
    }

I ran this model by the following code
(CELL is name of directory where my file is located)

    final String MODEL_FILE = ""file:///android_asset/"" + CELL + ""/optimized_graph.pb"" ;
    final String INPUT_NODE = ""input"" ;
    final String OUTPUT_NODE = ""output"" ;
    final int[] INPUT_SIZE = {1,68} ;
    float[] RESULT = new float[8];
    
    inferenceInterface = new TensorFlowInferenceInterface();
    inferenceInterface.initializeTensorFlow(getAssets(),MODEL_FILE) ;
    inferenceInterface.fillNodeFloat(INPUT_NODE,INPUT_SIZE,input);

and finally

    inferenceInterface.readNodeFloat(OUTPUT_NODE,RESULT);


But I get this error in Log

    12-28 16:42:48.622 9890-12178/com.getfocus.signalsimilarity I/native: tensorflow_inference_jni.cc:151 Initialization done in 52.275ms
    12-28 16:42:51.048 9890-12178/com.getfocus.signalsimilarity E/native: tensorflow_inference_jni.cc:170 Output [output] not found, aborting!


I have searched a lot for the solution but nothing seems to solve this. Thanks in advance",0,,2,2017-12-28T12:08:09Z,NONE,2017-12-29T02:32:28Z
15686,fix description of HASHTABLE_LOOKUP in smartreply doc,"awaiting testing (then merge),cla: yes",HASHTABLE_LOOKUP is a builtin op rather than a custom one.,1,,3,2017-12-28T09:30:22Z,CONTRIBUTOR,2017-12-29T00:47:52Z
15685,"fake_quant_with_min_max_vars doesn't change min, max vars ",stat:awaiting response,"# System Information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): pip3 install --upgrade tensorflow-gpu
- TensorFlow version (use command below): v1.4.0-19-ga52c8d9, 1.4.1
- Python version:3.5.2 
- Bazel version (if compiling from source): 0.7.0
- GCC/Compiler version (if compiling from source): GCC 5.4.0
- CUDA/cuDNN version: Cuda compilation tools, release 8.0, V8.0.61, cuDNN : 6.0.21
- GPU model and memory: Two GeForce GTX 1080 Ti devices.
- Exact command to reproduce: N/A
# Problem description
I've got a problem with tf-lite conversion tool. There is learned graph which should be converted in tflite format and quantinized.
I have read the answer https://stackoverflow.com/questions/47463204/tensorflow-lite-convert-error-for-the-quantized-graphdef where authors recommend to create new network with fake_quant operations and retrain it. However variables passed in tf.fake_quant_with_min_max_vars op did not change their values during training. Here is modeling code which shows the problem.
# Code to reproduce
```
import tensorflow as tf
import numpy as np

khe_init = tf.random_normal_initializer(mean=0.0, stddev=np.sqrt(2.0 / 1000))


def quant_conv_op(inpt, num_filters, filter_size=[3, 3], strides=[1, 1, 1, 1], padding=""VALID"", layer_name=""layer1"", use_acivation=True):
    num_input_map = inpt.get_shape().as_list()[-1]
    kernel_shape = filter_size + [num_input_map, num_filters]    
    with tf.variable_scope(layer_name):
        W = tf.get_variable(""weights"", shape=kernel_shape, initializer=khe_init)
        max_w = tf.get_variable(""max_quant_weights"", shape=[], initializer=tf.constant_initializer(1), trainable=True)
        min_w = tf.get_variable(""min_quant_weights"", shape=[], initializer=tf.constant_initializer(-1), trainable=True)
        b = tf.get_variable(""bias"", shape=[num_filters, ], initializer=tf.zeros_initializer)
 
        q_W = tf.fake_quant_with_min_max_vars(W, min_w, max_w)
        out = tf.nn.conv2d(inpt, q_W, strides=strides, padding=padding, name=""2d_convolution_operation"")
        out = tf.nn.bias_add(out, b)
        
        if use_acivation: 
            out = tf.nn.relu6(out, name=layer_name+""out"")
            out = tf.fake_quant_with_min_max_args(out, 0, 6)
        else:
            max_out = tf.get_variable(""max_quant_output"", shape=[], initializer=tf.constant_initializer(1), trainable=True)
            min_out = tf.get_variable(""min_quant_output"", shape=[], initializer=tf.constant_initializer(-1), trainable=True)  
            out = tf.fake_quant_with_min_max_vars(out, min_out, max_out, name=""fake_quant_with_min_max_out_quantinization"")
    
    return out, max_w, min_w, W


def loss(logits, batch):
    return tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf.ones(shape=[batch, 1]))


def build_net(ph_input):
    inp = tf.fake_quant_with_min_max_args(ph_input, -1, 1)
    out, max_w1, min_w1, W1 = quant_conv_op(inp, num_filters=64, filter_size=[3, 3], layer_name=""layer1"")    
    out, max_w2, min_w2, W2 = quant_conv_op(out, num_filters=128, filter_size=[3, 3], layer_name=""layer2"")
    out, max_w3, min_w3, W3 = quant_conv_op(out, num_filters=256, filter_size=[3, 3], layer_name=""layer3"")
    out = tf.reduce_mean(out, axis=[1, 2], keep_dims=True, name=""avg_pool"")
    out = tf.fake_quant_with_min_max_args(out, 0, 6, name=""fake_quant_with_min_max_avgpool_quantinization"")
    logits, max_w4, min_w4, W4 = quant_conv_op(out, num_filters=1, filter_size=[1, 1], layer_name=""layer4"", use_acivation=False)
    
    max_list = [max_w1, max_w2, max_w3, max_w4]
    min_list = [min_w1, min_w2, min_w3, min_w4]
    W_list = [W1, W2, W3, W4]
    logits = tf.reshape(logits, [-1, 1])
    sig_loss = tf.reduce_mean(loss(logits, batch=64))
    adam_op = tf.train.AdamOptimizer(10**-0)
    train_op = adam_op.minimize(sig_loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)) 
    return train_op, max_list, min_list, sig_loss, W_list


def main():
    sess = tf.InteractiveSession()
    ph_input = tf.placeholder(tf.float32, [None, 28, 28, 3], name=""network_input"") 
    train_op, max_list, min_list, sig_loss, W_list = build_net(ph_input)
    sess.run(tf.global_variables_initializer())
    tf.summary.FileWriter('.', graph=tf.get_default_graph())
    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
    for t in trainable_vars:
        if len(t.shape) == 0:
           print(t.name, sess.run(t))
        if len(t.shape) == 4:
           v = sess.run(t)
           print(t.name, np.max(v), np.min(v))

    for i in range(10):
        res = sess.run([train_op, sig_loss, W_list[0]]+max_list, feed_dict={ph_input:np.random.uniform(low=-1, high=1, size=[64, 28, 28, 3])})
    
    print(""========="")
    for t in trainable_vars:
        if len(t.shape) == 0:
           print(t.name, sess.run(t))
        if len(t.shape) == 4:
           v = sess.run(t)
           print(t.name, np.max(v), np.min(v))

if __name__ == ""__main__"":
    main()
```

  
  ",0,,1,2017-12-28T08:39:27Z,NONE,2018-01-03T07:35:16Z
15683,R1.4,cla: no,,0,,3,2017-12-28T06:45:36Z,NONE,2017-12-28T17:51:27Z
15682,Eager: tfe.implicit_value_and_gradients uses functions operating on raw tf variables,"comp:eager,type:support","## System information
- Tensorflow version: 1.5.0-dev20171126
- Python version: Python 3.5.0 (v3.5.0:374f501f4567, Sep 12 2015, 11:00:19)

## Problem
Forgive me if I'm re-iterating something that's discussed before. Even though I don't think the issue described here is a bug, I nevertheless believe it is worthy to point out. The specific issue is that when we pass a loss function, e.g. ```loss```, to ```tfe.implicit_value_and_gradients```, it seems that backprop only happens if the variables used by ```loss``` are is their ""raw states"". Here's an example:

```python
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()
v = tf.get_variable(name='v', initializer=1., trainable=True)
v_add_1 = v + 1.  # this causes the problem

def loss():
    return 2. * v_add_1
value_and_gradients_fn = tfe.implicit_value_and_gradients(loss)
print (value_and_gradients_fn())
```
In this case I get the error as follows:
```
Traceback (most recent call last):
  File ""test.py"", line 17, in <module>
    val = g()
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py"", line 360, in grad_fn
    raise ValueError(""No trainable variables were accessed while the ""
ValueError: No trainable variables were accessed while the function was being computed.
```
After a little bit of pondering, I found the problem to be the line ```v = v + 1.```. As soon as we **delete** this line, the program runs without bugs. My understanding of this behavior is that, the gradients and the backprop process somehow only ""live"" in the scope of the loss function. We **cannot** backprop to some variable that is modified outside of ```loss```, even if, implicitly, the computed loss depends on that variable. 

Here's a more obscure example:

```python
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

v = tf.get_variable(name='v', initializer=1., trainable=True)
v_add_1 = v + 1.
u = tf.get_variable(name='u', initializer=20., trainable=True)


def loss():
    result = v_add_1 + u
    return 2. * result

value_and_gradients_fn = tfe.implicit_value_and_gradients(loss)
optimizer = tf.train.AdamOptimizer(1e-1)

# before training
print (v)
print (u)

for i in range(100):
    _, gradients_and_variables = value_and_gradients_fn()
    optimizer.apply_gradients(gradients_and_variables)

# after training
print (v)
print (u)
```
After running, we could see that ```u``` is updated and ```v``` is not:
```
<tf.Variable 'v:0' shape=() dtype=float32, numpy=1.0>
<tf.Variable 'u:0' shape=() dtype=float32, numpy=20.0>
<tf.Variable 'v:0' shape=() dtype=float32, numpy=1.0>
<tf.Variable 'u:0' shape=() dtype=float32, numpy=9.9999638>
```

This might be irrelevant, but is there a way we could by pass the restriction and have gradient pass outside the function given to ```tfe.implicit_value_and_gradients```?",0,,2,2017-12-28T05:50:28Z,NONE,2017-12-29T22:05:16Z
15681,tf.image.decode_image does not support png grayscale 16bit.,,"Code to reproduce:
```
import tensorflow as tf
import numpy as np

print (""TF Version: %s"" % tf.__version__)

import urllib

png16 = urllib.request.urlopen('http://www.schaik.com/pngsuite/basn0g16.png').read(1000)
with tf.Session() as session:
    content = tf.placeholder(tf.string)
    tensor = tf.image.decode_image(
        contents=content,
        channels=1
    )
    
    out = (session.run(tensor, {content: png16}))
    np_out = np.array(out)
    print ('Shape', np_out.shape)
    print ('Max', np_out.max())
    

    
```
Outputs:
``` 
TF Version: 1.4.1
Shape (32, 32, 1)
Max 255
```
Same file, file command:
```
file basn0g16.png
basn0g16.png: PNG image data, 32 x 32, 16-bit grayscale, non-interlaced
```
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS
- **TensorFlow installed from (source or binary)**: pip3 install tensorflow
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: nope
- **GCC/Compiler version (if compiling from source)**: nope
- **CUDA/cuDNN version**: no gpu
- **GPU model and memory**: no gpu
- **Exact command to reproduce**: see abve


python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.4.0-19-ga52c8d9 1.4.1

### Describe the problem
It should return uint16, but returns uint8.


### Source code / logs
see above
",0,,2,2017-12-28T05:41:20Z,NONE,2017-12-28T08:16:39Z
15680,"Improve doc of TFRecordDataset, shuffle ahead of map","awaiting review,cla: yes","In the origin document, the code to demonstrate TFRecordDataset do `dataset.map(parser)` then do `dataset.shuffle(10000)`.
This code use a high number of buffer size (10000), and since `map` do ahead of `shuffle`, means when the first time this dataset yield one result it will need to run `map` over 10000 items and this can take a lot of time.
So, instead we can do `shuffle` ahead of `map`, since the item of `TFRecordDataset` is one `Example` raw data, `shuffle` ahead will not compromise the randomness and then the `map(parser)` only need to process one batch of items at a time. Which results much faster startup.
",0,,2,2017-12-28T03:56:00Z,CONTRIBUTOR,2017-12-28T19:34:39Z
15677,De-Bazel check_load_py_test sanity check,"awaiting testing (then merge),cla: yes","See https://github.com/tensorflow/tensorflow/pull/15368#issuecomment-354156560 for reference.
@gunan @martinwicke Friendly ping.",1,,7,2017-12-28T01:36:12Z,CONTRIBUTOR,2018-01-23T17:50:52Z
15676,Enabling tests to pass with python3.6. Updating dependencies for dock,cla: yes,er tests.,0,,1,2017-12-28T00:16:36Z,MEMBER,2017-12-28T00:48:31Z
15675,Branch 180224227,"awaiting testing (then merge),cla: yes",Need a push for the release.,0,,1,2017-12-28T00:11:27Z,MEMBER,2017-12-28T00:48:12Z
15674,Remove third_party/ prefixes,"cla: yes,pending merge internally",,1,,5,2017-12-27T23:54:21Z,CONTRIBUTOR,2017-12-27T23:55:42Z
15671,Turn check_futures_test into a sanity check,"awaiting testing (then merge),cla: yes","See https://github.com/tensorflow/tensorflow/pull/15368#issuecomment-354156560 for reference.
@gunan @martinwicke /cc",1,,11,2017-12-27T22:23:12Z,CONTRIBUTOR,2017-12-27T23:49:31Z
15670,[CMake] Add sanity tests for python file lists,"awaiting testing (then merge),cla: yes","Replaces #15166
See discussion in #15368
@gunan @martinwicke Friendly ping.",0,,8,2017-12-27T22:04:32Z,CONTRIBUTOR,2018-01-03T06:47:16Z
15668,MNIST dataset - gzip: train-images-idx3-ubyte.gz: not in gzip format,,"initiated from tensorflow-discuss From: greina@eng.ucsd.edu 

> 
> 
> I'm trying to use:
> 
> from tensorflow.examples.tutorials.mnist import input_data
> mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)
> 
> but I am getting the error that the downloaded file is not in GZip format.
> 
> There are several bug reports on this, but none of them seem to solve the issue. 
> 
> I found train-images-idx3-ubyte.gz in the local directory MNIST_data, but even trying `gunzip` fails:
> 
> ```(tf) [bduser@param03 MNIST_data]$ gunzip
> gzip: compressed data not read from a terminal. Use -f to force decompression.
> For help, type: gzip -h
> (tf) [bduser@param03 MNIST_data]$ gunzip train-images-idx3-ubyte.gz
> 
> gzip: train-images-idx3-ubyte.gz: not in gzip format
> ```
> 
> I'm thinking that this is a problem with the original datafile. Maybe the GZip format has changed or is in conflict?? Or perhaps my OS' version of GZip can't understand the GZip file??
> 
> I am using TF 1.4 on CentOS Linux release 7.4.1708 (Core)
> 
> Thanks.
> -Tony
> 
> 
> 
> ```
> >>> from tensorflow.examples.tutorials.mnist import input_data
> >>> mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)
> Successfully downloaded train-images-idx3-ubyte.gz 727 bytes.
> Extracting MNIST_data/train-images-idx3-ubyte.gz
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""/home/bduser/miniconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 242, in read_data_sets
>     train_images = extract_images(f)
>   File ""/home/bduser/miniconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 56, in extract_images
>     magic = _read32(bytestream)
>   File ""/home/bduser/miniconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 38, in _read32
>     return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]
>   File ""/home/bduser/miniconda2/envs/tf/lib/python2.7/gzip.py"", line 268, in read
>     self._read(readsize)
>   File ""/home/bduser/miniconda2/envs/tf/lib/python2.7/gzip.py"", line 303, in _read
>     self._read_gzip_header()
>   File ""/home/bduser/miniconda2/envs/tf/lib/python2.7/gzip.py"", line 197, in _read_gzip_header
>     raise IOError, 'Not a gzipped file'
> IOError: Not a gzipped file
> >>> exit()
> ```
> ",0,,3,2017-12-27T20:59:09Z,MEMBER,2017-12-27T21:01:03Z
15667,minor wording fix in (slim) readme,"awaiting testing (then merge),cla: yes","""we want to restore a model from a checkpoint
whose variables have different names **to** those in the current graph.""  (add the **to** in the line)",0,,3,2017-12-27T20:09:58Z,CONTRIBUTOR,2017-12-27T22:13:02Z
15666,updated Readme.md,"awaiting testing (then merge),cla: yes",tf-nightly whl files download links added for Linux of python 3.6 version,0,,2,2017-12-27T19:53:55Z,CONTRIBUTOR,2017-12-27T21:04:20Z
15664,Fix small typo in docstring which causes the API doc to render incorr,"awaiting testing (then merge),cla: yes","ectly
See
![screen shot 2017-12-27 at 19 53 06](https://user-images.githubusercontent.com/11613312/34390366-adc111e8-eb3f-11e7-8d31-bdb0c24c32a9.png)
on https://www.tensorflow.org/api_docs/python/tf/train/piecewise_constant",0,,5,2017-12-27T18:54:06Z,CONTRIBUTOR,2017-12-27T18:56:36Z
15661,Fix typo 'updaye' in audio_recognition.md,"awaiting testing (then merge),cla: yes",Fix typo 'updaye' to 'update' in audio_recognition.md,0,,7,2017-12-27T16:47:29Z,CONTRIBUTOR,2017-12-27T16:49:28Z
15660,Distributed training fault tolerance,"stat:awaiting tensorflower,type:support","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04.5 LTS
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: Python 3.6.3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: release 8.0, V8.0.44/ libcudnn v6.0.21
- **GPU model and memory**: Titan X (Pascal) 12 GB
- **Exact command to reproduce**: In the description

### Describe the problem
I am using the parameter server/master/worker paradigm to run model training in distributed mode. The master node does the training and evaluation, while the worker nodes only do the training (on their shard of the data). I should also note that I am using the `tf.contrib.learn.Experiment` interface.

This model of distributed training works as expected, however, sometimes one or more of the worker nodes fail. While they have failed, the master node and other worker nodes continue the training. The problem is that when this occurs (even when only one of the worker nodes have failed), the loss suddenly becomes zero, and as a result gradients as well become zero, while the metrics suddenly change to the value of a random model, as can be seen in the figures below.

* Loss curve. As can be seen one or more of the workers have failed three times during the training.
![loss_fail](https://user-images.githubusercontent.com/1921165/34386146-3ed3d652-eaf5-11e7-99de-7923862089e6.jpg)

* Gradient norm curve
![gradient_fail](https://user-images.githubusercontent.com/1921165/34386155-48760dba-eaf5-11e7-86f9-d653cda03a3e.PNG)

* Accuracy (on the validation set)
![accuracy_fail](https://user-images.githubusercontent.com/1921165/34386161-51b1f114-eaf5-11e7-8a6c-303d8972b2b8.PNG)


**Is there a way to prevent this behavior, either by stopping the training when one of the workers fails, or pausing the training until the failed worker comes back online again (like in the beginning of the training when training only starts when all of the workers come online)?**

### Source code / logs
As indicated above, I use the experiment interface. This is the configuration for distributed training:
```python
dist_config = {}
dist_config['cluster'] = {
    'master': ['127.0.0.1:{}'.format(dist_start_port + 1)],
    'ps': ['127.0.0.1:{}'.format(dist_start_port)],
    'worker': ['127.0.0.1:{}'.format(dist_start_port + 2 + i)
            for i in range(worker_count)]
}
dist_config['task'] = {
    'type': dist_type,
    'index': (worker_index if args.dist_type == 'worker' else 0)
}
dist_config['environment'] = 'cloud'
os.environ['TF_CONFIG'] = json.dumps(dist_config)
```
And this is how I start each node:
```python
if dist_type == 'master':
    experiment.train_and_evaluate()
elif dist_type == 'ps':
    experiment.run_std_server()
else:
    experiment.train()
```
",0,,5,2017-12-27T16:10:38Z,CONTRIBUTOR,2017-12-30T09:20:34Z
15659,Document Bazel-Tensorflow-Cuda interdependencies,stat:awaiting response,"Since each version of Tensorflow appears to require some specific release of bazel, it would be helpful to have documentation like [this](https://www.tensorflow.org/install/install_sources#tested_source_configurations) also for people who are stuck on an older version of CUDA. For instance, I cannot upgrade to CUDA 8 on the machine I am using and am now left with the exercise of finding a working config in a space of 5 dimensions (python version, bazel version, tf version, cuda version, cudnn version).
I had it once working with CUDA 7.0 and python 3.5 (and I think bazel 0.3), but cannot reproduce now.
In this concrete case, I am trying to build `r0.11` with python 3.6, cuda 7.5, cudnn 6 and bazel 0.3/0.4/0.8 and nothing's working.",0,,2,2017-12-27T16:04:11Z,CONTRIBUTOR,2017-12-28T19:41:34Z
15658,add a function to add extra logging handler,"API review,cla: yes","I want to separate different level of logs. So error message will not be buried by info/debug logs. In r1.4.  I use following code to do the job.

```
  file_hanlder = logging.FileHandler(""running_error.log"", mode='w', encoding=None, delay=False)
  file_hanlder.setLevel(logging.WARNING)
  tf.logging._logger.addHandler(file_hanlder)
```
but, when I change to r1.5, it don't work any more. because tensorflow rewrite tf_logging.py and _logger  is not exposed.  So I think add a function to add extra handler should be useful.",0,,5,2017-12-27T15:40:57Z,CONTRIBUTOR,2018-01-02T13:44:57Z
15657,Removed misplaced quote char,"awaiting testing (then merge),cla: yes",,0,,2,2017-12-27T15:35:48Z,CONTRIBUTOR,2017-12-27T21:25:18Z
15656,CUDA 9.1 and TensorFlow,"stat:awaiting tensorflower,type:support","I am using NVIDIA GeForce GTX 1050 and installed NVIDIA 387.26. I installed cuDNN 7.0.5 and CUDA 9.1. As of my understanding, I know that, tensorflow is not supported in CUDA 9.1. My question is when I can expect the next build/release of TF to support CUDA 9.1. For the time being, shall I make a link from CUDA 9.0 to CUDA 9.1 and expect to work? Or is there any better way to solve the problem?",0,,24,2017-12-27T14:10:43Z,NONE,2017-12-30T09:22:12Z
15655,"tf.layers.conv3d with ""channels_first"" does not accept batch dimension to be None",,"code to reproduce:

```python
import tensorflow as tf
x = tf.placeholder(dtype=tf.float32, shape=[None, 1, 32, 32, 32])
y = tf.layers.conv3d(x, 32, 9, data_format='channels_first')
```

traceback
```
Traceback (most recent call last):
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 468, in make_tensor_proto
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 468, in <listcomp>
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/util/compat.py"", line 65, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got None

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py"", line 809, in conv3d
    return layer.apply(inputs)
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 671, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 575, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py"", line 185, in call
    outputs_shape[4]])
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3938, in reshape
    ""Reshape"", tensor=tensor, shape=shape, name=name)
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 513, in _apply_op_helper
    raise err
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 926, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 229, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 208, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 472, in make_tensor_proto
    ""supported type."" % (type(values), values))
TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [None, 32, 576, 24]. Consider casting elements to a supported type.
```

The error source appears [here][1] and can be simply fixed by adding

```python
if outputs_shape[0] is None:
  outputs_shape[0] = -1
```

however you might suggest a deeper fix?


[1]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L181-L185",0,,7,2017-12-27T13:55:37Z,NONE,2017-12-27T21:32:53Z
15654,Enable `axis` support for `tf.unique`,"awaiting testing (then merge),cla: yes","The `axis` support for `Unique` has been Added in PR #12952 (defined in `UniqueV2` ops). The support for `axis` in python version of the `tf.unique` was not enabled yet, due to the API workflow porcess (3 weeks). This fix adds the support for `axis` with `tf.unique` by adds `Unique` to `hidden.txt`, and adds a python wrapper of `tf.unique` to pointing to `UniqueV2`.

This fix addresses part of the issue #15644.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,28,2017-12-27T12:07:40Z,MEMBER,2017-12-30T02:53:50Z
15653,Clarify batch_norm documentation to highlight that the dimensions for,"awaiting testing (then merge),cla: no,stat:awaiting response",... normalization depend on the shape of the tensor.,0,,11,2017-12-27T10:49:05Z,CONTRIBUTOR,2017-12-27T10:52:30Z
15652,fix variable name,"awaiting testing (then merge),cla: yes",,0,,3,2017-12-27T06:34:10Z,CONTRIBUTOR,2017-12-27T21:21:26Z
15650,Unable to compile tensorflow r1.4 from source with cuda 8.0 and cudnn 7 and after downgrading bazel?,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.4
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.8.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 8.0/7.0.4
- **GPU model and memory**: 1080 Ti
- **Exact command to reproduce**: 
`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""`


### Describe the problem
I'm trying to compile TF r1.4 from source but didn't manage to get this working although I tried several fixes:

1. Downgrading bazel from 0.9.0 to 0.8.1 based on #15492 

2. `sudo sh -c ""echo '/usr/local/cuda-8.0/lib64' >> /etc/ld.so.conf.d/nvidia.conf""`  and `sudo ldconfig` based on #13481

3. adding the `action_env` argument based on https://stackoverflow.com/questions/47080760/tensorflow-fails-to-compile/47295278#47295278

Note: When installing cudnn, I used both the runtime library and the tar file which i extracted and placed it in the /usr/local/cuda library respective folders.

### Source code / logs
```
ERROR: /home/kwotsin/tensorflow/tensorflow/core/BUILD:2131:1: C++ compilation of rule '//tensorflow/core:gpu_runtime_impl' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/kwotsin/.cache/bazel/_bazel_kwotsin/041f6cc3555a2d9f6211c6d126ede477/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64 \
    PATH=/usr/local/cuda/bin:/home/kwotsin/bin:/home/kwotsin/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -g0 -MD -MF bazel-out/host/bin/tensorflow/core/_objs/gpu_runtime_impl/tensorflow/core/common_runtime/gpu/gpu_device.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/_objs/gpu_runtime_impl/tensorflow/core/common_runtime/gpu/gpu_device.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DSNAPPY -iquote . -iquote bazel-out/host/genfiles -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/genfiles/external/local_config_cuda -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/host/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' -msse3 -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c tensorflow/core/common_runtime/gpu/gpu_device.cc -o bazel-out/host/bin/tensorflow/core/_objs/gpu_runtime_impl/tensorflow/core/common_runtime/gpu/gpu_device.pic.o)
In file included from ./tensorflow/stream_executor/stream_executor.h:35:0,
                 from ./tensorflow/core/platform/stream_executor.h:38,
                 from ./tensorflow/core/common_runtime/gpu/gpu_event_mgr.h:28,
                 from ./tensorflow/core/common_runtime/gpu/gpu_device.h:30,
                 from tensorflow/core/common_runtime/gpu/gpu_device.cc:22:
./tensorflow/stream_executor/stream_executor_pimpl.h:87:63: internal compiler error: Segmentation fault
   PlatformKind platform_kind() const { return platform_kind_; }
                                                               ^
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 145.189s, Critical Path: 22.73s
FAILED: Build did NOT complete successfully

```

Thank you.

====

Further updates:

1. I tried switching to the more updated 7.0.5 CuDNN (although some mentioned in #12052 that their build worked with 7.0.4. I'm now using CUDA 9.0 + CuDNN 7.05 for CUDA 9.0, and with bazel 0.8.1. The build unfortunately still doesn't work.

2. CUDA 8.0 + CuDNN 6.0.21 also doesn't work, with the similar reasons as such:

```
C++ compilation of rule '//tensorflow/core/kernels:sparse_conditional_accumulator_op' failed (Exit 1)
In file included from tensorflow/core/kernels/sparse_conditional_accumulator_op.cc:19:0:
./tensorflow/core/kernels/sparse_conditional_accumulator.h: In destructor 'tensorflow::SparseConditionalAccumulator<Device, T>::~SparseConditionalAccumulator() [with Device = Eigen::ThreadPoolDevice; T = float]':
./tensorflow/core/kernels/sparse_conditional_accumulator.h:68:3: internal compiler error: Segmentation fault
   };
   ^
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 505.239s, Critical Path: 114.19s
FAILED: Build did NOT complete successfully

```

```
ERROR: /home/kwotsin/tensorflow/tensorflow/core/kernels/BUILD:2554:1: C++ compilation of rule '//tensorflow/core/kernels:cwise_op' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/kwotsin/.cache/bazel/_bazel_kwotsin/041f6cc3555a2d9f6211c6d126ede477/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-9.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_VERSION=9.0 \
    TF_CUDNN_VERSION=7.0.5 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++11' '-march=native' '-D_GLIBCXX_USE_CXX11_ABI=0' -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/cwise_op/tensorflow/core/kernels/cwise_op_floor_div.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/cwise_op/tensorflow/core/kernels/cwise_op_floor_div.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DSNAPPY -iquote . -iquote bazel-out/k8-opt/genfiles -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/k8-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/k8-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' -msse3 -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c tensorflow/core/kernels/cwise_op_floor_div.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/cwise_op/tensorflow/core/kernels/cwise_op_floor_div.pic.o)
In file included from tensorflow/core/kernels/cwise_op_floor_div.cc:16:0:
./tensorflow/core/kernels/cwise_ops_common.h: In instantiation of 'void tensorflow::functor::BinaryFunctor<Eigen::ThreadPoolDevice, Functor, NDIMS, false>::BCast(const CPUDevice&, typename tensorflow::TTypes<typename Functor::out_type, NDIMS>::Tensor, typename tensorflow::TTypes<typename Functor::in_type, NDIMS>::ConstTensor, Eigen::array<long int, NDIMS>, typename tensorflow::TTypes<typename Functor::in_type, NDIMS>::ConstTensor, Eigen::array<long int, NDIMS>, bool*) [with Functor = tensorflow::functor::floor_div_real<double>; int NDIMS = 4; tensorflow::functor::CPUDevice = Eigen::ThreadPoolDevice; typename tensorflow::TTypes<typename Functor::out_type, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<double, 4, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<typename Functor::in_type, NDIMS>::ConstTensor = Eigen::TensorMap<Eigen::Tensor<const double, 4, 1, long int>, 16, Eigen::MakePointer>]':
./tensorflow/core/kernels/cwise_ops_common.h:136:7:   required from 'void tensorflow::BinaryOp<Device, Functor>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; Functor = tensorflow::functor::floor_div_real<double>]'
tensorflow/core/kernels/cwise_op_floor_div.cc:53:1:   required from here
./tensorflow/core/kernels/cwise_ops_common.h:417:3: internal compiler error: Segmentation fault
   }
   ^
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 430.853s, Critical Path: 77.17s
FAILED: Build did NOT complete successfully

```",0,,6,2017-12-27T03:02:31Z,CONTRIBUTOR,2018-01-03T01:42:29Z
15648,Predictor fixes for core estimators,"awaiting testing (then merge),cla: yes","Creating a predictor from a core estimator was broken due to:

- a wrong instance check
- wrong initialization of the ChiefSessionCreator

This PR fixes both.",0,,4,2017-12-26T21:56:54Z,CONTRIBUTOR,2017-12-27T02:36:34Z
15647,Tensorflow.org - master version is not updated,stat:awaiting tensorflower,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Not Relevant
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Not Relevant
- **TensorFlow installed from (source or binary)**: Not Relevant
- **TensorFlow version (use command below)**: Not Relevant
- **Python version**: Not Relevant
- **Bazel version (if compiling from source)**: Not Relevant
- **GCC/Compiler version (if compiling from source)**: Not Relevant
- **CUDA/cuDNN version**: Not Relevant
- **GPU model and memory**: Not Relevant
- **Exact command to reproduce**: Not Relevant

### Describe the problem
tensorflow.org master version should be updated to master, however it seems that it hasn't been regenerated for a while.

For example the latest addition of the performance guide for `tf.data` (https://github.com/tensorflow/tensorflow/commit/ba32ea1547af74d549f35a42e4de83c88652a636) hasn't yet made it to the website:

Doc source: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/performance_guide.md

Generated web page in tensorflow.org (master version):
https://www.tensorflow.org/versions/master/performance/performance_guide

(Currently shows ""Last updated November 14, 2017."" in the bottom)

",0,,3,2017-12-26T20:42:01Z,NONE,2017-12-28T19:42:43Z
15646,Branch 180147476,cla: yes,,0,,1,2017-12-26T20:02:43Z,MEMBER,2017-12-27T01:27:22Z
15642,contrib/all_reduce not update to latest nccl,,"send_op, dst_tensors = nccl.broadcast(level_2_output[w], dst_devices)

this line is out of date, hope update to latest",0,,4,2017-12-26T13:39:35Z,NONE,2018-01-03T01:41:42Z
15641,Compile with selective register on meta file,,"I want to apply the selective registration feature on my model to decrease the lib size. However, I need to apply it on a meta file saved via `saver` rather than a frozen pb file as shown in many posts I have found. When I try to run 

`bazel-bin/tensorflow/python/tools/print_selective_registration_header --graphs=model_test.ckpt-390760.meta`

it comes to the error

`[libprotobuf ERROR external/protobuf/src/google/protobuf/wire_format_lite.cc:621] String field 'tensorflow.NodeDef.op' contains invalid UTF-8 data when parsing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.`

I tried to add `--proto_fileformat=textproto` but another error comes up:

`raise self.ParseError('Expected identifier or number.')
google.protobuf.text_format.ParseError: 2:1 : Expected identifier or number.`

Is it even possible to do this at all? The ultimate goal of compiling this lib is to restore a pretrained model and incrementally train it on Android.",0,,4,2017-12-26T13:23:09Z,NONE,2017-12-28T19:48:34Z
15639,fix doc for benchmark_model for android,"awaiting review,cla: yes","after configure.py set framework_shared_object=true,
benchmark_model won't build for Android without tweeks. Add
'--config monolithic' to avoid confusion.",0,,8,2017-12-26T09:53:39Z,CONTRIBUTOR,2017-12-26T19:04:54Z
15637,Fix an lib error while building with CUDA9.1,"awaiting review,cla: yes",Some header files in CUDA9.1 was moved into dir cuda/include/crt causing when building with CUDA9.1 headers like math_functions.hpp could no long be loaded.  Adding the directory into BUILD file fixs the issue. ,0,,8,2017-12-26T07:48:16Z,CONTRIBUTOR,2017-12-26T07:50:33Z
15636,Read tflite file failed on iOS,"comp:lite,stat:awaiting response","Hi

I tried the examples on iOS according to TF Lite guide, but failed when assign data to tflite because address ""out"" is NULL. probably my tflite file is incorrect, but I am not sure, can anybody give some help? 

My test step is as follows:
1.  Try the following code(https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite),  get the tflite file converteds_model.tflite

```
import tensorflow as tf
img = tf.placeholder(name=""img"", dtype=tf.float32, shape=(1, 64, 64, 3))
val = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])
out = tf.identity(val, name=""out"")
with tf.Session() as sess:
  tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [img], [out])
  open(""converteds_model.tflite"", ""wb"").write(tflite_model)
```

2. Integrated the tflite into my app, which is from iOS sample code ""simple""(/Users/Sensteer/Software/tensorflowclone/tensorflow/tensorflow/contrib/lite/examples/ios/simple).But exception happed because address ""out"" is NULL

```
int input = interpreter->inputs()[0];                                 //input is 3
float* out = interpreter->typed_tensor<float>(input);          //out is NULL
```

So my questions are:
1. The tflite created above is right or not?
2. The reading tflite code is right or not?
2. If the tflite file is not right, do I must create tflite with  ""pb"", ""ckpt"" and ""FrozenGraphDef"" mentioned in guide?

Thanks
",0,,5,2017-12-26T07:37:09Z,NONE,2017-12-29T02:21:12Z
15634,ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory with python 2.7,,"Ubuntu 16.04
GPU: 1080 Ti
Cuda 9.0
CuDDN 7.0 v7
Using my PC, not a VM
Installed Intel MKL-DNN by this [guilde](https://github.com/mind/wheels#mkl), but looks like something wrong, because when trying to make a test for tensorflow, got error:

```
gagazet@woof:~/Desktop$` python 123.py 
Traceback (most recent call last):
  File ""123.py"", line 1, in <module>
    import tensorflow as tf
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

Recently someone created same problem and it was closed with reason: PR [#12975](https://github.com/tensorflow/tensorflow/pull/12975), but it was for a VM.
Can anyone explain, please, what i can be and how to fix it? Dont have any build_pip_package scrips at the TF folder. 
Its doesnt looks like same problem as PR [#12975](https://github.com/tensorflow/tensorflow/pull/12975)
",0,,1,2017-12-26T07:02:56Z,NONE,2017-12-29T02:19:56Z
15632,Branch 180053468,cla: yes,,0,,1,2017-12-26T01:09:22Z,MEMBER,2017-12-26T01:10:39Z
15631,ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory,,"GPU: 1080 Ti
cuda 9.0
cuddn 7.0 v7
centos 7.
installed Intel MKL-DNN but the error is still present.


here is the output:
````
ipython
Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49) 
Type 'copyright', 'credits' or 'license' for more information
IPython 6.1.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import tensorflow as tf
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~/anaconda3/lib/python3.6/imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

~/anaconda3/lib/python3.6/imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-41389fad42b5> in <module>()
----> 1 import tensorflow as tf

~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in <module>()
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     70 for some common reasons and solutions.  Include the entire stack trace
     71 above this error message when asking for help."""""" % traceback.format_exc()
---> 72   raise ImportError(msg)
     73 
     74 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/home/sb0709/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/sb0709/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/sb0709/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/sb0709/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/sb0709/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```
`



",0,,8,2017-12-25T21:05:09Z,NONE,2017-12-26T06:45:20Z
15630,tf.name_scope does not work with tf.layers,,"When I define a layer with
```
with tf.name_scope(""MY_SCOPE""):
    layer1 = tf.layers.dense(inputs = X,
                             units = 100,
                             kernel_initializer = he_init,
                             activation = tf.nn.elu,
                             name = ""layer1"")


```

and try to get its variables with

`tf.global_variables(scope=""MY_SCOPE"")`

it returns nothing because name scope did not apply to the layers from tf.layers

Shouldn't their name be

[<tf.Variable 'MY_SCOPE/layer1/kernel:0' shape=(784, 100) dtype=float32_ref>,
 <tf.Variable 'MY_SCOPE/layer1/bias:0' shape=(100,) dtype=float32_ref>]

instead of

[<tf.Variable 'layer1/kernel:0' shape=(784, 100) dtype=float32_ref>,
 <tf.Variable 'layer1/bias:0' shape=(100,) dtype=float32_ref>]

so that I can use my scope to reach my layers? This works with everything but tf.layers module.",0,,4,2017-12-25T18:39:45Z,NONE,2017-12-26T05:13:42Z
15623,"I cannot use Binomial.sample(), could you please help me?",,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,1,2017-12-25T07:28:37Z,NONE,2017-12-25T18:11:25Z
15622,Unable to compile from source on High Sierra (10.13.2) with bazel 0.9.0,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: tf 1.4
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **Exact command to reproduce**: bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=- msse4.1 --copt=-msse4.2 --config=opt -k //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
I'm unable to compile tensorflow from source. I get too many errors as shown below in the logs:

### Source code / logs
```
Rakshiths-MacBook-Pro:tensorflow rakshithgb$ ./configure
WARNING: Running Bazel server needs to be killed, because the startup options are different.
You have bazel 0.9.0-homebrew installed.
Please specify the location of python. [Default is /Users/rakshithgb/miniconda3/bin/python]: 


Found possible Python library paths:
  /Users/rakshithgb/miniconda3/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/Users/rakshithgb/miniconda3/lib/python3.6/site-packages]

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n
No Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n
No Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: n
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: n
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL support? [y/N]: n
No OpenCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Add ""--config=mkl"" to your bazel command to build with MKL support.
Please note that MKL on MacOS or windows is still not supported.
If you would like to use a local MKL instead of downloading, please set the environment variable ""TF_MKL_ROOT"" every time before build.
Configuration finished
Rakshiths-MacBook-Pro:tensorflow rakshithgb$ bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=- msse4.1 --copt=-msse4.2 --config=opt -k //tensorflow/tools/pip_package:build_pip_package
..............
ERROR: Skipping 'msse4.1': no such target '//:msse4.1': target 'msse4.1' not declared in package '' defined by /Users/rakshithgb/Documents/Tensorflow/tensorflow/BUILD
WARNING: Target pattern parsing failed.
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):
	File ""/private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD"", line 27
		cc_library(name = ""syclrt"", srcs = [sycl_libr..."")], <3 more arguments>)
	File ""/private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD"", line 30, in cc_library
		sycl_library_path
name 'sycl_library_path' is not defined
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD:39:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '@local_config_sycl//sycl:sycl'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/platform/default/build_config/BUILD:115:1: Target '@com_googlesource_code_re2//:re2' contains an error and its package is in error and referenced by '//tensorflow/core/platform/default/build_config:platformlib'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/third_party/eigen3/BUILD:20:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//third_party/eigen3:eigen3'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3169:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:pooling_ops'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3798:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:variable_ops'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:717:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:compare_and_bitpack_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3776:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:scatter_nd_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3764:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:dense_update_ops'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:643:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:gather_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:607:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:bitcast_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:619:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:constant_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:625:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:diag_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:631:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:edit_distance_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:687:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:mirror_pad_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:711:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:quantize_and_dequantize_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:787:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:split_v_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:774:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:slice_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3770:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:scatter_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3758:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:count_up_to_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/BUILD:2215:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//tensorflow/core:sycl_runtime'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:780:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:split_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:681:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:matrix_set_diag_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:601:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:bcast_ops'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:756:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:reverse_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:699:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:pack_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:813:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:transpose_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:705:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:pad_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:693:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:one_hot_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:649:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:identity_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:661:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:listdiff_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:533:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:immutable_constant_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:655:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:identity_n_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:613:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:concat_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:675:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:matrix_diag_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:839:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:where_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:762:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:reverse_sequence_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:794:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:inplace_ops'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:827:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:unique_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:768:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:shape_ops'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:667:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:matrix_band_part_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:637:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:gather_nd_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:750:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:reshape_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:800:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:tile_ops'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:833:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:unpack_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:550:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:debug_ops'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/tools/pip_package/BUILD:101:1: Target '@com_googlesource_code_re2//:LICENSE' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:licenses'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/tools/pip_package/BUILD:101:1: Target '@local_config_sycl//sycl:LICENSE.text' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:licenses'
WARNING: errors encountered while analyzing target '//tensorflow/tools/pip_package:build_pip_package': it will not be built
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (204 packages loaded).
INFO: Found 0 targets...
ERROR: command succeeded, but there were errors parsing the target pattern
INFO: Elapsed time: 51.902s, Critical Path: 0.02s
FAILED: Build did NOT complete successfully
```


",0,,17,2017-12-25T06:42:27Z,NONE,2017-12-25T13:42:27Z
15620,Tf Lite only support 4D l2_normalize?,"comp:lite,type:support","I build some feature extract network model and converted tflite using by toco successfully.
But I got error `""tensorflow/contrib/lite/kernels/l2norm.cc:47 NumDimensions(input) != 4 (2 != 4)`, when run interpreter->AllocateTensors().

I extract feature using by tf.nn.l2_normalize.
`embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name='embeddings')`
where prelogits is 2D tensor.
How can I extract normalized feature with tflite?
",1,,3,2017-12-25T04:05:11Z,NONE,2017-12-30T09:54:34Z
15619,Crelu should have axis,stat:awaiting tensorflower,"Currently, the `tf.nn.crelu` activation concatenates along the last axis. This would work fine for dense layers and conv layers where the `data_fromat=channels_last`, but this would be incorrect if invoked on the widely used `data_format=channels_first` for conv layers on the GPU.
",0,,3,2017-12-25T02:06:17Z,NONE,2017-12-28T19:19:07Z
15617,Fix periodic resample,"awaiting testing (then merge),cla: yes",,1,,20,2017-12-24T17:57:55Z,CONTRIBUTOR,2017-12-24T18:29:07Z
15616,[CMake] Include example compile script,"awaiting testing (then merge),cla: yes",@mrry Friendly ping.,0,,11,2017-12-24T17:30:07Z,CONTRIBUTOR,2017-12-25T02:36:44Z
15614,Support Negativo17 Fedora Packaging,"awaiting review,cla: yes","Support the [Negativo17](https://negativo17.org/nvidia-driver/) Nvidia driver packaging for Fedora. `libdevice` libraries are under `/usr/share/cuda`, includes are under `/usr/include/cuda` and libraries are under `/usr/lib64`. This PR should help #8264 too.

In addition, the gcc 5.3 in the Negativo17 repository (installed as `/usr/bin/gcc53`) only has a static non-PIC version of `libgomp.a`, so I have this local patch to force Tensorflow to link to the global (`/usr/lib64`) shared version:

````diff
diff --git a/tensorflow/contrib/cmake/tf_stream_executor.cmake b/tensorflow/contrib/cmake/tf_stream_executor.cmake
index 91ca33f4c4..7719ee096d 100644
--- a/tensorflow/contrib/cmake/tf_stream_executor.cmake
+++ b/tensorflow/contrib/cmake/tf_stream_executor.cmake
@@ -75,7 +75,7 @@ endif()
 #list(REMOVE_ITEM tf_stream_executor_srcs ${tf_stream_executor_test_srcs})
 
 if (NOT WIN32)
-  set (CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -lgomp"")
+  set (CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -l:libgomp.so.1"")
 endif (NOT WIN32)
 add_library(tf_stream_executor OBJECT ${tf_stream_executor_srcs})
 
diff --git a/third_party/gpus/cuda/BUILD.tpl b/third_party/gpus/cuda/BUILD.tpl
index b752734a08..0ce972291e 100644
--- a/third_party/gpus/cuda/BUILD.tpl
+++ b/third_party/gpus/cuda/BUILD.tpl
@@ -109,7 +109,7 @@ cc_library(
         ""."",
         ""cuda/include"",
     ],
-    linkopts = [""-lgomp""],
+    linkopts = [""-l:libgomp.so.1""],
     linkstatic = 1,
     visibility = [""//visibility:public""],
 )
diff --git a/third_party/toolchains/gpus/cuda/BUILD b/third_party/toolchains/gpus/cuda/BUILD
index 39136de99c..6f697919fd 100644
--- a/third_party/toolchains/gpus/cuda/BUILD
+++ b/third_party/toolchains/gpus/cuda/BUILD
@@ -114,7 +114,7 @@ cc_library(
         ""."",
         ""cuda/include"",
     ],
-    linkopts = [""-lgomp""],
+    linkopts = [""-l:libgomp.so.1""],
     linkstatic = 1,
     visibility = [""//visibility:public""],
 )
````

Building with clang is a lot more difficult, as it'd require making Tensorflow's CUDA symlink repo look enough like the unpacked tarball to pass [this detection logic](https://github.com/jyknight/llvm-monorepo/blob/6a6c3cae76a0839429c0b552572c46af9b194b86/clang/lib/Driver/ToolChains/Cuda.cpp)!

My `.tf_configure.bazelrc` (for FC 26) looks like:

````
build --action_env PYTHON_BIN_PATH=""/home/nicholas/miniconda3/bin/python""
build --action_env PYTHON_LIB_PATH=""/home/nicholas/miniconda3/lib/python3.6/site-packages""
build --force_python=py3
build --host_force_python=py3
build --python_path=""/home/nicholas/miniconda3/bin/python""
build --define with_jemalloc=true
build:gcp --define with_gcp_support=true
build:hdfs --define with_hdfs_support=true
build:s3 --define with_s3_support=true
build:xla --define with_xla_support=true
build:gdr --define with_gdr_support=true
build:verbs --define with_verbs_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env CUDA_TOOLKIT_PATH=""/usr""
build --action_env TF_CUDA_VERSION=""8.0""
build --action_env CUDNN_INSTALL_PATH=""/usr""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env NVVMIR_LIBRARY_DIR=""/usr/share/cuda""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --action_env TF_CUDA_CLANG=""0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc53""
build --config=cuda
test --config=cuda
build --define grpc_no_ares=true
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
build --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
build:mkl --define using_mkl=true
build:mkl -c opt
build:monolithic --define framework_shared_object=false
build --define framework_shared_object=true
build:android --crosstool_top=//external:android/crosstool
build:android --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
build:android_arm --config=android
build:android_arm --cpu=armeabi-v7a
build:android_arm64 --config=android
build:android_arm64 --cpu=arm64-v8a
````",0,,5,2017-12-24T13:05:18Z,NONE,2018-01-23T18:49:39Z
15612,Optimize FusedBatchNormGrad.,"awaiting review,cla: yes,stat:awaiting response","Reuse the output buffer and allocate only one temporary tensor, when data format is NHWC and
GPU is used. This is based on the observation that cudnn can perform the backward computation
in place. The same idea is used in PR #15601 . 

This lowers GPU memory consumption and may improve performance, because fewer distinct memory addresses are accessed. It also permits a higher batch size.

I've also added a new test for the gradient computation.",0,,7,2017-12-24T10:38:20Z,CONTRIBUTOR,2017-12-25T02:34:44Z
15611,'saved_model_cli.py' bug fix!,type:bug/performance,"In file `python/tools/saved_model_cli.py`  at function `def _print_tensor_info(tensor_info):`
The first line should be:

 `  print('    dtype: ' + {value:key for (key,value) in types_pb2.DataType.items()}[tensor_info.dtype])`

Not be : ` print('    dtype: ' + types_pb2.DataType.keyss()[tensor_info.dtype])`

because `tensor_info.dtype`  is an Integer which is the value of types(not the index of type values).",1,,7,2017-12-24T09:33:59Z,NONE,2017-12-24T15:34:24Z
15610,Cannot load pretrained models in Android via jcenter-provided library,,"It's a great step to support importing Android lib via jcenter. However, I'm having trouble in loading pretrained models into `TensorFlowInferenceInterface`, which says `NodeDef mentions attr 'dilations' not in Op...`. I think it's a compatibility issue, meaning that the model graph is not consistent with the graph interpreter. The models directly downloaded from `slim` and frozen by myself. Anyone can help me?",0,,1,2017-12-24T08:35:28Z,NONE,2017-12-30T10:06:50Z
15609,The relationship between neural network depth and accuracy,,"Hi:
I am learning to design a simple neural network, and try to identify 28x28 pixel greyscale images in the MNIST dataset.
I find that the neural network depth is not directly proportional to the recognition rate,

one layer neural network recognition rate: 92%
two layer neural network recognition rate: 94%
four layer neural network recognition rate: 91.8%

Can someone help me analyze it?
![6](https://user-images.githubusercontent.com/31270354/34325172-85768d48-e8c5-11e7-831a-6d11b9b02bf5.PNG)
(Please click on the picture to see the complete picture information)

**thank you very much!**",0,,1,2017-12-24T08:15:10Z,NONE,2017-12-28T19:08:04Z
15607,"Fix `tf.pow(x, y)` edge case with integer x and negative integer y","awaiting testing (then merge),cla: yes,kokoro:run","This fix tries to address the issue raised in #12156 and #9560 (and PR #11852) where pow(x, y) hangs with an integer x and a negative value of y.

This fix tries to throw out an error like numpy in this case:
```
>>> np.power([5, 5], [2, -2])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: Integers to negative integer powers are not allowed.
```

This fix adds error to the C++ functor like safe div/mod so that and InvalidArgument error could be triggered if any one of the values of y is negative.

NOTE: Similar to safe_div/mod this fix also does not cover GPU (not working yet).

This fix fix #12156. This fix is also related to #9560.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,5,2017-12-24T00:17:32Z,MEMBER,2017-12-25T02:31:55Z
15605,Make `frame` positions configurable,"API review,cla: yes","This also fixes a bug in `get_root_dir_with_all_resources`.

The value of `sys._getframe(1)` is caller-sensitive.
This leads to unexpected bugs where it gives the path of the wrong file.

For example, `get_root_dir_with_all_resources` is supposed to find the `runfiles` directoy.
But because of the extra call to `get_data_files_path`, everything shifts by one and now it resolves all paths on the location of the `resource_loader.py` file loaded (repository or PIP, but not a `runfiles` path). In the places it's used currently, it doesn't break anything, tho.

This PR fixes the behavior of `get_root_dir_with_all_resources` and makes the `frame` positions configurable on all methods (with a default for compatibility).

@martinwicke Friendly ping.",0,,4,2017-12-23T15:55:25Z,CONTRIBUTOR,2017-12-27T21:17:37Z
15604,ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory,,"I installed tf-nightly build and I get the following error on import of tensorflow.
`ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory`.

If I check for cuda 9, I get the following:
```
ldconfig -v
/usr/local/cuda-8.0/targets/x86_64-linux/lib:
	libnvgraph.so.8.0 -> libnvgraph.so.8.0.61
	libnppicom.so.8.0 -> libnppicom.so.8.0.61
	libnppial.so.8.0 -> libnppial.so.8.0.61
	libcufftw.so.8.0 -> libcufftw.so.8.0.61
	libcufft.so.8.0 -> libcufft.so.8.0.61
	libnppif.so.8.0 -> libnppif.so.8.0.61
	libcublas.so.8.0 -> libcublas.so.8.0.88
	libnvblas.so.8.0 -> libnvblas.so.8.0.88
	libnppi.so.8.0 -> libnppi.so.8.0.61
	libcusolver.so.8.0 -> libcusolver.so.8.0.61
	libnppidei.so.8.0 -> libnppidei.so.8.0.61
	libnvrtc-builtins.so.8.0 -> libnvrtc-builtins.so.8.0.61
	libnvrtc.so.8.0 -> libnvrtc.so.8.0.61
	libnpps.so.8.0 -> libnpps.so.8.0.61
	libcuinj64.so.8.0 -> libcuinj64.so.8.0.61
	libnppig.so.8.0 -> libnppig.so.8.0.61
	libOpenCL.so.1 -> libOpenCL.so.1.0.0
	libnppicc.so.8.0 -> libnppicc.so.8.0.61
	libnppist.so.8.0 -> libnppist.so.8.0.61
	libnppisu.so.8.0 -> libnppisu.so.8.0.61
	libnppim.so.8.0 -> libnppim.so.8.0.61
	libcurand.so.8.0 -> libcurand.so.8.0.61
	libcudart.so.8.0 -> libcudart.so.8.0.61
	libnvToolsExt.so.1 -> libnvToolsExt.so.1.0.0
	libnppitc.so.8.0 -> libnppitc.so.8.0.61
	libnppc.so.8.0 -> libnppc.so.8.0.61
	libcusparse.so.8.0 -> libcusparse.so.8.0.61
/usr/local/cuda-9.1/targets/x86_64-linux/lib:
	libnppicc.so.9.1 -> libnppicc.so.9.1.85
	libnppisu.so.9.1 -> libnppisu.so.9.1.85
	libcufftw.so.9.1 -> libcufftw.so.9.1.85
	libcufft.so.9.1 -> libcufft.so.9.1.85
	libnppial.so.9.1 -> libnppial.so.9.1.85
	libnppist.so.9.1 -> libnppist.so.9.1.85
	libcublas.so.9.1 -> libcublas.so.9.1.85
	libnvblas.so.9.1 -> libnvblas.so.9.1.85
	libnppitc.so.9.1 -> libnppitc.so.9.1.85
	libcusolver.so.9.1 -> libcusolver.so.9.1.85
	libnvrtc.so.9.1 -> libnvrtc.so.9.1.85
	libnvrtc-builtins.so.9.1 -> libnvrtc-builtins.so.9.1.85
	libnppidei.so.9.1 -> libnppidei.so.9.1.85
	libOpenCL.so.1 -> libOpenCL.so.1.0.0
	libnppig.so.9.1 -> libnppig.so.9.1.85
	libnppc.so.9.1 -> libnppc.so.9.1.85
	libcudart.so.9.1 -> libcudart.so.9.1.85
	libnvToolsExt.so.1 -> libnvToolsExt.so.1.0.0
	libnvgraph.so.9.1 -> libnvgraph.so.9.1.85
	libnppif.so.9.1 -> libnppif.so.9.1.85
	libcusparse.so.9.1 -> libcusparse.so.9.1.85
	libaccinj64.so.9.1 -> libaccinj64.so.9.1.85
	libcuinj64.so.9.1 -> libcuinj64.so.9.1.85
	libnppim.so.9.1 -> libnppim.so.9.1.85
	libnppicom.so.9.1 -> libnppicom.so.9.1.85
	libnpps.so.9.1 -> libnpps.so.9.1.85
	libcurand.so.9.1 -> libcurand.so.9.1.85
```
I that due to a name mismatch. `libcublas.so.9.0 =! libcublas.so.9.1`? And if so how can we overcome this?",0,,18,2017-12-23T13:58:25Z,NONE,2017-12-29T22:12:20Z
15603,[CMake] Remove invalid python modules,"awaiting testing (then merge),cla: yes","Split from #15368
@mrry Friendly ping.",0,,12,2017-12-23T13:43:51Z,CONTRIBUTOR,2017-12-26T18:46:54Z
15602,[CMake] Test existence of python entries,"awaiting testing (then merge),cla: yes","Split from #15166
@mrry Friendly ping,",0,,9,2017-12-23T13:34:36Z,CONTRIBUTOR,2017-12-29T00:55:23Z
15601,Optimize FusedBatchNorm (and fix a bug).,"awaiting review,cla: yes","I discovered experimentally that `cudnn` computations can be performed in place. Therefore there is no need to allocate two temporary tensors in `FusedBatchNorm` for GPU and data format NHWC. One is enough. This lowers memory consumption, and hence increases the maximum possible batch size.

This might seem risky (because NVIDIA doesn't mention the property), but in fact the current implementation already uses it: by doing forward_input_or_allocate_output in `FusedBatchNormOp`.
If data format is NCHW, and the input is forwarded, then `cudnn` would be forced
to do the computation in place (see line 247). This is how I discovered that the whole approach works:
I was trying to see if forwarding the input is a bug or not.

I added several tests to ensure that the change is correct.

While doing this, I discovered that ops_testutil does not properly synchronize at the end.
The reason seems to be the call `context_->eigen_gpu_device().synchronize()`. Somehow
it does nothing. I think the problem is that Eigen is not compiled with the flag `EIGEN_CUDACC`.
So I changed it to `GPUUtil::Sync(device_.get())`.",0,,5,2017-12-23T13:22:22Z,CONTRIBUTOR,2017-12-25T02:24:08Z
15600,Exclude tf_stream_executor.cmake for CPU only,"awaiting testing (then merge),cla: yes","Bug introduced in #15099
Fixes #3996",0,,3,2017-12-23T12:46:56Z,CONTRIBUTOR,2017-12-23T12:50:09Z
15599,Freeze pb model will not remove is_training flag of bn attr and it is moreover still set as true,stat:awaiting response,"Using /tensorflow/python/framework/graph_util_impl.py # **convert_variables_to_constants** to freeze graph to pb model will not remove **is_training** flag of **batch_normalization** as well as **fused batch_normalization** ops (tf.layers.batch_normalziation) in the inference graph.  Moreover, the **is_training** flag is still set as true. This issue will not cause any errors to use the pb model on Android. However, it will cause converting errors to tflite model by toco. Is it possible to optimize this conversion tool to resolve the problem in further versions. 

_Following is the node output of the pb model during loading:
name: ""convolution_layer/block_layer1/batch_normalization/FusedBatchNorm""
op: ""FusedBatchNorm""
input: ""convolution_layer/block_layer1/conv2d/Conv2D""
input: ""batch_normalization/gamma/read""
input: ""batch_normalization/beta/read""
input: ""convolution_layer/block_layer1/batch_normalization/Const""
input: ""convolution_layer/block_layer1/batch_normalization/Const_1""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""data_format""
  value {
    s: ""NHWC""
  }
}
attr {
  key: ""epsilon""
  value {
    f: 0.0010000000475
  }
}
**attr {
  key: ""is_training""
  value {
    b: true
  }**
}_",0,,4,2017-12-23T08:54:29Z,NONE,2017-12-25T02:21:27Z
15598,R1.4,cla: no,,0,,3,2017-12-23T07:29:24Z,NONE,2017-12-25T02:18:33Z
15597,Adding installation dependencies for python3.6 based on failures from,cla: yes, the release job.,0,,2,2017-12-23T00:39:58Z,MEMBER,2017-12-23T00:46:17Z
15596,Update tf-learn reference to TF Estimators,cla: yes,,0,,1,2017-12-22T21:57:47Z,MEMBER,2017-12-23T22:47:17Z
15594,MKL: Adding MKL-DNN support for LRN op,"awaiting testing (then merge),cla: yes",,0,,2,2017-12-22T21:06:09Z,CONTRIBUTOR,2017-12-22T22:12:11Z
15593,Branch 179953488,cla: yes,push,0,,7,2017-12-22T21:05:48Z,CONTRIBUTOR,2017-12-22T21:06:01Z
15592,Refactor methods for path calculation,cla: yes,"@jart I really appreciate your advice. I've rewritten #15315 to be more ""boring"":
* `get_grandparent(path, degree)` -> a files grandparent of the given degree
* `get_abs_data_path(path, depth, frame=0)` -> path relative to a file upwards the callstack
* `get_data_files_path(frame=0)` -> no change in behavior, but now shared with other methods

They should be about boring enough to be useful and to be shared.

* This should resolve the issues with `get_data_files_path` by making it less brittle...
* And I think `get_abs_data_path` is how `get_path_to_datafile` should really look like?

Further thoughts on this are appreciated!",0,,4,2017-12-22T19:48:27Z,CONTRIBUTOR,2017-12-22T20:00:32Z
15589,Segmentation fault on get_session_handle after 1.4.1 upgrade,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 (from 1.4.1 pip install)
- **Python version**: 2.7.6
- **Bazel version (if compiling from source)**: N/a
- **GCC/Compiler version (if compiling from source)**: N/a
- **CUDA/cuDNN version**: N/a
- **GPU model and memory**: N/a
- **Exact command to reproduce**: 

After upgrading from 1.3 to 1.4.1, running the following code produces a segmentation fault:

```
import tensorflow as tf

with tf.Session() as S: S.run( tf.get_session_handle(tf.constant(1, dtype=tf.float32)) )
```


### Source code / logs

GDB stack traces:

```
(gdb) py-bt 
0x00007fffb765b307 in nsync::nsync_mu_lock(nsync::nsync_mu_s_*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
(gdb) py-bt
#22 Frame 0xfea2d0, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 1302, in _run_fn (session=<SwigPyObject at remote 0x7fff6e664330>, feed_dict={}, fetch_list=['GetSessionHandle:0'], target_list=[], options=None, run_metadata=None, status=<SwigPyObject at remote 0x7fff6e670f00>)
    status, run_metadata)
#27 Frame 0xff8a00, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 1323, in _do_call (self=<Session(_config=None, _graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={}, _name_stack='', _gradient_override_map={}, _seed=None, _handle_movers={}, _op_to_kernel_label_map={}, _nodes_by_id={1: <Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fffeb2f2150>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=1) at remote 0x7ffff377d950>, _consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fff6e612810>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=7) at remote 0x7ffff377db10>, _consumers=[], _id=1L) at remote 0x7fff6e612750>], _control_flow_context=None, _id_value=2, _original_op=None, _traceback=[('minimal_core_dump.py', 3, '<module>', {'__builtins__': <module at remote 0x7ffff7f9ab08>, '__fi...(truncated)
    return fn(*args)
#31 Frame 0xfe2fe0, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 1317, in _do_run (self=<Session(_config=None, _graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={}, _name_stack='', _gradient_override_map={}, _seed=None, _handle_movers={}, _op_to_kernel_label_map={}, _nodes_by_id={1: <Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fffeb2f2150>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=1) at remote 0x7ffff377d950>, _consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fff6e612810>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=7) at remote 0x7ffff377db10>, _consumers=[], _id=1L) at remote 0x7fff6e612750>], _control_flow_context=None, _id_value=2, _original_op=None, _traceback=[('minimal_core_dump.py', 3, '<module>', {'__builtins__': <module at remote 0x7ffff7f9ab08>, '__fil...(truncated)
    options, run_metadata)
#35 Frame 0xf12bc0, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 1120, in _run (self=<Session(_config=None, _graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={}, _name_stack='', _gradient_override_map={}, _seed=None, _handle_movers={}, _op_to_kernel_label_map={}, _nodes_by_id={1: <Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fffeb2f2150>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=1) at remote 0x7ffff377d950>, _consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fff6e612810>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=7) at remote 0x7ffff377db10>, _consumers=[], _id=1L) at remote 0x7fff6e612750>], _control_flow_context=None, _id_value=2, _original_op=None, _traceback=[('minimal_core_dump.py', 3, '<module>', {'__builtins__': <module at remote 0x7ffff7f9ab08>, '__file__...(truncated)
    feed_dict_tensor, options, run_metadata)
#39 Frame 0xfa91e0, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 889, in run (self=<Session(_config=None, _graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={}, _name_stack='', _gradient_override_map={}, _seed=None, _handle_movers={}, _op_to_kernel_label_map={}, _nodes_by_id={1: <Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fffeb2f2150>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=1) at remote 0x7ffff377d950>, _consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fff6e612810>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=7) at remote 0x7ffff377db10>, _consumers=[], _id=1L) at remote 0x7fff6e612750>], _control_flow_context=None, _id_value=2, _original_op=None, _traceback=[('minimal_core_dump.py', 3, '<module>', {'__builtins__': <module at remote 0x7ffff7f9ab08>, '__file__':...(truncated)
    run_metadata_ptr)
#43 Frame 0x7ffff7f4da00, for file minimal_core_dump.py, line 3, in <module> ()
    with tf.Session() as S: S.run( tf.get_session_handle(tf.constant(1, dtype=tf.float32)) )
```


```
(gdb) bt  
#0  0x00007fffb765b307 in nsync::nsync_mu_lock(nsync::nsync_mu_s_*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#1  0x00007fffb494de1f in tensorflow::SessionState::GetNewId() () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/../libtensorflow_framework.so
#2  0x00007fffb636bd24 in tensorflow::GetSessionHandleOp::Compute(tensorflow::OpKernelContext*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007fffb7852729 in tensorflow::grappler::ConstantFolding::EvaluateNode(tensorflow::NodeDef const&, tensorflow::gtl::InlinedVector<tensorflow::TensorValue, 4> const&, tensorflow::gtl::InlinedVector<tensorflow::TensorValue, 4>*) const () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007fffb7858a03 in tensorflow::grappler::ConstantFolding::EvaluateOneFoldable(tensorflow::NodeDef const&, std::vector<tensorflow::NodeDef, std::allocator<tensorflow::NodeDef> >*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007fffb7859486 in tensorflow::grappler::ConstantFolding::FoldNode(tensorflow::NodeDef*, tensorflow::GraphDef*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007fffb785ab03 in tensorflow::grappler::ConstantFolding::FoldGraph(tensorflow::GraphDef*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007fffb785b526 in tensorflow::grappler::ConstantFolding::RunOptimizationPass(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007fffb785b9c9 in tensorflow::grappler::ConstantFolding::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007fffb7843657 in tensorflow::grappler::MetaOptimizer::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007fffb7844925 in tensorflow::grappler::RunMetaOptimizer(tensorflow::grappler::GrapplerItem const&, tensorflow::RewriterConfig const&, tensorflow::DeviceBase*, tensorflow::grappler::Cluster*, tensorflow::GraphDef*)
    () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007fffb782f316 in tensorflow::GraphExecutionState::OptimizeGraph(tensorflow::BuildGraphOptions const&, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#12 0x00007fffb782fe80 in tensorflow::GraphExecutionState::BuildGraph(tensorflow::BuildGraphOptions const&, std::unique_ptr<tensorflow::ClientGraph, std::default_delete<tensorflow::ClientGraph> >*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#13 0x00007fffb76678ae in tensorflow::DirectSession::CreateGraphs(tensorflow::BuildGraphOptions const&, std::unordered_map<std::string, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> > > > >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*, tensorflow::DirectSession::RunStateArgs*, tensorflow::gtl::InlinedVector<tensorflow::DataType, 4>*, tensorflow::gtl::InlinedVector<tensorflow::DataType, 4>*)
    () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#14 0x00007fffb7669daa in tensorflow::DirectSession::GetOrCreateExecutors(tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#15 0x00007fffb766b5bb in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#16 0x00007fffb5d5e0fa in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#17 0x00007fffb5d5e434 in TF_Run () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#18 0x00007fffb5a7c9da in tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#19 0x00007fffb5a7cdd1 in tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#20 0x00007fffb5a410b1 in _wrap_TF_Run () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#21 0x00000000004c7f54 in call_function (oparg=<optimized out>, pp_stack=0x7fffffffd1a0) at ../Python/ceval.c:4020
#22 PyEval_EvalFrameEx (
    f=f@entry=Frame 0xfea2d0, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 1302, in _run_fn (session=<SwigPyObject at remote 0x7fff6e664330>, feed_dict={}, fetch_list=['GetSessionHandle:0'], target_list=[], options=None, run_metadata=None, status=<SwigPyObject at remote 0x7fff6e670f00>), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666
#23 0x00000000004704ea in PyEval_EvalCodeEx (closure=<optimized out>, defcount=<optimized out>, defs=0x0, kwcount=<optimized out>, kws=<optimized out>, argcount=<optimized out>, args=<optimized out>, locals=0x0,
    globals=<optimized out>, co=<optimized out>) at ../Python/ceval.c:3252
#24 function_call.15337 (func=<optimized out>, arg=<optimized out>, kw=<optimized out>) at ../Objects/funcobject.c:526
#25 0x00000000004c9aa5 in PyObject_Call (kw=0x0, arg=(<SwigPyObject at remote 0x7fff6e664330>, {}, ['GetSessionHandle:0'], [], None, None), func=<function at remote 0x7fff6e610d70>) at ../Objects/abstract.c:2529
#26 ext_do_call (nk=<optimized out>, na=<optimized out>, flags=<optimized out>, pp_stack=0x7fffffffd3e0, func=<function at remote 0x7fff6e610d70>) at ../Python/ceval.c:4333
... # partial output
```
",0,,6,2017-12-22T17:50:43Z,NONE,2017-12-22T22:23:01Z
15587,lite model file size,type:support,"how to reduce my model size(my own trained tensorflow model )
i want to use tensorflow lite converter get a smaller tflite file, but the tflite file has the same size with my trained model,
-------------------------------------------------------------------------

bazel-bin/tensorflow/contrib/lite/toco/toco \
  --allow_custom_ops \
  --input_file=/data/log/frozen_model.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \
  --output_file=/data/log//mobilenet.tflite --inference_type=FLOAT \
  --input_data_types=FLOAT --input_arrays=input \
  --output_arrays=output --input_shapes=1,224,224,3

--------------------------------------------------------------------------",0,,1,2017-12-22T13:22:38Z,NONE,2017-12-22T22:30:39Z
15583,tensorflow logging glog redefined warning,stat:awaiting response,"I just integrated a tensorflow module to my existing c++ project. I was already using glog for logging in my existing project, logging to files.

Now when I build the new project it throws warnings like:
`/usr/local/include/google/tensorflow/tensorflow/core/platform/default/logging.h:254:0: warning: ""CHECK_GT"" redefined
#define CHECK_GT(val1, val2) CHECK_OP(Check_GT, >, val1, val2)
/usr/local/include/glog/logging.h:793:0: note: this is the location of the previous definition
#define CHECK_GT(val1, val2) CHECK_OP(_GT, > , val1, val2)`

Is this behavior safe? I am not concerned that much with the logs that tensorflow generates, I need my existing modules to continue logging to files.

If this behavior is unsafe, this could be a good feature request to have glog logging not affected by tensorflow logging.",0,,3,2017-12-22T11:19:13Z,NONE,2017-12-22T19:00:14Z
15581,TfLiteCameraDemo only contains 32-bit libtensorflowlite_jni.so,"comp:lite,stat:awaiting tensorflower","I strictly followed the steps on https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/ to
build the TfLiteCameraDemo. But I found only 32-bit libtensorflowlite_jni.so was contained in the final APK.

I modified the source code of libneuralnetworks.so to test my employer's NN accelerator, which needed the 64-bit libtensorflowlite_jni.so.

even I configed bazel with
./configure --config=android_arm64

and built with 
bazel build --cxxopt=--std=c++11 --cpu=arm64-v8a //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo

could not work.

Where was my mistake? Or how can I pack the 64-bit  libtensorflowlite_jni.so into the final APK?
",1,,7,2017-12-22T09:47:47Z,NONE,2017-12-28T19:03:09Z
15579,[Bazel/MSVC] Fix build error since aae439c,"awaiting testing (then merge),cla: yes",#15213,0,,5,2017-12-22T09:29:53Z,CONTRIBUTOR,2017-12-22T10:51:30Z
15578,Portability fix for StrAppend,"cla: yes,stat:awaiting response",#15557,0,,6,2017-12-22T08:22:38Z,CONTRIBUTOR,2017-12-25T02:12:33Z
15577,update README.md,"awaiting review,cla: yes",Added whl file downloading links of Python 3.6 for Linux with CPU only and GPU both :),0,,6,2017-12-22T08:13:44Z,CONTRIBUTOR,2017-12-23T01:32:36Z
15575,TensorFlow automatically modify variable scope name,,"check the following code:

```python
with tf.variable_scope('test'):
      v1 = tf.placeholder(tf.float32, shape=(10,10), name='v1')

with tf.variable_scope('test'):
      v2 = tf.placeholder(tf.float32, shape=(5, 5), name='v2')

print(v1)
print(v2)
```


the code output:

 ```python
Tensor(""test/v1:0"", shape=(10, 10), dtype=float32)
Tensor(""test_1/v2:0"", shape=(5, 5), dtype=float32)
```

It looks ridiculous for TF modfiy variable scope name 'test' into 'test_1' while my new placeholder named 'v2' is different from 'v1'. However, if i add variables, everything is going to be normal. code like:

```python
    with tf.variable_scope('test'):
        v1 = tf.placeholder(tf.float32, shape=(10,10), name='v1')
        w1 = tf.get_variable('w1', shape=(2,2))

    with tf.variable_scope('test'):
        v2 = tf.placeholder(tf.float32, shape=(5, 5), name='v2')
        w2 = tf.get_variable('w2', shape=(3, 3))
    print(v1)
    print(v2)
    print(w1)
    print(w2)
```
outputs:
```python
Tensor(""test/v1:0"", shape=(10, 10), dtype=float32)
Tensor(""test_1/v2:0"", shape=(5, 5), dtype=float32)
<tf.Variable 'test/w1:0' shape=(2, 2) dtype=float32_ref>
<tf.Variable 'test/w2:0' shape=(3, 3) dtype=float32_ref>
```

I dont know why TF **just modify the variable scope name** while i try to add a new placeholder into the existed scope.Is it a BUG?
",0,,2,2017-12-22T07:08:33Z,NONE,2017-12-22T07:31:38Z
15574,Fix adding elements to collections.deque.,"awaiting testing (then merge),cla: yes","In some cases, [word2vec_basic.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L123) will fail at L123 and throw the following error:
```
TypeError: sequence index must be integer, not 'slice'
```
It seams the ```text8``` dataset and parameters will not touch that line code, but I hit this issue when I run this script against my own dataset. This is caused by Python ```collections.deque``` doesn't support index by slice:
```
>>> from collections import deque
>>> d = deque('tensorflow')
>>> d[:]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: sequence index must be integer, not 'slice'
```
This PR fix this issue by using ```deque.extend```, and this is consistent with L114.",0,,5,2017-12-22T07:07:18Z,CONTRIBUTOR,2017-12-22T07:16:30Z
15573,'tensorflow/contrib/reduce_slice_ops/_objs/python/ops/_reduce_slice_ops_gpu/tensorflow/contrib/reduce_slice_ops/kernels/reduce_slice_ops_gpu.cu.pic.o' was not created,,"Hi i'm running the tensorflow from the source and after building the tensorflow using bazel using below command 

**bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package** 

**The error message pasted below**

10 errors detected in the compilation of ""/home/lb/.cache/bazel/_bazel_lb/144f5944ef8ff562c57e67fdaa41565f/execroot/org_tensorflow/tmpae8_5f0170552fd082f4/tmpxft_0000245c_00000000-6_reduce_slice_ops_gpu.cu.cpp1.ii"".
ERROR: /home/lb/WorkSpace/AI-WORK/tensorflow/tensorflow/contrib/reduce_slice_ops/BUILD:14:1: output 'tensorflow/contrib/reduce_slice_ops/_objs/python/ops/_reduce_slice_ops_gpu/tensorflow/contrib/reduce_slice_ops/kernels/reduce_slice_ops_gpu.cu.pic.o' was not created
ERROR: /home/lb/WorkSpace/AI-WORK/tensorflow/tensorflow/contrib/reduce_slice_ops/BUILD:14:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 74.030s, Critical Path: 8.38s
FAILED: Build did NOT complete successfully
",0,,16,2017-12-22T06:40:28Z,NONE,2017-12-22T20:05:30Z
15571,why keras run slower and slower,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,1,2017-12-22T03:35:22Z,NONE,2017-12-22T18:58:36Z
15570,Using grid_rnn_cell.Grid1LSTMCell makes stack_bidirectional_dynamic_rnn Throw different shape errors,,"I encountered a problem using grid_rnn_cell.Grid1LSTMCell. I have no problems using it on contrib static_rnn and contrib static_bidirectional_rnn.

```
class GridRNNTest(tf.test.TestCase):
    def setUp(self):
        self.num_features = 1
        self.time_steps = 1
        self.batch_size = 1
        tf.reset_default_graph()
        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])
        self.cell = grid_rnn_cell.Grid1LSTMCell(num_units=8)

    def test_simple_grid_rnn(self):
        self.input_layer = tf.unstack(self.input_layer, self.time_steps, 1)
        rnn.static_rnn(self.cell, self.input_layer, dtype=tf.float32)

class BidirectionalGridRNNTest(tf.test.TestCase):
    def setUp(self):
        self.num_features = 1
        self.time_steps = 1
        self.batch_size = 1
        tf.reset_default_graph()
        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])
        self.cell_fw = grid_rnn_cell.Grid1LSTMCell(num_units=8)
        self.cell_bw = grid_rnn_cell.Grid1LSTMCell(num_units=8)

    def test_simple_bidirectional_grid_rnn(self):
        self.input_layer = tf.unstack(self.input_layer, self.time_steps, 1)
        rnn.static_bidirectional_rnn(self.cell_fw, self.cell_bw, self.input_layer, dtype=tf.float32)
```

However, when I test it on contrib stack_bidirectional_dynamic_rnn:

```
class StackBidirectionalGridRNNTest(tf.test.TestCase):
    def setUp(self):
        self.num_features = 1
        self.time_steps = 1
        self.batch_size = 1
        tf.reset_default_graph()
        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])
        self.cells_fw = [grid_rnn_cell.Grid1LSTMCell(num_units=8) for _ in range(2)]
        self.cells_bw = [grid_rnn_cell.Grid1LSTMCell(num_units=8) for _ in range(2)]

    def test_stack_bidirectional_grid_rnn(self):
        self.input_layer = tf.unstack(self.input_layer, self.time_steps, 1)
        rnn.stack_bidirectional_dynamic_rnn(self.cells_fw, self.cells_fw, self.input_layer, dtype=tf.float32)
```

I encounter these shape errors:

(1) When the input_layer is unstacked: `Shape (1, 1) must have rank at least 3`
(2) When the input_layer is not unstacked: Shape `(1, 2, 8) must have rank 2`

Which are, apparently,  conflicting with each other.",1,,7,2017-12-22T03:34:15Z,CONTRIBUTOR,2017-12-22T03:40:54Z
15569,Dataset shuffle operation is not deterministic,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:elementary OS 0.4.1 Loki
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: Not related
- **GPU model and memory**: Not related 
- **Exact command to reproduce**: mentioned below


### Describe the problem
Dataset Shuffle operation is not determenastic 

### Source code / logs
```python
import numpy as np
import tensorflow as tf

def test():
  np.random.seed(42)
  tf.set_random_seed(42)
  numbers = np.arange(1,100)

  def get_data():
    dataset = tf.data.Dataset.from_tensor_slices(numbers)  # some initial dataset
    dataset =  dataset.shuffle(100)
    x = dataset.make_one_shot_iterator().get_next()
    return x

  # execution 1
  x = get_data()
  with tf.Session() as sess:
    x_batch1 = sess.run(x)
    print(x_batch1)
  # clear out everything
  tf.reset_default_graph()

  # execution 2
  x = get_data()
  with tf.Session() as sess:
    x_batch2 = sess.run(x)
    print(x_batch2)
  # results should be equivalent
  assert np.allclose(x_batch1, x_batch2)
test()
```
test code sample taken from @dusenberrymw",0,,2,2017-12-22T01:07:43Z,NONE,2017-12-22T14:29:46Z
15568,InvalidArgumentError: slice index 0 of dimension 0 out of bounds.,,"Hi,

### Describe the problem
I am trying to encode and decode an image using tf.image.decode_jpeg and b64 encoding. The code is so simple but it seems I have an error regarding StrideSlice. I am not sure if the problem coming from the undefined shape of input placeholder or related to jpeg_decode.  Thanks. 

### Source code / logs
```
import tensorflow as tf
import base64
import functools


def build_graph(input_len=None):
    graph = tf.Graph()
    with graph.as_default():
        image_bytes = tf.placeholder(tf.string, name='input_node')
        images = tf.map_fn(
            functools.partial(tf.image.decode_jpeg, channels=3),
            image_bytes,
            dtype=tf.uint8
        )
        image_floats = tf.cast(images, tf.float32, name=""output_node"") / 255.0

    return graph


if __name__ == '__main__':
    file_name = ""test_1.jpg""
    with open(file_name, ""rb"") as imageFile:
        image_string = imageFile.read()
        image_encode = base64.b64encode(image_string).decode(""utf-8"")

    graph = build_graph(input_len=len(image_encode))

    with tf.Session(graph=graph) as session:
        init_op = tf.group(tf.global_variables_initializer(),
                           tf.local_variables_initializer())
        session.run(init_op)
        input_node = graph.get_tensor_by_name(""input_node:0"")
        output_node = graph.get_tensor_by_name(""output_node:0"")
        image_out = session.run(output_node, feed_dict={input_node: image_encode})
```

These is the traceback:

```
Caused by op u'map/TensorArrayUnstack/strided_slice', defined at:
  File ""/.../Simple_Graph_Design/ImageTest.py"", line 26, in <module>
    graph = build_graph(input_len=len(image_encode))
  File ""/.../Simple_Graph_Design/ImageTest.py"", line 13, in build_graph
    dtype=tf.uint8
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/functional_ops.py"", line 354, in map_fn
    elem_ta.unstack(elem) for elem_ta, elem in zip(elems_ta, elems_flat)]
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py"", line 107, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_ops.py"", line 412, in unstack
    num_elements = array_ops.shape(value)[0]
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 538, in _SliceHelper
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 706, in strided_slice
    shrink_axis_mask=shrink_axis_mask)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 5430, in strided_slice
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): slice index 0 of dimension 0 out of bounds.
	 [[Node: map/TensorArrayUnstack/strided_slice = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](map/TensorArrayUnstack/Shape, map/TensorArrayUnstack/strided_slice/stack, map/TensorArrayUnstack/strided_slice/stack_1, map/TensorArrayUnstack/strided_slice/stack_1)]]

```
### System information
- TF version: 1.4.0
- Linux Ubuntu 16.04
- Python version: 2.7
- No CUDA or GPU 




",0,,6,2017-12-21T23:52:52Z,NONE,2017-12-22T07:26:22Z
15567,Recognize more environments as interactive,"awaiting review,cla: yes,kokoro:run","... and log to stdout in those cases, and show INFO messages by default.

Fixes #6438.",0,,2,2017-12-21T23:15:02Z,OWNER,2017-12-25T02:04:46Z
15566,"MKL: Reverting PR #14478, which breaks Inception v3 with MKL.","awaiting testing (then merge),cla: yes",PR #14478 https://github.com/tensorflow/tensorflow/pull/14478 (based on commit e7b69e4) removed line 276 from  #ifdef INTEL_MKL section and breaks Inception v3 with MKL. Perhaps someone meant to remove line 202?,0,,2,2017-12-21T23:12:36Z,CONTRIBUTOR,2017-12-22T22:11:39Z
15565,Linux GPU build failing (__CUDACC_VER__ is no longer supported),,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 17.10, kernel 4.14.8
- **TensorFlow installed from (source or binary)**:
Source (failed build)
- **TensorFlow version (use command below)**:
master
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
0.9.0
- **GCC/Compiler version (if compiling from source)**:
4.9
- **CUDA/cuDNN version**:
9.0/7.0.4
- **GPU model and memory**:
Two 1080 Ti (11Gb each)
- **Exact command to reproduce**:
bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=cuda //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
I note the build bot is failing too for Linux GPU, since the latest merge in the past 1 hour.

### Source code / logs
ERROR: /home/daniel/build/tensorflow/tensorflow/contrib/image/BUILD:106:1: error while parsing .d file: /home/daniel/.cache/bazel/_bazel_daniel/a40ff47569db15fec953d4e5b5812083/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/contrib/image/_objs/python/ops/_distort_image_ops_gpu/tensorflow/contrib/image/kernels/adjust_hsv_in_yiq_op_gpu.cu.pic.d (No such file or directory)
In file included from /usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/common_functions.h:50:0,
                 from /usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/cuda_runtime.h:115,
                 from <command-line>:0:
/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token """"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead."""" is not valid in preprocessor expressions
 #define __CUDACC_VER__ ""__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.""
                        ^
./tensorflow/core/util/cuda_device_functions.h:37:7: note: in expansion of macro '__CUDACC_VER__'
 #elif __CUDACC_VER__ >= 7050
       ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 237.382s, Critical Path: 22.79s
",0,,3,2017-12-21T21:53:54Z,NONE,2017-12-21T23:28:16Z
15563,Cannot statically link against Tensorflow library in Golang,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.4.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: `go build -a -v -o hellotf --ldflags '-linkmode external -extldflags ""-static -L /usr/local/lib""' -x <GITHUB_PROJ_NAME>/hellotf`

### Describe the problem
I'm trying to compile a **statically-linked** binary of the hello world app against the TF 1.4.0 binary per the [golang install & hello-world instructions](https://www.tensorflow.org/install/install_go) and am not able to successfully link against the TF library as it reports that `ltensorflow` cannot be found by `/usr/bin/ld` in the `go build`.

The command used is:
`go build -a -v -o hellotf --ldflags '-linkmode external -extldflags ""-static -L /usr/local/lib""' -x <GITHUB_PROJ_NAME>/hellotf`

I've tried linking against `/usr/local/lib` (where libtensorflow lives) using `-extldflags`, CGO_LDFLAGS, LDFLAGS etc, done `sudo ldconfig` and the env setup with LD_LIBRARY_PATH and LIBRARY_PATH, but cannot move past this step and always get the error status:

```
/.gvm/gos/go1.8.3/pkg/tool/linux_amd64/link: running gcc failed: exit status 1
/usr/bin/ld: cannot find -ltensorflow
```

The only related issue I've encountered for this is: https://stackoverflow.com/questions/44428816/tensorflow-for-go-demo-example-run-failed, but that didn't help much either.

That being said, I can dynamically link and build the helloworld go code e.g. `go build hello_tf.go` and `go test github.com/tensorflow/tensorflow/tensorflow/go` all work successfully; the issue arises when I try to statically link and cannot link to libtensorflow after it compiles, no matter what settings I try using.

Any help or advice would be greatly appreciated. Thanks!",0,,2,2017-12-21T20:35:00Z,NONE,2017-12-22T07:26:29Z
15561,_Assert3DImage that adds a control dependency for the shape check.,"awaiting review,cla: yes,stat:awaiting response","I noticed that the `_Check3DImage` was always used in exactly the same way and extracted this into its own convenience function. 

The only remaining use of `_Check3DImage` was an import in `gen_image_ops.py` saying
""# TODO(drpng): remove these once internal use has discontinued."", so maybe it could be removed entirely.",0,,3,2017-12-21T19:14:42Z,CONTRIBUTOR,2017-12-29T00:17:55Z
15560,Branch 179822007,cla: yes,,0,,2,2017-12-21T19:02:46Z,MEMBER,2017-12-21T19:27:55Z
15558,ONNX Model Support,,Please add support for the [ONNX](https://github.com/onnx/onnx) open model standard or a conversion tool would be great.,0,,1,2017-12-21T16:51:24Z,NONE,2017-12-22T00:13:48Z
15556,Little error in example how to read data,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 3.5.4
- **Exact command to reproduce**: 

```
python tensorflow/tensorflow/examples/how_tos/reading_data/convert_to_record.py
python tensorflow/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py
```

### Describe the problem

Error when doing the above second command: `AttributeError: module 'tensorflow' has no attribute 'data'` The data attribute is actually in the contrib module then you should change the line 108 of the file `tensorflow/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py`:

`dataset = tf.data.TFRecordDataset(filename)`
to
`dataset = tf.contrib.data.TFRecordDataset(filename)`

Sorry, I don't know if I should have submitted a pull request for such a little change.

",0,,4,2017-12-21T14:31:56Z,NONE,2017-12-21T15:17:56Z
15555,RGB<->YIQ colorspace conversion,"awaiting testing (then merge),cla: yes","Implementation of functions for colorspace conversion RGB <-> YIQ, according to [https://en.wikipedia.org/wiki/YIQ#Formulas](Wikipedia).
This PR adds CPP functions and python wrappers in tf.image namespace",1,,23,2017-12-21T14:11:06Z,CONTRIBUTOR,2017-12-21T18:40:00Z
15553,[XLA] Replace GCC vector extension with portable Intel SIMD,"awaiting testing (then merge),cla: yes","MSVC does not support GCC vector extension, so replace it with Intel SIMD library. Eigen also uses Intel SIMD library to implement vector functions.

MSVC does not have `__SSE4_1__` macro, although it does define `__AVX__` when building with `/arch:AVX`. Since Eigen enables SSE4.1 when `__AVX__` is defined ([`Eigen/Core`](https://bitbucket.org/eigen/eigen/src/034b6c3e101792a3cc3ccabd9bfaddcabe85bb58/Eigen/Core?at=default&fileviewer=file-view-default#Core-156)), we just follow Eigen.

#15213",0,,6,2017-12-21T12:14:31Z,CONTRIBUTOR,2017-12-21T18:41:37Z
15552,Tensorflow 1.4 C++ API considerably slower than Python,stat:awaiting response,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source with all optimizations
- **TensorFlow version (use command below)**: 1.4
- **Python version**:  3.5.2
- **Bazel version (if compiling from source)**: 0.8.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 8.0 / 6.0
- **GPU model and memory**: GTX960M

### Describe the problem

I was trying to run several models and evaluate the performance with different batch sizes in python and c++ and noticed that the c++ API version is considerably slower than the python one. Both were compiled with the same optimizations and with cuda support. 

When I try to predict the output of a single 256x256 image in python it takes me 0.5 seconds, and when i do it in tensorflow c++ api it takes me 1.7 seconds. Notice that in python I was using a non deployed model (without freezing and transforming graph), whereas in C++ I did those transformations. 

Does anyone knows why this is happening? Is it because of the frozen and transformed graph?

I always thought the C++ API would be at least as fast as the Python version. 
",0,,5,2017-12-21T11:13:17Z,NONE,2017-12-21T23:39:31Z
15550,fix random_distributions_test,"awaiting testing (then merge),cla: yes","[iota](http://en.cppreference.com/w/cpp/algorithm/iota) is declared in \<numeric\>
",0,,2,2017-12-21T08:28:01Z,CONTRIBUTOR,2017-12-21T18:09:27Z
15548,Bug fix for example_parsing_ops_test,"awaiting testing (then merge),cla: yes","Delay tensor allocation.

Static variables' initialization order is not determined in C++.
In one static variable's constructor, you can't access other static variables unless they are constexpr, which is not true for tensor's allocators.

Replacing it with std::call_once.

https://isocpp.org/wiki/faq/ctors#static-init-order",0,,3,2017-12-21T07:45:12Z,CONTRIBUTOR,2017-12-22T06:07:46Z
15547,"how to build tensorflow-lite.so  for linux  ubuntu, thanks?",type:support,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,1,2017-12-21T07:29:59Z,NONE,2017-12-22T22:46:30Z
15544,support unknown shape for `sparse_multiclass_hing_loss`,"awaiting testing (then merge),cla: yes","Fix #15480.

### How to test

+ [x] add test case.
+ [ ] pass all tests.",0,,2,2017-12-21T06:29:42Z,CONTRIBUTOR,2017-12-21T18:12:15Z
15543,Maven Nightlies,stat:awaiting response,"@asimshankar Would it be possible to start releasing Maven nightlies for the Java API packages? I only depend on the org.tensorflow Proto package, but I guess it would be more consistent to do that for all the Java API packages.",0,,2,2017-12-21T05:39:27Z,CONTRIBUTOR,2018-01-04T09:31:19Z
15540,No PyPi package for Tensorflow 1.4 on Windows?,stat:awaiting tensorflower,"Running Windows 10 with Python 3.6.
Just wondering when that's gonna come out.",0,,4,2017-12-21T03:58:55Z,NONE,2017-12-21T15:21:08Z
15539,Remove quantized_matmul_op_for_hexagon_test in Windows build scripts,"awaiting testing (then merge),cla: yes",,0,,2,2017-12-21T03:55:07Z,CONTRIBUTOR,2017-12-21T04:23:40Z
15538,[solved]session->Create(graph_def) No OpKernel was registered to support Op 'RandomUniform',,"issue: crashes when loading pb file in C++ program:

>     Session * session;
>     GraphDef graph_def;
>     SessionOptions opts;
>     TF_CHECK_OK(ReadBinaryProto(Env::Default(), heartPrintPbPathFile, &graph_def));
>     TF_CHECK_OK(NewSession(opts, &session));
>     TF_CHECK_OK(session->Create(graph_def));
> 

os: linux
train: python
inference: C++ interface

runtime error on inference part:

> Non-OK-status: session->Create(graph_def) status: Invalid argument: No OpKernel was registered to support Op 'RandomUniform' with these attrs.  Registered devices: [CPU], Registered kernels:
>   <no registered kernels>
> 
> 	 [[Node: rnn_net/rnn/dropout_63/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0](rnn_net/rnn/dropout_63/Shape)]]

python train code:

>         with tf.variable_scope(""rnn_net"") as scope:
>             cell = []
>             for i in range(num_layers):
>                 cell.append(tf.nn.rnn_cell.LSTMCell(hidden_size, state_is_tuple=True))
> 
>             cell = tf.nn.rnn_cell.MultiRNNCell(cell)
> 
>             cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = self.keep_prob)
> 
>             initial_state = cell.zero_state(batch_size, tf.float32)
> 
>             input_list = tf.unstack(conv_output, axis=1)
> 
>             rnn_output, _ = tf.nn.static_rnn(cell, input_list, dtype=tf.float32)
>             self.rnn_output = rnn_output[-1]
>             print ""rnn output shape: ""
>             print self.rnn_output.get_shape()

hint one : i found that if i delete this line:
cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = self.keep_prob)
then the trained pb file can be loaded normally.

hint two: beside rnn part, i also have cnn part in the network, and the dropout in cnn works just fine:

>         with tf.variable_scope(""conv_net"") as scope:
>             filters = [15, 11, 7, 5]
>             kernel_size = [64, 64, 32, 32]
>             input_layer = tf.reshape(self.x, [-1, seq_len, 1])
>             conv1_1 = tf.layers.conv1d(inputs=input_layer, filters=filters[0], kernel_size=kernel_size[0], padding=""same"", activation=tf.nn.tanh, name='conv1_1')
>             conv1_2 = tf.layers.conv1d(inputs=conv1_1, filters=filters[1], kernel_size=kernel_size[1], padding=""same"", activation=tf.nn.tanh)
>             pool1 = tf.layers.max_pooling1d(inputs=conv1_2, pool_size=4, strides=4)
> 
>             bn1 = batch_norm(pool1, self.is_train, scope='bn1')
>             dropout1 = tf.layers.dropout(inputs=bn1, rate=(1 - self.keep_prob))
> 
>             conv2_1 = tf.layers.conv1d(inputs=dropout1, filters=filters[2], kernel_size=kernel_size[2], padding=""same"", activation=tf.nn.tanh, name='conv2_1')
>             conv2_2 = tf.layers.conv1d(inputs=conv2_1, filters=filters[3], kernel_size=kernel_size[3], padding=""same"", activation=tf.nn.tanh)
>             pool2 = tf.layers.max_pooling1d(inputs=conv2_2, pool_size=4, strides=4)
> 
>             bn2 = batch_norm(pool2, self.is_train, scope='bn2')
>             dropout2 = tf.layers.dropout(inputs=bn2, rate=(1 - self.keep_prob))
> 
>             conv3_1 = tf.layers.conv1d(inputs=dropout2, filters=filters[2], kernel_size=kernel_size[2], padding=""same"", activation=tf.nn.tanh)
>             conv3_2 = tf.layers.conv1d(inputs=conv3_1, filters=filters[3], kernel_size=kernel_size[3], padding=""same"", activation=tf.nn.tanh)
>             pool3 = tf.layers.max_pooling1d(inputs=conv2_2, pool_size=2, strides=2)
> 
>             conv_output = pool3

",0,,6,2017-12-21T03:35:22Z,NONE,2017-12-21T18:43:29Z
15537,tensorflow lite converter: why empty lite file,"comp:lite,stat:awaiting response","I have a tensorflow pb file and try to use tflite converter to convert it to lite file,but the result file is empty and no error. I don't know why
`bazel run --config=opt --copt=-msse4.1 --copt=-msse4.2  //tensorflow/contrib/lite/toco:toco --   --input_file=/Users/Lavector/code/keras_to_tensorflow/mobilenet_regression.pb   --output_file=/tmp/mobilenet_regression.lite   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --inference_type=FLOAT   --input_shape=1,224,224,3   --input_array=input   --output_array=output_node0`

`WARNING: Config values are not defined in any .rc file: opt
INFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).
INFO: Found 1 target...
Target //tensorflow/contrib/lite/toco:toco up-to-date:
  bazel-bin/tensorflow/contrib/lite/toco/toco
INFO: Elapsed time: 0.223s, Critical Path: 0.01s
INFO: Build completed successfully, 1 total action

INFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/Users/Lavector/code/keras_to_tensorflow/mobilenet_regression.pb' '--output_file=/tmp/mobilenet_regression.lite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=FLOAT' '--input_shape=1,224,224,3' '--input_array=input' '--output_array=output_node0'
2017-12-21 11:20:11.351544: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 472 operators, 695 arrays (0 quantized)
2017-12-21 11:20:11.384604: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 85 operators, 200 arrays (0 quantized)
2017-12-21 11:20:11.387067: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 85 operators, 200 arrays (0 quantized)`

I use mobilenet to do regression, and it is successful when test.
I test the example code of [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md](url), and it is correct.",0,,8,2017-12-21T03:19:09Z,NONE,2017-12-22T07:27:03Z
15534,mpi_collectives: Refactor to fix build issues,"awaiting testing (then merge),cla: yes","After TF commit 5c7f9e3, the mpi_collectives package would no longer build
ops and kernels. This build issue caused mpi_collectives import to fail in
Python with the following error: ""NameError: Could not find operator MPISize
in dynamic library mpi_collectives.so"" (ref: issue #13875).

To fix this issue, add build targets to ensure both ops and kernels are built.
Note, also refactored the build targets and directory structure to more
closely match other contrib packages.",0,,3,2017-12-21T00:35:51Z,CONTRIBUTOR,2017-12-25T23:43:49Z
15533,added note about weights gradient in compute_weighted_loss,"awaiting testing (then merge),cla: yes","Added a note concerning the gradient computation w.r.t. weights in `losses.compute_weighted_loss`, 
see #15046.

I have only added it to this function, and not the other losses (like mean_squared_error) because it is a rare cornercase that should be documented somewhere, but is of no relevance to most users. ",0,,5,2017-12-21T00:20:07Z,CONTRIBUTOR,2017-12-21T00:27:40Z
15532,Update API Docs,"cla: yes,stat:awaiting response",Updated api dos for SampleDistortedBoundingBox v1 and v2 to update tf.image_summary to tf.summary.image,0,,6,2017-12-20T23:08:57Z,CONTRIBUTOR,2017-12-20T23:13:46Z
15531,Fix sample_distorted_bounding_box where min_object_covered could be None,"awaiting testing (then merge),cla: yes","This fix tried to address the issue raised in #15529 where not providing min_object_covered a value will result in a ValueError. In the docstring, however, min_object_covered has been described as default to 0.1.

The reason for the issue is that when sample_distorted_bounding_box switched to V2, min_object_covered has been changed form an attr to an input. As input could not have a default value, min_object_covered=None will result in an error.

This fix adds the check so that a default value 0.1 will be provided if min_object_covered=None.

This fix fixes #15529.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
",1,,3,2017-12-20T22:10:24Z,MEMBER,2018-01-23T21:21:49Z
15529,Sample Distorted Bounding Box Bug,"awaiting review,stat:awaiting tensorflower","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Stock Example
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
16.04.3 LTS
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
Python 3.5.2
- **Bazel version (if compiling from source)**:
0.7.0
- **GCC/Compiler version (if compiling from source)**:
5.4.0
- **CUDA/cuDNN version**:
cuda_9.0.176_384.81 / cudnn 7
- **GPU model and memory**:
GTX 755M 2gb Memory x2
- **Exact command to reproduce**:
begin, size, bbox_for_draw = tf.image.sample_distorted_bounding_box(
        tf.shape(image),
        bounding_boxes=bounding_boxes)


### Describe the problem
When using the tf.image.sample_distorted_bounding_box() function the parameter min_object_covered seems to default to value None which causes an error (ValueError: None values not supported.)  If you give an argument for min_object_covered it seems to work fine.

There seems to be two versions of this function in the source [v2 which takes min_object_covered](https://github.com/tensorflow/tensorflow/blob/fc49f43817e363e50df3ff2fd7a4870ace13ea13/tensorflow/core/ops/image_ops.cc#L930) as a argument and a [v1](https://github.com/tensorflow/tensorflow/blob/fc49f43817e363e50df3ff2fd7a4870ace13ea13/tensorflow/core/ops/image_ops.cc#L844) which has the default value of 0.1 as an attribute.  It appears v2 is the one [being used](https://github.com/tensorflow/tensorflow/blob/73658420db2498ad7f07363bfa72cba6e2d9fdd2/tensorflow/python/ops/image_ops_impl.py#L1536).  Not sure what approach is best to take for fixing this bug but believe the root of the issue is coming from tensorflow/core/ops/image_ops.cc

### Source code / logs
Attached
[boundingbox.txt](https://github.com/tensorflow/tensorflow/files/1576836/boundingbox.txt)

Examples of code being implemented [here](https://github.com/wagonhelm/image_augment/blob/master/DataAug.ipynb).

",0,,2,2017-12-20T20:03:22Z,CONTRIBUTOR,2017-12-20T22:11:10Z
15528,Update debugger.md,"awaiting testing (then merge),cla: yes","When working thru the examples, I'm seeing just `Softmax` rather than `softmax:Softmax`.  If this is indeed correct the output image also needs to be updated.",0,,5,2017-12-20T19:54:48Z,CONTRIBUTOR,2017-12-20T20:00:16Z
15527,[Bug] Tensorflow serving loads incorrect model weights when using saved model main_op,,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 3.16.36 x86_64
- **TensorFlow installed from (source or binary)**: Binary (pip)
- **TensorFlow version (use command below)**: Git version 1.4.0-19-ga52c8d9 Release version 1.4.1
- **Python version**: 2.7.9
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: See attached gist

Hey there,

I've documented a lot of this bug over on this [issue](https://github.com/tensorflow/serving/issues/656) on the tensorflow serving repository, which was closed with the direction to open an issue here.

Essentially, the bug is as follows: I have an embedding layer in Keras that uses some pre-initialized weights. When I export the model for use by TF serving, I note the following behavior:

- The keras model itself has no issue outputting the correct results
- The exported model itself (upon inspection) has the correct weights
- The result values from TF serving are incorrect.

I've narrowed it down to an issue with `tensorflow.python.ops.variables.global_variables_initializer`, as follows:

When specifying the `main_op` argument in `tensorflow.saved_model.builder.SavedModelBuilder.add_meta_graph_and_variables`, if I use `tensorflow.saved_model.main_op.main_op()` I encounter this issue. 

If instead, I use a control flow group that excludes the global variables initializer as follows:

```
main_op_new = control_flow_ops.group(                 
            lookup_ops.tables_initializer(),          
            variables.local_variables_initializer(),  
)                                                     
```

I do not encounter this issue. I've attached the full code for reproducing this issue (with the caveat of needing `tensorflow_model_server` running) [here](https://gist.github.com/zmjjmz/ce9c7a896933a02953cae0069a2ca21e)

Here's the example output with the global variables initializer: https://gist.github.com/zmjjmz/d739cdfa52148eb814450e48cbf8ddb6

If you comment out line 83 of the repro code and uncomment line 84, you should get the correct output as shown here: https://gist.github.com/zmjjmz/9edee5b4eeff94f383122545d80ee55f

Thanks for helpin out




",0,,6,2017-12-20T19:06:51Z,NONE,2017-12-20T23:27:33Z
15526,Branch 179628764,"awaiting testing (then merge),cla: yes",Push,0,,9,2017-12-20T19:03:55Z,CONTRIBUTOR,2017-12-20T19:04:07Z
15525,Op type not registered HashTableV2 error while deploying model in cloud ml,type:support,"Problem while deploying model in Tensorflow 1.4.

**Code :**

```python
def model_fn(features, labels, mode):
    if mode == tf.estimator.ModeKeys.TRAIN:
        tf.keras.backend.set_learning_phase(True)
    else:
        tf.keras.backend.set_learning_phase(False)

    input_feature = features['x']
    table = lookup.index_table_from_file(vocabulary_file='vocab.txt', num_oov_buckets=1, default_value=-1)
    text = tf.squeeze(input_feature, [1])
    words = tf.string_split(text)
    dense_words = tf.sparse_tensor_to_dense(words, default_value=PADWORD)
    numbers = table.lookup(dense_words)
    padding = tf.constant([[0, 0], [0, MAX_LEN]])
    padded = tf.pad(numbers, padding)
    sliced = tf.slice(padded, [0, 0], [-1, MAX_LEN])
    print('words_sliced={}'.format(words))

    embeds = tf.keras.layers.Embedding(MAX_FEATURES+1, 128, input_length=MAX_LEN)(sliced)

    print('words_embed={}'.format(embeds))
    f1 = tf.keras.layers.Dropout(0.2)(embeds)
    f1 = tf.keras.layers.Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)(f1)
    f1 = tf.keras.layers.GlobalAveragePooling1D()(f1)
    f1 = tf.keras.layers.Dense(hidden_dims)(f1)
    f1 = tf.keras.layers.Dropout(0.5)(f1)
    f1 = tf.keras.layers.Activation('relu')(f1)
    logits = tf.keras.layers.Dense(11)(f1)

    predictions_dict = {
        'class': tf.argmax(logits, 1),
        'prob': tf.nn.softmax(logits)
    }

    '''prediction_output = tf.estimator.export.PredictOutput({""classes"": tf.argmax(input=logits, axis=1),
                                                           ""probabilities"": tf.nn.softmax(logits,
                                                                                          name=""softmax_tensor"")})'''

    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions_dict, export_outputs={
            'prediction': tf.estimator.export.PredictOutput(predictions_dict)
        })

    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits=logits)

    if mode == tf.contrib.learn.ModeKeys.TRAIN:
        train_op = tf.contrib.layers.optimize_loss(loss, tf.contrib.framework.get_global_step(), optimizer='Adam',
                                                   learning_rate=0.001)
        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

    eval_metrics_ops = {
        'accuracy': tf.metrics.accuracy(labels=labels, predictions=predictions_dict['class']),
        'precision': tf.metrics.precision(labels=labels, predictions=predictions_dict['class']),
        'recall': tf.metrics.recall(labels=labels, predictions=predictions_dict['class'])
    }
    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics_ops)

def get_train_record(record):
    vector = tf.decode_csv(record, DEFAULTS, use_quote_delim=True)
    return vector[1:], vector[0]

def preprocess(text):
    text = text.lower()
    result = ' '.join([word for word in text.split() if word not in (stop_words)])
    return result


def build_vocab(file_name, vocab_file_name):
    df = pd.read_csv(file_name, header=None, sep=',', skiprows=[1], names=['product', 'consumer_complaint_narrative'])
    df['consumer_complaint_narrative'] = df['consumer_complaint_narrative'].apply(preprocess)
    print(df['consumer_complaint_narrative'][0])
    vocab_processor = tflearn.preprocessing.VocabularyProcessor(max_document_length=MAX_FEATURES, min_frequency=10,
                                                                tokenizer_fn=tflearn.preprocessing.tokenizer)
    vocab_processor.fit(df['consumer_complaint_narrative'])
    with gfile.Open(vocab_file_name, 'wb') as f:
        f.write(""{}\n"".format(PADWORD))
        for word, index in vocab_processor.vocabulary_._mapping.items():
            f.write(""{}\n"".format(word))
    nwords = len(vocab_processor.vocabulary_)
    print('{} words into {}'.format(nwords, vocab_file_name))


def input_fn(file_name, batch_size, repeat_count, shuffle=False):
    def _input_fn():
        data_set = tf.data.TextLineDataset(filenames=file_name)
        data_set = data_set.map(get_train_record)
        if shuffle:
            data_set = data_set.shuffle(shuffle)
        data_set = data_set.repeat(repeat_count)
        batch = data_set.batch(batch_size)
        iterator = batch.make_one_shot_iterator()
        features, labels = iterator.get_next()
        return {'x': features}, labels

    return _input_fn()


def get_train_spec(file_name, batch_size, repeat_count):
    return tf.estimator.TrainSpec(input_fn=lambda: input_fn(file_name, batch_size, repeat_count, shuffle=True), max_steps=1000)


def get_test_spec(file_name, batch_size, repeat_count=1):
    return tf.estimator.EvalSpec(input_fn=lambda: input_fn(file_name, batch_size, repeat_count, shuffle=True))


def serving_input_fn():
    feature_tensor = tf.placeholder(tf.string, [None])
    # features = tf.py_func(preprocess, [feature_tensor], tf.string)
    features = tf.expand_dims(feature_tensor, -1)
    return tf.estimator.export.ServingInputReceiver({'x': features}, {'x': features})

finance_classifier = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir)

print('\n Training .....')
finance_classifier.train(input_fn=lambda: input_fn('dataset/train.csv', batch_size, repeat_count=5, shuffle=True))

print('\n Evaluating.....')
eval_results = finance_classifier.evaluate(input_fn=lambda: input_fn('dataset/valid.csv', batch_size, repeat_count=1,
                                                                  shuffle=False))
for key in eval_results:
    print("" {} was {}"".format(key, eval_results[key]))

print('\n Exporting')
exported_model_dir = finance_classifier.export_savedmodel(model_dir, serving_input_receiver_fn=serving_input_fn)
decoded_model_dir = exported_model_dir.decode(""utf-8"")
```

[Screenshot](https://drive.google.com/open?id=1FAmoo9zCBJBAG2IFdySb_r4s1OV6YExn)

But when I tried with tf1.2 with some changes in the code in model_fn. Basically not using tf.keras but using tf.contrib.keras it was working. **is this bug ?**",0,,5,2017-12-20T18:22:04Z,NONE,2017-12-20T23:04:58Z
15521,Bug/Feature: constant_folding FP16 ,stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
RHEL 7
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
('v1.3.0-rc1-6207-ge210cb1', '1.4.0')
- **Python version**: 
python2.7
- **Bazel version (if compiling from source)**:
bazel 0.9.0
- **GCC/Compiler version (if compiling from source)**:
gcc 4.8
- **Exact command to reproduce**:

python tensorflow/models/tutorial/images/convolutional.py --use_fp16

### Describe the problem
When using FP16 in e.g. the case listed above, the following error occurs:
E tensorflow/core/grappler/optimizers/constant_folding.cc:1272] Unexpected type half
E tensorflow/core/grappler/optimizers/constant_folding.cc:1242] Unexpected type half

When looking into constant_folding.cc, i found out FP16 support exists at given lines, but is commented out.
Why is this not yet included in Tensorflow?

As far as I can see, this is more of a feature request than a bug report, since these code lines simply check if computational effort can be reduced (if the matrix is zero or one)

Also when using above command, the model trains until step 1100, then the learning rate drops to 0 (a known FP16 problem). Still it's annoying to encounter this in an official tutorial file.

",0,,4,2017-12-20T16:09:52Z,NONE,2017-12-20T23:13:59Z
15518,Possible memory leak with tf.py_func() with distributed Tensorflow?,"stat:awaiting tensorflower,type:bug/performance","When running Tensorflow as an distributed process to provide data with tf.data, it gradually consumes more and more memory, and finally consumes all memory of the system.

Scripts to reproduce:
We use a dummy dataset which produce [128, 28, 28, 1] tensors. 
Case1: Without distribute, which works fine, it will only consume 429Mb memory, no matter how many batches we run.
Codes in `test1.py`:
```
#test1.py
import tensorflow as tf
import numpy as np
from tqdm import tqdm

def dataset_generator():
    while True:
        yield np.random.uniform(size=[28, 28, 1]).astype(np.float32)
dataset = tf.data.Dataset.from_generator(dataset_generator, tf.float32)
dataset = dataset.batch(128)
value = dataset.make_one_shot_iterator().get_next()

sess = tf.Session()
for _ in tqdm(range(100000), ascii=True):
    sess.run(value)
```

Case: With distribute, it will consumes more and more memory while running more and more batches. It consumes 10+Gb with less than 1M batches. Use the following two commands in two processes to run the `test2.py`:
```
CUDA_VISIBLE_DEVICES="""" python test2.py dataset
CUDA_VISIBLE_DEVICES="""" python test2.py test
```
Codes in `test2.py`
```
# test2.py
import tensorflow as tf
import numpy as np
from tqdm import tqdm
import sys
def main(role):
    def dataset_generator():
        while True:
            yield np.random.uniform(size=[28, 28, 1]).astype(np.float32)
    cluster = tf.train.ClusterSpec({'dataset': ['localhost:2001'], 'test': ['localhost:2002']})
    if role == 'dataset':
        server = tf.train.Server(cluster, 'dataset', 0)
    elif role == 'test':
        server = tf.train.Server(cluster, 'test', 0)
    else:
        raise ValueError(""Uknown role {}."".format(role))
    with tf.device('/job:dataset/task:0')    :
        dataset = tf.data.Dataset.from_generator(dataset_generator, tf.float32)
        dataset = dataset.batch(128)
        value = dataset.make_one_shot_iterator().get_next()
    if role == 'dataset':
        server.join()
    elif role == 'test':
        sess = tf.Session(target=server.target)
        for _ in tqdm(range(100000000), ascii=True):
            sess.run(value)
            
if __name__ == ""__main__"":
    main(sys.argv[1])
```

Tensorflow: v1.4.0-rc1-11-g130a514 1.4.0
OS: ubuntu mate 16.04.1
Python: 3.6.1 (conda 4.3.30)


",1,,3,2017-12-20T09:47:28Z,NONE,2017-12-20T16:05:16Z
15517,fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h',stat:awaiting tensorflower,"I am new to tensorflow, when I compile tensorfolw with VS2015 in Windows 7, I got the following error, could anyone help on this, thanks!
fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h'",0,,7,2017-12-20T09:15:06Z,NONE,2017-12-22T02:20:51Z
15516,[XLA] Define LANG_CXX11 for >= VS 2015,"awaiting testing (then merge),cla: yes","In MSVC, __cplusplus == 199711 even when /std:c++latest is set because MSVC is still not ""fully"" C++11 compliant.

Split from #15310.

#15213",1,,5,2017-12-20T08:39:07Z,CONTRIBUTOR,2017-12-20T09:45:57Z
15515,[XLA] Use simplified version of TF_ASSIGN_OR_RETURN for MSVC,"awaiting testing (then merge),cla: yes","This simplified version also works for GCC as well actually.

Split from #15310.

#15213",1,,7,2017-12-20T08:38:46Z,CONTRIBUTOR,2017-12-20T22:48:33Z
15514,[XLA] Remove unused dlfcn.h and implement sincos[f] for MSVC,"awaiting testing (then merge),cla: yes","Split from #15310.

#15213",1,,2,2017-12-20T08:37:31Z,CONTRIBUTOR,2017-12-26T00:57:58Z
15513,[XLA] Define ssize_t for Windows,"awaiting testing (then merge),cla: yes","Split from #15310.

#15213",1,,6,2017-12-20T08:37:06Z,CONTRIBUTOR,2017-12-21T00:51:52Z
15512,[XLA] Hide GCC 7.1.1 workaround from MSVC,"awaiting testing (then merge),cla: yes","Split from #15310.

#15213",1,,4,2017-12-20T08:36:53Z,CONTRIBUTOR,2017-12-20T23:06:10Z
15511,[XLA] Fix std::array initialization,"awaiting testing (then merge),cla: yes","Split from #15310.

#15213",1,,2,2017-12-20T08:36:27Z,CONTRIBUTOR,2017-12-20T23:07:57Z
15510,"[XLA] Use tensorflow::port::Aligned{Malloc,Free}","awaiting testing (then merge),cla: yes","Split from #15310.

#15213",1,,2,2017-12-20T08:36:06Z,CONTRIBUTOR,2017-12-21T02:03:29Z
15509,[XLA] Explicitly include <numeric>,"awaiting testing (then merge),cla: yes","`std::accumulate` comes from `<numeric>`, but `<numeric>` is not implicitly included by `<algorithm>` in MSVC.

Split from #15310.

#15213",1,,2,2017-12-20T08:35:35Z,CONTRIBUTOR,2017-12-20T23:10:18Z
15508,[XLA] Add 'const' to custom comparator,"awaiting testing (then merge),cla: yes","Split from #15310.

#15213",1,,2,2017-12-20T08:34:14Z,CONTRIBUTOR,2017-12-20T23:11:45Z
15507,[XLA] Use os.path for path manipulation,"awaiting testing (then merge),cla: yes","Split from #15310.

#15213",0,,2,2017-12-20T08:33:39Z,CONTRIBUTOR,2017-12-20T20:13:11Z
15506,bugfix: long is 32 bits on Windows,cla: yes,"Please avoid to use 'long' as data type.

```c++
int main()
{
	long long threshold = 1L << 31;    
	std::cout << threshold << std::endl;
	return 0;
}
```
result:
-2147483648

",0,,2,2017-12-20T08:25:56Z,CONTRIBUTOR,2017-12-20T19:59:26Z
15503,Behavior change of tf.app.flags parsing boolean args,stat:awaiting tensorflower,"tf version 1.5.0-dev20171219
for example below args
flags.DEFINE_boolean('pre_calc_image_feature', False, '')

when using tf 1.4.1 it is ok to do --pre_calc_image_feature 0
which got FLAGS.pre_calc_image_feature == False.
But for tf 1.5 you must use --pre_calc_image_feature=0 if you still use --pre_calc_image_feature 0
then you will get FLAGS.pre_calc_image_feature == True. 

Not sure if this is a bug or just by design but personally I think tf version 1.4.1 is better handling this case.
",0,,8,2017-12-20T07:22:34Z,NONE,2017-12-22T07:27:17Z
15502,Use BSD fnmatch on Windows,"awaiting review,cla: yes","#15501 

It has a BSD-3-Clause copyright. 
",0,,2,2017-12-20T06:54:56Z,CONTRIBUTOR,2018-01-03T13:05:01Z
15500,fix pooling1D dimension bug,"awaiting review,cla: yes","When `data_format` is `channels_last`, input is `NWC`, so we have to `expand_dim(1)` to make it become `NHWC`. Then we apply pooling on `W` which is the 3rd dimention.

When `data_format` is `channels_first`, input is `NCW`, so we have to `expand_dim(2)` to make it become `NCHW`. Then we apply pooling on `W`, which is the 4th dimention.

RELNOTES: Fixed wrong handling of 1D pooling (pooling was not happening on the correct dimension).",1,,13,2017-12-20T06:27:33Z,CONTRIBUTOR,2017-12-21T09:05:38Z
15499,Go bindings: No shape inference function exists for op 'CreateSummaryFileWriter',,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: NA (using Go bindings)
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: cuda-9.1.85-1 cudnn-7.0.5-1
- **GPU model and memory**: GTX 1060 6GB
- **Exact command to reproduce**: See below

### Describe the problem
Calling `op.CreateSummaryFileWriter()` panics with: `panic: failed to add operation ""CreateSummaryFileWriter"": No shape inference function exists for op 'CreateSummaryFileWriter', did you forget to define it?`

### Source code / logs
```
package main

import (
	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func main() {
	s := op.NewScope()
	writer := op.SummaryWriter(s)
	createSummaryWriter := op.CreateSummaryFileWriter(s,
		writer,
		op.Const(s.SubScope(""log_dir""), ""tb_logs""),
		op.Const(s.SubScope(""max_queue""), int32(10)),
		op.Const(s.SubScope(""flush_millis""), int32(1000)),
		op.Const(s.SubScope(""filename_suffix""), ""tb_demo""),
	)
	scalar := op.Const(s.SubScope(""scalar""), float32(3.1415))
	step := op.Const(s.SubScope(""step""), int64(1))
	tag := op.Const(s.SubScope(""tag""), ""foo_scalar"")
	summary := op.ScalarSummary(s, tag, scalar)
	merged := op.MergeSummary(s, []tf.Output{summary})
	write := op.WriteSummary(s, writer, step, scalar, tag, merged)
	closeSummaryWriter := op.CloseSummaryWriter(s, writer)
	graph, err := s.Finalize()
	if err != nil {
		panic(err)
	}
	sess, err := tf.NewSession(graph, nil)
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{createSummaryWriter})
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{write})
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{closeSummaryWriter})
	if err != nil {
		panic(err)
	}
}
```
Returns:
```
2017-12-19 20:39:06.314322: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
panic: failed to add operation ""CreateSummaryFileWriter"": No shape inference function exists for op 'CreateSummaryFileWriter', did you forget to define it? (Stacktrace: goroutine 1 [running]:
runtime/debug.Stack(0xc420082130, 0x13eb530, 0x14298a0)
	/usr/lib/go/src/runtime/debug/stack.go:24 +0xa7
github.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).UpdateErr(0xc42007c180, 0x4d9527, 0x17, 0x750140, 0xc42000e068)
	/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:120 +0x72
github.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).AddOperation(0xc42007c180, 0x4d9527, 0x17, 0x4d9527, 0x17, 0xc4200820f0, 0x5, 0x5, 0x0, 0xc4200181a0)
	/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:85 +0xfd
github.com/tensorflow/tensorflow/tensorflow/go/op.CreateSummaryFileWriter(0xc42007c180, 0xc420010200, 0x0, 0xc420010210, 0x0, 0xc420010220, 0x0, 0xc420010230, 0x0, 0xc420010240, ...)
	/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/wrappers.go:16474 +0x2cc
main.main()
	/home/isaac/go/src/github.com/is8ac/gotf/tb_demo.go:11 +0x282
)

goroutine 1 [running]:
main.main()
	/home/isaac/go/src/github.com/is8ac/gotf/tb_demo.go:27 +0x7cc
exit status 2
```",0,,2,2017-12-20T05:05:17Z,NONE,2017-12-20T18:17:34Z
15498,gradients_1 generated in graph but not connected with AdamOptimizer ,,"System information: 
OS platform: Linux Unbuntu 16.04
Tensorflow install from: pip install...
Tensorflow version: 1.4.0
Python version: py2.7

Problem:
I want to write a CNN classification network for MNIST dataset. And I write a class named MNIST_classification, then define '_build_model()' and '_train_phase()' in this class. In main function, I define a object of MNIST_classification class, and callback the '_train_phase()' to start a training process. But I found in Graph(), there are two gradients generated at each computation node, i.e. ""gradients"" and ""gradients_1"". I use tf.gradients(loss, train_vars) to print all gradients' names and get '/gradients_1/*', but within the Graph(), gradients_1 are not connected with a AdamOptimizer, which leads to no backward propagation update for each trainable variable...

 
![image](https://user-images.githubusercontent.com/33562173/34189925-f3c6c4ac-e578-11e7-8b8f-3de98611d415.png)

Source code:

import tensorflow as tf
from utils import utils
import numpy as np

class mnist_classification(object):

    def __init__(self, sess, graph, train_param={'num_of_epoches': 1000,
                                        'num_of_classes': 10,
                                        'log_dir': './log',
                                        'model_dir': './model',
                                        'batch_size': 128,
                                        'learn_rate': 1e-4,
                                        'max_iter': 5000,
                                        'dim_feat': 28}):

        self.num_of_epoches = train_param['num_of_epoches']
        self.log_dir      = train_param['log_dir']
        self.model_dir    = train_param['model_dir']
        self.batch_size   = train_param['batch_size']

        self.learn_rate = train_param['learn_rate']
        self.max_iter   = train_param['max_iter']

        self.dim_feat = train_param['dim_feat']
        self.num_of_classes = train_param['num_of_classes']

        self.sess = sess
        self.graph = graph

        # !Build-up MNIST classification model...
        assert self.sess.graph is self.graph
        self._build_model()



    def _convolution_block(self, inp_feat, kernel_size, num_of_kernel_channels, conv_strides, conv_padding, var_scope):
        '''
            Function:
                        _convolution_block, i.e. convolution + Maxpooling + ReLU
            Input:
                    [1] <tensor> inp_feat, i.e. input feature, dimension->[batch_size, height, width, channel]
                    [2] <int32>  kernel_size
                    [3] <int32> num_of_kernel_channels
                    [4] <int32> conv_strides
                    [5] <string> conv_padding
                    [5] <string> var_scope
            Output:
                    <tensor> activ
        '''
        try:
            num_of_feat_channels = inp_feat.shape[3].value
        except:
            num_of_feat_channels = 1

        with tf.variable_scope(var_scope):
            weights = tf.get_variable(name='conv_weights', shape=[kernel_size, kernel_size, num_of_feat_channels, num_of_kernel_channels],
                                      initializer=tf.random_normal_initializer(stddev=0.1))

            biases = tf.get_variable(name='conv_biases', shape=[num_of_kernel_channels], initializer=tf.zeros_initializer())

        # !Convolution layer...
        conv = tf.nn.conv2d(inp_feat, weights, strides=conv_strides, padding=conv_padding, name='conv', data_format='NHWC') + biases

        # !Activation layer...
        activ = tf.nn.relu(conv)

        # !Pooling layer...
        pool = tf.nn.max_pool(activ,ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')

        return pool





    def _fc_layer(self, inp_feat, num_of_outputs, var_scope):
        '''
            Function:
                        _fc_layer
            Input:
                        [1] inp_feat, dimension->[batch_size, dim_feat]
                        [2] num_of_outputs
                        [3] var_scope: reuse=True
            Output:
                        fc
        '''

        dim_feat = inp_feat.shape[1].value

        with tf.variable_scope(var_scope):

            fc_weights = tf.get_variable(name='fc_weights', dtype=tf.float32, shape=[dim_feat, num_of_outputs], initializer=tf.random_normal_initializer(stddev=0.1))
            fc_bias    = tf.get_variable(name='fc_bias',    dtype=tf.float32, shape=[num_of_outputs], initializer=tf.zeros_initializer())


            # tf.nn.xw_plus_b(x, weights, bias) = tf.matmul(x, weights) + biases
            fc = tf.nn.xw_plus_b(x=inp_feat, weights=fc_weights, biases=fc_bias)

            return fc



    def _build_model(self):

        self.digit = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, self.dim_feat, self.dim_feat, 1])
        self.label = tf.placeholder(dtype=tf.int64, shape=[self.batch_size, self.num_of_classes])

        # # !One-hot encoding for label...
        # with tf.name_scope('label_trans'):
        #     self.new_label = utils._array_sparse_to_dense(self.label, self.num_of_classes)

        conv1 = self._convolution_block(inp_feat=self.digit, conv_strides=[1,1,1,1], conv_padding='SAME', kernel_size=5, num_of_kernel_channels=32, var_scope='conv1')

        conv2 = self._convolution_block(inp_feat=conv1, conv_strides=[1,1,1,1], conv_padding='SAME', kernel_size=5, num_of_kernel_channels=64, var_scope='conv2')

        conv3 = self._convolution_block(inp_feat=conv2, conv_strides=[1, 1, 1, 1], conv_padding='SAME', kernel_size=7,num_of_kernel_channels=64, var_scope='conv3')

        flatt = tf.layers.flatten(conv3, name='flatten')

        fc1 = self._fc_layer(inp_feat=flatt, num_of_outputs=1024, var_scope='fc1')
        fc1 = tf.nn.relu(fc1)

        fc2 = self._fc_layer(inp_feat=fc1, num_of_outputs=10, var_scope='fc2')

        self.predict = tf.nn.softmax(logits=fc2, dim=-1)

        self.loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=self.label, logits=fc2))

        # !Define MNIST classification accuracy...
        correct = tf.equal(tf.argmax(self.predict, 1), tf.argmax(self.label, 1))
        self.accuracy = tf.reduce_mean(tf.cast(correct, 'float'))

        # !Define training operations...
        self.train_op = tf.train.AdamOptimizer(self.learn_rate).minimize(self.loss)





    def _train(self, img_file, lab_file):

        # !Load data into memory...
        img, n_rows, n_cols = utils._read_MNIST_file(img_file, fmt='>IIII')
        lab, _, _ = utils._read_MNIST_file(lab_file, fmt='>II')


        # !Intialize the global_variables and local_variables...
        self.sess.run(tf.global_variables_initializer())
        self.sess.run(tf.local_variables_initializer())

        # !List all trainable variables...
        train_vars = tf.trainable_variables()
        for var in train_vars:
            print var.name

        # !Add gradients into tensorboard summary...
        gradients = tf.gradients(self.loss, train_vars)
        for ii in range(len(gradients)):
            if gradients[ii]!=None:
                tf.summary.histogram(train_vars[ii].name, gradients[ii])

        # !Add loss into tensorboard summary...
        tf.summary.scalar('loss', self.loss)
        tf.summary.scalar('accuracy', self.accuracy)

        tf.summary.image('input_image', self.digit)

        summary_writer = tf.summary.FileWriter(""./log"", self.sess.graph)

        merged = tf.summary.merge_all()

        # !Start training process...
        for ii_epoch in range(self.num_of_epoches):
            for ii_iter in range(self.max_iter):


                img_batch = utils._randomly_sample(img, self.batch_size)
                img_batch = np.expand_dims(img_batch, axis=3) / 255

                lab_batch = utils._randomly_sample(lab, self.batch_size)
                lab_batch = utils._array_sparse_to_dense(np.int64(lab_batch), num_of_classes=self.num_of_classes)

                _, pred, los, acc, summary= self.sess.run([self.train_op, self.predict, self.loss, self.accuracy, merged], feed_dict={self.digit: img_batch, self.label: lab_batch})

                if ( (ii_epoch * self.max_iter + ii_iter) % 100 == 0):
                    summary_writer.add_summary(summary, ii_epoch * self.max_iter + ii_iter)
                    print(pred[0,:])
                    print(lab_batch[0])

                print('!Loss at No.%d Epoch, No.%d Iteration=%.5f' % (ii_epoch, ii_iter, los))
                print('!Accuracy at No.%d Epoch, No.%d Iteration=%.5f' % (ii_epoch, ii_iter, acc))



Main function:
from utils.utils import _array_sparse_to_dense
from utils.utils import _read_MNIST_file
from matplotlib import pyplot
import numpy as np
import tensorflow as tf
from model.MNIST_classification import mnist_classification
import os
import sys


# !Check if tensorflow version == '1.4.0', API
assert tf.__version__ == '1.4.0'



# !Set system default encoding method == 'utf-8'
reload(sys)
sys.setdefaultencoding('utf-8')


os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""


with tf.Graph().as_default() as graph:
    with tf.Session() as sess:
        MNIST_inst = mnist_classification(sess=sess, graph=graph)

        MNIST_inst._train(img_file='./data/train-images.idx3-ubyte', lab_file='./data/train-labels.idx1-ubyte')

",0,,1,2017-12-20T03:31:05Z,NONE,2017-12-20T05:17:33Z
15497,Image Input for GridLSTM,type:support,"I would like to use GridLSTM for a handwriting recognition task. Unfortunately, the documentation is lacking info on how to input images into GridLSTMs. 
",0,,5,2017-12-20T03:23:17Z,CONTRIBUTOR,2017-12-20T03:59:38Z
15496,Add ws2_32.lib to gcs_dns_cache_test's linkopts,"awaiting testing (then merge),cla: yes",Needed for Windows,0,,2,2017-12-20T03:20:14Z,CONTRIBUTOR,2017-12-26T00:54:03Z
15495,Update mnist.py,"awaiting testing (then merge),cla: no",Comment modification,0,,6,2017-12-20T02:29:12Z,NONE,2017-12-20T02:35:26Z
15494,Enables 0-D indexing for Gather() in TFLite,"awaiting testing (then merge),cla: yes,comp:lite","Hi,

This patch enables use of scalar (0-D) index for Gather() in TFLite.

Since `Dims<>` still works correctly (which becomes {1, 1, 1, 1}) when calculating a scalar tensor,
no actual internal change is needed. Still, it only supports indexing the first dimension.

At [here](https://github.com/tensorflow/tensorflow/compare/master...scottcjt:gather_0d?expand=1#diff-3aa3a18bc0cba3f7fe969252dbd8ed09L51), `int32` is changed to `int32_t` because my clang on OSX 10.12 complains that `int32` is undefined.",1,,6,2017-12-19T23:27:10Z,CONTRIBUTOR,2017-12-27T01:14:27Z
15493,Add S3 logging to TensorFlow's logging system,"awaiting testing (then merge),cla: yes","This fix is an attempt to help the issue raised in #15159 where there is no logging in S3 file system and it is not easy to debug to diagnose.

This fix adds S3 logging to TensFlow's logging with
```
LogLevel::Info -> INFO
LogLevel::Warn -> WARNING
LogLevel::Error -> ERROR
LogLevel::Fatal -> FATAL
```

This fix is related to #15159.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,15,2017-12-19T20:15:18Z,MEMBER,2017-12-26T05:47:49Z
15491,Branch 179578952,cla: yes,Push,0,,4,2017-12-19T19:28:03Z,CONTRIBUTOR,2017-12-19T19:28:18Z
15489,Hotfix/fix android example for focus mode continuous picture - #15487,"awaiting testing (then merge),cla: yes",,1,,5,2017-12-19T18:22:40Z,CONTRIBUTOR,2017-12-19T18:24:49Z
15487,Android Example breaks for old cameras not having support for FOCUS_MODE_CONTINUOUS_PICTURE,stat:contributions welcome,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

No fixed a bug.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Ubuntu 16.04, Tried on Android 21,22,23,24,25

- **TensorFlow installed from (source or binary)**:

Android App

- **TensorFlow version (use command below)**:

Latest in Android App

- **Python version**: 

3.2

- **Bazel version (if compiling from source)**:

Not Applicable

- **GCC/Compiler version (if compiling from source)**:

Not Applicable


- **CUDA/cuDNN version**:

Not Applicable


- **GPU model and memory**:

Not Applicable

- **Exact command to reproduce**:

Compile the Android Example as it is and execute on any Android device with old camera not supporting FOCUS_MODE_CONTINUOUS_PICTURE

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Android Example breaks for old cameras not having support for FOCUS_MODE_CONTINUOUS_PICTURE

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Creating a pull request for the fix.
",0,,1,2017-12-19T18:00:02Z,CONTRIBUTOR,2017-12-23T04:38:31Z
15486,fix _Pooling1D data format bug,cla: yes,"When `data_format` is `channels_last`, input is `NWC`, so we have to `expand_dim(1)` to make it become `NHWC`. Then we apply pooling on `W` which is the 3rd dimention.

When `data_format` is `channels_first`, input is `NCW`, so we have to `expand_dim(2)` to make it become `NCHW`. Then we apply pooling on `W`, which is the 4th dimention.",0,,6,2017-12-19T17:48:08Z,CONTRIBUTOR,2017-12-19T17:49:58Z
15485,Possible Bug with GPU: matmul_op.cc ,,"Hello,

I am trying to run a matrix multiplication on GPU. This is the code i am running on python. 
```
import tensorflow as tf

with tf.device('/gpu:0'):
  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
  c = tf.matmul(a, b)

sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
print(sess.run(c))
```

In the matmul_op.cc i have added an ```std::cout << ""Device: "" << ctx->device()->name << endl;``` in function ```Compute``` that is on line 456. The weird thing is that this ```cout``` prints: ```Device:  /job:localhost/replica:0/task:0/device:CPU:0 ``` while the log_device_placement from the session reports: 
```
2017-12-19 18:50:35.432196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GT 720 major: 3 minor: 5 memoryClockRate(GHz): 0.797
pciBusID: 0000:82:00.0
totalMemory: 1.95GiB freeMemory: 1.95GiB
2017-12-19 18:50:35.432273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GT 720, pci bus id: 0000:82:00.0, compute capability: 3.5)
Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GT 720, pci bus id: 0000:82:00.0, compute capability: 3.5
2017-12-19 18:50:35.455437: I tensorflow/core/common_runtime/direct_session.cc:300] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GT 720, pci bus id: 0000:82:00.0, compute capability: 3.5

Tensor(""MatMul:0"", shape=(2, 2), dtype=float32, device=/device:GPU:0)
MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2017-12-19 18:50:35.456469: I tensorflow/core/common_runtime/placer.cc:874] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
b: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2017-12-19 18:50:35.456525: I tensorflow/core/common_runtime/placer.cc:874] b: (Const)/job:localhost/replica:0/task:0/device:GPU:0
a: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2017-12-19 18:50:35.456555: I tensorflow/core/common_runtime/placer.cc:874] a: (Const)/job:localhost/replica:0/task:0/device:GPU:0
```

I have also checked the ```bool USE_CUBLAS``` and it has value 0, in function ```Compute```. Is it a possible bug or i am doing something wrong?",0,,3,2017-12-19T17:00:18Z,NONE,2017-12-19T20:41:17Z
15484,[Building Error] clang: error: no such file or directory: 'x86_64',"comp:lite,stat:awaiting response","Hi all,

I met this problem when I was trying to build the Tensorflow Lite for iOS following [this instruction](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/ios.md).



Every thing went well until I tried the command:
`tensorflow/contrib/lite/build_ios_universal_lib.sh`

The error occured:
`clang: error: no such file or directory: 'x86_64'`

What could be the problem? Thank you very much!
My environment: Mac Sierra 10.12.6, XCode 9.2",1,,5,2017-12-19T15:38:54Z,NONE,2017-12-19T22:36:17Z
15483,What is the best practice for running training and evaluation on the same machine?,,"I ask the question at [stackoverflow](https://stackoverflow.com/questions/47883570/what-is-the-best-practice-for-running-training-and-evaluation-on-the-same-machin). No answer so far, maybe I can find people have similar needs.

Here is the question:

**What I want to do?**

1. I only have 1 machine. 

2. I want to evaluate the mode periodically. 

**What I have now?** 


1.  use a placeholder. Say I run 1000 step of training by feeding the training data. then I feed in validation dataset for evaluation. put it in a loop.

    But as google suggested, placeholder is not a good way for long run training.


2. So, I use slim dataset to feed in data. Now, the model is bonded with training dataset like this:
    >      net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID',
    >                                 scope='conv1')

 I have to construct another model(in another graph) which is bonded with validation dataset. 

**Is there a better way of doing that?**

I know that google is focusing on distribution training on large scale, but I think as tensorflow  is a low-level and flexible framwork. There must be a way can do what I want. ",0,,1,2017-12-19T15:00:54Z,CONTRIBUTOR,2017-12-19T17:40:18Z
15482,Fix a compile error in file_block_cache_test.cc,"awaiting testing (then merge),cla: yes","tensorflow/core/platform/cloud/file_block_cache_test.cc(461): error C3493: 'block_size' cannot be implicitly captured because no default capture mode has been specified

Compiler: Visual Studio 2017 v15.4.4",0,,2,2017-12-19T12:52:38Z,CONTRIBUTOR,2017-12-19T22:35:13Z
15481,"[BUG] ""no viable conversion "" ERROR raised in ""tensorflow/core/kernels/eigen_pooling.h"" when build v1.4.1 for opencl",stat:awaiting response,"### System information
- **OS Platform and Distribution**:Linux Ubuntu 17.10
- **TensorFlow installed from**: source
- **TensorFlow version**:1.4.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:0.8.1
- **GCC/Compiler version (if compiling from source)**:7.2

### Describe the problem
""no viable conversion "" ERROR raised when build v1.4.1 with opencl (computecpp CE 0.5.0)

### Source code / logs
ERROR: .../tensorflow/tensorflow/core/kernels/BUILD:3169:1: C++ compilation of rule '//tensorflow/core/kernels:pooling_ops' failed (Exit 1)
In file included from tensorflow/core/kernels/pooling_ops_3d.cc:29:
./tensorflow/core/kernels/eigen_pooling.h:338:12: error: no viable conversion from '__m128' (vector of 4 'float' values) to 'cl::sycl::vec<float, 4>'
    Packet skip_mask =
           ^
./tensorflow/core/kernels/eigen_pooling.h:333:5: note: in instantiation of function template specialization 'Eigen::internal::AvgPoolMeanReducer<float>::reducePacketWithType<cl::sycl::vec<float, 4> >' requested here
    reducePacketWithType(static_cast<T>(0), p, accum);
    ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:186:15: note: in instantiation of function template specialization 'Eigen::internal::AvgPoolMeanReducer<float>::reducePacket<cl::sycl::vec<float, 4> >' requested here
      reducer.reducePacket(self.m_impl.template packet<Unaligned>(firstIndex + j), &p);
              ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:279:56: note: in instantiation of member function 'Eigen::internal::InnerMostDimReducer<Eigen::TensorEvaluator<const Eigen::TensorReductionOp<Eigen::internal::AvgPoolMeanReducer<float>, const Eigen::IndexList<Eigen::type2index<1>>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 3>, const Eigen::TensorVolumePatchOp<-1, -1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 5, 1, long>, 16, MakePointer> > >, MakePointer>, Eigen::ThreadPoolDevice>, Eigen::internal::AvgPoolMeanReducer<float>, true>::reduce' requested here
          InnerMostDimReducer<Self, Op, Vectorizable>::reduce(self, 0, num_coeffs, reducer);
                                                       ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:523:48: note: in instantiation of member function 'Eigen::internal::FullReducer<Eigen::TensorEvaluator<const Eigen::TensorReductionOp<Eigen::internal::AvgPoolMeanReducer<float>, const Eigen::IndexList<Eigen::type2index<1>>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 3>, const Eigen::TensorVolumePatchOp<-1, -1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 5, 1, long>, 16, MakePointer> > >, MakePointer>, Eigen::ThreadPoolDevice>, Eigen::internal::AvgPoolMeanReducer<float>, Eigen::ThreadPoolDevice, true>::run' requested here
      internal::FullReducer<Self, Op, Device>::run(*this, reducer, m_device, data);
                                               ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:133:19: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorReductionOp<Eigen::internal::AvgPoolMeanReducer<float>, const Eigen::IndexList<Eigen::type2index<1>>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 3>, const Eigen::TensorVolumePatchOp<-1, -1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 5, 1, long>, 16, MakePointer> > >, MakePointer>, Eigen::ThreadPoolDevice>::evalSubExprsIfNeeded' requested here
    return m_impl.evalSubExprsIfNeeded(data);
                  ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:132:24: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    return m_rightImpl.evalSubExprsIfNeeded(m_leftImpl.data());
                       ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:59: note: in instantiation of member function 'Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 5, 1, long>, 16, MakePointer>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 5>, const Eigen::TensorReductionOp<Eigen::internal::AvgPoolMeanReducer<float>, const Eigen::IndexList<Eigen::type2index<1>>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 3>, const Eigen::TensorVolumePatchOp<-1, -1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 5, 1, long>, 16, MakePointer> > >, MakePointer> > >, Eigen::ThreadPoolDevice, true>::run' requested here
      internal::TensorExecutor<const Assign, DeviceType>::run(assign, m_device);
                                                          ^
tensorflow/core/kernels/pooling_ops_3d.cc:108:71: note: in instantiation of function template specialization 'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<float, 5, 1, long>, 16, MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorReshapingOp<const Eigen::DSizes<long, 5>, const Eigen::TensorReductionOp<Eigen::internal::AvgPoolMeanReducer<float>, const Eigen::IndexList<Eigen::type2index<1>>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 3>, const Eigen::TensorVolumePatchOp<-1, -1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 5, 1, long>, 16, MakePointer> > >, MakePointer> > >' requested here
    output->tensor<T, 5>().device(context->eigen_device<CPUDevice>()) =
                                                                      ^
tensorflow/core/kernels/pooling_ops_3d.cc:194:39: note: in instantiation of member function 'tensorflow::LaunchPoolingOp<Eigen::ThreadPoolDevice, float, tensorflow::PoolingType::AVG>::launch' requested here
    LaunchPoolingOp<Device, T, Type>::launch(context, tensor_in, window, stride,
                                      ^
tensorflow/core/kernels/pooling_ops_3d.cc:133:12: note: in instantiation of member function 'tensorflow::Pooling3DOp<Eigen::ThreadPoolDevice, float, tensorflow::PoolingType::AVG>::Compute' requested here
  explicit Pooling3DOp(OpKernelConstruction* context) : UnaryOp<T>(context) {
           ^
tensorflow/core/kernels/pooling_ops_3d.cc:738:15: note: in instantiation of member function 'tensorflow::Pooling3DOp<Eigen::ThreadPoolDevice, float, tensorflow::PoolingType::AVG>::Pooling3DOp' requested here
TF_CALL_float(REGISTER_CPU_KERNELS);
              ^
external/local_config_sycl/crosstool/../sycl/include/SYCL/vec.h:9461:3: note: candidate constructor not viable: no known conversion from '__m128' (vector of 4 'float' values) to 'const vec<float, 4> &' for 1st argument
  vec(const vec<dataT, kElems> &rhs) {
  ^
external/local_config_sycl/crosstool/../sycl/include/SYCL/vec.h:9437:3: note: candidate template ignored: could not match 'swizzled_vec<float, kElemsRhs, kIndexRhsN...>' against '__attribute__((__vector_size__(4 * sizeof(float)))) float' (vector of 4 'float' values)
  vec(const swizzled_vec<dataT, kElemsRhs, kIndexRhsN...> &rhs) {
  ^
1 error generated.
",0,,3,2017-12-19T12:26:34Z,NONE,2017-12-20T01:05:50Z
15480, sparse_multiclass_hinge_loss() Error,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows/MacOS
- **TensorFlow installed from (source or binary)**:N
- **TensorFlow version (use command below)**:1.4
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:N
- **GCC/Compiler version (if compiling from source)**:N
- **CUDA/cuDNN version**:NA
- **GPU model and memory**:NA
- **Exact command to reproduce**:See below

### Describe the problem
There seems to be a bug in sparse_multiclass_hinge_loss(), as per the example below.

### Source code / logs

```python
import numpy as np
import tensorflow as tf

x = np.random.uniform(0, 1, size = (100, 5))
y = np.random.choice(3, 100) 
y = y.reshape(100, 1)

X = tf.placeholder(""float32"", [None, 5])
Y = tf.placeholder(""int32"", [None, 1])

weights = {'w': tf.Variable(tf.random_uniform([5, 3]))}
biases = {'b': tf.Variable(tf.zeros([3]))}

logits = tf.add(tf.matmul(X, weights['w']), biases['b'])

loss = tf.reduce_mean(tf.contrib.kernel_methods.sparse_multiclass_hinge_loss(logits=logits, labels=Y))

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    res = sess.run(loss, feed_dict={X: x, Y: y}) 


res
```

 make_tensor_proto(values, dtype, shape, verify_shape)
    369   else:
    370     if values is None:
--> 371       raise ValueError(""None values not supported."")
    372     # if dtype is provided, forces numpy array to be the type
    373     # provided if possible.

ValueError: None values not supported.

",1,,7,2017-12-19T11:52:38Z,NONE,2017-12-19T22:46:21Z
15479,Fix issue building memory_stats with opencl,"awaiting testing (then merge),cla: yes","This PR fixes #15477 
This bug exists in at least 1.4, 1.5 and master branch, should I send PR for every branch?",0,,2,2017-12-19T11:51:55Z,CONTRIBUTOR,2017-12-19T22:47:04Z
15478,[XLA/tfcompile] Add Env::CreateUniqueFileName and use it in SaveGraph,"awaiting testing (then merge),cla: yes","Split part of `tensorflow::Env::LocalTempFilename` into `tensorflow::Env::CreateTempFilename` so that it can be used in `SaveGraph`.

This PR is to replace #15335.

I am terrible in creating descriptive function name, better name suggestion for `CreateTempFilename` is welcomed.

/cc @jlebar.

 #15213",0,,6,2017-12-19T10:36:24Z,CONTRIBUTOR,2017-12-20T02:15:31Z
15477,"[BUG] Undeclared error in: ""tensorflow/contrib/memory_stats/kernels/memory_stats_ops.cc""",,"### System information
- **Linux Ubuntu 17.10**:
- **TensorFlow installed from source**:
- **TensorFlow version 1.4.1**:
- **Python version 3.6**: 
- **Bazel version (0.8.1)**:
- **GCC/Compiler version (7.2)**:

### Describe the problem
undeclared error raised in ""tensorflow/contrib/memory_stats/kernels/memory_stats_ops.cc"" when build v1.4.1 with jemalloc, OpenCL

### Source code / logs
ERROR: .../tensorflow/tensorflow/contrib/memory_stats/BUILD:17:1: C++ compilation of rule '//tensorflow/contrib/memory_stats:python/ops/_memory_stats_ops.so' failed (Exit 1)
tensorflow/contrib/memory_stats/kernels/memory_stats_ops.cc:64:5: error: unknown type name '**MaxBytesInUseOp**'; did you mean 'BytesInUseOp'?
    MaxBytesInUseOp);
    ^~~~~~~~~~~~~~~
    BytesInUseOp
./tensorflow/core/framework/op_kernel.h:1209:68: note: expanded from macro 'REGISTER_KERNEL_BUILDER'
  REGISTER_KERNEL_BUILDER_UNIQ_HELPER(__COUNTER__, kernel_builder, __VA_ARGS__)
                                                                   ^
./tensorflow/core/framework/op_kernel.h:1212:53: note: expanded from macro 'REGISTER_KERNEL_BUILDER_UNIQ_HELPER'
  REGISTER_KERNEL_BUILDER_UNIQ(ctr, kernel_builder, __VA_ARGS__)
                                                    ^
./tensorflow/core/framework/op_kernel.h:1225:24: note: expanded from macro 'REGISTER_KERNEL_BUILDER_UNIQ'
            return new __VA_ARGS__(context);                          \
                       ^
tensorflow/contrib/memory_stats/kernels/memory_stats_ops.cc:44:7: note: 'BytesInUseOp' declared here
class BytesInUseOp : public MemoryStatsOp {
      ^
1 error generated.
",0,,1,2017-12-19T10:34:05Z,NONE,2017-12-19T11:23:14Z
15475,[WIP]  Add bazel runfiles manifest support for Windows,cla: yes,"To implement the ideas in:
https://groups.google.com/forum/#!msg/bazel-discuss/Po8xN8dhWkI/sWPUYV9YBAAJ

Now //tensorflow/core:example_example_parser_configuration_test is disabled on Windows, because it cannot find the data files.

testing::RunFileRelocator::GetInstance().Relocate() is the replacement of testing::TensorFlowSrcRoot(). We should mark TensorFlowSrcRoot as deprecated or completely remove it.

Places need to change:
c/c_api_test.cc
cc/saved_model/loader_test.cc
compiler/aot/codegen_test.cc
compiler/xla/service/gpu/llvm_gpu_backend/utils_test.cc
compiler/xla/tests/sample_file_test.cc
contrib/ffmpeg/default/ffmpeg_lib_test.cc
contrib/lite/models/test_utils.h
contrib/lite/testing/generated_examples_zip_test.cc
contrib/session_bundle/bundle_shim_test.cc
contrib/session_bundle/test_util.cc
core/distributed_runtime/rpc/grpc_testlib.cc
core/grappler/costs/graph_properties_test.cc
core/grappler/utils/scc_test.cc
core/kernels/hexagon/graph_transferer_test.cc
core/kernels/spectrogram_test.cc
core/platform/cloud/google_auth_provider_test.cc
core/profiler/internal/tfprof_show_test.cc
core/profiler/internal/tfprof_stats_test.cc
core/profiler/internal/tfprof_tensor_test.cc
core/profiler/internal/tfprof_timeline_test.cc
core/example/example_parser_configuration_test.cc
core/platform/cloud/oauth_client_test.cc

Ref: https://github.com/bazelbuild/bazel/issues/4215

TODO:
1. deal with folders
2. do not translate abs path.

",0,,14,2017-12-19T09:55:24Z,CONTRIBUTOR,2017-12-19T09:57:51Z
15474,"Revert changes in TrainingHelper that cause ""no gradient"" error","awaiting testing (then merge),cla: yes","Change occured in https://github.com/tensorflow/tensorflow/commit/69c324591ba4dfeafb403ee59de56ffe063c1e94

Discussed in https://github.com/tensorflow/tensorflow/issues/15278",0,,7,2017-12-19T09:32:21Z,CONTRIBUTOR,2017-12-19T22:49:18Z
15471,BUG: fix name scope collision in `tf.layers`,"awaiting testing (then merge),cla: yes","Fix  #13429.

Since  #14390 has been merged into master branch, we can easily solve the problem with `auxiliary_name_scope=False`.

### How to test

+ [x] add test case.
+ [x] pass all tests.
",0,,6,2017-12-19T05:41:42Z,CONTRIBUTOR,2017-12-19T20:19:50Z
15466,Add a wrapper for cc_library: tf_cc_library,"cla: yes,stat:awaiting response,stat:awaiting tensorflower",I'll replace every cc_library under //tensorflow with tf_cc_library,0,,14,2017-12-19T03:01:16Z,CONTRIBUTOR,2017-12-20T01:06:28Z
15464,[Feature Request] Sparse compute_gradient,stat:awaiting response,"I am working on an extremely large scale linear model and have been trying to optimize the performance of the TF optimizer.

**Have I written custom code**
Version I.
My feature size is huge (500Mil) and sparse, so I was testing if I could make TF only compute the necessary gradients and apply it using some function like tf.scatter_sub()
My graph is like this:
```
cost = fn(w)
vars_to_update = tf.gather(w, non_zero_indices)
grads = tf.gradients(cost, vars_to_update)
update_op = tf.scatter_sub(w, non_zero_indeces, grads)
```
I found that tf.graidents() always returns None for tf.gather(). Similar condition if I pass tf.gather() to any optimizer like tf.train.GradientDescentOpitmizer(cost, tf.gather(w, indices)), it will throw unsupported error for tf.gather(). 

I was wondering if I did anything wrong or TF just doesn't support sparse gradient computation? If latter does TF team plan to have that implemented in short future?

Version II.
In stead of creating a sparse tensor and do sparse_tensor_dense_matmul(), I also tried using tf.gather() follow by tf.segment_sum() to implement W*X. By doing this the optimizer apparently automatically performed sparse grad computation and sparse update. However, the speed of the optimizer was **horribly slower (15seconds)** than the sparse tensor approach. And idea why?

Pseudo code:
```
active_weights = tf.gather(weights, non_zero_indices)
total = tf.segment_sum(
                tf.reshape(activated_weights, [-1]),
                segment_ids //which is the row number e.g. [0,0,0,0,1,1,1,2,2,2,3,3,...]
                )
update_op = tf.train.GradientDescentOptimizer().minimize(total, active_weights)
```

**OS Platform and Distribution**
Centos Linux version 3.10.0-229.4.2.el7.x86_64 (gcc version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC) )

**TensorFlow installed from (source or binary)**
pip install tensorflow

**TF version**
1.3.0

**Python version**
2.7.5

**Bazel version, CUDA/cuDNN, GPU model and memory, Exact command to reproduce**
N/A

**Also there might be a potential bug**
If in the second approach (tf.gather() and tf.segment_sum()) I replace GradientDescentOptimizer with Adam or Adagrad optimizer, the memory would blow up very quickly. I did not look into why that happened so I am not sure if this worth a bug ticket.",0,,9,2017-12-19T00:42:07Z,CONTRIBUTOR,2017-12-19T13:25:58Z
15461,Updating MKL to the latest release.,"awaiting testing (then merge),cla: yes",,1,,6,2017-12-19T00:17:48Z,CONTRIBUTOR,2017-12-19T00:18:36Z
15460,Branch 179464468,,Push,0,,3,2017-12-18T22:44:38Z,CONTRIBUTOR,2017-12-19T00:11:24Z
15458,Upgrade all TF base images to ubuntu 16.,"awaiting testing (then merge),cla: yes",,0,,9,2017-12-18T21:06:34Z,OWNER,2017-12-22T00:44:59Z
15457,Small fix in the example,"awaiting testing (then merge),cla: no,stat:awaiting response","There's a typo in the example

tflite_modeL -> tflite_model",0,,6,2017-12-18T20:11:34Z,NONE,2017-12-18T21:13:50Z
15453,Update math_ops.py,"awaiting testing (then merge),cla: yes",Corrected documentation of tf.reduce_mean(),0,,2,2017-12-18T17:11:05Z,CONTRIBUTOR,2017-12-18T21:33:08Z
15451,Tensorflow Installation Problem in Anaconda3-5.0.1Environment,,"After installing the Anaconda3-5.0.1 on Ubuntu 17.10, I have followed the following steps to install the Tesnorflow -

$ conda create -n tensorflow python=3.6
$ source activate tensorflow
(tensorflow)$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl
After installing the above pakages, I have verified the above installation in Anaconda environment, following issues are faced -
we6aisol@we6aisol-H170-Gaming-3:$ source activate tensorflow
(tensorflow) we6aisol@we6aisol-H170-Gaming-3:$ python
Python 3.6.3 |Anaconda, Inc.| (default, Nov 20 2017, 20:41:42)
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

import tensorflow as tf
/home/we6aisol/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
return f(*args, **kwds)
Please help me to resolve this issue.

Thanks & Regards
Manoj Bansal",0,,2,2017-12-18T15:06:40Z,NONE,2017-12-23T04:44:57Z
15450,TensorFlow binary was not compiled to use: AVX AVX2 from Java 1.8,,"
![image](https://user-images.githubusercontent.com/12063612/34111971-2b81c42a-e446-11e7-8482-6e3f3867e8d6.png)

![image](https://user-images.githubusercontent.com/12063612/34112074-6ad1b824-e446-11e7-8541-02ae054aefa0.png)

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform from Windows 10
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version 1.4.0
- **Java version** : 1.8.0_144
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 
- **GPU model and memory**: GTX 1060 6G and Memory 8G
- **Exact command to reproduce**: javac

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
""C:\Program Files\Java\jdk1.8.0_144\bin\java"" -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:56173,suspend=y,server=n -javaagent:C:\Users\Administrator\.IntelliJIdea2017.3\system\captureAgent\debugger-agent.jar=C:\Users\Administrator\AppData\Local\Temp\capture.props -Dfile.encoding=UTF-8 -classpath ""C:\Program Files\Java\jdk1.8.0_144\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\rt.jar;C:\gitspase\zczdemo\target\classes;E:\maven\Jars\org\tensorflow\tensorflow\1.4.0\tensorflow-1.4.0.jar;E:\maven\Jars\org\tensorflow\libtensorflow\1.4.0\libtensorflow-1.4.0.jar;E:\maven\Jars\org\tensorflow\libtensorflow_jni\1.4.0\libtensorflow_jni-1.4.0.jar;E:\maven\Jars\org\projectlombok\lombok\1.16.18\lombok-1.16.18.jar;C:\Program Files\JetBrains\IntelliJ IDEA 2017.3.1\lib\idea_rt.jar"" com.zcz.tensorflow.zczdemo.HelloTF
Connected to the target VM, address: '127.0.0.1:56173', transport: 'socket'
Hello from 1.4.0
Disconnected from the target VM, address: '127.0.0.1:56173', transport: 'socket'
",0,,10,2017-12-18T14:59:54Z,NONE,2017-12-18T15:15:40Z
15449,Tensorflow-gpu 1.4.1 windows binaries couldn't be found,,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 1709
- **TensorFlow installed from (source or binary)**: looking for the binary
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 3.6.3 64-bit
- **Exact command to reproduce**: ""pip3 install --upgrade tensorflow-gpu""

### Describe the problem

Can't find the binaries for the 1.4.1 windows version",0,,2,2017-12-18T14:56:40Z,NONE,2017-12-18T15:17:24Z
15448,[Feature] Dataset API - Reinitializable Iterator resets to first dataset element,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: slightly altered stock example (see below)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: 1.4.0 from source
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: Nvidia 1060

### Describe the problem
Currently the re-initializable iterator API using .from_structure and Iterator.make_initializer always resets the get_next() op of the iterator to fetch the first element of its Dataset instance again after running `sess.run(training_init_op)` or `sess.run(validation_init_op)`, respectively. 

Is this really the intended behavior? This means, that if you want to switch between training and validation Datasets within one epoch (i.e. in a shorter rhythm than the full dataset length) you will always only iterate over the first `batch_size * number-of-training-steps-before-validation-step` elements of the training set during training. 

### Source code / logs
I guess the code example will make it clearer:
```
# Define training and validation datasets with the same structure.
training_dataset = tf.data.Dataset.range(100)
validation_dataset = tf.data.Dataset.range(50)

# A reinitializable iterator is defined by its structure. We could use the
# `output_types` and `output_shapes` properties of either `training_dataset`
# or `validation_dataset` here, because they are compatible.
iterator = tf.data.Iterator.from_structure(training_dataset.output_types,
                                  training_dataset.output_shapes)

next_element = iterator.get_next()


training_init_op = iterator.make_initializer(training_dataset)
validation_init_op = iterator.make_initializer(validation_dataset)

with tf.Session() as sess:
   # Run 20 epochs in which the training dataset is traversed, followed by the
   # validation dataset.
   for i in range(5):
       # Initialize an iterator over the training dataset.
       print(""#########################  "", i)
       sess.run(training_init_op)
       for _ in range(10):
           nel = sess.run(next_element)
           print(""train: "", type(nel), nel)

       # Initialize an iterator over the validation dataset.
       sess.run(validation_init_op)
       for _ in range(5):
           nel = sess.run(next_element)
           print(""valid: "", type(nel), nel)
```

Produces the output:

```
#########################   0
train:  <class 'numpy.int64'> 0
train:  <class 'numpy.int64'> 1
train:  <class 'numpy.int64'> 2
train:  <class 'numpy.int64'> 3
train:  <class 'numpy.int64'> 4
train:  <class 'numpy.int64'> 5
train:  <class 'numpy.int64'> 6
train:  <class 'numpy.int64'> 7
train:  <class 'numpy.int64'> 8
train:  <class 'numpy.int64'> 9
valid:  <class 'numpy.int64'> 0
valid:  <class 'numpy.int64'> 1
valid:  <class 'numpy.int64'> 2
valid:  <class 'numpy.int64'> 3
valid:  <class 'numpy.int64'> 4
#########################   1
train:  <class 'numpy.int64'> 0
train:  <class 'numpy.int64'> 1
train:  <class 'numpy.int64'> 2
train:  <class 'numpy.int64'> 3
train:  <class 'numpy.int64'> 4
train:  <class 'numpy.int64'> 5
train:  <class 'numpy.int64'> 6
train:  <class 'numpy.int64'> 7
train:  <class 'numpy.int64'> 8
train:  <class 'numpy.int64'> 9
valid:  <class 'numpy.int64'> 0
valid:  <class 'numpy.int64'> 1
valid:  <class 'numpy.int64'> 2
valid:  <class 'numpy.int64'> 3
valid:  <class 'numpy.int64'> 4
...
```
Apparently, the latter 90 elements of the training set and the latter 45 elements of the validation set never get evaluated. I don't really see the real-world use-case for this behavior. 


I know that you can implement the other functionality via the feedable iterator scheme using one_shot_iterators (but not using initializable iterators) as highlighted by the code below (pay attention to the different iterators used for training and validation here):

```
# Define training and validation datasets with the same structure.
training_dataset = tf.data.Dataset.range(10000000).repeat(2)
validation_dataset = tf.data.Dataset.range(5000000).repeat(2)

# A feedable iterator is defined by a handle placeholder and its structure. We
# could use the `output_types` and `output_shapes` properties of either
# `training_dataset` or `validation_dataset` here, because they have
# identical structure.
handle = tf.placeholder(tf.string, shape=[])
iterator = tf.data.Iterator.from_string_handle(
    handle, training_dataset.output_types, training_dataset.output_shapes)
next_element = iterator.get_next()

# You can use feedable iterators with a variety of different kinds of iterator
training_iterator = training_dataset.make_one_shot_iterator()
validation_iterator = validation_dataset.make_initializable_iterator()

with tf.Session() as sess:
    # The `Iterator.string_handle()` method returns a tensor that can be evaluated
    # and used to feed the `handle` placeholder.
    training_handle = sess.run(training_iterator.string_handle())
    validation_handle = sess.run(validation_iterator.string_handle())
    # Loop forever, alternating between training and validation.
    for i in range(5):
        print(""######################## "", i)
        i += 1
        # Run 10 steps using the training dataset. Note that the training dataset is
        # 2 * the original set, i.e. we run 2 epochs (see .repeat() argument), and we resume from where
        # we left off in the previous `while` loop iteration.
        for _ in range(10):
            nel = sess.run(next_element, feed_dict={handle: training_handle})
            print(""train: "", type(nel), nel)

        # Run one pass over the validation dataset.
        sess.run(validation_iterator.initializer)
        for _ in range(5):
            nel = sess.run(next_element, feed_dict={handle: validation_handle})
            print(""valid: "", type(nel), nel)
```

creates output:

```
########################  0
train:  <class 'numpy.int64'> 0
train:  <class 'numpy.int64'> 1
train:  <class 'numpy.int64'> 2
train:  <class 'numpy.int64'> 3
train:  <class 'numpy.int64'> 4
train:  <class 'numpy.int64'> 5
train:  <class 'numpy.int64'> 6
train:  <class 'numpy.int64'> 7
train:  <class 'numpy.int64'> 8
train:  <class 'numpy.int64'> 9
valid:  <class 'numpy.int64'> 0
valid:  <class 'numpy.int64'> 1
valid:  <class 'numpy.int64'> 2
valid:  <class 'numpy.int64'> 3
valid:  <class 'numpy.int64'> 4
########################  1
train:  <class 'numpy.int64'> 10
train:  <class 'numpy.int64'> 11
train:  <class 'numpy.int64'> 12
train:  <class 'numpy.int64'> 13
train:  <class 'numpy.int64'> 14
train:  <class 'numpy.int64'> 15
train:  <class 'numpy.int64'> 16
train:  <class 'numpy.int64'> 17
train:  <class 'numpy.int64'> 18
train:  <class 'numpy.int64'> 19
valid:  <class 'numpy.int64'> 0
valid:  <class 'numpy.int64'> 1
valid:  <class 'numpy.int64'> 2
valid:  <class 'numpy.int64'> 3
valid:  <class 'numpy.int64'> 4
########################  2
train:  <class 'numpy.int64'> 20
train:  <class 'numpy.int64'> 21
train:  <class 'numpy.int64'> 22
train:  <class 'numpy.int64'> 23
train:  <class 'numpy.int64'> 24
train:  <class 'numpy.int64'> 25
train:  <class 'numpy.int64'> 26
train:  <class 'numpy.int64'> 27
train:  <class 'numpy.int64'> 28
train:  <class 'numpy.int64'> 29
valid:  <class 'numpy.int64'> 0
valid:  <class 'numpy.int64'> 1
valid:  <class 'numpy.int64'> 2
valid:  <class 'numpy.int64'> 3
valid:  <class 'numpy.int64'> 4
```",1,,8,2017-12-18T14:49:11Z,NONE,2017-12-18T14:57:26Z
15446,call within the loop,"awaiting testing (then merge),cla: yes",optimize call within the loop,0,,3,2017-12-18T12:55:23Z,CONTRIBUTOR,2017-12-19T02:34:02Z
15444,Fix lib_strings_str_util_test on Windows,"awaiting testing (then merge),cla: yes",,0,,3,2017-12-18T11:41:24Z,CONTRIBUTOR,2017-12-18T17:56:15Z
15443,New metric: Cohen's kappa,"awaiting testing (then merge),cla: yes","resolve #15285.

Add new metric: `cohen_kappa`, which is equivalent to `sklearn.metrics.cohen_kappa_score`(>=0.19), but the implementation doesn't support weighted matrix yet.

Ref:
+ [Cohen's kappa - Wiki](https://en.wikipedia.org/wiki/Cohen's_kappa)
+ [Cohen's kappa: Index of Inter-rater Reliability](http://psych.unl.edu/psycrs/handcomp/hckappa.PDF)
+ [sklearn.metrics.cohen_kappa_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html)

### How to test

+ [x] add test cases.
+ [ ] pass all tests.",0,,8,2017-12-18T11:23:25Z,CONTRIBUTOR,2017-12-19T05:59:21Z
15441,[WIP] Add tf_copts to XLA libraries,"cla: yes,stat:awaiting response","Split from #14531 
Required by #15213
Depends on #15466

To fix a build error in //tensorflow/compiler/xla:util
There is a name conflict in  xla_data.pb.h
```
enum PrimitiveType {
  PRIMITIVE_TYPE_INVALID = 0,
  PRED = 1,
  S8 = 2,
  S16 = 3,
  S32 = 4,
  S64 = 5,
  U8 = 6,
  U16 = 7,
  U32 = 8,
  U64 = 9,
  F16 = 10,
  F32 = 11,
  BF16 = 16,
  F64 = 12,
  C64 = 15,
  TUPLE = 13,
  OPAQUE = 14,
  PrimitiveType_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  PrimitiveType_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
```
However, **OPAQUE** is already defined in wingdi.h as:
```
#define OPAQUE              2
```
So, ""/DNOGDI compiler flag  is a must for who includes this file.

@rongjiecomputer 
@jhseu ",0,,13,2017-12-18T09:48:49Z,CONTRIBUTOR,2017-12-18T23:57:34Z
15439,Add an is_external arg to tf_copts,"awaiting testing (then merge),cla: yes","It's for support build custom ops on Windows
@meteorcloudy This is what we talked last week.",0,,6,2017-12-18T09:04:41Z,CONTRIBUTOR,2017-12-18T09:14:05Z
15436,Simple Recurrent Unit,"awaiting testing (then merge),cla: yes",As per https://github.com/tensorflow/tensorflow/pull/15434#issuecomment-352343576.,0,,10,2017-12-18T07:27:09Z,CONTRIBUTOR,2017-12-18T09:00:35Z
15434,"Revert ""Initial SRU Implementation (#13978)""",cla: yes,This reverts commit e3e2ac9181c42eb82548726d8a250944b56180fd.,0,,2,2017-12-18T05:30:45Z,OWNER,2017-12-18T06:52:20Z
15432,[feature request] Switch to nvidia-docker v2?,"stat:awaiting response,type:build/install","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4.1
- **Python version**: 
3.5.4
- **Bazel version (if compiling from source)**:
None
- **GCC/Compiler version (if compiling from source)**:
None
- **CUDA/cuDNN version**:
None
- **GPU model and memory**:
None
- **Exact command to reproduce**:
None

### Describe the problem
Now we are using nvidia-docker v1 for CI build. We should switch to v2 because v1 is now being deprecated. (https://github.com/NVIDIA/nvidia-docker/wiki/About-version-2.0) 

### Source code / logs

",1,,3,2017-12-18T02:57:44Z,CONTRIBUTOR,2017-12-18T06:24:18Z
15431,Add shape function SingleImageRandomDotStereograms,"awaiting testing (then merge),cla: yes,kokoro:run","This fix tries to address the issue raised in #15429 where there is no shape function for SingleImageRandomDotStereograms.

This fix adds the shape function for `SingleImageRandomDotStereograms`.

NOTE: `SingleImageRandomDotStereograms` takes an attribute of `output_image_shape` which is in the format of `[X, Y, C]` (`[ImageX, ImageY, Channel]`. However, the actual
data output is in the format of `[ImageY, ImageX, Channel]` (`[h, w, c]`). So by default the output_image_shape has the value of [1024, 768, 1] but the output data will be [768, 1024, 1].
And if `[1200, 800, 1]` is used explicitly then the output data shape will be `[800, 1200, 1]`.

This fix does not change the behavior for now.

This fix fixes #15429.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,1,2017-12-18T01:26:47Z,MEMBER,2017-12-26T00:19:07Z
15429,RuntimeError: No C++ shape function registered for standard op: SingleImageRandomDotStereograms,,"single_image_random_dot_stereograms (windows bug)
the function works fine on ubuntu 16.04 / tf 1.4.0 (CPU)

I know, that the support for unsupported libraries in tf.contrib on Windows is incomplete, and left up to the individual contributors. I found some sketchy solutions by editing gen_single_image_random_dot_stereograms_ops.py, but i could not figure it out correctly.


**Source code**
```
import tensorflow as tf
from tensorflow.contrib.image import single_image_random_dot_stereograms
img=[[1,2,3,3,2,1],
     [1,2,3,4,5,2],
     [1,2,3,4,5,3],
     [1,2,3,4,5,4],
     [6,5,4,4,5,5]]
session = tf.InteractiveSession()
sirds = single_image_random_dot_stereograms(
    img,
    convergence_dots_size=8,
    number_colors=256,normalize=True)
out = sirds.eval()
png = tf.image.encode_png(out).eval()
with open('picture_out.png', 'wb') as f:
  f.write(png)
```

**Error**
Windows 10 Pro, 1709
conda version : 4.3.30
python version: 3.6.3
tensorflow-gpu 1.4.0

```
File ""C:\Users\###\Anaconda3\lib\site-packages\tensorflow\python\framework\common_shapes.py"", line 696, in _call_cpp_shape_fn_impl
    ""No C++ shape function registered for standard op: %s"" % op.type)
RuntimeError: No C++ shape function registered for standard op: SingleImageRandomDotStereograms
```

Windows 10 Pro, 1709
conda version : 4.3.30
python version: 3.5.4
tensorflow 1.3.0/tensorflow 1.2.0

`AttributeError: 'NoneType' object has no attribute 'single_image_random_dot_stereograms`
",0,,1,2017-12-17T19:44:34Z,NONE,2017-12-18T01:28:59Z
15427,Fix failing test //tensorflow/python:function_test,cla: yes,"@benoitsteiner It seems `//tensorflow/python:function_test` is failing with my last commit from #13998 (sorry about that).

It looks like the `function_test` created a graph with python through a helper class and invoked tests through c api.

I am not familiar with the `function_test`, though as `ClipByValue` is actually hidden, and is exposed through python only, with the gradient defined in python as well, I think replacing it will fix the issue?

Please take a look. Again, really sorry for the caused inconvenience.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,5,2017-12-17T18:21:03Z,MEMBER,2017-12-17T18:21:44Z
15424,"""Not a valid TensorFlow Graph serialization"" at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph.loadGraph",stat:awaiting response,"Hi,
I build libtensorflow_inference.so & libandroid_tensorflow_inference_java.jar from source, then use them in an android studio project. But when I run
TensorFlowInferenceInterface tflite = new TensorFlowInferenceInterface(assetManager, MODEL_PATH);
the program always crashes and prints
""
Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Invalid GraphDef
at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:551)
at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:105)
......
""
The tflite file in MODEL_PATH is generated successfully by bazel toco from a model graph, and the tflite file is successfully read into byte array. Are there any special restrictions on the graph? What are the possible reasons why function TensorFlowInferenceInterface.loadGraph() throws an Exception?

Thanks a lot.",0,,1,2017-12-17T09:45:49Z,NONE,2017-12-17T18:59:43Z
15423,Can't build with basel Python Configuration Error: --define PYTHON_BIN_PATH,stat:awaiting tensorflower,"Can't get what im doing wrong.
Win7 python 3.6, tensorflow from master, cuda 9.0, cudnn 7.0.5 for cuda 9.0, basel and swig loaded today 

https://github.com/tensorflow/tensorflow/issues/12052

> C:\Users\Andrey\Desktop\tensorflow>bazel clean
> ...........
> INFO: Starting clean (this may take a while). Consider using --async if the clea
> n takes more than several minutes.
> 
> C:\Users\Andrey\Desktop\tensorflow>python configure.py
> WARNING: Running Bazel server needs to be killed, because the startup options ar
> e different.
> You have bazel 0.8.1 installed.
> Please specify the location of python. [Default is C:\Users\Andrey\Anaconda3\pyt
> hon.exe]:
> 
> 
> Found possible Python library paths:
>   C:\Users\Andrey\Anaconda3\lib\site-packages
> Please input the desired Python library path to use.  Default is [C:\Users\Andre
> y\Anaconda3\lib\site-packages]
> 
> Do you wish to build TensorFlow with XLA JIT support? [y/N]: y
> XLA JIT support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with GDR support? [y/N]: y
> GDR support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with VERBS support? [y/N]: y
> VERBS support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with CUDA support? [y/N]: y
> CUDA support will be enabled for TensorFlow.
> 
> Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to d
> efault to CUDA 9.0]:
> 
> 
> Please specify the location where CUDA 9.0 toolkit is installed. Refer to README
> .md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/
> CUDA/v9.0]:
> 
> 
> Please specify the cuDNN version you want to use. [Leave empty to default to cuD
> NN 7.0]:
> 
> 
> Please specify the location where cuDNN 7 library is installed. Refer to README.
> md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/C
> UDA/v9.0]:
> 
> 
> Please specify a list of comma-separated Cuda compute capabilities you want to b
> uild with.
> You can find the compute capability of your device at: https://developer.nvidia.
> com/cuda-gpus.
> Please note that each additional compute capability significantly increases your
>  build time and binary size. [Default is: 3.5,5.2]
> 
> 
> Do you wish to build TensorFlow with MPI support? [y/N]: n
> No MPI support will be enabled for TensorFlow.
> 
> Please specify optimization flags to use during compilation when bazel option ""-
> -config=opt"" is specified [Default is -march=native]:
> 
> 
> Add ""--config=mkl"" to your bazel command to build with MKL support.
> Please note that MKL on MacOS or windows is still not supported.
> If you would like to use a local MKL instead of downloading, please set the envi
> ronment variable ""TF_MKL_ROOT"" every time before build.
> 
> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
>  n
> Not configuring the WORKSPACE for Android builds.
> 
> 
> C:\Users\Andrey\Desktop\tensorflow>bazel build --config=opt --config=win-cuda //
> tensorflow/tools/pip_package:build_pip_package
> ...........
> Loading:
> Loading: 0 packages loaded
> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (3 packages l
> oaded)
> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (24 packages
> loaded)
> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (34 packages
> loaded)
> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (72 packages
> loaded)
> ERROR: C:/users/andrey/desktop/tensorflow/third_party/py/numpy/BUILD:11:1: no su
> ch package '@local_config_python//': Traceback (most recent call last):
>         File ""C:/users/andrey/desktop/tensorflow/third_party/py/python_configure
> .bzl"", line 291
>                 _create_local_python_repository(repository_ctx)
>         File ""C:/users/andrey/desktop/tensorflow/third_party/py/python_configure
> .bzl"", line 251, in _create_local_python_repository
>                 _check_python_bin(repository_ctx, python_bin)
>         File ""C:/users/andrey/desktop/tensorflow/third_party/py/python_configure
> .bzl"", line 204, in _check_python_bin
>                 _fail((""--define %s='%s' is not execut...)))
>         File ""C:/users/andrey/desktop/tensorflow/third_party/py/python_configure
> .bzl"", line 27, in _fail
>                 fail((""%sPython Configuration Error:%...)))
> Python Configuration Error: --define PYTHON_BIN_PATH='C:/Users/Andrey/Anaconda3/
> python.exe' is not executable. Is it the python binary?
>  and referenced by '//third_party/py/numpy:headers'
> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (72 packages
> loaded)
> ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' fai
> led; build aborted: Loading failed
> INFO: Elapsed time: 6,710s
> FAILED: Build did NOT complete successfully (72 packages loaded)",0,,5,2017-12-17T02:51:21Z,NONE,2017-12-17T12:53:04Z
15422,Add common error documentation,"awaiting review,awaiting testing (then merge),cla: yes",See https://github.com/tensorflow/tensorflow/issues/15258,0,,5,2017-12-17T01:12:28Z,CONTRIBUTOR,2017-12-17T01:14:24Z
15420,IteratorGetNext should have a None gradient defined,,"Currently `IteratorGetNext` has no gradient defined. This can cause failures like below in `tf.gradients`. The solution is to define `None` gradient, like the `tf.stop_gradient` op. A work-around when this failure occurs is to wrap dataset ops inside `tf.stop_gradient`

```
  File ""/Users/yaroslav/anaconda/envs/sep22/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 590, in gradients
    (op.name, op.type))
LookupError: No gradient defined for operation 'IteratorGetNext' (op type: IteratorGetNext)

```",0,,1,2017-12-16T23:17:24Z,CONTRIBUTOR,2017-12-17T00:03:35Z
15418,Eager:  `gradients_function` can't compute the gradient for simple functions,comp:eager,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64-bit
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0-dev20171215
- **Python version**: 3.6.3 |Anaconda, Inc.| (default, Nov  8 2017, 15:10:56) [MSC v.1900 64 bit (AMD64)]
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: See description

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

First, define a loss function:

```
import tensorflow as tf
import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()

def loss(w):
    prediction = 2 * w + 1
    true_value = 11
    return tf.cast((true_value - prediction)**2, tf.float32)
```

Then, compute the gradient when w=0.1:

`tfe.gradients_function(loss)(0.1)`

The output is as expected. Next, compute the gradient when w=50:

`tfe.gradients_function(loss)(50)`

The output is:

`[None]`

I expected the output to be 360 because the gradient is -40 + 8 w.",1,,3,2017-12-16T20:55:58Z,CONTRIBUTOR,2017-12-17T01:33:27Z
15417,no protobuf package for macos Python 3.6,stat:awaiting tensorflower,"Following instructions on
https://www.tensorflow.org/install/install_mac

`pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp35-none-macosx_10_11_x86_64.whl
`

This doesn't work for Python 3.6 with error
`protobuf-3.1.0-cp35-none-macosx_10_11_x86_64.whl is not a supported wheel on this platform.
`
If I just change URL cp36, there's no such file

```pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl

  Could not install requirement protobuf==3.1.0 from https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl because of error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl
Could not install requirement protobuf==3.1.0 from https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl because of HTTP error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl for URL https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl

```",0,,6,2017-12-16T20:33:16Z,CONTRIBUTOR,2017-12-17T07:19:07Z
15415,sparse_multiclass_hinge_loss() error,stat:awaiting response,"Hello, 

I'm getting the error below using `sparse_multiclass_hinge_loss()`. Any hints would be highly appreciated.

```python
import numpy as np
import tensorflow as tf

x = np.random.uniform(0, 1, size = (100, 5))
y = np.random.choice(3, 100) 
y = y.reshape(100, 1)

X = tf.placeholder(""float32"", [None, 5])
Y = tf.placeholder(""int32"", [None, 1])

weights = {'w': tf.Variable(tf.random_uniform([5, 3]))}
biases = {'b': tf.Variable(tf.zeros([3]))}

logits = tf.add(tf.matmul(X, weights['w']), biases['b'])

loss = tf.reduce_mean(tf.contrib.kernel_methods.sparse_multiclass_hinge_loss(logits=logits, labels=Y))

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    res = sess.run(loss, feed_dict={X: x, Y: y}) 


res
```
 make_tensor_proto(values, dtype, shape, verify_shape)
    369   else:
    370     if values is None:
--> 371       raise ValueError(""None values not supported."")
    372     # if dtype is provided, forces numpy array to be the type
    373     # provided if possible.

ValueError: None values not supported.


",0,,2,2017-12-16T12:10:12Z,NONE,2017-12-17T01:20:32Z
15414,Failed to convert a .pb file to a .lite file where there is a custom lite op sin,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04.5 LTS 
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.4.0
- **Python version**: 2.7.6
- **Bazel version (if compiling from source)**: 0.8.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

I followed ""[How to use custom operators](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/custom_operators.md)"" to create a .pb file and ""sin.cc"" in //tensorflow/contrib/lite/kernel/. Then I added ""TfLiteRegistration* Register_SIN();"" and ""AddCustom(""Sin"", Register_SIN());"" in ""register.cc""
But when I used the bazel command to covert the pb file, the sin op can not be converted

Here is the command I used:
bazel build //tensorflow/contrib/lite/toco:toco
bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=tftest/sin.pb --input_format=TENSORFLOW_GRAPHDEF --output_file=tftest/sin.tflite --output_format=TFLITE --inference_type=FLOAT --input_array=input --input_shape=1 --output_array=output
Here is the corresponding ERROE information:
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: Sin.

I tried to use ----allow_custom_ops, but it did not work. Here is the ERROR information:
Converting unsupported operation: Sin

I think I have to modify more files, but I do not know which files I should modify and how to modify. Could you please give a detailed demo?
Thx",0,,3,2017-12-16T08:04:56Z,NONE,2017-12-16T18:57:13Z
15409,MKL: Fixes for concat and elementwise ops,"cla: yes,stat:awaiting response",Fixes concat for Svd test cases and disable 4 elementwise ops to get SSG/VGG-16 model to work with MKL DNN. Disabled tanh due to differences between Tensorflow and DNN primitives.,0,,12,2017-12-15T23:37:18Z,CONTRIBUTOR,2017-12-18T21:46:30Z
15406,MKL: Adding missing reorders in ReLU and AddN,"awaiting testing (then merge),cla: yes",Commit to add missing reorder primitive in ReLU and AddN operators.,0,,2,2017-12-15T21:33:25Z,CONTRIBUTOR,2017-12-26T02:14:17Z
15403,Computing gradients of loop variables return None,,"### Problem description
I got `None` when computing gradient of the two loop variables that are supposed to have gradient..

### Minimum code to reproduce the error
```python
def loop_cond(i, *_):
    with tf.control_dependencies([tf.Print(i, [i])]):
        return i < 5

def loop_body(i, a, b):
    c = tf.gradients(b, a)[0]
    b = b + c
    return i + 1, b,  b ** 2
        
a = tf.constant(3.0)
f_i, f_a, f_b = tf.while_loop(loop_cond, loop_body, [0, a, a])
```

It seems to have failed in the first iteration as there was no output from print statement.

### Complete logs
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-23-a766942a3da2> in <module>()
      9 
     10 a = tf.constant(3.0)
---> 11 f_i, f_a, f_b = tf.while_loop(loop_cond, loop_body, [0, a, a])

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)
   2814     loop_context = WhileContext(parallel_iterations, back_prop, swap_memory)  # pylint: disable=redefined-outer-name
   2815     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)
-> 2816     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
   2817     return result
   2818 

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)
   2638       self.Enter()
   2639       original_body_result, exit_vars = self._BuildLoop(
-> 2640           pred, body, original_loop_vars, loop_vars, shape_invariants)
   2641     finally:
   2642       self.Exit()

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)
   2588         structure=original_loop_vars,
   2589         flat_sequence=vars_for_body_with_tensor_arrays)
-> 2590     body_result = body(*packed_vars_for_body)
   2591     if not nest.is_sequence(body_result):
   2592       body_result = [body_result]

<ipython-input-23-a766942a3da2> in loop_body(i, a, b)
      5 def loop_body(i, a, b):
      6     c = tf.gradients(b, a)[0]
----> 7     b = b + c
      8     return i + 1, b,  b ** 2
      9 

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
    883       if not isinstance(y, sparse_tensor.SparseTensor):
    884         try:
--> 885           y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
    886         except TypeError:
    887           # If the RHS is not a tensor, it might be a tensor aware object

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)
    834       name=name,
    835       preferred_dtype=preferred_dtype,
--> 836       as_ref=False)
    837 
    838 

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
    924 
    925     if ret is None:
--> 926       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    927 
    928     if ret is NotImplemented:

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    227                                          as_ref=False):
    228   _ = as_ref
--> 229   return constant(v, dtype=dtype, name=name)
    230 
    231 

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)
    206   tensor_value.tensor.CopyFrom(
    207       tensor_util.make_tensor_proto(
--> 208           value, dtype=dtype, shape=shape, verify_shape=verify_shape))
    209   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
    210   const_tensor = g.create_op(

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)
    369   else:
    370     if values is None:
--> 371       raise ValueError(""None values not supported."")
    372     # if dtype is provided, forces numpy array to be the type
    373     # provided if possible.

ValueError: None values not supported.
```

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 7.11 (wheezy)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0-4-g9283868 1.4.0
- **Python version**:  3.5.4
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA",0,,3,2017-12-15T18:27:24Z,NONE,2017-12-15T21:00:37Z
15401,module 'tensorflow.contrib' has no attribute 'lite',"comp:lite,stat:awaiting response","Hello folks.

Everytime I try to run the example fo TOCO:

```
import tensorflow as tf

img = tf.placeholder(name=""img"", dtype=tf.float32, shape=(1, 64, 64, 3))
val = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])
out = tf.identity(val, name=""out"")
with tf.Session() as sess:
  tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [img], [out])
  open(""test.tflite"", ""wb"").write(tflite_modeL)
``` 

I get the error: **module 'tensorflow.contrib' has no attribute 'lite'**

OS Platform and Distribution: Mac OS High Sierra
Tensorflow installed from pip version 1.4.1. 
Python version: 2.7.14 and 3.6.3 :: Anaconda custom (64-bit)

The informations about my system are those:

== cat /etc/issue ===============================================
Darwin Leandros-MacBook-Pro.local 17.3.0 Darwin Kernel Version 17.3.0: Thu Nov  9 18:09:22 PST 2017; root:xnu-4570.31.3~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.2

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.0.0 (clang-900.0.37)
Target: x86_64-apple-darwin17.3.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin Leandros-MacBook-Pro.local 17.3.0 Darwin Kernel Version 17.3.0: Thu Nov  9 18:09:22 PST 2017; root:xnu-4570.31.3~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.13.3)
numpydoc (0.7.0)
protobuf (3.5.0.post1)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.8)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

Seams like tensorflow lite is not available for reason =/

Sorry if this is not a bug and I am just being dumb about how to make TOCO works. 
",0,,3,2017-12-15T18:02:55Z,NONE,2017-12-15T22:19:10Z
15398,Feature request: tf.info to show docstrings in jupyter notebook,,"When working with jupyter notebooks, it is really helpful to see the documentation within the notebook. Numpy has a function called `np.info` which takes a numpy function as argument and prints its docstring. For example, np.info(np.mean) prints the docstring for `np.mean`. It would be really helpful to have an equivalent `tf.info` for those of us who work with jupyter notebooks (and who doesn't?).",0,,2,2017-12-15T14:36:36Z,NONE,2017-12-15T21:21:08Z
15397,Tensorflow c++ memory leak - Valgrind,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 17.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.1
- **GCC/Compiler version (if compiling from source)**: 6.0.3
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Describe the problem**:


I am executing simple tensorflow code to create graph def as shown below

```
tensorflow::NewSession (options, &session)
ReadBinaryProto (tensorflow::Env::Default(), ""/home/ashok/eclipseWorkspace/faceRecognition-x86_64/Data/models/optimized_facenet.pb"", &graph_def));
session->Create (graph_def);
```

But when I run Valgrind as shown below
```
valgrind --leak-check=full --show-leak-kinds=all --vex-guest-max-insns=25 ./faceRecognition-x86_64 -r -i
```
 I get below errors

```
==12366== 16,000 bytes in 1 blocks are still reachable in loss record 47,782 of 47,905
==12366==    at 0x4C2E19F: operator new(unsigned long) (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==12366==    by 0xBF875DC: std::vector<tensorflow::CostModel::MemUsage, std::allocator<tensorflow::CostModel::MemUsage> >::reserve(unsigned long) (in /usr/lib/libtensorflow_cc.so)
==12366==    by 0xBF90128: tensorflow::CostModel::InitFromGraph(tensorflow::Graph const&) (in /usr/lib/libtensorflow_cc.so)
==12366==    by 0xBEE48D3: tensorflow::SimpleGraphExecutionState::InitBaseGraph(tensorflow::BuildGraphOptions const&) (in /usr/lib/libtensorflow_cc.so)
==12366==    by 0xBEE52CF: tensorflow::SimpleGraphExecutionState::MakeForBaseGraph(tensorflow::GraphDef*, tensorflow::SimpleGraphExecutionStateOptions const&, std::unique_ptr<tensorflow::SimpleGraphExecutionState, std::default_delete<tensorflow::SimpleGraphExecutionState> >*) (in /usr/lib/libtensorflow_cc.so)
==12366==    by 0xBE68B9D: tensorflow::DirectSession::MaybeInitializeExecutionState(tensorflow::GraphDef const&, bool*) (in /usr/lib/libtensorflow_cc.so)
==12366==    by 0xBE68CF9: tensorflow::DirectSession::ExtendLocked(tensorflow::GraphDef const&) (in /usr/lib/libtensorflow_cc.so)
==12366==    by 0xBE68FC7: tensorflow::DirectSession::Create(tensorflow::GraphDef const&) (in /usr/lib/libtensorflow_cc.so)
==12366==    by 0x26B899: TensorFlow::initializeRecognition() (in /home/ashok/eclipseWorkspace/faceRecognition-x86_64/Debug/faceRecognition-x86_64)
==12366==    by 0x24197D: RecognitionWithImages::RecognitionWithImages() (in /home/ashok/eclipseWorkspace/faceRecognition-x86_64/Debug/faceRecognition-x86_64)
==12366==    by 0x12F27C: main (in /home/ashok/eclipseWorkspace/faceRecognition-x86_64/Debug/faceRecognition-x86_64)

```
These type of errors are also generated when I do **session -> run ()** 

Due to the above issues, the memory needed to run the program keeps increasing as time passes and the application crashes due to insufficient memory after  a certain point of time.

I have also posted the issue in stack overflow - https://stackoverflow.com/questions/47834054/tensorflow-c-memory-leak-valgrind",0,,1,2017-12-15T14:14:46Z,NONE,2017-12-15T21:08:24Z
15391,tf.nn.leaky_relu does not work with float64,"stat:contributions welcome,type:feature","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 26
- **TensorFlow installed from (source or binary)**: binary, pip3 install tensorflow
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: Python 3.6.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: None
- **GPU model and memory**: Doesn't apply
- **Exact command to reproduce**: See below

### Describe the problem

`tf.nn.leaky_relu` does not work with float64. It only seems to work with float32. I can't think of a reason why it should not work with float64, so I consider this a bug, your mileage might vary. I'm also not familiar enough with tf code to fix it myself without any help. :-)

It seems at least [ops.py#L926](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/framework/ops.py#L926) should be in a `try` block just as [ops.py#L912](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/framework/ops.py#L912)

Also the unit tests only test float32.

### Source code / logs

```
In [1]: import tensorflow as tf
/usr/lib64/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)

In [2]: c = tf.constant(5.0, dtype=tf.float32)

In [3]: lr = tf.nn.leaky_relu(c)

In [4]: c = tf.constant(5.0, dtype=tf.float64)

In [5]: lr = tf.nn.leaky_relu(c)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-5-71e83d060e7b> in <module>()
----> 1 lr = tf.nn.leaky_relu(c)

~/.emacs.d/.python-environments/jedi/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py in leaky_relu(features, alpha, name)
   1541     features = ops.convert_to_tensor(features, name=""features"")
   1542     alpha = ops.convert_to_tensor(alpha, name=""alpha"")
-> 1543     return math_ops.maximum(alpha * features, features)
   1544 
   1545 

~/.emacs.d/.python-environments/jedi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
    883       if not isinstance(y, sparse_tensor.SparseTensor):
    884         try:
--> 885           y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
    886         except TypeError:
    887           # If the RHS is not a tensor, it might be a tensor aware object

~/.emacs.d/.python-environments/jedi/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)
    834       name=name,
    835       preferred_dtype=preferred_dtype,
--> 836       as_ref=False)
    837 
    838 

~/.emacs.d/.python-environments/jedi/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
    924 
    925     if ret is None:
--> 926       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    927 
    928     if ret is NotImplemented:

~/.emacs.d/.python-environments/jedi/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
    772     raise ValueError(
    773         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r"" %
--> 774         (dtype.name, t.dtype.name, str(t)))
    775   return t
    776 

ValueError: Tensor conversion requested dtype float32 for Tensor with dtype float64: 'Tensor(""Const_1:0"", shape=(), dtype=float64)'
```",0,,4,2017-12-15T10:58:31Z,NONE,2017-12-15T13:11:18Z
15390,Update C API test data comparison for s390x,"awaiting testing (then merge),cla: yes",Base64 encode/decode is not endian-dependent. The hash passed to Base64 will generate different string on big-endian platform. Correcting the test data comparison for s390x architecture. ,0,,3,2017-12-15T09:55:10Z,CONTRIBUTOR,2017-12-20T12:48:53Z
15389,Fatal error while compiling Tensorflow with CUDA 9.1,type:build/install,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-104-generic x86_64)
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
unknown 1.4.0 (Source code is cloned from 798fa36d11119e6fdc13b90a14abfe1805e7de90)
- **Python version**: 
3.6.3
- **Bazel version (if compiling from source)**:
0.8.1
- **GCC/Compiler version (if compiling from source)**:
gcc version 5.4.0 20160609
- **CUDA/cuDNN version**:
CUDA 9.1
cuDNN 7.0.5
- **GPU model and memory**:
2 * Tesla V100-PCIE-16GB
- **Exact command to reproduce**:
See description below.

### Describe the problem
While trying to compile the latest TensorFlow(cloned from 798fa36d11119e6fdc13b90a14abfe1805e7de90), such error will be raised:
```
ERROR: /home/ubuntu/tensorflow/tensorflow/contrib/seq2seq/BUILD:64:1: error while parsing .d file: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/contrib/seq2seq/_objs/python/ops/_beam_search_ops_gpu/tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.pic.d (No such file or directory)
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14:0,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/contrib/seq2seq/kernels/beam_search_ops.h:19,
                 from tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.cc:20:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:59:34: fatal error: math_functions.hpp: No such file or directory
```

It turns out that in CUDA 9.1, `math_functions.hpp` is located at `cuda/include/crt/math_functions.hpp`, rather than `cuda/include/math_functions.hpp` (CUDA 9.0 does), which leads to this error.
`ln -s /usr/local/cuda/include/crt/math_functions.hpp /usr/local/cuda/include/math_functions.hpp` will fix this problem and complete the compiling process.

#### Reference
https://stackoverflow.com/a/47807106/2666624

### Source code / logs
Traceback is available above.
",0,,19,2017-12-15T09:19:39Z,NONE,2017-12-15T22:35:11Z
15388,cannot install with virtualenv python3.6,stat:awaiting response,"Have I written custom code      No
OS Platform and Distribution .  MacOS 10.13.2
TensorFlow installed from        conda  URL of the TensorFlow Python package
TensorFlow version .                1.4
Bazel version                             N/A
CUDA/cuDNN version              N/A
GPU model and memory          N/A
Exact command to reproduce  I just follow Install with conda, My python was installed with conda, I think maybe that is the problem.



virtualenv --system-site-packages -p python3 ~/tensorflow
Running virtualenv with interpreter /usr/local/bin/python3
Using base prefix '/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6'
New python executable in /Users/xinhai/tensorflow/bin/python3.6
Also creating executable in /Users/xinhai/tensorflow/bin/python
Installing setuptools, pip, wheel...
  Complete output from command /Users/xinhai/tensorflow/bin/python3.6 - setuptools pip wheel:
  stringstringstringstringstringstringstringstring
Traceback (most recent call last):
  File ""<stdin>"", line 7, in <module>
  File ""/usr/local/lib/python2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/__init__.py"", line 5, in <module>
  File ""/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/logging/__init__.py"", line 28, in <module>
    from string import Template
ImportError: cannot import name 'Template'



...Installing setuptools, pip, wheel...done.
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/virtualenv.py"", line 2328, in <module>
    main()
  File ""/usr/local/lib/python2.7/site-packages/virtualenv.py"", line 713, in main
    symlink=options.symlink)
  File ""/usr/local/lib/python2.7/site-packages/virtualenv.py"", line 945, in create_environment
    download=download,
  File ""/usr/local/lib/python2.7/site-packages/virtualenv.py"", line 901, in install_wheel
    call_subprocess(cmd, show_stdout=False, extra_env=env, stdin=SCRIPT)
  File ""/usr/local/lib/python2.7/site-packages/virtualenv.py"", line 797, in call_subprocess
    % (cmd_desc, proc.returncode))
OSError: Command /Users/xinhai/tensorflow/bin/python3.6 - setuptools pip wheel failed with error code 1",0,,4,2017-12-15T08:54:21Z,NONE,2017-12-15T18:30:12Z
15383,Feature request: Use placeholders to specify the inputs of TFGAN model.,stat:awaiting response,,0,,3,2017-12-15T06:02:16Z,CONTRIBUTOR,2017-12-15T19:00:07Z
15380,CMake: external package: PIC option not working,,"In many of external cmake files (```/tensorflow/contrib/cmake/external/*.cmake```), it tries to turn PIC on and off with:
```
      CMAKE_CACHE_ARGS
                if(tensorflow_ENABLE_POSITION_INDEPENDENT_CODE)
                        -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=ON
                else()
                        -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=OFF
                endif()
```

However, the result (with cmake 3.5) is not what we wanted in CMakeCache:
```
CMAKE_POSITION_INDEPENDENT_CODE:BOOL=OFF; endif();
```
Where it should be
```
CMAKE_POSITION_INDEPENDENT_CODE:BOOL=ON
```
because ```tensorflow_ENABLE_POSITION_INDEPENDENT_CODE``` is ON.

This can be corrected by:
```
  -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=${tensorflow_ENABLE_POSITION_INDEPENDENT_CODE}
```

I'll send a PR after some testing along with other fixes (e.g., linking to ZLIB as shared object, not static linking)

---- update requested from @tensorflowbutler ----
Have I written custom code: No.
OS Platform and Distribution: PR-tested at Ubuntu-16.04 and Tizen
TensorFlow installed from: N/A
TensorFlow version: github master branch of last week: 00f8b97fc601381546aea89315dee549bdbbbdfc
Bazel version: N/A (doing it without bazel)
CUDA/cuDNN version: Tizen: 9/7 / Ubuntu: 8/6
GPU model and memory: Titan Xp
Exact command to reproduce: N/A (it is about build)",0,,5,2017-12-15T05:30:13Z,CONTRIBUTOR,2017-12-15T12:59:21Z
15376,"With tf-nightly-gpu, getting error: ImportError: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.23' not found",stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: `tf-nightly-gpu` -- I can't import TensorFlow to check the version
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: Cuda 9.0, cuDNN 7.0.4
- **GPU model and memory**: GTX 1080 8GB
- **Exact command to reproduce**:

```
virtualenv --system-site-packages ~/tftest
source ~/tftest/bin/activate
pip install tf-nightly-gpu
python -c 'import tensorflow'
```

### Describe the problem
When I run the commands above, I get the following error:
```
(tftest) reedwm@reedwm2:~$ python -c 'import tensorflow'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/buildtools/current/sitecustomize/sitecustomize.py"", line 152, in SetupPathsAndImport
    return real_import(name, globals, locals, fromlist, level)
  File ""/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 73, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.23' not found (required by /usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

This occurs if I `pip install tf-nightly-gpu==1.5.0.dev20171212`, which is the earliest version of `tf-nightly-gpu` it occurs on. When I pip install the previous version with `pip install tf-nightly-gpu==1.5.0.dev20171207`, the issue does not occur.

This issue is similar to #53 and #3127.

/CC @gunan @jhseu @martinwicke, any ideas what the issue could be?
",0,,7,2017-12-15T00:43:20Z,MEMBER,2017-12-15T16:03:53Z
15375,Performance  problem TF VS Keras,,"Hello , 
I just got huge difference in results using Keras (Back-end TensorFlow) and TensorFlow. I want to know if the difference in performances is normal .

The keras model produces a loss of 0.2
`model = k.models.Sequential()
model.add(k.layers.convolutional.Conv2D(64, kernel_size=(3,3), input_shape=(75,75,3)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(k.layers.convolutional.MaxPooling2D(pool_size=(3,3), strides=(2,2)))
model.add(k.layers.Dropout(0.2))
model.add(k.layers.convolutional.Conv2D(128, kernel_size=(3, 3)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(k.layers.Dropout(0.2))
model.add(k.layers.convolutional.Conv2D(128, kernel_size=(3, 3)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(k.layers.Dropout(0.3))
model.add(k.layers.convolutional.Conv2D(64, kernel_size=(3, 3)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(k.layers.Dropout(0.3))
model.add(k.layers.Flatten())
model.add(k.layers.Dense(512))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(k.layers.Dropout(0.2))
model.add(k.layers.Dense(256))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(k.layers.Dropout(0.2))
model.add(k.layers.Dense(1))
model.add(Activation('sigmoid'))
mypotim=Adam(lr=0.01, decay=0.0)
model.compile(loss='binary_crossentropy', optimizer = mypotim, metrics=['accuracy'])`

The TensorFlow normal model produces 0.7 :

`x = tf.placeholder(tf.float32, [None, 75,75,3], name=""DNN_Input"") 
learningRateIn= tf.placeholder(tf.float32)
keep_prob = tf.placeholder(tf.float32)
isTrainPlace=tf.placeholder(tf.bool)
with tf.name_scope('conv_1'):  
    conv_1=tf.layers.conv2d(x,64,[3,3],activation=tf.nn.relu)
    batch_n1 = tf.contrib.layers.batch_norm(conv_1,center=True, scale=True, is_training=isTrainPlace, scope='bn1')
    mpool_1=tf.layers.max_pooling2d(batch_n1,pool_size=(2,2),strides=(2,2))
    dropout_1=tf.layers.dropout(mpool_1,rate=0.8,training=isTrainPlace)
with tf.name_scope('conv_2'):      
    conv_2=tf.layers.conv2d(dropout_1,128,[3,3],activation=tf.nn.relu)
    batch_n2 = tf.contrib.layers.batch_norm(conv_2, center=True, scale=True,is_training=isTrainPlace,scope='bn2')
    mpool_2=tf.layers.max_pooling2d(batch_n2,pool_size=(2,2),strides=(2,2))
    dropout_2=tf.layers.dropout(mpool_2,rate=0.8,training=isTrainPlace)
with tf.name_scope('conv_3'):      
    conv_3=tf.layers.conv2d(dropout_2,128,[3,3],activation=tf.nn.relu)
    batch_n3 = tf.contrib.layers.batch_norm(conv_3, center=True, scale=True,is_training=isTrainPlace,scope='bn3')
    mpool_3=tf.layers.max_pooling2d(batch_n3,pool_size=(2,2),strides=(2,2))
    dropout_3=tf.layers.dropout(mpool_3,rate=0.7,training=isTrainPlace)
with tf.name_scope('conv_4'):     
    conv_4=tf.layers.conv2d(dropout_3,64,[3,3],activation=tf.nn.relu)
    batch_n4 = tf.contrib.layers.batch_norm(conv_4,center=True, scale=True,is_training=isTrainPlace,scope='bn4')
    mpool_4=tf.layers.max_pooling2d(batch_n4,pool_size=(2,2),strides=(2,2))
    dropout_4=tf.layers.dropout(mpool_4,rate=0.7,training=isTrainPlace)
    h4=tf.contrib.layers.flatten(dropout_4)
with tf.name_scope('dense_1'):      
    y_dense_1=tf.layers.dense(h4,512,activation=tf.nn.relu)
    batch_dense_1 = tf.contrib.layers.batch_norm(y_dense_1,center=True, scale=True,is_training=isTrainPlace, scope='bn5')
    dropout_dense_1=tf.layers.dropout(batch_dense_1,rate=0.8,training=isTrainPlace)
with tf.name_scope('dense_2'):      
    y_dense_2=tf.layers.dense(dropout_dense_1,256,activation=tf.nn.relu)
    batch_dense_2 = tf.contrib.layers.batch_norm(y_dense_2, center=True, scale=True, is_training=isTrainPlace,scope='bn6')
    dropout_dense_2=tf.layers.dropout(batch_dense_2 ,rate=0.8, training=isTrainPlace)
    y_estimated=tf.layers.dense(dropout_dense_2,2)  `

PS : the code above , is inspired from : [ https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d](url)
Can anybody help me, please, to undersand , if it's normal or not ? does Keras, uses different tensorflow parameters than the default parameters of tensorflow ?
Thanks in advance.
Toetoe.",0,,1,2017-12-14T23:54:03Z,NONE,2017-12-15T01:55:27Z
15374,tf.matching_files order of returned files,stat:contributions welcome,"As far as I can tell from `https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/file_system.cc`, the order of filenames returned by `tf.matching_files` can be non-determinstic. 

If that is correct, it would be nice if that were stated in the documentation (and also for `Dataset.list_files` and `train.match_filenames_once`).

Even better would be to guarantee alphabetical order, but I am not sure about the performance overhead that would incur.

This would enable to process files given as e.g.
A/1.png, A/2.png, ... and B/1.png, ... jointly by doing to match_files followed by a zip.

Have I written custom code No
OS Platform and Distribution N/A
TensorFlow installed from pip 
TensorFlow version 1.4.1
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce N/A",0,,7,2017-12-14T22:57:06Z,CONTRIBUTOR,2017-12-15T07:28:19Z
15373,GPU memory usage changed from TF 1.3.0 to 1.4.0 - runs out of memory,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code included below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: From pip
- **TensorFlow version (use command below)**: 1.3.0 and 1.4.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: cuda-8.0 cudnn-6.0
- **GPU model and memory**: GTX 1080 8GB
- **Exact command to reproduce**: python <example_script.py>

### Describe the problem
Bug. TensorFlow runs out of GPU memory (ResourceExhaustedError) when using version 1.4.0 when running code that runs fine on version 1.3.0. Please see the following script to reproduce.

### Source code / logs
```
import tensorflow as tf
import tensorflow.contrib.slim.nets as nets
import tensorflow.contrib.slim as slim
from tensorflow.contrib.slim.nets import resnet_v2

kBatchSize = 4
kCropSize = 500
kNumClasses = 10

with tf.device('/gpu:0'):
  images = tf.random_normal([kBatchSize, kCropSize, kCropSize, 3])
  labels = tf.constant(0, dtype=tf.int32, shape=[kBatchSize, kCropSize, kCropSize])

  with slim.arg_scope(resnet_v2.resnet_arg_scope()):
    backbone, end_points = resnet_v2.resnet_v2_101(
        images, None, is_training=True, global_pool=False,
        output_stride=8)

    final_conv = tf.layers.conv2d(backbone, kNumClasses, [1, 1], name='final_conv')
    logits = tf.image.resize_bilinear(final_conv, tf.slice(tf.shape(images), [1], [2]))

  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(
        labels=labels, logits=logits)

optimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)
train_op = slim.learning.create_train_op(loss, optimizer)
slim.learning.train(train_op, '/tmp/resnet')
```
",0,,1,2017-12-14T22:55:32Z,NONE,2017-12-15T01:54:55Z
15371,Standardize arguments in SessionRunHook APIs.,stat:awaiting tensorflower,"Some hooks inheriting from `SessionRunHook` (https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/training/basic_session_run_hooks.py) use different input argument keywords, while implementing the exact same functionality. This should be ironed out.

I wanted to make a PR for this, but I realised this will be backwards incompatible. Still, I think we should standardise this.

E.g.:
`every_secs` (by `SecondOrStepTimer`)
`every_n_secs`  (by `LoggingTensorHook`) [this seems like most descriptive one, to me]
`save_secs` (by `CheckpointSaverHook`)
",0,,2,2017-12-14T21:10:30Z,NONE,2017-12-14T22:18:51Z
15368,Clean bazel `all_files` targets,"awaiting testing (then merge),cla: yes",Broken out of #15166,0,,19,2017-12-14T16:58:32Z,CONTRIBUTOR,2017-12-14T17:17:36Z
15365,Automatic node placement (allocating graph nodes to multiple devices) feature  in distributed tensorflow,,"I read tensorflow white papaer and found node placement which allocates graph nodes to devices without manual configuration.

https://www.reddit.com/r/MachineLearning/comments/4n6a0e/distributed_tensorflow_resource_allocation/

This post says this feature was removed because it did not perform well.
However, it was posted a year ago and I think you are still developing this feature.

Is it included in the current version of tensorflow?
If so, what code do i need to see?
If inot, do you plan to add this feature?
",0,,1,2017-12-14T13:58:56Z,NONE,2017-12-14T22:17:08Z
15362,Fix broken image link in TensorFlow Lite's docs,"awaiting review,awaiting testing (then merge),cla: yes",I fixed the link of image in the same way as other documents in [tensorflow/tensorflow/docs_src/](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/docs_src).,0,,4,2017-12-14T10:51:25Z,CONTRIBUTOR,2017-12-19T22:01:36Z
15361,//tensorflow/python:bfloat16_test and //tensorflow/python:framework_dtypes_test failing on Windows,,"http://ci.tensorflow.org/job/tf-master-win-bzl/2063/console
```
13:00:56 INFO: From Testing //py_test_dir/tensorflow/python:framework_dtypes_test:
13:00:56 ==================== Test output for //py_test_dir/tensorflow/python:framework_dtypes_test:
13:00:56 .........F\\?\C:\tmp\Bazel.runfiles_fnb6t73_\runfiles\org_tensorflow\py_test_dir\tensorflow\python\framework\dtypes_test.py:277: DeprecationWarning: Please use assertEqual instead.
13:00:56   self.assertEquals(dtype.min, np.finfo(numpy_dtype).min)
13:00:56 ......
13:00:56 ======================================================================
13:00:56 FAIL: testIsUnsigned (__main__.TypesTest)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_fnb6t73_\runfiles\org_tensorflow\py_test_dir\tensorflow\python\framework\dtypes_test.py"", line 219, in testIsUnsigned
13:00:56     self.assertEqual(dtypes.as_dtype(""bfloat16"").is_unsigned, False)
13:00:56 AssertionError: True != False
13:00:56 
13:00:56 ----------------------------------------------------------------------
13:00:56 Ran 16 tests in 0.009s
13:00:56 
13:00:56 FAILED (failures=1)
13:00:56 <dtype: 'float32'>: -3.40282e+38 - 3.40282e+38
13:00:56 <dtype: 'float64'>: -1.79769313486e+308 - 1.79769313486e+308
13:00:56 <dtype: 'int32'>: -2147483648 - 2147483647
13:00:56 <dtype: 'uint8'>: 0 - 255
13:00:56 <dtype: 'int16'>: -32768 - 32767
13:00:56 <dtype: 'int8'>: -128 - 127
13:00:56 <dtype: 'int64'>: -9223372036854775808 - 9223372036854775807
13:00:56 <dtype: 'bfloat16'>: 0 - 0
13:00:56 <dtype: 'uint16'>: 0 - 65535
13:00:56 <dtype: 'float16'>: -65504.0 - 65504.0
13:00:56 <dtype: 'uint32'>: 0 - 4294967295
13:00:56 <dtype: 'uint64'>: 0 - 18446744073709551615
13:00:56 <dtype: 'float32_ref'>: -3.40282e+38 - 3.40282e+38
13:00:56 <dtype: 'float64_ref'>: -1.79769313486e+308 - 1.79769313486e+308
13:00:56 <dtype: 'int32_ref'>: -2147483648 - 2147483647
13:00:56 <dtype: 'uint8_ref'>: 0 - 255
13:00:56 <dtype: 'int16_ref'>: -32768 - 32767
13:00:56 <dtype: 'int8_ref'>: -128 - 127
13:00:56 <dtype: 'int64_ref'>: -9223372036854775808 - 9223372036854775807
13:00:56 <dtype: 'bfloat16_ref'>: 0 - 0
13:00:56 <dtype: 'uint16_ref'>: 0 - 65535
13:00:56 <dtype: 'float16_ref'>: -65504.0 - 65504.0
13:00:56 <dtype: 'uint32_ref'>: 0 - 4294967295
13:00:56 <dtype: 'uint64_ref'>: 0 - 18446744073709551615
13:00:56 ================================================================================
13:00:56 INFO: From Testing //py_test_dir/tensorflow/python:bfloat16_test:
13:00:56 ==================== Test output for //py_test_dir/tensorflow/python:bfloat16_test:
13:00:56 FFF.F.FFFFFFFFFFFFFFFF.
13:00:56 ======================================================================
13:00:56 FAIL: testAdd (__main__.Bfloat16NumPyTest)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 189, in testAdd
13:00:56     self.assertAllClose(np.array([[5, 7, 9]]), x + y)
13:00:56   File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\framework\test_util.py"", line 1083, in assertAllClose
13:00:56     self._assertArrayLikeAllClose(a, b, rtol=rtol, atol=atol)
13:00:56   File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\framework\test_util.py"", line 1053, in _assertArrayLikeAllClose
13:00:56     np.testing.assert_allclose(a, b, rtol=rtol, atol=atol, err_msg=msg)
13:00:56   File ""C:\Program Files\Anaconda3\lib\site-packages\numpy\testing\utils.py"", line 1411, in assert_allclose
13:00:56     verbose=verbose, header=header, equal_nan=equal_nan)
13:00:56   File ""C:\Program Files\Anaconda3\lib\site-packages\numpy\testing\utils.py"", line 796, in assert_array_compare
13:00:56     raise AssertionError(msg)
13:00:56 AssertionError: 
13:00:56 Not equal to tolerance rtol=1e-06, atol=1e-06
13:00:56 None
13:00:56 (mismatch 100.0%)
13:00:56  x: array([[5, 7, 9]])
13:00:56  y: array([[ 0.,  0.,  0.]], dtype=float16)
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testArray (__main__.Bfloat16NumPyTest)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 172, in testArray
13:00:56     self.assertEqual(""[[bfloat16(1) bfloat16(2) bfloat16(3)]]"", str(x))
13:00:56 AssertionError: '[[bfloat16(1) bfloat16(2) bfloat16(3)]]' != '[[bfloat16(0) bfloat16(0) bfloat16(0)]]'
13:00:56 - [[bfloat16(1) bfloat16(2) bfloat16(3)]]
13:00:56 ?            ^           ^           ^
13:00:56 + [[bfloat16(0) bfloat16(0) bfloat16(0)]]
13:00:56 ?            ^           ^           ^
13:00:56 
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testCasts (__main__.Bfloat16NumPyTest)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 181, in testCasts
13:00:56     self.assertTrue(np.all(x == y))
13:00:56 AssertionError: False is not true
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testLogSumExp (__main__.Bfloat16NumPyTest)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 196, in testLogSumExp
13:00:56     atol=2e-2)
13:00:56   File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\framework\test_util.py"", line 1083, in assertAllClose
13:00:56     self._assertArrayLikeAllClose(a, b, rtol=rtol, atol=atol)
13:00:56   File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\framework\test_util.py"", line 1053, in _assertArrayLikeAllClose
13:00:56     np.testing.assert_allclose(a, b, rtol=rtol, atol=atol, err_msg=msg)
13:00:56   File ""C:\Program Files\Anaconda3\lib\site-packages\numpy\testing\utils.py"", line 1411, in assert_allclose
13:00:56     verbose=verbose, header=header, equal_nan=equal_nan)
13:00:56   File ""C:\Program Files\Anaconda3\lib\site-packages\numpy\testing\utils.py"", line 796, in assert_array_compare
13:00:56     raise AssertionError(msg)
13:00:56 AssertionError: 
13:00:56 Not equal to tolerance rtol=1e-06, atol=0.02
13:00:56 None
13:00:56 (mismatch 100.0%)
13:00:56  x: array([[ 4.048587,  5.048587,  6.048587]], dtype=float32)
13:00:56  y: array([[ 0.693359,  0.693359,  0.693359]], dtype=float16)
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testAdd (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 89, in testAdd
13:00:56     self._assertFloatIdentical(1, float(bfloat16(1) + bfloat16(0)))
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 48, in _assertFloatIdentical
13:00:56     self.assertEqual(v, w)
13:00:56 AssertionError: 1 != 0.0
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testDiv (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 123, in testDiv
13:00:56     self.assertTrue(math.isnan(float(bfloat16(0) / bfloat16(0))))
13:00:56 AssertionError: False is not true
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testEqual (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 156, in testEqual
13:00:56     self.assertEqual(v == w, bfloat16(v) == bfloat16(w))
13:00:56 AssertionError: False != True
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testGreater (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 146, in testGreater
13:00:56     self.assertEqual(v > w, bfloat16(v) > bfloat16(w))
13:00:56 AssertionError: True != False
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testGreaterEqual (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 151, in testGreaterEqual
13:00:56     self.assertEqual(v >= w, bfloat16(v) >= bfloat16(w))
13:00:56 AssertionError: False != True
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testHash (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 79, in testHash
13:00:56     self.assertEqual(0x3f80, hash(bfloat16(1.0)))
13:00:56 AssertionError: 16256 != 0
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testLess (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 136, in testLess
13:00:56     self.assertEqual(v < w, bfloat16(v) < bfloat16(w))
13:00:56 AssertionError: True != False
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testLessEqual (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 141, in testLessEqual
13:00:56     self.assertEqual(v <= w, bfloat16(v) <= bfloat16(w))
13:00:56 AssertionError: False != True
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testMul (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 114, in testMul
13:00:56     self._assertFloatIdentical(-1, float(bfloat16(1) * bfloat16(-1)))
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 48, in _assertFloatIdentical
13:00:56     self.assertEqual(v, w)
13:00:56 AssertionError: -1 != 0.0
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testNegate (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 85, in testNegate
13:00:56     self._assertFloatIdentical(-v, float(-bfloat16(v)))
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 48, in _assertFloatIdentical
13:00:56     self.assertEqual(v, w)
13:00:56 AssertionError: -0.0 != 4.591774807899561e-41
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testNotEqual (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 161, in testNotEqual
13:00:56     self.assertEqual(v != w, bfloat16(v) != bfloat16(w))
13:00:56 AssertionError: True != False
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testRepr (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 69, in testRepr
13:00:56     self.assertEqual(""bfloat16(1)"", repr(bfloat16(1)))
13:00:56 AssertionError: 'bfloat16(1)' != 'bfloat16(0)'
13:00:56 - bfloat16(1)
13:00:56 ?          ^
13:00:56 + bfloat16(0)
13:00:56 ?          ^
13:00:56 
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testRoundTripToFloat (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 52, in testRoundTripToFloat
13:00:56     self._assertFloatIdentical(v, float(bfloat16(v)))
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 48, in _assertFloatIdentical
13:00:56     self.assertEqual(v, w)
13:00:56 AssertionError: 1.0 != 0.0
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testRoundTripToInt (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 56, in testRoundTripToInt
13:00:56     self.assertEqual(v, int(bfloat16(v)))
13:00:56 AssertionError: -256 != 0
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testStr (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 60, in testStr
13:00:56     self.assertEqual(""1"", str(bfloat16(1.0)))
13:00:56 AssertionError: '1' != '0'
13:00:56 - 1
13:00:56 + 0
13:00:56 
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testSub (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 101, in testSub
13:00:56     self._assertFloatIdentical(1, float(bfloat16(1) - bfloat16(0)))
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 48, in _assertFloatIdentical
13:00:56     self.assertEqual(v, w)
13:00:56 AssertionError: 1 != 0.0
13:00:56 
13:00:56 ----------------------------------------------------------------------
13:00:56 Ran 23 tests in 0.014s
13:00:56 
13:00:56 FAILED (failures=20)
13:00:56 not close where =  (array([0, 0, 0], dtype=int64), array([0, 1, 2], dtype=int64))
13:00:56 not close lhs =  [5 7 9]
13:00:56 not close rhs =  [ 0.  0.  0.]
13:00:56 not close dif =  [ 5.  7.  9.]
13:00:56 not close tol =  [  1.01327896e-06   1.01327896e-06   1.01327896e-06]
13:00:56 dtype = int32, shape = (1, 3)
13:00:56 not close where =  (array([0, 0, 0], dtype=int64), array([0, 1, 2], dtype=int64))
13:00:56 not close lhs =  [ 4.04858732  5.04858732  6.04858732]
13:00:56 not close rhs =  [ 0.69335938  0.69335938  0.69335938]
13:00:56 not close dif =  [ 3.35522795  4.35522795  5.35522795]
13:00:56 not close tol =  [ 0.02000427  0.02000427  0.02000427]
13:00:56 dtype = float32, shape = (1, 3)
13:00:56 ================================================================================
```


@gunan ",0,,2,2017-12-14T09:50:57Z,MEMBER,2017-12-14T10:55:21Z
15359,code is jammed when evaluate,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:win10
- **TensorFlow installed from (source or binary)**:pip3
- **TensorFlow version (use command below)**:1.4
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:8.0 6.46
- **GPU model and memory**:2GB
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

the code is jammed when run `eval_results = pc_classifier.evaluate(input_fn = pcd.test_input_fn_np)`
I built model under the guidance of `cnn_mnist.py` . the differences are that I change the network architecture  and using my input_functioin. Everything is normal during the training process, but it jammed during the evaluate process. 
the call stack is:
![default](https://user-images.githubusercontent.com/22407275/33982216-b1cba1e0-e0ea-11e7-85e6-68d8f91457ff.JPG)
and it jammed at the code
`    for hook in self._hooks:
      hook.after_run(
          run_context,
          session_run_hook.SessionRunValues(
              results=outputs[hook] if hook in outputs else None,
              options=options,
              run_metadata=run_metadata))`

what's the problem?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,1,2017-12-14T08:22:43Z,NONE,2017-12-14T18:59:35Z
15355,Dockerfile.devel-gpu: optimize the size of the generated image,cla: yes,"- Use `nvidia/cuda:9.0-base-ubuntu16.04` as the base image to select
  just the CUDA libraries we need.
- Remove the installed static libraries.
- Remove the dependency on openjdk-8 since Bazel ships with a local copy.
- Perform a shallow clone of the repository.

The image is 2.94GB, down from 4.87GB.

Signed-off-by: Felix Abecassis <fabecassis@nvidia.com>

See initial discussion here: https://github.com/tensorflow/tensorflow/issues/15284
@gunan @martinwicke @yongtang ",0,,6,2017-12-14T05:39:52Z,CONTRIBUTOR,2017-12-14T05:40:50Z
15353,[Go]: Make op wrapper generation more robust.,"awaiting testing (then merge),cla: yes","- Since Go 1.8, GOPATH has a default value, so handle
  that (https://golang.org/doc/go1.8#gopath)
- generate.sh expected bash (for the string substitution syntax)
  while 'sh' may point to another shell. So explicitly require bash.",0,,1,2017-12-14T02:09:33Z,MEMBER,2017-12-15T07:09:44Z
15351,questions about shared variables between CPU and GPU ,,"Dear developers:

I looked at cifar10_multi_gpu_train.py, the idea about sharing model params among CPU and GPUs is inspiring. However, I have a few questions that I want to understand well before I can apply to my own problem. 

As far as I can tell, the model params are stored in CPU by looking into the tower_loss() function since cifar10.py explicitly pinned down all variables at ""/cpu:0"". Then function train() wraps up tower_loss() with gpu device like this:

`for i in xrange(FLAGS.num_gpus):`
`       with tf.device('/gpu:%d' % i):`
`           loss = tower_loss(scope)`
`           tf.get_variable_scope().reuse_variables()`

Using this way, I bet model params are stored in CPU and there is no extra copy anywhere because it is set to just reuse the same variables in the scope, while GPU stored gradient operations written in tower_loss(). In the way, I believe the model params have to transfer from CPU to GPU whenever GPU calls for these params to operate upon. It would be inefficient if doing multiple transfer to GPU I believe. I notice ""identity"" operation in the end of tower_loss(). Is ""tf.identity(total_loss)"" doing the trick so CPU transfers model params to the GPU only once, then GPU just holds the local copy from then on?

",0,,1,2017-12-13T23:50:48Z,NONE,2017-12-13T23:58:58Z
15350,Branch 178965261,cla: yes,Fixed a minor merge conflict in tensorflow/core/platform/cloud/gcs_dns_cache.cc,0,,2,2017-12-13T23:29:02Z,MEMBER,2017-12-14T01:12:11Z
15347,Object Detection frozen graph issue,,"# Training environment 

### System information
- MAC OSX 10.13.2
- Tensorflow 1.4-rc1 (GPU support)
- Installation through source
- Python 3.6 (Anaconda)
- Bazel 0.8
- CUDA 9/ cuDNN7
- GPU 1080Ti

I have trained my own model for object detection. Extracted graph from checkpoint as well. This graph is working with the Mac GPU and CPU. It is also working with Raspberry PI but when I am trying to run it on AWS EC2 (Deep Learning AMI (Amazon Linux) and Deep Learning AMI (Ubuntu) both) instance. Built tensorflow 1.4-rc1 from source as well as installed using pip but it keep giving me following error:

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1322     try:
-> 1323       return fn(*args)
   1324     except errors.OpError as e:

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1301                                    feed_dict, fetch_list, target_list,
-> 1302                                    status, run_metadata)
   1303 

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    472             compat.as_text(c_api.TF_Message(self.status.status)),
--> 473             c_api.TF_GetCode(self.status.status))
    474     # Delete the underlying status object from memory otherwise it stays alive

InvalidArgumentError: NodeDef mentions attr 'T' not in Op<name=Where; signature=input:bool -> index:int64>; NodeDef: Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Greater). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Greater)]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-25-7493eea60222> in <module>()
     20       (boxes, scores, classes, num) = sess.run(
     21           [detection_boxes, detection_scores, detection_classes, num_detections],
---> 22           feed_dict={image_tensor: image_np_expanded})
     23       # Visualization of the results of a detection.
     24       vis_util.visualize_boxes_and_labels_on_image_array(

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    887     try:
    888       result = self._run(None, fetches, feed_dict, options_ptr,
--> 889                          run_metadata_ptr)
    890       if run_metadata:
    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1119       results = self._do_run(handle, final_targets, final_fetches,
-> 1120                              feed_dict_tensor, options, run_metadata)
   1121     else:
   1122       results = []

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1315     if handle is None:
   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1317                            options, run_metadata)
   1318     else:
   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1334         except KeyError:
   1335           pass
-> 1336       raise type(e)(node_def, op, message)
   1337 
   1338   def _extend_graph(self):

InvalidArgumentError: NodeDef mentions attr 'T' not in Op<name=Where; signature=input:bool -> index:int64>; NodeDef: Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Greater). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Greater)]]

Caused by op 'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Where', defined at:
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py"", line 3, in <module>
    app.launch_new_instance()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 478, in start
    self.io_loop.start()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 281, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 232, in dispatch_shell
    handler(stream, idents, msg)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 397, in execute_request
    user_expressions, allow_stdin)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2728, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2850, in run_ast_nodes
    if self.run_code(code, result):
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-21-0d8b8f2357e8>"", line 7, in <module>
    tf.import_graph_def(od_graph_def, name='')
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 313, in import_graph_def
    op_def=op_def)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): NodeDef mentions attr 'T' not in Op<name=Where; signature=input:bool -> index:int64>; NodeDef: Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Greater). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Greater)]]
```

When I am extracting frozen graph on EC2 instance from the same previous checkpoint it gives me much lower accuracy than my MAC and raspberryPI",0,,2,2017-12-13T19:45:53Z,NONE,2017-12-13T23:58:35Z
15345,Using wrong location for x86_64 android build,"stat:awaiting tensorflower,type:bug/performance","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
A: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
A: OSX 10.13.1
- **TensorFlow installed from (source or binary)**:
A: Source
- **TensorFlow version (use command below)**:
A: 1.4.1
- **Python version**: 
A: 2.7
- **Bazel version (if compiling from source)**:
A: 0.8
- **GCC/Compiler version (if compiling from source)**:
A:
```
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1
Apple LLVM version 9.0.0 (clang-900.0.38)
Target: x86_64-apple-darwin17.2.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
```

- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
`make -f tensorflow/contrib/makefile/Makefile TARGET=ANDROID ANDROID_ARCH=x86_64`

### Describe the problem
Android x86_64 build fails with Makefile using make -f tensorflow/contrib/makefile/Makefile TARGET=ANDROID ANDROID_ARCH=x86_64 because it cannot find the binary `x86-64-linux-android-g++`

It can be fixed by changing the `tensorflow/contrib/makefile/Makefile` at line 303 from 
`BIN_PREFIX := x86-64-linux-android` to
`BIN_PREFIX := x86_64-linux-android`
",1,,2,2017-12-13T18:38:09Z,NONE,2017-12-13T19:14:56Z
15344,Fix broken link in tensorflow lite readme,cla: yes,,0,,4,2017-12-13T18:21:36Z,CONTRIBUTOR,2017-12-13T18:24:27Z
15343,Iterator on cached tf.data Dataset cannot be reinitialized ,"stat:awaiting response,type:bug/performance","Found a likely bug when trying to use a reinitializable iterator to read from two cached datasets, one for validation and one for training. The iterator can however only be initialized once per cached dataset. Seems to me like the iterator should remove the lock file when being reinitialized, it is not in my case and that is why I get this issue. Here's a minimal example with only one cached dataset.

(basic system information below)

### Example
```python
import os

import numpy as np
import tensorflow as tf


data = np.random.rand(10, 3).astype(np.float32)
dataset = tf.data.Dataset.from_tensor_slices(data)
batches = dataset.shuffle(10).repeat().batch(5)

config = tf.ConfigProto(device_count = {'GPU': 0})
sess = tf.Session(config=config)

cache_dir = os.path.join(os.getcwd(), 'cache_dir')
try:
    os.makedirs(cache_dir)
except OSError:
    print('Cache directory already exists')

cached = batches.cache(os.path.join(cache_dir, 'cache'))
iterator = tf.data.Iterator.from_structure(output_types=tf.float32, output_shapes=(5, 3))
batch = iterator.get_next()

init1 = iterator.make_initializer(cached)
init2 = iterator.make_initializer(batches)

sess.run(init1)
sess.run(batch)
```
> array([[ 0.11960778,  0.3081578 ,  0.96522039],
       [ 0.90339011,  0.12458269,  0.30650312],
       [ 0.58160347,  0.55877644,  0.50363588],
       [ 0.2350398 ,  0.33509603,  0.4165386 ],
       [ 0.76757395,  0.50134581,  0.93601096]], dtype=float32)

```python
sess.run(init2)
sess.run(batch)
```
> array([[ 0.76757395,  0.50134581,  0.93601096],
       [ 0.2350398 ,  0.33509603,  0.4165386 ],
       [ 0.90339011,  0.12458269,  0.30650312],
       [ 0.13266359,  0.82675195,  0.26691398],
       [ 0.58160347,  0.55877644,  0.50363588]], dtype=float32)

```python
sess.run(init1)
sess.run(batch)
```
> AlreadyExistsError (see above for traceback): There appears to be a concurrent caching iterator running - cache lockfile already exists ('/home/ubuntu/ai_notebooks/notebooks/projects/deep-purple/cache_dir/cache.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1513187725
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[5,3]], output_types=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]


### Sytem information
Tensorflow version: v1.4.0-rc1-11-g130a514 1.4.0 (installed from pip)
Python version: 3.5.2
OS: Linux Ubuntu 16.04.3
CUDA: 8.0.61
cuDNN: 6",1,,9,2017-12-13T18:15:07Z,NONE,2017-12-13T18:41:41Z
15342,Windows Environment and TF 1.4.1 - Unavailable through PyPI,stat:awaiting response,"Hello dear Tensorflowers,

When running the following code `pip install tensorflow==1.4.1`, I obtain the following error:

```
Could not find a version that satisfies the requirement tensorflow==1.4.1 (from versions: 1.2.0rc2, 1.2.0, 1.2.1, 1.3.0rc0, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.4.0rc0, 1.4.0rc1, 1.4.0)
No matching distribution found for tensorflow==1.4.1
```

This error seem logical because the _wheel_ file does not exist for the windows distribution: 

- TF 1.4.1 - No Windows Compiled Library: https://pypi.python.org/pypi/tensorflow/1.4.1
- TF 1.4.0 - Windows Compiled Library is present: https://pypi.python.org/pypi/tensorflow/1.4.0

Is the support for the windows platform dropped ? Or maybe some compilation pipeline broke somewhere.

Thanks for your help,

Jonathan",0,,5,2017-12-13T16:09:02Z,CONTRIBUTOR,2017-12-14T01:32:54Z
15341,Feature Request: enable rechanging tf.device of a tensor,stat:awaiting tensorflower,"It seems that once a tensor's GPU was defined using tf.device, the GPU cannot be changed anymore. 
When loading saved graphs, the graph will use the same GPU that was chosen years ago and it cannot be set again.

thanks.",0,,8,2017-12-13T15:01:38Z,NONE,2017-12-13T19:17:37Z
15339,opencv cannot read frame from video with tensorflow,,"I am using tensorflow r1.4 and opencv3.1 in ubuntu14.04.
When I include #include <tensorflow/core/public/session.h> or 
#include ""tensorflow/cc/ops/standard_ops.h"" I cannot read images from cv::VideoCapture adn I got empty mat. When I didn't include these tensorflow headers, I can read frame successfully. Anyone could help me? Thanks a lot!!!
I noticed other issues like [#1924](https://github.com/tensorflow/tensorflow/issues/1924?from=singlemessage) [#6496](https://github.com/tensorflow/tensorflow/issues/6496) but got no idea.

Here is my cpp file:
#include <tensorflow/core/platform/env.h>
#include <tensorflow/core/public/session.h>
#include ""tensorflow/cc/ops/standard_ops.h""
#include <opencv2/opencv.hpp>
#include <iostream>
using namespace std;
using namespace tensorflow;

int main()
{
cv::VideoCapture cap;
if(!cap.open(""/home/kx/project/RM-dataset/01.avi"")){
std::cout<<""cannot open video ""<<std::endl;
}
cv::Mat frame;
while(1){
cap>>frame;
if(frame.empty()){
std::cout<<""no frame""<<std::endl;
continue;
}
cv::imshow(""frame"",frame);
cv::waitKey(0);
}
return 0;
}

my cmake file:

cmake_minimum_required (VERSION 2.8)
project (tf_example)

set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -g -std=c++11 -W"")
find_package(OpenCV 3.1.0 REQUIRED)
include_directories(
/home/kx/something/tensorflow-r1.4
/home/kx/something/tensorflow-r1.4/tensorflow/bazel-genfiles
/home/kx/something/tensorflow-r1.4/tensorflow/contrib/makefile/gen/protobuf/include
/home/kx/something/tensorflow-r1.4/tensorflow/contrib/makefile/gen/host_obj
/home/kx/something/tensorflow-r1.4/tensorflow/contrib/makefile/gen/proto
/home/kx/something/tensorflow-r1.4/tensorflow/contrib/makefile/downloads/nsync/public
/home/kx/something/tensorflow-r1.4/tensorflow/contrib/makefile/downloads/eigen
/home/kx/something/tensorflow-r1.4/bazel-out/local-py3-opt/genfiles
${OPENCV_INCLUDE_DIRS}
)

add_executable(tf_test tf_test.cpp)
target_link_libraries(tf_test
/home/kx/something/tensorflow-r1.4/bazel-bin/tensorflow/libtensorflow_cc.so
/home/kx/something/tensorflow-r1.4/bazel-bin/tensorflow/libtensorflow_framework.so
${OpenCV_LIBS}
)

The results:
no frame

",0,,2,2017-12-13T13:15:26Z,NONE,2017-12-13T19:18:09Z
15335,[XLA/tfcompile] Implement mkstemps for MSVC,"awaiting review,cla: yes","`mkstemps` used in `SaveGraph` is not available on Windows.

Implementation adapted from https://github.com/git-for-windows/git/blob/master/wrapper.c#L470.",0,,8,2017-12-13T08:28:16Z,CONTRIBUTOR,2017-12-18T03:22:44Z
15334,Quantized graph on ssd mobilenet fails with InvalidArgumentError,stat:awaiting response,"I am using ssd_mobilenet_v1_coco_2017_11_17 and quantized it using following command:

bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
  --in_graph=ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb \
  --out_graph=ssd_mobilenet_v1_coco_2017_11_17/frozen_quant.pb \
  --inputs='image_tensor' \
  --outputs='detection_boxes,detection_scores,detection_classes' \
  --transforms='add_default_attributes strip_unused_nodes(type=float, shape=""1,299,299,3"")
    remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true)
    fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes
    strip_unused_nodes sort_by_execution_order'

I am using tensorflow v 1.4.1 for detecting bounding box and it throws following error:

InvalidArgumentError: The node 'Preprocessor/map/while/ResizeImage/ResizeBilinear/eightbit' has inputs from different frames. The input 'Preprocessor/map/while/ResizeImage/size' is in frame 'Preprocessor/map/while/while_context'. The input 'Preprocessor/map/while/ResizeImage/ResizeBilinear_eightbit/Preprocessor/map/while/ResizeImage/ExpandDims/quantize' is in frame ''.",0,,3,2017-12-13T08:24:37Z,NONE,2017-12-13T19:02:03Z
15331,Graph building and optimization is really slow,,"I'm using tensorflow 1.4 and tflearn 0.3.2. The os of my machine is Win 8.1. Apparently, the graph building and cost optimization in my code is real slow which I think is a bug in Tensorflow (either that or my machine is just real slow). Here's the code:

```
class Model(object):
    def __init__(self):
        self.num_classes = 80
        self.num_time_steps = 1596
        self.input_dimension = 48
        self.inputs = network_utils.input_data([None, self.num_time_steps, self.input_dimension], name=""input"")
        self.labels = network_utils.sparse_input_data()
        self.seq_lens = network_utils.input_data([None], name=""seq_len"", input_type=network_utils.get_type('int32'))
        self.learning_rate = 0.01

    def _inference(self):
        model = network_utils.bidirectional_lstm(self.inputs, 50, return_seq=True)
        model = network_utils.bidirectional_lstm(model, 100, return_seq=True)
        model = network_utils.bidirectional_lstm(model, 200)
        logits = network_utils.get_time_major(model, self.num_classes, network_utils.get_shape(self.inputs)[0], 200)
        return logits

    def loss(self):
        y_predict = self._inference()
        loss = network_utils.ctc_loss(predictions=y_predict, labels=self.labels, sequence_length=self.seq_lens)
        cost = network_utils.cost(loss)
        decoded = network_utils.decode(inputs=y_predict, sequence_length=self.seq_lens)
        label_error_rate = network_utils.label_error_rate(y_pred=decoded[0], y_true=self.labels)
        return loss, label_error_rate, cost

    def optimize(self, cost, optimizer):
        return network_utils.optimize(loss=cost, optimizer=optimizer, learning_rate=self.learning_rate)
```

```
import tensorflow as tf
import tflearn
from tflearn import bidirectional_rnn, BasicLSTMCell

from optimizer_enum import Optimizers

def ctc_loss(predictions, labels, sequence_length,
             preprocess_collapse_repeated_labels=True,
             ctc_merge_repeated=True,
             inputs_are_time_major=True):
    return tf.nn.ctc_loss(inputs=predictions, labels=labels, sequence_length=sequence_length,
                          preprocess_collapse_repeated=preprocess_collapse_repeated_labels,
                          ctc_merge_repeated=ctc_merge_repeated,
                          time_major=inputs_are_time_major)

def input_data(shape, name: str = 'InputData', input_type=tf.float32):
    return tflearn.input_data(shape=shape, dtype=input_type, name=name)

def reshape(tensor: tf.Tensor, new_shape: list):
    return tf.reshape(tensor, new_shape, name=""reshape"")

def bidirectional_lstm(inputs, num_hidden: int, return_seq=False):
    return bidirectional_rnn(inputs, BasicLSTMCell(num_hidden), BasicLSTMCell(num_hidden), return_seq=return_seq)


def decode(inputs, sequence_length, merge_repeated=True):
    decoded, _ = tf.nn.ctc_beam_search_decoder(inputs, sequence_length, merge_repeated)
    return decoded

def label_error_rate(y_pred, y_true):
    return tf.reduce_mean(tf.edit_distance(tf.cast(y_pred, tf.int32), y_true))

def optimize(loss, optimizer, learning_rate):
    if optimizer == Optimizers.MOMENTUM:
        return tf.train.MomentumOptimizer(learning_rate, momentum=0.9).minimize(loss)
    if optimizer == Optimizers.ADAM:
        return tf.train.AdamOptimizer(learning_rate).minimize(loss)
    if optimizer == Optimizers.ADADELTA:
        return tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)
    if optimizer == Optimizers.RMSPROP:
        return tf.train.RMSPropOptimizer(learning_rate).minimize(loss)
    raise NotImplementedError(""{} is not implemented."".format(optimizer))

def sparse_input_data(input_type=tf.int32):
    return tf.sparse_placeholder(input_type)

def get_time_major(model, num_classes, batch_size, num_hidden_units):
    outputs = reshape(model, [-1, num_hidden_units])

    W = tf.Variable(tf.truncated_normal([num_hidden_units,
                                         num_classes],
                                        stddev=0.1, dtype=tf.float32), name='W')
    b = tf.Variable(tf.constant(0., dtype=tf.float32, shape=[num_classes], name='b'))

    logits = tf.matmul(outputs, W) + b
    logits = tf.reshape(logits, [batch_size, -1, num_classes])
    logits = tf.transpose(logits, (1, 0, 2))
    return logits

def cost(loss):
    return tf.reduce_mean(loss)


def get_type(type_str):
    if type_str == 'int32':
        return tf.int32
    return tf.float32


def get_shape(tensor):
    return tf.shape(tensor)
```

Can anyone help me address this issue? To reproduce it, you can run main/train.py in this [repository](https://github.com/selcouthlyBlue/simplified_bi_lstm_ocr)",0,,1,2017-12-13T05:26:31Z,CONTRIBUTOR,2017-12-13T06:50:06Z
15330,Fix issues in doc `tf.Placeholder` should be `tf.placeholder`,cla: yes,"This fix fixes issues in the doc (data_feeder.py) where `tf.Placeholder` should be `tf.placeholder`
",0,,1,2017-12-13T03:47:59Z,MEMBER,2017-12-13T16:49:30Z
15327,Feature Request: Easy way to predict after training model with Estimator and Dataset API,,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Windows 10
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: Python 3.5.2 :: Anaconda custom (64-bit)
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None

### Describe the problem

I have beed trained a image classification cnn model with the Estimator and Dataset(`tf.data.TFRecordsDataset`) API. The relative model files have bee saved in `model_dir`. The last few days I try very hard to figure out how to predict one or more images label using the saved model files. 

However I failed and don't know what to do. I can't find relative contents in the official doc. So adding an easy method to do this may be a good idea. Or is there another solution I missed?

FYI, my training code is [here](https://github.com/secsilm/understaing-datasets-estimators-tfrecords/blob/master/cifar10-estimator-dataset.py).",0,,5,2017-12-13T01:45:45Z,CONTRIBUTOR,2017-12-13T07:49:45Z
15325,Minor documentation mistake,,"https://www.tensorflow.org/programmers_guide/tensors
Under the ""Getting a tf.Tensor object's rank"" subheading
`r = tf.rank(my3d)` should be `r = tf.rank(my_image)`",1,,1,2017-12-12T23:22:56Z,NONE,2017-12-13T00:02:05Z
15324,Make GANEstimator global_step_inc dependent on gen and dis losses,"awaiting review,cla: yes","This solves a non-deterministic problem that prevents to use `global_step` in `tf.cond` during training.
See also https://github.com/tensorflow/tensorflow/issues/15271#issuecomment-351212183",0,,8,2017-12-12T22:40:46Z,CONTRIBUTOR,2017-12-13T12:38:18Z
15323,Beam Search Decoder API,type:feature,"@ebrevdo Why is it that the user needs to call `tile_batch` explicitly for beam search decoders when using attention models? Couldn't the beam search decoder internally tile the provided `initial_state` in its constructor? It seems that this API is prone to wrong usage so I'm trying to understand why it's necessary.

Thank you!",0,,5,2017-12-12T21:45:03Z,CONTRIBUTOR,2018-01-03T07:41:34Z
15320,Multi-core CPU performance dropped for MKL TF build,type:bug/performance,"### System information

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS (64-bit)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: Tensorflow r1.4
- **Python version**: Python version: 2.7.12
- **Bazel version (if compiling from source)**: Bazel release 0.7.0
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
- **CUDA/cuDNN version**: no CUDA
- **GPU model and memory**: no GPU, but i7-6850K with 32Gb ddr4
- **Exact command to reproduce**: run the script below

Tested on two machines:
1) i7-6850K with 32Gb ddr4
2) two Xeon x5650 with 24Gb ddr3

### Describe the problem
When I build Tensorflow with MKL it dropped CPU performance in a strange way. Performance of individual core is much higher, but for multicore is much worse.
It's a big epic bottleneck for my project and I can't solve it by myself. I will appreciate any help!

1) TF installation from sources with MKL support
> Tensorflow r1.4 installed from source. Configured with jemalloc as malloc support and other configure settings ignored.
> $ bazel build --config=mkl -c opt //tensorflow/tools/pip_package:build_pip_package
> $ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
> $ pip install /tmp/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27mu-linux_x86_64.whl

**Run tests:** 
one core: 0.03s
all cores: 0.12s

2) TF installation with pip
> $ pip install tensorflow
> (tensorflow-1.4.1-cp27-cp27mu-manylinux1_x86_64.whl installed)

**Run tests:**
one core: 0.16s
all cores: 0.03s


### Source code / logs

```
    import time
    import numpy as np
    import tensorflow as tf

    from tensorflow.contrib import slim
    from tensorflow.contrib.slim.python.slim.nets.inception_v1 import inception_v1, inception_v1_arg_scope


    input_shape = (1, 224, 224, 3)
    features = tf.placeholder(tf.float32, input_shape)

    with slim.arg_scope(inception_v1_arg_scope()):
        predictions, end_points = inception_v1(features, is_training=False)

    # remove to utilize all cores
    session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,
                                  inter_op_parallelism_threads=1)

    with tf.Session(config=session_conf) as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())

        images = np.random.random(input_shape)
        consumption = []
        for i in range(10):
            tick = time.time()
            sess.run(predictions, feed_dict={features: images})
            consumption.append(time.time() - tick)

        print np.mean(consumption)
```",1,,13,2017-12-12T19:43:31Z,NONE,2017-12-13T07:30:56Z
15319,Cannot import tensorflow after installing tensorflow-gpu - Windows,,"I am running windows 10 64bit. 
Tensorflow-gpu version - 1.4.0
CUDA version - 8.0
cuDNN - v6.0

I installed it using the cmd command: `pip install tensorflow-gpu ` and do not have any other version of tensorflow installed. now when I try to execute 

```
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
```
I get the error `ModuleNotFoundError: No module named 'tensorflow'`

I am hoping getting this working will solve my bigger issue of tensorflow not recognizing my gpu for gpu processing. ",0,,2,2017-12-12T18:58:42Z,NONE,2017-12-12T22:07:55Z
15318,tensorflow multigpu with dataset api is not converging,,"
I am trying to train a resNet50  on multi_gpus,
I have used 
[https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py)](url)



and I have added tensorflow Dataset input pipeline to the link code
But after some steps loss is some value and does not changes any more!(value =0.693147 always)

I have tested this network with dataset API on single gpu,and worked fine.
Im sure something is wrong with my data feeding,because when I feed a simple random nd_array,it converges!!!!!

I have tested my tfrecord file,it was OK .
but I dont know the problem.

I use
ubuntu 16.04,

tensorflow 1.4,

python 2.7.12,

gtx 1080 Gpus

this is all my train code:





[github.txt](https://github.com/tensorflow/tensorflow/files/1552082/github.txt)
",0,,4,2017-12-12T15:23:58Z,NONE,2017-12-13T01:25:07Z
15316,Exclude tests from contrib_py,cla: yes,@gunan @mrry Don't you think?,0,,2,2017-12-12T14:05:53Z,CONTRIBUTOR,2017-12-12T15:54:44Z
15315,Refactor methods for path calculation,"awaiting testing (then merge),cla: yes","This allows other scripts to access this logic.
@gunan This is what I meant.",0,,38,2017-12-12T13:56:10Z,CONTRIBUTOR,2017-12-13T16:32:56Z
15314,tensorflow.python.framework.errors_impl.NotFoundError: Can not get size for:,,"D:\Python\Python35\models-master\research\object_detection>D:\Python\Python35\python train.py --logtostderr --train_dir=training\ --pipline_config_path=training\ssd_mobilenet_v1_pets.config
Traceback (most recent call last):
  File ""train.py"", line 163, in <module>
    tf.app.run()
  File ""C:\Users\USER\AppData\Roaming\Python\Python35\site-packages\tensorflow\python\platform\app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""train.py"", line 106, in main
    overwrite=True)
  File ""C:\Users\USER\AppData\Roaming\Python\Python35\site-packages\tensorflow\python\lib\io\file_io.py"", line 384, in copy
    compat.as_bytes(oldpath), compat.as_bytes(newpath), overwrite, status)
  File ""D:\Python\Python35\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""C:\Users\USER\AppData\Roaming\Python\Python35\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Can not get size for:  : path d\udc92acc\udce8s sp\udce9cifi\udce9 not found",0,,8,2017-12-12T13:43:35Z,NONE,2017-12-13T01:26:34Z
15312,iOS build_all_ios_ssd.sh 'double-conversion/double-conversion.h' file not found,stat:awaiting response,"iOS `build_all_ios_ssd.sh` `'double-conversion/double-conversion.h' file not found`

```

export TF_ROOT=$(~/tensorflow-master)


cd ~/tensorflow_ios_detector/config
bash config.sh

export TF_ROOT=~/tensorflow-master
cd $TF_ROOT
tensorflow/contrib/makefile/build_all_ios_ssd.sh

gcc --std=c++11 -I. -I/Users/admin/tensorflow-master/tensorflow/contrib/makefile/downloads/ -I/Users/admin/tensorflow-master/tensorflow/contrib/makefile/downloads/eigen -I/Users/admin/tensorflow-master/tensorflow/contrib/makefile/downloads/gemmlowp -I/Users/admin/tensorflow-master/tensorflow/contrib/makefile/downloads/nsync/public -I/Users/admin/tensorflow-master/tensorflow/contrib/makefile/downloads/fft2d -I/Users/admin/tensorflow-master/tensorflow/contrib/makefile/gen/host_obj/ -I/Users/admin/tensorflow-master/tensorflow/contrib/makefile/gen/protobuf-host/include -I/usr/local/include -c tensorflow/core/lib/random/simple_philox.cc -o /Users/admin/tensorflow-master/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/lib/random/simple_philox.o
tensorflow/core/lib/strings/numbers.cc:26:10: fatal error: 
      'double-conversion/double-conversion.h' file not found
#include ""double-conversion/double-conversion.h""
         ^
1 error generated.
make: *** [/Users/admin/tensorflow-master/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/lib/strings/numbers.o] Error 1
make: *** Waiting for unfinished jobs....
+ '[' 2 -ne 0 ']'
+ echo 'arm64 compilation failed.'
arm64 compilation failed.
+ exit 1
Mac-Admin:tensorflow-master admin$ 

```
but script downloaded `double-conversion` in `downloads` folder located at `tensorflow/contrib/makefile`
",0,,3,2017-12-12T13:33:18Z,NONE,2017-12-13T01:35:59Z
15311,Support python3.6 Linux,,Python3.6 support Linux,0,,2,2017-12-12T13:02:00Z,CONTRIBUTOR,2017-12-12T13:26:57Z
15310,[XLA/tfcompile] Various fixes for MSVC Part 1,cla: yes,"This is the initial work to solve #15213. More changes will come in other PRs.

- `tensorflow/compiler/aot/tests/make_test_graphs.py`: Use `os.path` for path manipulation.

- `tensorflow/compiler/xla/array.h`: Explicitly include `<numeric>` for `std::accumulate`.

- `tensorflow/compiler/xla/service/cpu/external_constant_pool.{cc,h}`: Use `tensorflow::port::Aligned{Malloc,Free}`.

- `tensorflow/compiler/xla/service/cpu/llvm_ir_runtime.cc`: Fix `std::array` initialization.

- `tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc`: Remove unused `<dlfcn.h>` and polyfill `sincos[f]` for MSVC.

- `tensorflow/compiler/xla/service/heap_simulator.h`: Add `const` to custom comparator.

- `tensorflow/compiler/xla/status_macros.h`: Use simplified version of `TF_ASSIGN_OR_RETURN` for MSVC. The simplified version also works for GCC as well actually.

- `tensorflow/compiler/xla/util.h`: Hide the workaround for GCC 7 from MSVC's eyes.

- `tensorflow/core/lib/gtl/compactptrset.h`: Define `ssize_t` for MSVC.

- `tensorflow/core/platform/macros.h`: Define `LANG_CXX11` for >= VS 2015. In MSVC, `__cplusplus == 199711` even when `/std:c++latest` is set because MSVC is still not ""fully"" C++11 compliant.",0,,3,2017-12-12T12:39:35Z,CONTRIBUTOR,2017-12-13T10:05:20Z
15309,Correct channels_first format in 1d_pooling for tf.layers,"awaiting testing (then merge),cla: yes","The code was equivalent in the case of self.data_format == 'channels_last' or self.data_format == 'channels_first'.
My modifications fix it when data_format == 'channels_first'.",1,,11,2017-12-12T12:10:03Z,NONE,2018-01-24T13:17:45Z
15308,improve compute high rank hessians,"awaiting testing (then merge),cla: yes",fix possible compute high rank hessians,0,,13,2017-12-12T12:03:42Z,CONTRIBUTOR,2017-12-12T12:19:52Z
15307,Setting proper sonames on Linux,"awaiting testing (then merge),cla: yes","Setting proper soname prevents from linking with absolute path when using `cmake`.

The reference below contains long conversation about the issue with linking, but the general idea is when a library has no `DT_SONAME` field, executable is linked with absolute path and can't be used in a different environment, setting `LD_LIBRARY_PATH` doesn't help.

http://cmake.3232098.n2.nabble.com/How-to-avoid-the-explicit-library-location-when-linking-with-imported-library-targets-td5542269.html",1,,6,2017-12-12T11:33:48Z,CONTRIBUTOR,2017-12-26T01:58:32Z
15306,GO Tests Fail for Tensorflow 1.4.0 but example code works on amd64 and arm64,stat:awaiting response,"### System information
- ~**Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**~:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04 on amd64 and arm64
- **TensorFlow installed from (source or binary)**: amd64: binary (https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.4.0.tar.gz); arch64: source (d752244f) `//tensorflow:libtensorflow.so`
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: python3.5
- **Bazel version (if compiling from source)**:  arm64: 0.7.0
- **GCC/Compiler version (if compiling from source)**: arm64: 5.4.0-6ubuntu1~16.04.5
- **CUDA/cuDNN version**: amd64: cuda-8.0 | libcudnn.so.6
- **GPU model and memory**: amd64: GeForce GTX 1060 6071MiB
- **Exact command to reproduce**: `go test github.com/tensorflow/tensorflow/tensorflow/go`

### Describe the problem
Test fail but [example code](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go#example-package) work on both platforms. 

### Source code / logs
amd64:
```
2017-12-12 11:37:29.916413: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2
2017-12-12 11:37:29.918709: F tensorflow/core/framework/tensor.cc:822] Unexpected type: 23
SIGABRT: abort
PC=0x7f9b403b4428 m=4 sigcode=18446744073709551610
signal arrived during cgo execution

goroutine 29 [syscall, locked to thread]:
runtime.cgocall(0x657730, 0xc4200479d0, 0xc4200479f8)
	/usr/local/go/src/runtime/cgocall.go:132 +0xe4 fp=0xc4200479a0 sp=0xc420047960 pc=0x405574
github.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_SetAttrTensor(0x7f9b0800c380, 0x7f9b0800e190, 0x7f9b0800c840, 0x7f9b0800dc80)
	github.com/tensorflow/tensorflow/tensorflow/go/_test/_obj_test/_cgo_gotypes.go:919 +0x45 fp=0xc4200479d0 sp=0xc4200479a0 pc=0x52d725
github.com/tensorflow/tensorflow/tensorflow/go.setAttr.func18(0x7f9b0800c380, 0x7f9b0800e190, 0x7f9b0800c840, 0x7f9b0800dc80)
	/home/meldron/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:306 +0xec fp=0xc420047a08 sp=0xc4200479d0 pc=0x539cdc
github.com/tensorflow/tensorflow/tensorflow/go.setAttr(0x7f9b0800c380, 0xc42000e0c0, 0x6f01be, 0x5, 0x6b70c0, 0xc4200ec4c0, 0x0, 0x0)
	/home/meldron/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:306 +0x10f1 fp=0xc420047c00 sp=0xc420047a08 pc=0x530481
github.com/tensorflow/tensorflow/tensorflow/go.(*Graph).AddOperation(0xc42000e080, 0x6f0024, 0x5, 0xc42050a3b8, 0x6, 0x0, 0x0, 0x0, 0xc420080f00, 0x4b393b, ...)
	/home/meldron/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:209 +0x4a0 fp=0xc420047d60 sp=0xc420047c00 pc=0x52f190
github.com/tensorflow/tensorflow/tensorflow/go.Const(0xc42000e080, 0xc42050a3b8, 0x6, 0x683760, 0xc4200ec300, 0xc42050a3b8, 0x6, 0x4d499d, 0x7abe18)
	/home/meldron/go/src/github.com/tensorflow/tensorflow/tensorflow/go/util_test.go:38 +0x221 fp=0xc420047e38 sp=0xc420047d60 pc=0x52a781
github.com/tensorflow/tensorflow/tensorflow/go.TestOutputDataTypeAndShape.func1(0xc420102780)
	/home/meldron/go/src/github.com/tensorflow/tensorflow/tensorflow/go/operation_test.go:137 +0x11e fp=0xc420047fa8 sp=0xc420047e38 pc=0x53708e
testing.tRunner(0xc420102780, 0xc4200ec480)
	/usr/local/go/src/testing/testing.go:746 +0xd0 fp=0xc420047fd0 sp=0xc420047fa8 pc=0x4d4a40
runtime.goexit()
	/usr/local/go/src/runtime/asm_amd64.s:2337 +0x1 fp=0xc420047fd8 sp=0xc420047fd0 pc=0x45fa31
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:789 +0x2de

goroutine 1 [chan receive]:
testing.(*T).Run(0xc420102000, 0x6f62b2, 0x1a, 0x703770, 0x47b401)
	/usr/local/go/src/testing/testing.go:790 +0x2fc
testing.runTests.func1(0xc420102000)
	/usr/local/go/src/testing/testing.go:1004 +0x64
testing.tRunner(0xc420102000, 0xc420057de0)
	/usr/local/go/src/testing/testing.go:746 +0xd0
testing.runTests(0xc4200ec220, 0xa423e0, 0x11, 0x11, 0xc420057e78)
	/usr/local/go/src/testing/testing.go:1002 +0x2d8
testing.(*M).Run(0xc420057f18, 0xc420057f70)
	/usr/local/go/src/testing/testing.go:921 +0x111
main.main()
	github.com/tensorflow/tensorflow/tensorflow/go/_test/_testmain.go:84 +0xdb

goroutine 25 [chan receive]:
testing.(*T).Run(0xc4201023c0, 0xc4200145a0, 0x13, 0xc4200ec480, 0x2)
	/usr/local/go/src/testing/testing.go:790 +0x2fc
github.com/tensorflow/tensorflow/tensorflow/go.TestOutputDataTypeAndShape(0xc4201023c0)
	/home/meldron/go/src/github.com/tensorflow/tensorflow/tensorflow/go/operation_test.go:136 +0x56e
testing.tRunner(0xc4201023c0, 0x703770)
	/usr/local/go/src/testing/testing.go:746 +0xd0
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:789 +0x2de

rax    0x0
rbx    0x7f9b1fbfca30
rcx    0x7f9b403b4428
rdx    0x6
rdi    0x20a0
rsi    0x20a3
rbp    0x7f9b1fbfca20
rsp    0x7f9b1fbfc8e8
r8     0x7f9b0800eb80
r9     0x0
r10    0x8
r11    0x206
r12    0x7f9b1fbfcc50
r13    0x17
r14    0x5
r15    0x7f9b1fbfcc50
rip    0x7f9b403b4428
rflags 0x206
cs     0x33
fs     0x0
gs     0x0
FAIL	github.com/tensorflow/tensorflow/tensorflow/go	0.059s
```

arm64:
```
2017-12-12 11:40:20.357262: F tensorflow/core/framework/tensor.cc:822] Unexpected type: 23
SIGABRT: abort
PC=0x7f7a686528 m=0 sigcode=18446744073709551610
signal arrived during cgo execution

goroutine 56 [syscall, locked to thread]:
runtime.cgocall(0x5e9a48, 0x442003d9d8, 0x29)
	/usr/local/go/src/runtime/cgocall.go:132 +0xa0 fp=0x442003d9a0 sp=0x442003d960 pc=0x405280
github.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_SetAttrTensor(0xa2bac10, 0xa24bfa0, 0x9fcbb50, 0xa289620)
	github.com/tensorflow/tensorflow/tensorflow/go/_test/_obj_test/_cgo_gotypes.go:919 +0x38 fp=0x442003d9d0 sp=0x442003d9a0 pc=0x508468
github.com/tensorflow/tensorflow/tensorflow/go.setAttr.func18(0xa2bac10, 0xa24bfa0, 0x9fcbb50, 0xa289620)
	/home/rock64/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:306 +0xa4 fp=0x442003da00 sp=0x442003d9d0 pc=0x511e04
github.com/tensorflow/tensorflow/tensorflow/go.setAttr(0xa2bac10, 0x442008a0c0, 0x6809f9, 0x5, 0x648200, 0x4420106480, 0x0, 0x0)
	/home/rock64/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:306 +0xd7c fp=0x442003dc00 sp=0x442003da00 pc=0x50aa3c
github.com/tensorflow/tensorflow/tensorflow/go.(*Graph).AddOperation(0x442008a088, 0x680864, 0x5, 0x4420086728, 0x6, 0x0, 0x0, 0x0, 0x442007ef00, 0x4a1f7c, ...)
	/home/rock64/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:209 +0x3b8 fp=0x442003dd60 sp=0x442003dc00 pc=0x509b38
github.com/tensorflow/tensorflow/tensorflow/go.Const(0x442008a088, 0x4420086728, 0x6, 0x615100, 0x44201062c0, 0x4420086728, 0x6, 0x445a01, 0x80)
	/home/rock64/go/src/github.com/tensorflow/tensorflow/tensorflow/go/util_test.go:38 +0x190 fp=0x442003de30 sp=0x442003dd60 pc=0x505a90
github.com/tensorflow/tensorflow/tensorflow/go.TestOutputDataTypeAndShape.func1(0x442011e780)
	/home/rock64/go/src/github.com/tensorflow/tensorflow/tensorflow/go/operation_test.go:137 +0xc4 fp=0x442003dfa0 sp=0x442003de30 pc=0x50fd44
testing.tRunner(0x442011e780, 0x4420106440)
	/usr/local/go/src/testing/testing.go:746 +0xb0 fp=0x442003dfc0 sp=0x442003dfa0 pc=0x4bd240
runtime.goexit()
	/usr/local/go/src/runtime/asm_arm64.s:931 +0x4 fp=0x442003dfc0 sp=0x442003dfc0 pc=0x456a44
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:789 +0x244

goroutine 1 [chan receive]:
testing.(*T).Run(0x442011e000, 0x686c26, 0x1a, 0x694188, 0x5a2fb201)
	/usr/local/go/src/testing/testing.go:790 +0x258
testing.runTests.func1(0x442011e000)
	/usr/local/go/src/testing/testing.go:1004 +0x54
testing.tRunner(0x442011e000, 0x4420051dd0)
	/usr/local/go/src/testing/testing.go:746 +0xb0
testing.runTests(0x44201061c0, 0x7d8300, 0x11, 0x11, 0x3e7)
	/usr/local/go/src/testing/testing.go:1002 +0x280
testing.(*M).Run(0x4420051f18, 0x42d01c)
	/usr/local/go/src/testing/testing.go:921 +0xf0
main.main()
	github.com/tensorflow/tensorflow/tensorflow/go/_test/_testmain.go:84 +0xd0

goroutine 52 [chan receive]:
testing.(*T).Run(0x442011e3c0, 0x44200c02c0, 0x13, 0x4420106440, 0x2)
	/usr/local/go/src/testing/testing.go:790 +0x258
github.com/tensorflow/tensorflow/tensorflow/go.TestOutputDataTypeAndShape(0x442011e3c0)
	/home/rock64/go/src/github.com/tensorflow/tensorflow/tensorflow/go/operation_test.go:136 +0x540
testing.tRunner(0x442011e3c0, 0x694188)
	/usr/local/go/src/testing/testing.go:746 +0xb0
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:789 +0x244

r0      0x0
r1      0x363f
r2      0x6
r3      0x7f7c729000
r4      0x363f
r5      0x7f7c7296f0
r6      0x0
r7      0x0
r8      0x83
r9      0x9fa59e0
r10     0x7fdb6f6760
r11     0x7fdb6f6760
r12     0xa3d70a3d70a3d70b
r13     0x7fdb6f6706
r14     0x0
r15     0x1db
r16     0x7f7a639120
r17     0x7f7a687830
r18     0x14
r19     0x7f7a797000
r20     0x7f7c729000
r21     0x7f7a7979d8
r22     0xa322700
r23     0x5
r24     0xa2bac10
r25     0x0
r26     0x6941e0
r27     0x10
r28     0x80b700
r29     0x7fdb6f66e0
lr      0x7f7a6879e0
sp      0x7fdb6f66e0
pc      0x7f7a686528
fault   0x0
FAIL	github.com/tensorflow/tensorflow/tensorflow/go	0.261s
```",0,,3,2017-12-12T10:41:30Z,NONE,2017-12-13T01:58:34Z
15303,Fix initialization of tf.contrib.layers.spatial_softmax temperature,"awaiting testing (then merge),cla: yes",It's now possible to initialize a trainable `temperature` with values other than 1,0,,3,2017-12-12T09:07:58Z,CONTRIBUTOR,2017-12-29T01:02:33Z
15302,Fix #15297: bfloat16 is unsigned on Windows,cla: yes,"#15297 

__BYTE_ORDER__ is not a builtin macro in VC++.  Need to include ""tensorflow/core/platform/cpu_info.h"" before using it. ",0,,8,2017-12-12T08:05:53Z,CONTRIBUTOR,2017-12-12T08:35:54Z
15301,"Removing extra ""d"" after close() method in SessionTest.java","awaiting testing (then merge),cla: yes",,0,,7,2017-12-12T07:57:21Z,CONTRIBUTOR,2017-12-12T07:57:42Z
15300,Documentation - Fixing 'if' spelling,cla: yes,Fixing 'if' spelling,0,,5,2017-12-12T07:30:03Z,CONTRIBUTOR,2017-12-12T07:34:08Z
15299,Model Average Optimizer,"awaiting testing (then merge),cla: yes",I have implemented model average optimizer and open a new pr since the [original one](https://github.com/tensorflow/tensorflow/pull/11581) is closed. ,0,,11,2017-12-12T07:26:09Z,CONTRIBUTOR,2017-12-12T07:30:50Z
15295,Add name scope to tf.image,"awaiting testing (then merge),cla: yes","This PR fixes #1560 
I searched through the tf.image API list and found the following APIs need to add name scope

> crop_to_bounding_box
> pad_to_bounding_box
> flip_left_right
> random_flip_left_right
> flip_up_down
> random_flip_up_down
> per_image_standardization
> central_crop
> resize_images
> resize_image_with_crop_or_pad
> transpose_image

@martinwicke Thanks for quick reply before and could you please take a look?

- [x] Add name scope to above functions
- [x] Add test case(s)

RELNOTES: Add name scopes to tf.image functions.",0,,6,2017-12-12T05:32:02Z,CONTRIBUTOR,2017-12-26T17:00:05Z
15291,Dockerfile.devel-gpu: infinite prompt loop,type:build/install,"```sh
$ docker build -f Dockerfile.devel-gpu github.com/tensorflow/tensorflow.git#master:tensorflow/tools/docker
[...]
Step 19/22 : RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 &&     LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs:${LD_LIBRARY_PATH}     tensorflow/tools/ci_build/builds/configured GPU     bazel build -c opt --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""         tensorflow/tools/pip_package:build_pip_package &&     rm /usr/local/cuda/lib64/stubs/libcuda.so.1 &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip --no-cache-dir install --upgrade /tmp/pip/tensorflow-*.whl &&     rm -rf /tmp/pip &&     rm -rf /root/.cache
 ---> Running in 3e8560e2d441
/tensorflow /tensorflow
INFO: Reading 'startup' options from /etc/bazel.bazelrc: --batch
Extracting Bazel installation...
You have bazel 0.5.4 installed.
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]
Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL support? [y/N]: No OpenCL support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 

Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

Invalid path to CUDA 8.0 toolkit. /usr/local/cuda/lib64/libcudart.so.8.0 cannot be found
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 

Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

Invalid path to CUDA 8.0 toolkit. /usr/local/cuda/lib64/libcudart.so.8.0 cannot be found
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 

Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

Invalid path to CUDA 8.0 toolkit. /usr/local/cuda/lib64/libcudart.so.8.0 cannot be found
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 

Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

Invalid path to CUDA 8.0 toolkit. /usr/local/cuda/lib64/libcudart.so.8.0 cannot be found
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 

Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

Invalid path to CUDA 8.0 toolkit. /usr/local/cuda/lib64/libcudart.so.8.0 cannot be found
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 

Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

Invalid path to CUDA 8.0 toolkit. /usr/local/cuda/lib64/libcudart.so.8.0 cannot be found
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 

Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

[... ad infinitum ... ]
```

Is it because the `1.4` branch doesn't support CUDA 9.0?
https://github.com/tensorflow/tensorflow/blob/abd5375ba8d373045321d1eebdb4501c36ab0ccd/tensorflow/tools/docker/Dockerfile.devel-gpu#L74-L76

@gunan",0,,7,2017-12-12T02:16:44Z,CONTRIBUTOR,2017-12-12T06:51:24Z
15288,Eager: eager mode considerably slower than standard TensorFlow for large matrix multiplications ,"comp:eager,stat:awaiting response","We have been benchmarking eager mode versus standard TensorFlow for large square matrix multiplications, specifically the time to run

m = tf.matmul(A, B) (in eager mode) 

versus 

m = sess.run(self.c, feed_dict={self.A:A, self.B:B})

in non-eager mode. 

We find that while runtimes are comparable for small matrices, eager mode is considerably slower for repeated multiplications of large matrices (eg, of dimension 15,000). The first multiplication is fast, but subsequent multiplications take much longer, even after resetting the computation graph. Is this expected behavior? We are running everything on a GPU. 
",0,,8,2017-12-12T01:46:28Z,NONE,2017-12-13T01:33:59Z
15287,Use base_dtype for self._dtype in tf.layers,cla: yes,"This avoids mismatch dtype (ref vs no_ref) when using variables as inputs to a layer.
See #15262",0,,3,2017-12-12T01:23:28Z,CONTRIBUTOR,2017-12-14T17:30:05Z
15286,Branch 178689056,cla: yes,,0,,1,2017-12-12T01:02:50Z,CONTRIBUTOR,2017-12-12T01:46:08Z
15285,Add kappa coefficient as a new metric?,"stat:contributions welcome,type:feature","Currently tensorflow supports using accuracy as a metric for model's performance.

However, for unbalanced datasets, kappa coefficient (https://www.wikiwand.com/en/Cohen%27s_kappa#) is a commonly used metric. Would it be possible to add this one in the model.metrics?",0,,3,2017-12-12T00:42:48Z,NONE,2017-12-12T01:24:02Z
15283,Fix minor typo in CUDNN_VERSION check,"awaiting testing (then merge),cla: yes","Effectively enables CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED in
CudnnSupport::GetConvolveBackwardFilterAlgorithms() for cuDNN v5.1.",0,,3,2017-12-11T23:10:06Z,CONTRIBUTOR,2017-12-19T21:10:29Z
15282,Fixed memory_stats_ops_test,"awaiting testing (then merge),cla: yes",Added explicit dependency to avoid matrix free prior to stats op execution.,0,,2,2017-12-11T21:04:22Z,CONTRIBUTOR,2017-12-19T21:23:35Z
15281,Remove redundant dependencies.,"cla: yes,pending merge internally",Looks like they got added back during merge. #15136,0,,1,2017-12-11T19:42:16Z,MEMBER,2017-12-11T21:21:03Z
15280,Missing dlcose()/FreeLibrary() after dlopen()/LoadLibrary(),stat:awaiting response,"Have I written custom code: N/A
 OS Platform and Distribution: N/A
 TensorFlow installed from: N/A
 TensorFlow version: N/A
 Bazel version: N/A
 CUDA/cuDNN version: N/A
 GPU model and memory: N/A
 Exact command to reproduce: N/A

Problem description:
A. Looking at code below, there are couple of issues:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/nnapi/NeuralNetworksShim.h#L34-L59
   1.There is no dlcose() call after dlopen() and dysym() in the code above.
   2.There can be two successful getLibraryHandle() calls without dlclose() in loadFunction().

B. More generally, the code below shows there is no interface to unload DLL either.
https://github.com/tensorflow/tensorflow/blob/359d6f9716c0bb9bd8201ce600da98b0481a8049/tensorflow/core/platform/env.h#L254-L280",0,,4,2017-12-11T19:28:27Z,NONE,2017-12-12T07:27:47Z
15277,[XLA] Make the client_test able to be disabled using a manifest file,"awaiting testing (then merge),cla: yes","This change allows these 3 tests in the XLA ClientTest to be selectively disabled using the manifest mechanism.

",0,,2,2017-12-11T15:32:39Z,CONTRIBUTOR,2017-12-26T01:23:28Z
15276,[XLA] Fix OS/X compile error with std::transform and  std::addressof,"awaiting review,cla: yes","I don't know if this has been flagged up elsewhere...

on my platform (OS/X, XLA), I receive the following error when trying to compile tensorflow/compiler/xla/service/llvm_ir/kernel_support_library.cc:

```
tensorflow/compiler/xla/service/llvm_ir/kernel_support_library.cc:101:5: error: no matching function for call to 'transform'
    std::transform(function->arg_begin(), function->arg_end(),
    ^~~~~~~~~~~~~~
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/algorithm:1922:1: note: candidate template ignored: couldn't infer template argument '_UnaryOperation'
transform(_InputIterator __first, _InputIterator __last, _OutputIterator __result, _UnaryOperation __op)
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/algorithm:1932:1: note: candidate function template not viable: requires 5 arguments, but 4 were provided
transform(_InputIterator1 __first1, _InputIterator1 __last1, _InputIterator2 __first2,
^
```

It is possible that std::addressof cannot be matched with the unary function template.

This code change replaces addressof with an function, which does match std::function<> ok.

",0,,7,2017-12-11T15:30:36Z,CONTRIBUTOR,2017-12-18T19:58:49Z
15275,[XLA] Allow components in the plugins directory to create devices,"awaiting review,cla: yes","Due to a change in the visibility of sub-components of the XLA JIT, the Graphcore device was unable to use the target needed for creating devices.

This change adds the plugin directory to the set of directories allowed to use the JIT.



",0,,3,2017-12-11T15:23:55Z,CONTRIBUTOR,2018-01-09T10:13:44Z
15273,Dataset Iterator is not an iterator,"stat:awaiting tensorflower,type:feature","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: confidential
- **TensorFlow installed from (source or binary)**: [Pypi](https://pypi.python.org/pypi/tensorflow-gpu/1.4.1)
- **TensorFlow version (use command below)**: `v1.4.0-19-ga52c8d9 1.4.1`
- **Python version**: 3.5.3
- **CUDA/cuDNN version**: (sensitive information replaced by `xxx`)
```
$ apt search cud | grep installed
libcublas8.0/xxx,now 8.0.44-4 amd64 [installed]
libcuda1/xxx,now 375.66-1 amd64 [installed,automatic]
libcuda1-i386/xxx,now 375.66-1 i386 [installed,automatic]
libcudart8.0/xxx,now 8.0.44-4 amd64 [installed]
libcudnn6/now 6.0.21-1+cuda8.0 amd64 [installed,local]
libcufft8.0/xxx,now 8.0.44-4 amd64 [installed]
libcurand8.0/xxx,now 8.0.44-4 amd64 [installed]
libnvidia-fatbinaryloader/xxx,now 375.66-1 amd64 [installed,automatic]
libnvidia-ptxjitcompiler/xxx,now 375.66-1 amd64 [installed,automatic]
```
- **GPU model and memory**: Quadro K1200, 4019 MiB
- **Exact command to reproduce**:
Run [convert_to_records.py](https://github.com/tensorflow/models/tree/5a5d330539dff11eef79ca2e716fb477baf13cf9/official/mnist) from the official MNIST example, then:
```python
>>> import tensorflow as tf
>>> ds = tf.data.TFRecordDataset(['/tmp/mnist_data'])
>>> i  = ds.make_one_shot_iterator()
>>> next(i)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'Iterator' object is not an iterator
```

### Describe the problem

The returned ""iterator"" is not an iterator, because it does not provide a `__next__` or `next` method. It does provide a `get_next` method, but that is not what Python expects.
",0,,4,2017-12-11T12:49:31Z,NONE,2017-12-11T13:14:49Z
15272,Wrongly used dropout bug,,"Hi, 

    I guess it should use **tf.layers.dropout** instead of **tf.nn.dropout** here? Because in the inference stage, all the nodes should be used instead of dropout. I guess this is a bug. 

https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/examples/tutorials/mnist/mnist_deep.py#L92

    Many thanks! 

Best wishes, 

Qiuqiang
    ",0,,2,2017-12-11T12:00:13Z,NONE,2017-12-11T14:54:54Z
15271,TFGAN ideas for pretraining/training,tag:awaiting response,"With the PR #14723 enabling `get_hooks_fn` to be set manually instead of the default `1` step generator and `1` step discriminator there comes a set of new ""problems"" / things to consider.

1. The `Estimator` saves configured tensor summaries in the background. This does not work when doing something like a `2/2` split for generator/discriminator or a `2/3`. Then `Estimator` will only save one step instead of the actual amounts of steps taken (right?). This means that we have to consider an option to configure the `FileWriter`. I believe that is possible for the vanilla `Estimator` with `scaffolds` but not for the `GANEstimator` currently. This `FileWriter` has to be accessible by the `RunTrainOpsHook`. We would also need a generator + discriminator specific ""global_step"" that will be used in the `RunTrainOpsHook`. A quick idea would be to use the `overall_global_step * train_steps` of a `RunTrainOpsHook` or to use `dummy_global_step_generator` and `dummy_global_step_discriminator`.

2. When ""pre-training"" (with a normal call to `.train()` and a modified `sequential_train_hook(10,10)`) the generator for e.g. `10` steps and then the discriminator for `10` steps. Does the discriminator `loss_fn` receive 10 times new data from the generator through `gan_model.discriminator_real_outputs` or will it always be the same data? For pre-training I would assume I can feed in the same output data from the generator in batches multiple epochs. I believe that is not possible in the current setup, but correct me if I am wrong.

3. I have different loss functions for both pre-training and training. There is not `ModeKeys.PRETRAIN` to switch between them.

If I find more things I'll add them here.",1,,19,2017-12-11T11:44:24Z,CONTRIBUTOR,2017-12-11T11:51:15Z
15268,correct the misspell of Quantize,cla: no,,0,,3,2017-12-11T11:04:02Z,NONE,2017-12-26T02:33:28Z
15266, //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel  test fails on ppc64le,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
     Ubuntu 16.04 (ppc64le)
- **TensorFlow installed from (source or binary)**:
      Installed from source (v1.3.1)
- **TensorFlow version (use command below)**:
      TF1.3.1
- **Python version**: 
     Python 2.7.5
- **Bazel version (if compiling from source)**:
       0.5.4
- **CUDA/cuDNN version**:
     NA
- **GPU model and memory**:
      NA
- **Exact command to reproduce**:
      bazel test -c opt //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel       


**Describe the problem**

Here 2 sub tests are failing on ppc64le i.e.` IsFiniteScalarF32`  and `IsFiniteR1F32s` in file array_elementwise_ops_test.cc

For IsFiniteScalarF32 sub-test , error at line 100 : 
https://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/tests/array_elementwise_ops_test.cc#L100
`ComputeAndCompareR0<bool>(&builder, false, {});`
 The failure due to expected: false  vs actual: true

For IsFiniteR1F32s sub-test , error at line 126 :
https://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/tests/array_elementwise_ops_test.cc#L126
`ComputeAndCompareR1<bool>(&builder, {false, true, false, true, false, false}, {});`
The failure due to expected: {010100}  vs actual: {111100}

Currently trying to find the root cause , started debugging further on this. Any inputs/help appreciated.Thanks!

**Source code / logs**

1 .**IsFiniteScalarF32 sub-test log :**
```
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
-----------------------------------------------------------------------------
Note: This is test shard 5 of 25.
[==========] Running 6 tests from 2 test cases.
[----------] Global test environment set-up.
[----------] 5 tests from ArrayElementwiseOpTest
[ RUN      ] ArrayElementwiseOpTest.IsFiniteScalarF32
2017-12-11 09:36:20.653075: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-12-11 09:36:20.654000: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-12-11 09:36:20.654482: I tensorflow/compiler/xla/service/service.cc:187] XLA service 0x100062a6ea0 executing computations on platform Host. Devices:
2017-12-11 09:36:20.654493: I tensorflow/compiler/xla/service/service.cc:195]   StreamExecutor device (0): <undefined>, <undefined>
tensorflow/compiler/xla/tests/literal_test_util.cc:157: Failure
Value of: Equal(expected, actual)
  Actual: false (expected: false
actual:   true)
Expected: true
expected:
false
        vs actual:
true
tensorflow/compiler/xla/tests/literal_test_util.cc:157: Failure
Value of: Equal(expected, actual)
  Actual: false (expected: false
actual:   true)
Expected: true
expected:
false
        vs actual:
true
[  FAILED  ] ArrayElementwiseOpTest.IsFiniteScalarF32 (60 ms)
[ RUN      ] ArrayElementwiseOpTest.LogicalNotZeroElement
2017-12-11 09:36:20.712501: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.LogicalNotZeroElement (6 ms)
[ RUN      ] ArrayElementwiseOpTest.LogOfPowerF32
2017-12-11 09:36:20.719016: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.LogOfPowerF32 (12 ms)
[ RUN      ] ArrayElementwiseOpTest.Max3DAndScalarZeroElementS32s
2017-12-11 09:36:20.731087: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.Max3DAndScalarZeroElementS32s (7 ms)
[ RUN      ] ArrayElementwiseOpTest.Compare1DTo2DS32Ne
2017-12-11 09:36:20.738675: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.Compare1DTo2DS32Ne (13 ms)
[----------] 5 tests from ArrayElementwiseOpTest (99 ms total)

[----------] 1 test from ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount
[ RUN      ] ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount.SquareManyValues/0
2017-12-11 09:36:20.751273: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount.SquareManyValues/0 (34 ms)
[----------] 1 test from ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount (34 ms total)

[----------] Global test environment tear-down
[==========] 6 tests from 2 test cases ran. (133 ms total)
[  PASSED  ] 5 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] ArrayElementwiseOpTest.IsFiniteScalarF32

 1 FAILED TEST

```
2. **IsFiniteR1F32s sub-test log:**

```
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
-----------------------------------------------------------------------------
Note: This is test shard 6 of 25.
[==========] Running 6 tests from 2 test cases.
[----------] Global test environment set-up.
[----------] 5 tests from ArrayElementwiseOpTest
[ RUN      ] ArrayElementwiseOpTest.IsFiniteR1F32s
2017-12-11 09:36:21.201704: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-12-11 09:36:21.202563: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-12-11 09:36:21.203145: I tensorflow/compiler/xla/service/service.cc:187] XLA service 0x10015cc6ea0 executing computations on platform Host. Devices:
2017-12-11 09:36:21.203154: I tensorflow/compiler/xla/service/service.cc:195]   StreamExecutor device (0): <undefined>, <undefined>
tensorflow/compiler/xla/tests/literal_test_util.cc:157: Failure
Value of: Equal(expected, actual)
  Actual: false (expected: {010100}
actual:   {111100})
Expected: true
expected:
{010100}
        vs actual:
{111100}
[  FAILED  ] ArrayElementwiseOpTest.IsFiniteR1F32s (17 ms)
[ RUN      ] ArrayElementwiseOpTest.CompareEqF32s
2017-12-11 09:36:21.218227: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.CompareEqF32s (10 ms)
[ RUN      ] ArrayElementwiseOpTest.MulOfExpF32
2017-12-11 09:36:21.228744: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.MulOfExpF32 (12 ms)
[ RUN      ] ArrayElementwiseOpTest.Min2DTo1DF32s
2017-12-11 09:36:21.240780: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.Min2DTo1DF32s (15 ms)
[ RUN      ] ArrayElementwiseOpTest.Compare1DTo2DS32Ge
2017-12-11 09:36:21.255836: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.Compare1DTo2DS32Ge (19 ms)
[----------] 5 tests from ArrayElementwiseOpTest (73 ms total)

[----------] 1 test from ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount
[ RUN      ] ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount.SquareManyValues/1
2017-12-11 09:36:21.274589: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount.SquareManyValues/1 (73 ms)
[----------] 1 test from ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount (73 ms total)

[----------] Global test environment tear-down
[==========] 6 tests from 2 test cases ran. (146 ms total)
[  PASSED  ] 5 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] ArrayElementwiseOpTest.IsFiniteR1F32s

 1 FAILED TEST

```
",0,,30,2017-12-11T10:06:30Z,CONTRIBUTOR,2017-12-11T19:02:49Z
15265,Test remove __force_inline [DO NOT MERGE],cla: yes,"This is only a test for fixing https://github.com/tensorflow/tensorflow/issues/10521, please don't merge this PR.",0,,4,2017-12-11T09:28:47Z,MEMBER,2017-12-11T09:30:33Z
15264,Support empty input tensor for some ops (fix #14657),"awaiting testing (then merge),cla: yes","Cudnn kernels doesn't work for empty input tensors.
This PR adds support for empty input tensor for FusedBatchNorm,FusedBatchNormGrad,Conv2DBackpropFilter, and cudnn pooling. (fix #14657)",1,,27,2017-12-11T08:20:04Z,CONTRIBUTOR,2017-12-20T00:16:09Z
15263,Cannot parse tensor from proto: dtype: DT_INT32 when using tf.extract_image_patches and tf.reshape,,"Hi,
I'm experiencing a problem when using TensorFlow to extract image patches and then reshape the output. I'm using TensorFlow 1.3.0, what am i doing wrong?

That's my code:
```
import tensorflow as tf
import numpy as np
c = 3
h = 1024
p = 32

image = tf.random_normal([h,h,c])
patch_size = [1,p,p,1]
patches = tf.extract_image_patches([image],
   patch_size, patch_size, [1, 1, 1, 1], 'VALID')
patches = tf.reshape(patches, [h, p, p, c])

sess = tf.Session()
I,P,R_n = sess.run([image,patches])
print(I.shape)
print(P.shape)
```

The error i'm getting is this:
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1326     try:
-> 1327       return fn(*args)
   1328     except errors.OpError as e:

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1305                                    feed_dict, fetch_list, target_list,
-> 1306                                    status, run_metadata)
   1307 

~\AppData\Local\conda\conda\envs\tf\lib\contextlib.py in __exit__(self, type, value, traceback)
     65             try:
---> 66                 next(self.gen)
     67             except StopIteration:

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\errors_impl.py in raise_exception_on_not_ok_status()
    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 466           pywrap_tensorflow.TF_GetCode(status))
    467   finally:

InvalidArgumentError: Cannot parse tensor from proto: dtype: DT_INT32
tensor_shape {
  dim {
    size: 3
  }
}
tensor_content: ""\000\004\000\000\000\004\000\000\003\000\000\000""

	 [[Node: random_normal_21/shape = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [3] values: 1024 1024 3>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-159-454594e78931> in <module>()
     12 
     13 sess = tf.Session()
---> 14 I,P,R_n = sess.run([image,patches])
     15 print(I.shape)
     16 print(P.shape)

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    893     try:
    894       result = self._run(None, fetches, feed_dict, options_ptr,
--> 895                          run_metadata_ptr)
    896       if run_metadata:
    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1122     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1123       results = self._do_run(handle, final_targets, final_fetches,
-> 1124                              feed_dict_tensor, options, run_metadata)
   1125     else:
   1126       results = []

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1319     if handle is None:
   1320       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1321                            options, run_metadata)
   1322     else:
   1323       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1338         except KeyError:
   1339           pass
-> 1340       raise type(e)(node_def, op, message)
   1341 
   1342   def _extend_graph(self):

InvalidArgumentError: Cannot parse tensor from proto: dtype: DT_INT32
tensor_shape {
  dim {
    size: 3
  }
}
tensor_content: ""\000\004\000\000\000\004\000\000\003\000\000\000""

	 [[Node: random_normal_21/shape = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [3] values: 1024 1024 3>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

Caused by op 'random_normal_21/shape', defined at:
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\__main__.py"", line 3, in <module>
    app.launch_new_instance()
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\traitlets\config\application.py"", line 658, in launch_instance
    app.start()
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\zmq\eventloop\ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tornado\ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\zmq\eventloop\zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\zmq\eventloop\zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\zmq\eventloop\zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\IPython\core\interactiveshell.py"", line 2698, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\IPython\core\interactiveshell.py"", line 2802, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\IPython\core\interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-159-454594e78931>"", line 7, in <module>
    image = tf.random_normal([h,h,c])
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\ops\random_ops.py"", line 71, in random_normal
    shape_tensor = _ShapeTensor(shape)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\ops\random_ops.py"", line 42, in _ShapeTensor
    return ops.convert_to_tensor(shape, dtype=dtype, name=""shape"")
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 611, in convert_to_tensor
    as_ref=False)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 676, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 121, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 106, in constant
    attrs={""value"": tensor_value, ""dtype"": dtype_value}, name=name).outputs[0]
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Cannot parse tensor from proto: dtype: DT_INT32
tensor_shape {
  dim {
    size: 3
  }
}
tensor_content: ""\000\004\000\000\000\004\000\000\003\000\000\000""

	 [[Node: random_normal_21/shape = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [3] values: 1024 1024 3>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
```",0,,2,2017-12-11T08:16:17Z,NONE,2017-12-11T08:39:00Z
15262,Bug with tf.layers.Dense,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
8.0
- **GPU model and memory**:
GTX 1080 ti
- **Exact command to reproduce**:

with tf.variable_scope('hello') as var_scope:
    var = tf.get_variable('var', [3,4,5])
    dense = tf.layers.Dense(5, name = 'dense_layer')
    b = dense(a)

And the error was : 
RuntimeError: Conversion function <function _TensorConversionFunction at 0x7f5bed522b90> for type <class 'tensorflow.python.ops.variables.Variable'> returned incompatible dtype: requested = float32_ref, actual = float32",1,,11,2017-12-11T07:51:25Z,NONE,2017-12-11T08:30:18Z
15261,"ValueError: Variable Model/LSTMenc/rnn/basic_lstm_cell/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem
If I put reuse=None while creating BasicLSTMCell in the following code, I get this error:
```
Traceback (most recent call last):
  File ""pretrain.py"", line 358, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""pretrain.py"", line 251, in main
    valid_model = build_model(word_vocab, train=False)
  File ""pretrain.py"", line 200, in build_model
    dropout=FLAGS.dropout))
  File ""/home/raghuram.vadapalli/styletransfer/NeuralSum/model.py"", line 218, in lstm_doc_enc
    initial_state=initial_rnn_state, dtype=tf.float32)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py"", line 197, in static_rnn
    (output, state) = call_cell()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py"", line 184, in <lambda>
    call_cell = lambda: cell(input_, state)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py"", line 235, in __call__
    with _checked_scope(self, scope or ""basic_lstm_cell"", reuse=self._reuse):
  File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py"", line 93, in _checked_scope
    ""the argument reuse=True."" % (scope_name, type(cell).__name__))
ValueError: Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'Model/LSTMenc/rnn/basic_lstm_cell'; and the cell was not constructed as BasicLSTMCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True.
```
If I put reuse=True, I get this error
```
Traceback (most recent call last):
  File ""pretrain.py"", line 358, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""pretrain.py"", line 244, in main
    train_model = build_model(word_vocab, train=True)
  File ""pretrain.py"", line 146, in build_model
    dropout=FLAGS.dropout))
  File ""/home/raghuram.vadapalli/styletransfer/NeuralSum/model.py"", line 218, in lstm_doc_enc
    initial_state=initial_rnn_state, dtype=tf.float32)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py"", line 197, in static_rnn
    (output, state) = call_cell()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py"", line 184, in <lambda>
    call_cell = lambda: cell(input_, state)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py"", line 713, in __call__
    output, new_state = self._cell(inputs, state, scope)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py"", line 241, in __call__
    concat = _linear([inputs, h], 4 * self._num_units, True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py"", line 1044, in _linear
    _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size], dtype=dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 1049, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 948, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 356, in get_variable
    validate_shape=validate_shape, use_resource=use_resource)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 341, in _true_getter
    use_resource=use_resource)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 671, in _get_single_variable
    ""VarScope?"" % name)
ValueError: Variable Model/LSTMenc/rnn/basic_lstm_cell/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?
```
### Source code / logs
```
def lstm_doc_enc(input_cnn,
                   batch_size=20,
                   num_rnn_layers=2,
                   rnn_size=650,
                   max_doc_length=35,
                   dropout=0.0):

    # lstm document encoder
    with tf.variable_scope('LSTMenc') as scope:
        def create_rnn_cell():
            cell = tf.contrib.rnn.BasicLSTMCell(rnn_size, state_is_tuple=True, forget_bias=0.0, reuse=True)
            if dropout > 0.0:
                cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.-dropout)
            return cell

        if num_rnn_layers > 1:
            cell = tf.contrib.rnn.MultiRNNCell([create_rnn_cell() for _ in range(num_rnn_layers)], state_is_tuple=True)
        else:
            cell = create_rnn_cell()

        initial_rnn_state = cell.zero_state(batch_size, dtype=tf.float32)

        input_cnn = tf.reshape(input_cnn, [batch_size, max_doc_length, -1])
        input_cnn2 = [tf.squeeze(x, [1]) for x in tf.split(input_cnn, max_doc_length, 1)]

        outputs, final_rnn_state = tf.contrib.rnn.static_rnn(cell, input_cnn2,
                                         initial_state=initial_rnn_state, dtype=tf.float32)

    return adict(
        initial_enc_state=initial_rnn_state,
        final_enc_state=final_rnn_state,
        enc_outputs=outputs
    )

```
",0,,1,2017-12-11T06:59:55Z,NONE,2017-12-11T19:11:20Z
15259,Using tensorflow to compute gradients w.r.t. spectral decomposition error,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.1
- **TensorFlow installed from (source or binary)**: pip install tensorflow
- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: see code

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Hi,

I'm having issues with evaluating the gradients of eigenvalues/eigenvectors with respect to the underlying matrix. Tensorflow evaluates the gradients, however when compared against analytical derivations the gradients are generally inconsistent (I understand numerical can be unstable). I'd like to understand is there's a bug in the gradients or if a different methodology was employed to the one cited below.

We are using tf.self_adjoint_eig to evaluate the spectral decomposition for the tensorflow variable input. We have initialised this with a symmetric matrix to satisfy the self adjoint operator property. We wish to take the derivative of individual eigenvalues or eigenvector with respect to the original matrix (note for eigenvectors we take one element from one eigenvector, e.g. element 2 in eigenvector 1 to avoid the issue of gradient aggregation for now).

The methodology for evaluating analytical gradients of eigenvalues and vectors of a symmetric real matrix can be found in the paper ""On differentiating Eigenvalues and Eigenvectors"" by Magnus (1985), Theorem 1 eqn (6) + (7). We evaluated using our input matrix the gradients under this paper and compared it to tensorflow evaluated gradients and the gradients from finite difference approximation. For the eigenvalues, the gradients are similar (identical on diagonal entries, off by a factor of 2 on off-diagonal elements), however the eigenvectors are off by quite a bit outside the diagonal entries. To start, define a matrix A as (excuse the matrix output formatting from Python)

A=[[-3 -2  4]
      [-2  1  1]
      [ 4  1  5]]

using np.linalg.eig and tf.self_adjoint_eig on A (I simply initialised a variable with A and computed the gradient for the tf implementation)

Tensorflow eigenvalues: [-5.43071561  1.76904987  6.66166575]
Python eigenvalues: [-5.43071561  6.66166575  1.76904987]

I now wish to evaluate the gradient of eigenvalue 1 (-5.43071561) w.r.t. A. 

Analytical gradient:
[[ 0.75896178  0.28555906 -0.31842553]
 [ 0.28555906  0.10744148 -0.11980748]
 [-0.31842553 -0.11980748  0.13359674]]

Tensorflow gradient:
[[[ 0.75896178,  0.        ,  0.        ],
   [ 0.57111812,  0.10744148,  0.        ],
  [-0.63685107, -0.23961495,  0.13359674]]]

The diagonal entries are the same but the off-diagonal entries are clearly off by a factor of 2.  Now we try and evaluate gradients for the eigenvectors.

Tensorflow eigenvectors:
[[-0.87118413 -0.31452619 -0.37697678]
 [-0.32778267  0.94426587 -0.0303395 ]
 [ 0.36550888  0.09713517 -0.92572567]]
Python eigenvectors:
[[ 0.87118413  0.37697678 -0.31452619]
 [ 0.32778267  0.0303395   0.94426587]
 [-0.36550888  0.92572567  0.09713517]]

We try and find the gradient of the eigenvector 1 element 2 (+/-0.32778267). We expect the tensorflow gradient to the equivalent to the analytical gradient (after taking into account the sign difference).

Analytical gradient:
[[ 0.03511309 -0.10795607 -0.01312188]
 [ 0.01321128 -0.04061843 -0.0049371 ]
 [-0.01473184  0.04529341  0.00550534]]

Tensorflow gradient:
[[[-0.03511309,  0.        ,  0.        ],
   [ 0.09474478,  0.04061843,  0.        ],
   [ 0.02785372, -0.04035631, -0.00550534]]]

Besides the entries being different on the off-diagonal, one issue is that tensorflow only returns the lower triangle of the gradient. Despite A being symmetric, the contribution to the gradient is not symmetric as shown in the analytical evaluation above. Thank you for reading!

TL:DR, I'd like to understand where the gradient computations are coming from and why they differ substantially to the results we have been using in our research. 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Here is a script that reproduces the above.

[eigen_decomp_examplev2.py.zip](https://github.com/tensorflow/tensorflow/files/1546631/eigen_decomp_examplev2.py.zip)
",0,,1,2017-12-11T05:32:08Z,NONE,2017-12-11T19:15:56Z
15258,[no such file or directory: 'x86_64'] when building the library in TensorFlow Lite for iOS,stat:awaiting tensorflower,"Hi all, I encountered with an issur when trying to [setting up the environment to build TensorFlow Lite for iOS. ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/ios.md#building).
But when I was trying to build the library for all five supported architectures on iOS using `tensorflow/contrib/lite/build_ios_universal_lib.sh`, I have the follwing issue. 


    xcrun: error: SDK ""iphonesimulator"" cannot be located
    xcrun: error: SDK ""iphonesimulator"" cannot be located
    xcrun: error: unable to lookup item 'PlatformPath' in SDK 'iphonesimulator'
    xcrun: error: SDK ""iphonesimulator"" cannot be located
    xcrun: error: SDK ""iphonesimulator"" cannot be located
    xcrun: error: unable to lookup item 'Path' in SDK 'iphonesimulator'
    xcrun: error: SDK ""iphoneos"" cannot be located
    xcrun: error: SDK ""iphoneos"" cannot be located
    xcrun: error: unable to lookup item 'SDKVersion' in SDK 'iphoneos'
    gcc --std=c++11 -O3 -DNDEBUG -miphoneos-version-min=9.0 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -fembed-bitcode -Wno-c++11-narrowing -mno-thumb -fno-exceptions -isysroot  -arch x86_64 -O3 -I. -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/../../../ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/allocation.cc -o /Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/allocation.o
    gcc -miphoneos-version-min=9.0 -fembed-bitcode -mno-thumb -isysroot  -arch x86_64 -O3 -I. -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/../../../ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/context.c -o /Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/context.o
    gcc --std=c++11 -O3 -DNDEBUG -miphoneos-version-min=9.0 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -fembed-bitcode -Wno-c++11-narrowing -mno-thumb -fno-exceptions -isysroot  -arch x86_64 -O3 -I. -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/../../../ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/downloads/farmhash/src/farmhash.cc -o /Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/downloads/farmhash/src/farmhash.o
    gcc --std=c++11 -O3 -DNDEBUG -miphoneos-version-min=9.0 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -fembed-bitcode -Wno-c++11-narrowing -mno-thumb -fno-exceptions -isysroot  -arch x86_64 -O3 -I. -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/../../../ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/error_reporter.cc -o /Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/error_reporter.o
    gcc --std=c++11 -O3 -DNDEBUG -miphoneos-version-min=9.0 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -fembed-bitcode -Wno-c++11-narrowing -mno-thumb -fno-exceptions -isysroot  -arch x86_64 -O3 -I. -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/../../../ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/interpreter.cc -o /Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/interpreter.o
    clang: error: no such file or directory: 'x86_64'
    clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
    make: *** [/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/allocation.o] Error 1
    make: *** Waiting for unfinished jobs....
    clang: error: no such file or directory: 'x86_64'
    clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
    clang: error: no such file or directory: 'x86_64'
    clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
    make: *** [/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/context.o] Error 1
    make: *** [/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/downloads/farmhash/src/farmhash.o] Error 1
    clang: error: no such file or directory: 'x86_64'
    clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
    make: *** [/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/error_reporter.o] Error 1
    clang: error: no such file or directory: 'x86_64'
    clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
    make: *** [/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/interpreter.o] Error 1


### System information
- **OS Platform and Distribution**: macOS High Sierra 10.13.2(17C88)
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 3.6.1
- **GCC/Compiler version (if compiling from source)**: GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)
- **Exact command to reproduce**: `bash tensorflow/contrib/lite/build_ios_universal_lib.sh`
- **Have I written custom code?**: no
- **TensorFlow installed from**: source
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A

Has anyone else had the same problem?",0,,6,2017-12-11T04:34:36Z,NONE,2017-12-11T12:58:59Z
15256,Problem about tf.data.Dataset.from_sparse_tensor_slices,type:docs,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.4
- **Python version**: 3.52
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:8.0,6.46
- **GPU model and memory**:2GB
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I used the tf.data.Dataset.from_sparse_tensor_slices to built a dataset. But the website has no enought information.
Here's my code
`    point_cloud_feature_dataset = tf.data.Dataset.from_sparse_tensor_slices(sparse_feature)
    point_cloud_feature_dataset = point_cloud_feature_dataset.shuffle(buffer_size = 100000)
    point_cloud_feature_dataset = point_cloud_feature_dataset.batch(batch_size = BATCH_SIZE)
    point_cloud_feature_dataset = point_cloud_feature_dataset.repeat()
    iterator_feature = point_cloud_feature_dataset.make_one_shot_iterator()`

when I called the iterator_feature.get_nest(). It return 3 Tensors of shape [none,none,1]. Instead of a SparseTensor.  The input Sparse Tensor of dataset has a shape of [1000000,300000]. Each row is a example. I hope talents can replenish the Doc. Thanks!!  

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,13,2017-12-11T02:12:45Z,NONE,2017-12-11T02:22:20Z
15255,Add int64 support for BroadcastArgs and BroadcastGradientArgs,"awaiting testing (then merge),cla: yes","In `array_ops.cc`, both int32 and int64 are expected to be supported for `BroadcastArgs` and `BroadcastGradientArgs`. However, this was not the case as only int32 kernel are registered even though `T` is part of the `TypeConstraint`.

This fix adds the int64 kernel support for `BroadcastArgs` and `BroadcastGradientArgs`, and adds related test cases.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,4,2017-12-11T01:40:34Z,MEMBER,2017-12-26T02:32:39Z
15253,include _solib_* in pip package,"awaiting testing (then merge),cla: yes","See issue #15252 for detail, also should fixes issue #13711.
The problem is that when compile tensorflow with --config=mkl on virtual machines, mkl libraries won't be included because they locate under _solib_local, however, setup.py only includes _solib_k8.",0,,8,2017-12-10T16:46:09Z,CONTRIBUTOR,2017-12-26T02:30:56Z
15252,vm compiled tensorflow - libmklml_intel.so ImportError,stat:contributions welcome,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04 VM
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
Branch `r1.4` and branch `master`
- **Python version**: 
3.6.3
- **Bazel version (if compiling from source)**:
Build label: 0.8.1 (Install using apt repository)
- **GCC/Compiler version (if compiling from source)**:
gcc-6 (Ubuntu 6.4.0-10ubuntu1~16.04.york0) 6.4.0 20171112

- **Exact command to reproduce**:
python -c ""import tensorflow as tf;""

### Describe the problem
After compiling Tensorflow from source with tutorial: `https://www.tensorflow.org/install/install_sources` and install the compiled pip package, I import tensorflow on python console and get those errors:

~~~
python -c ""import tensorflow as tf;""
Traceback (most recent call last):
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/potatoman/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/potatoman/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 73, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/potatoman/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/potatoman/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
~~~

My machine is a headless KVM virtual machine running on a ubuntu 17.10, btw.

I figure out where the problem is somehow, which is the pip packaging script don't include those mkl so files, and in my case, replace `_solib_k8` in script `tensorflow/tools/pip_package/setup.py` at line 185 with `_solib_local` solves the issue.

",0,,4,2017-12-10T16:43:46Z,CONTRIBUTOR,2017-12-10T19:06:50Z
15251,different output size for avg_pool and max_pool,,"Hello,

I have the same bug as this user.

https://stackoverflow.com/questions/47423172/tensorflow-why-does-avg-pool-ignore-one-stride-dimension

My version is uo-to-date

Have I written custom code: Yes
OS Platform and Distribution: Linux Mint 18.3, codename ""Sylvia""
TensorFlow installed from Tensorflow Website
TensorFlow version 14.1.0
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory GeForce 940MX
Exact command to reproduce: see link",0,,4,2017-12-10T16:34:43Z,NONE,2017-12-11T01:01:17Z
15250,Batch Norm variance output mismatches with tf 1.4.0,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from  binary**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**:  2.7.0
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/5.0
- **GPU model and memory**: GTX1080 and 8 GB
- **Exact command to reproduce**: python test_google_bn.py

### Describe the problem
Batch normalization test failed with tensorflow version 1.4.0 but the same test passed with tensorflow version 1.3.0. 


### Source code / logs
```Python
# test_google_bn.py
import numpy as np
import pytest
import tensorflow as tf
from numpy.testing import assert_array_almost_equal
from tensorflow.python.ops import control_flow_ops
def test_delayed_update_moving_vars():
    with tf.Session() as sess:
        height, width = 3, 3
        image_shape = (10, height, width, 3)
        image_values = np.random.rand(*image_shape)
        expected_mean = np.mean(image_values, axis=(0, 1, 2))
        expected_var = np.var(image_values, axis=(0, 1, 2))
        images = tf.constant(image_values, shape=image_shape, dtype=tf.float32)
        decay = 0.1
        epsilon = 1e-5
        output = tf.contrib.layers.batch_norm(images, is_training=True, reuse=None, decay=decay, epsilon=epsilon,
                            updates_collections=tf.GraphKeys.UPDATE_OPS, name='BatchNorm')
        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
        # updates_ops are added to UPDATE_OPS collection.
        assert len(update_ops) == 2
        with tf.control_dependencies(update_ops):
            barrier = tf.no_op(name='barrier')
        output = control_flow_ops.with_dependencies([barrier], output)
        # Initialize all variables
        sess.run(tf.global_variables_initializer())
        moving_mean = tf.contrib.framework.get_variables('BatchNorm/moving_mean')[0]
        moving_variance = tf.contrib.framework.get_variables('BatchNorm/moving_variance')[0]
        mean, variance = sess.run([moving_mean, moving_variance])
        # After initialization moving_mean == 0 and moving_variance == 1.
        assert_array_almost_equal(mean, [0] * 3)
        assert_array_almost_equal(variance, [1] * 3)
        for _ in range(10):
            sess.run([output])
        mean = moving_mean.eval()
        variance = moving_variance.eval()
        # After 10 updates with decay 0.1 moving_mean == expected_mean and
        # moving_variance == expected_var.
        assert_array_almost_equal(mean, expected_mean, decimal=4)
        assert_array_almost_equal(variance, expected_var, decimal=4)
```

>           assert_array_almost_equal(variance, expected_var, decimal=4)
           AssertionError: 
           Arrays are not almost equal to 4 decimals
           
           (mismatch 100.0%)
            x: array([ 0.08  ,  0.0908,  0.0773], dtype=float32)
            y: array([ 0.0792,  0.0898,  0.0764])


",1,,7,2017-12-10T09:58:04Z,CONTRIBUTOR,2017-12-10T19:22:20Z
15248,"Instead of python, use PYTHON_BIN_PATH in pip.sh.",cla: yes,,1,,8,2017-12-10T08:22:47Z,OWNER,2017-12-10T17:59:13Z
15247,Replace `variables.get_global_step()` use `training_util.get_global_step()`,"cla: yes,stat:awaiting response",,1,,6,2017-12-10T08:16:57Z,CONTRIBUTOR,2017-12-10T18:33:36Z
15245,add c++ gradient for op: Pow,"awaiting testing (then merge),cla: yes","Fix  #15239.

### How to test

+ [x] add test case.
+ [ ] pass all tests.",0,,9,2017-12-10T06:48:33Z,CONTRIBUTOR,2017-12-10T18:34:01Z
15244,Switched optimization mode for Pi builds to avoid internal compiler error,cla: yes,"The nightly Pi3 builds have been failing with:

```
In file included from external/eigen_archive/unsupported/Eigen/MatrixFunctions:57:0,
                 from ./third_party/eigen3/unsupported/Eigen/MatrixFunctions:1,
                 from tensorflow/core/kernels/matrix_exponential_op.cc:19:
external/eigen_archive/unsupported/Eigen/src/MatrixFunctions/MatrixFunction.h: In member function 'MatrixType Eigen::internal::MatrixFunctionAtomic<MatrixType>::compute(const MatrixType&) [with MatrixType = Eigen::Matrix<std::complex<float>, -1, -1>]':
external/eigen_archive/unsupported/Eigen/src/MatrixFunctions/MatrixFunction.h:101:1: internal compiler error: in decompose_normal_address, at rtlanal.c:5799
 }
 ^
Please submit a full bug report,
with preprocessed source if appropriate.
See <http://gcc.gnu.org/bugs.html> for instructions.
```

It appears to be the same problem that Chrome hit with gcc here:

https://bugs.chromium.org/p/chromium/issues/detail?id=675648

Their solution was to change the optimization flags to avoid this, so I've followed their lead and switched to -O3. I hope this won'tl make a big latency or size difference, but it does solve the compiler crash at least.",0,,1,2017-12-10T06:40:23Z,MEMBER,2017-12-11T20:49:35Z
15240,"in minimize     ([str(v) for _, v in grads_and_vars], loss)) ValueError: No gradients provided for any variable - This error occurs while using minimize for an adam optimizer()",,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,2,2017-12-09T20:19:06Z,NONE,2017-12-10T00:07:42Z
15239,No gradient defined for op: Pow,"stat:contributions welcome,type:bug/performance","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Bazel version**: N/A
- **Python version**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: -

### Describe the problem
It seems there is no gradient defined for the Pow operation in the C++ API.

I am actually transferring this issue from https://github.com/migueldeicaza/TensorFlowSharp/issues/187. Similar to the case of Select (#14845), it seems there is also no gradient for the Pow operation in the C++ API.",0,,4,2017-12-09T20:09:30Z,NONE,2017-12-10T06:54:32Z
15238,CPU usage drops after several steps,stat:awaiting response,"I am running a tensorflow computation graph. The same graph was running fine before but all of a sudden I noticed that after several steps and checkpoint the CPU usage drops from more than 600% to 100% for the rest of the session. It seems like all executors are dying (when saving maybe) and the computation keeps running on only 1 Core shooting up and down from 100 to 200%.

I also posted the issue on stack overflow: https://stackoverflow.com/questions/47720893/tensor-flow-cpu-usage-drops-after-saving

  ",0,,6,2017-12-09T16:46:23Z,NONE,2017-12-10T00:09:20Z
15236,"Error while using cuda-9.0, libcublas.so.8.0: cannot open shared object file: No such file or directory",stat:awaiting response,"Hi,

I just installed cuda-9.0
and done all the requirements for cuda to run tensorflow like libcudnn7_7.0.5.15-1+cuda9.0_amd64.deb, libcudnn7-dev_7.0.5.15-1+cuda9.0_amd64.deb.
The test example of libcudnn are working fine.

I installed tensorflow-gpu, it finished normally while using ""$ sudo pip3 install tensorflow-gpu"" but at the time of import, it gives me  error "" 
ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory""
Look like tensorflow doesn't support Cuda-9.0?

Error is as follows:

$ ipython
Python 3.5.2 (default, Nov 23 2017, 16:37:01) 
Type 'copyright', 'credits' or 'license' for more information
IPython 6.2.1 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import tensorflow as tf
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

/usr/lib/python3.5/imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

/usr/lib/python3.5/imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py in <module>()
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     70 for some common reasons and solutions.  Include the entire stack trace
     71 above this error message when asking for help."""""" % traceback.format_exc()
---> 72   raise ImportError(msg)
     73 
     74 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.



Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: No use pip install tensorflow
- **TensorFlow version (use command below)**: tensorflow_gpu-1.4.1-cp35-cp35m-manylinux1_x86_64.whl 
- **Python version**:  Python 3.5.2 (default, Nov 23 2017, 16:37:01)
- **Bazel version (if compiling from source)**:No
- **GCC/Compiler version (if compiling from source)**: No
- **CUDA/cuDNN version**:  cuda-9.0
- **GPU model and memory**: GeForce GT 750M
- **Exact command to reproduce**: No just import tensorflow as tf
                                                         
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,15,2017-12-09T14:35:25Z,NONE,2017-12-10T00:10:43Z
15235,Loss should change depending on the number of epochs chosen even if I set the seed?,,"Hi colleagues,

I am using Keras to train different NNs. I would like to know why if I increment the epochs, the result until the same number of epochs is not the same for the evolution of loss. I am using shuffle=False, and np.random.seed(2017), tf.set_random_seed(2017), and I have check that if I repeat with the same number of epochs, the result is the same, so the random initialization is working. After the epochs training, I am deleting the ANN so the training begins at 0 again.

Here I attach the picture of the resulting training with 10 epochs:

<img width=""676"" alt=""captura de pantalla 2017-12-09 a las 15 02 25"" src=""https://user-images.githubusercontent.com/23745991/33796312-05efe5da-dcf2-11e7-9780-09e0be902557.png"">

And here I attach the picture of the resulting training with 8 epochs:

<img width=""679"" alt=""captura de pantalla 2017-12-09 a las 15 02 34"" src=""https://user-images.githubusercontent.com/23745991/33796313-106a6b34-dcf2-11e7-820c-e5d279665785.png"">

Also, I would like to know why the training time is not exactly (8/10) the 10 epochs attempt and how is it possible that some of them have less accuracy with 2 more epochs!

Here is the link to open code Jupyter Notebook. [GitHub Jupyter Notebook - ANN Comparison](https://github.com/PabloRR100/Pruning-Algorithm-Method/blob/master/NN-Comparison.ipynb)

Thanks a lot!",0,,3,2017-12-09T14:03:15Z,NONE,2017-12-10T00:55:15Z
15232,"Install new TensorFlow via anaconda, but failed",stat:awaiting response,"Hello,

When I tried to install TensorFlow via anaconda.
I`ve searched the suitable cite to submit the issue, but failed to find out it.
If this is not sorry for it and advice me.

The target os is 'windows 10', and via anaconda 5.0.1 For Windows (Python 3.6 version 64bit).
After intall anaconda, from my terminal 'cmd.exe',
$ conda create -n tensorflow python=3.6
but received an error message.

But from 'Anaconda Prompt' below 'Anaconda3' of 'start button',it works correctly.

Thus I recommend to include the following description,
start 'Anaconda Prompt' below 'Anaconda3' of 'start button' between the steps.",0,,3,2017-12-09T08:04:49Z,NONE,2017-12-09T18:55:26Z
15230,tensorflow-gpu on mac,,"I can use the following to install tensorflow on mac.

Python 2

~~~
pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.4.0-py2-none-any.whl
~~~

Python 3

~~~
pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.4.0-py3-none-any.whl
~~~

But I don't find the correponding files for tensorflow-gpu. Can it be generated and posted over there? Thanks.
",0,,1,2017-12-09T04:55:15Z,NONE,2017-12-09T08:37:32Z
15229,[XLA] Fix another XLA/tfcompile compile error on OSX ,"awaiting testing (then merge),cla: yes","This fixes issue https://github.com/tensorflow/tensorflow/issues/15196 (though that was accidentally closed)

kernel_support_library.cc:99:5: error: no matching function
for call to 'transform' std::transform(function->arg_begin(),..

TEST=build tfcompile on OSX (also requires PR#14893)",0,,6,2017-12-09T04:54:25Z,CONTRIBUTOR,2017-12-09T05:18:51Z
15227,MKL: Fixing MKL-DNN convolution filter propagation to backprop,"awaiting testing (then merge),cla: yes",Added code to propagate the 2D convolution filter to the backward pass.,0,,2,2017-12-09T00:28:55Z,CONTRIBUTOR,2017-12-22T22:12:54Z
15225,Update .gitignore,"awaiting testing (then merge),cla: yes",,1,,3,2017-12-08T21:58:57Z,CONTRIBUTOR,2017-12-09T04:41:31Z
15224,Add TensorFlow support for tf.repeat (equivalent to np.repeat),cla: yes,"This fix tries to address the feature request proposed in #8246 where there was no equivalent of numpy.repeat in TensorFlow.

This fix adds the support for tf.repeat that is equivalent to np.repeat.

NOTE: in order to allow optional `axis` parameter, this fix adds `Repeat` and `RepeatFlat`  ops where one takes `axis` and another does not take `axis`.

This fix fixes #8246.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,1,2017-12-08T21:28:59Z,MEMBER,2017-12-12T16:08:18Z
15223,stdin,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,1,2017-12-08T19:24:08Z,NONE,2017-12-08T19:44:29Z
15222,tfcompile error - no matching function for call to 'transform',,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.  This is on a clean checkout of tensorflow.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  OSX Sierra
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: building from master (command outputs 1.3.0)
- **Python version**:  2.7
- **Bazel version (if compiling from source)**:  0.6.0
- **GCC/Compiler version (if compiling from source)**: 
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1
Apple LLVM version 9.0.0 (clang-900.0.38)
Target: x86_64-apple-darwin16.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
- **CUDA/cuDNN version**: N/A (ran ./configure without CUDA)
- **GPU model and memory**: N/A (no GPU)
- **Exact command to reproduce**:
1. Check out tensorflow
2. Run ./configure, enable XLA support
3. cd tensorflow/compiler/aot
4.  bazel build :tfcompile

### Describe the problem
```
ERROR: /Users/mattrunchey/gitrepos/tensorflow/tensorflow/compiler/xla/service/llvm_ir/BUILD:171:1: C++ compilation of rule '//tensorflow/compiler/xla/service/llvm_ir:kernel_support_library' failed (Exit 1).
tensorflow/compiler/xla/service/llvm_ir/kernel_support_library.cc:101:5: error: no matching function for call to 'transform'
    std::transform(function->arg_begin(), function->arg_end(),
    ^~~~~~~~~~~~~~
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/algorithm:1922:1: note: candidate template ignored: couldn't infer template argument '_UnaryOperation'
transform(_InputIterator __first, _InputIterator __last, _OutputIterator __result, _UnaryOperation __op)
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/algorithm:1932:1: note: candidate function template not viable: requires 5 arguments, but 4 were provided
transform(_InputIterator1 __first1, _InputIterator1 __last1, _InputIterator2 __first2,
^
1 error generated.
```
This happens across multiple OSes, as well (we tried to compile on a unix distro with the same error).  

### Source code / logs
This seems to stem from a recent change in kernel_support_library.cc (specifically https://github.com/tensorflow/tensorflow/commit/c572bc4fd7c73f4b8014ae43cdf9da5b99592f59#diff-877daea43ebeb1cd4756f960400ee922 ).  ",0,,3,2017-12-08T19:23:08Z,NONE,2017-12-08T20:06:40Z
15221,Dataset.from_generator doesn't support strings,,"I have hit the same problem described in https://stackoverflow.com/questions/47705684/tesnorflow-dataset-generator-does-not-work-with-strings

I think it is a missing feature instead of a bug?",0,,1,2017-12-08T19:10:31Z,NONE,2017-12-08T19:26:17Z
15220,ResidualWrapper and HighwayWrapper require rnn inputs' last dimension to be equal to num_units,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: -
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  v1.4.0-rc1-11-g130a514 1.4.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: - 
- **GPU model and memory**: - 
- **Exact command to reproduce**:
~~~python
from tensorflow.contrib import rnn as rnn_cell
from tensorflow.python.ops import rnn
import tensorflow as tf

rnn_inputs = tf.random_uniform([10, 10, 10])
cell = rnn_cell.LSTMCell(128)
cell = rnn_cell.HighwayWrapper(cell) # rnn_cell.ResidualWrapper(cell)
_, _ = rnn.dynamic_rnn(cell, rnn_inputs, dtype=tf.float32)
~~~

### Problem
`rnn_cell.ResidualWrapper` and `rnn_cell.HighwayWrapper` throw an exception when rnn inputs' last dimension is not equal to `num_units`. Without wrappers, the input tensor can be different from `num_units`.
What's the reason for the different behaviour? Is it intended?

### Full Traceback
https://pastebin.com/YvTb3WM3",1,,2,2017-12-08T18:49:46Z,CONTRIBUTOR,2017-12-09T05:36:44Z
15218,stupid question but please help me to answer this,stat:awaiting response,"I have a problem with parallel computing. I know that CUDA or pyCUDA can do such thing like 3D convolution in parallel. 
I think about using tensorflow GPU by calling tf.nn.conv3d(input, filter), does it uses build in functions in CUDA to operate 3D convolution with parallel computing or there are somethings else? 
I am not familiar with C++ or even with pyCuda I am not sure how to implement 3D convolution with it.

Same question with all type of other implement with tensorflow GPU.

Bests,   ",0,,2,2017-12-08T17:27:13Z,NONE,2017-12-09T01:03:56Z
15217,pip.sh: unify the way virtualenv is invoked,cla: yes,"between python3.6 and the other versions

""python -m"" also seems to be a more robust way of calling virtualenv
than relying on the virtualenv command on path.",1,,1,2017-12-08T16:49:10Z,CONTRIBUTOR,2017-12-09T21:30:19Z
15216,"When data become large,parition variables can not initialized successfully",,"i use tensorflow to distributed trainning models,  i use the partition valriables to store an array data, when the data is not so bigger, everything looks ok,but when the array data become larger, when the session initialize, the partition variables can not  initialized and the session will wait util time out.
### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request. the feat_info can initialize successfully, but the adj_info cannot initialized. the adj_info is larger than feat_info
### Source code / logs
i use ps_num = 4, worker_num =4 and i also try some other distributed config, like ps_num=1, worker_num=4, the result is the same
source code:
    with tf.device(tf.train.replica_device_setter(
        worker_device=""/job:worker/task:%d"" % task_id,
        cluster=cluster_spec)):
      
      feat_info = tf.get_variable(""feature_info"", (len(id_map),FLAGS.features_column), tf.float32, trainable=False, partitioner=tf.fixed_size_partitioner(num_workers))
      adj_info = tf.get_variable(""adj_info"", (len(id_map),FLAGS.max_degree), tf.int64, trainable=False, partitioner=tf.fixed_size_partitioner(num_workers))
     
      with tf.device('/job:worker/task:%d' %task_id):
          adj_local = tf.Variable(tf.constant(minibatch.adj, dtype=tf.int64), trainable=False, name=""adj_local"", collections=[tf.GraphKeys.LOCAL_VARIABLES])
          feat_local = tf.Variable(tf.constant(features, dtype=tf.float32), trainable=False, name=""feat_local"", collections=[tf.GraphKeys.LOCAL_VARIABLES])
     
      length, begin, end = split_node_by_task(len(id_map), task_id, num_workers)
      adj = tf.nn.embedding_lookup(adj_info, [x for x in range(begin, end)])
      adj = adj_local
      
      feat = tf.nn.embedding_lookup(feat_info, [x for x in range(begin, end)])
      feat = feat_local

log:
2017-12-08 23:54:17.377290: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session c2b3ba9b700261ba with config: 
INFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  None, ready: Variables not initialized: adj_info/part_0, adj_info/part_1, adj_info/part_2, adj_info/part_3, adj_info/part_4, adj_info/part_5, adj_info/part_6, adj_info/part_7, adj_info/part_8, adj_info/part_9, adj_info/part_10, adj_info/part_11, adj_info/part_12, adj_info/part_13, adj_info/part_14, adj_info/part_15
2017-12-09 00:00:35.637019: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session f35fcf332e3908ec with config: 
INFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  None, ready: Variables not initialized: adj_info/part_0, adj_info/part_1, adj_info/part_2, adj_info/part_3, adj_info/part_4, adj_info/part_5, adj_info/part_6, adj_info/part_7, adj_info/part_8, adj_info/part_9, adj_info/part_10, adj_info/part_11, adj_info/part_12, adj_info/part_13, adj_info/part_14, adj_info/part_15
and it will alway waiting adj_info to initialize",0,,11,2017-12-08T16:03:41Z,NONE,2017-12-09T01:03:52Z
15215,Tensorflow - Unable to import frozen graph with batchnorm : uninitialized value batch_normalization/moving_mean,,"I am trying to freeze in a pbtxt file a checkpoint containing batchnorm layers (ubuntu, python 2.7, tf 1.1.0).

context : 
**Have I written custom code**
Yes, see below

**OS Platform and Distribution**
Docker with Ubuntu 14.04

**TensorFlow installed from**
pip installer

**TensorFlow version**
tensorflow and tensorflow-gpu 1.1.0

**Bazel version**
N/A

**CUDA/cuDNN version**
Cuda 8, CUDNN 5.1

**GPU model and memory**
Nvidia titan-x * 2, 12Go Ram each

**Exact command to reproduce**

For this, following these posts and issues :

https://github.com/davidsandberg/facenet/issues/161

https://github.com/davidsandberg/facenet/pull/172/commits/0f3ece502550714c91056f3a8630ce8c037f613f

I use this function:

    freeze_and_prune_graph(model_path_and_name, output_file=None):
		""""""
		freezes a model trained and saved by the trainer by :
		    - extracting the trainable variables between input_node and output_node
		    - turning them to constants
		    - changing the 1rst dim of input_node to None
		    -saving the resulting graph as a single .pb file

		:param model_path_and_name: must finish by .ckpt, and the checkpoint must be composed of
		3+ files : .ckpt.index, .ckpt.meta, and .ckpt.data-0000X-of-0000Y

		:param model_path_and_name: path to the trained model
		:param output_file: file to save to. If None, model_path_and_name.[-ckpt][+pb]
		:return: None
		""""""
		config_proto = tf.ConfigProto(allow_soft_placement=True)

		with tf.Session(config=config_proto) as sess:
		    new_saver = tf.train.import_meta_graph(model_path_and_name + '.meta', clear_devices=True)
		    tf.get_default_session().run(tf.global_variables_initializer())
		    tf.get_default_session().run(tf.local_variables_initializer())
		    new_saver.restore(sess, model_path_and_name)

		    # get graph definition
		    gd = sess.graph.as_graph_def()
		    # fix batch norm nodes
		    for node in gd.node:
		        if node.op == 'RefSwitch':
		            node.op = 'Switch'
		            for index in xrange(len(node.input)):
		                if 'moving_' in node.input[index]:
		                    node.input[index] = node.input[index] + '/read'
		        elif node.op == 'AssignSub':
		            node.op = 'Sub'
		            if 'use_locking' in node.attr: del node.attr['use_locking']
		        elif node.op == 'AssignAdd':
		            node.op = 'Add'
		            if 'use_locking' in node.attr: del node.attr['use_locking']

		    # tf.get_collection() returns a list. In this example we only want the
		    input_node = sess.graph.get_tensor_by_name('input_node:0')
		    new_shape = [None] + input_node.get_shape().as_list()[1:]

		    trainables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
		    new_graph_def = tf.graph_util.convert_variables_to_constants(sess, gd, [""output_node""],
		                                                                 variable_names_whitelist=[t.name[:-2] for t in trainables] + ['output_node'])

		    for node in new_graph_def.node:
		        if node.name == 'input_node':
		            node.attr['shape'].CopyFrom(attr_value_pb2.AttrValue(shape=tf.TensorShape(new_shape).as_proto()))
		            break

		    with tf.gfile.GFile(output_file, ""wb"") as f:
		        f.write(new_graph_def.SerializeToString())
		    print(""{0} / {1} ops in the final graph."".format(len(new_graph_def.node), len(sess.graph.as_graph_def().node)))

This goes well and creates the pbtxt file with the following output :

> Converted 201 variables to const ops.
5287 / 41028 ops in the final graph.

I then try to load the pbtxt model using this function :

    def load_frozen_graph(frozen_graph_file):
	    """"""
	    loads a graph frozen via freeze_and_prune_graph and returns the graph, its input placeholder and output tensor

	    :param frozen_graph_file: .pb file to load
	    :return: tf.graph, tf.placeholder, tf.tensor
	    """"""
	    # We load the protobuf file from the disk and parse it to retrieve the
	    # unserialized graph_def
	    with tf.gfile.GFile(frozen_graph_file, ""rb"") as f:
	        graph_def = tf.GraphDef()
	        graph_def.ParseFromString(f.read())

	    # Then, we can use again a convenient built-in function to import a graph_def into the
	    # current default Graph
	    with tf.Graph().as_default() as graph:
	        tf.import_graph_def(
	            graph_def,
	            input_map=None,
	            return_elements=None,
	            name=""prefix"",
	            op_dict=None,
	            producer_op_list=None
	        )

	    input_images_placeholder = graph.get_tensor_by_name('prefix/input_node:0')
	    input_phase_placeholder = None
	    try:
	        input_phase_placeholder = graph.get_tensor_by_name('prefix/phase:0')
	    except KeyError:
	        pass
	    output = graph.get_tensor_by_name('prefix/output_node:0')

	    return graph, input_images_placeholder, input_phase_placeholder, output

using the following snippet:

    graph, input_images_placeholder, is_training_placeholder, output = load_frozen_graph(model_pbtxt)
	sess = tf.Session(config=tf_config, graph=graph)
	feed_dict = {input_images_placeholder: prepared_input}
	if is_training_placeholder is not None:
	    feed_dict[is_training_placeholder] = False
	ret = sess.run([output], feed_dict=feed_dict)

This, however, leads to the following error:

> FailedPreconditionError (see above for traceback): Attempting to use uninitialized value prefix/conv0/BatchNorm/batch_normalization/moving_mean
> [[Node: prefix/conv0/BatchNorm/batch_normalization/moving_mean/read = Identity[T=DT_FLOAT, _class=[""loc:@prefix/conv0/BatchNorm/batch_normalization/moving_mean""], _device=""/job:localhost/replica:0/task:0/gpu:0""](prefix/conv0/BatchNorm/batch_normalization/moving_mean)]]
	 [[Node: prefix/output_node/_381 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_2447_prefix/output_node"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Following the question :
https://stackoverflow.com/questions/36007883/tensorflow-attempting-to-use-uninitialized-value-in-variable-initialization
I tried initializing variables:

    graph, input_images_placeholder, is_training_placeholder, output = load_frozen_graph(model_pbtxt)
	sess = tf.Session(config=tf_config, graph=graph)
	init = [tf.global_variables_initializer(), tf.local_variables_initializer()]
	sess.run(init)

	feed_dict = {input_images_placeholder: prepared_input}
	if is_training_placeholder is not None:
	    feed_dict[is_training_placeholder] = False
	ret = sess.run([self.output], feed_dict=feed_dict)

This, however, changes the error to:

> ValueError: Fetch argument <tf.Operation 'init' type=NoOp> cannot be interpreted as a Tensor. 
(Operation name: ""init"" op: ""NoOp"" is not an element of this graph.)

which seems to show that there is no variable that needs to be initialized.

What am I missing ? How to I freeze and reload the relevant values of a batch_normalization layer ?

PS: I do realize that this might better be on stackoverflow, but I posted there first and got no answer in 2 weeks:
https://stackoverflow.com/questions/47434139/tensorflow-unable-to-import-frozen-graph-with-batchnorm-uninitialized-value",0,,3,2017-12-08T15:19:04Z,NONE,2017-12-09T01:03:47Z
15212,lookup.index_table_from_tensor() emits an error in eager mode when invoked more than once.,comp:eager,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: +
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **TensorFlow installed from (source or binary)**: tf-nightly
- **TensorFlow version (use command below)**: 1.5.0-dev20171206
- **Python version**: 3
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: 
~~~python
from tensorflow.python.ops import lookup_ops
import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()

inpt = ['1611', '1612', '1613', '1615', '1616', '1617', '1618', '1619', '1621']

a = lookup_ops.index_table_from_tensor(inpt, name='a')
b = lookup_ops.index_table_from_tensor(inpt, name='b')
~~~

### Problem
When the eager execution is enabled, the following error occurs:
~~~console
FailedPreconditionError: Table already initialized. [Op:InitializeTableV2] name: string_to_index/hash_table/string_to_index/hash_table//string_to_index/hash_table/table_init//
~~~

### Source code / logs
Full trace:
https://pastebin.com/GsKBjyV6

### Question
Is there a way to specify more than one lookup table (a `shared_name` or a special scope)?",1,,7,2017-12-08T13:14:40Z,CONTRIBUTOR,2017-12-08T13:35:50Z
15210,Incorrect Result from Add Function,,"### System information

- Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 16.04.3 LTS
- Bazel version: Not applicable 
- TensorFlow installed from binary
- TensorFlow version: 1.4.0
- Python version: 3.5.2
- CUDA/cuDNN version: 8.0 / 6.0 for CUDA 8.0
- GPU model and memory: GM107M [GeForce GTX 960M] 4GB
- Exact command to reproduce:

Here is a simple program to add: 

session = tf.Session()

a = tf.placeholder(tf.float32)
#print(""first"")
b = tf.placeholder(tf.float32)
#print(""second"")
result_node = tf.add(a,b)
#print(""starting"")
x = session.run(result_node, {a:2.0, b: 3.5})
print(x)

Output should be 5.5, while I am getting 2.0 on 2 machines

![screenshot from 2017-12-08 17-59-49](https://user-images.githubusercontent.com/19254286/33766026-af9682da-dc41-11e7-8f81-1ff054903deb.png)
![screenshot from 2017-12-08 18-00-21](https://user-images.githubusercontent.com/19254286/33766027-afd8aba6-dc41-11e7-9a85-02f2764a9d67.png)

",0,,7,2017-12-08T12:31:12Z,NONE,2017-12-09T01:03:38Z
15207,Feature request: automatically pick all defaults in ./configure,stat:awaiting tensorflower,"Everytime a new configuration option is added my CI halts on the ./configure step as it's prompted for input. Instead of setting each flag could we have some `USE_DEFAULTS=1 ./configure` flag instead?

I assume the intention is to slowly move away from the shell script to something better (hence configure.py, I wager) but for now, when testing projects with the master release it would be very useful to be able to just pick all default options for the WORKSPACE without knowing about them.",0,,4,2017-12-08T11:02:05Z,CONTRIBUTOR,2017-12-08T12:09:24Z
15205,Delete Dockerfile.devel-gpu-cuda9-cudnn7,cla: yes,"our default dockerfiles now use cuda9-cudnn7.
No need for this file anymore.

CC @flx42 ",0,,4,2017-12-08T07:28:47Z,OWNER,2017-12-08T07:47:34Z
15204,compile error with config sycl,stat:community support,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.4,the latest version I git from tensorflow
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:0.8.1
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:no
- **GPU model and memory**:no
- **Exact command to reproduce**:bazel build -c opt --config=sycl //tensorflow/tools/pip_package:build_pip_package


##something else
OpenCl version:1.2
computecpp path:    /usr/local/computecpp  version 0.5.0


computecpp_info:
Device Info:
Discovered 1 devices matching:
  platform    : <any>
  device type : <any>
--------------------------------------------------------------------------------
Device 0:
  Device is supported                     : UNTESTED - Vendor not tested on this OS
  CL_DEVICE_NAME                          : Hainan
  CL_DEVICE_VENDOR                        : Advanced Micro Devices, Inc.
  CL_DRIVER_VERSION                       : 2482.3
  CL_DEVICE_TYPE                          : CL_DEVICE_TYPE_GPU 



clinfo:
Number of platforms                               1
  Platform Name                                   AMD Accelerated Parallel Processing
  Platform Vendor                                 Advanced Micro Devices, Inc.
  Platform Version                                OpenCL 2.0 AMD-APP (2482.3)
  Platform Profile                                FULL_PROFILE
  Platform Extensions                             cl_khr_icd cl_amd_event_callback cl_amd_offline_devices 
  Platform Extensions function suffix             AMD

  Platform Name                                   AMD Accelerated Parallel Processing
Number of devices                                 1
  Device Name                                     Hainan
  Device Vendor                                   Advanced Micro Devices, Inc.
  Device Vendor ID                                0x1002
  Device Version                                  OpenCL 1.2 AMD-APP (2482.3)
  Driver Version                                  2482.3
  Device OpenCL C Version                         OpenCL C 1.2 
  Device Type                                     GPU
  Device Profile                                  FULL_PROFILE
  Device Board Name (AMD)                         AMD Radeon HD 8500M
  Device Topology (AMD)                           PCI-E, 04:00.0
  Max compute units                               4
  SIMD per compute unit (AMD)                     4
  SIMD width (AMD)                                16
  SIMD instruction width (AMD)                    1
  Max clock frequency                             850MHz
  Graphics IP (AMD)                               6.0
  Device Partition                                (core)
    Max number of sub-devices                     4
    Supported partition types                     none specified
  Max work item dimensions                        3
  Max work item sizes                             256x256x256
  Max work group size                             256
  Preferred work group size multiple              64
  Wavefront width (AMD)                           64
  Preferred / native vector sizes                 
    char                                                 4 / 4       
    short                                                2 / 2       
    int                                                  1 / 1       
    long                                                 1 / 1       
    half                                                 1 / 1        (n/a)
    float                                                1 / 1       
    double                                               1 / 1        (cl_khr_fp64)
  Half-precision Floating-point support           (n/a)
  Single-precision Floating-point support         (core)
    Denormals                                     No
    Infinity and NANs                             Yes
    Round to nearest                              Yes
    Round to zero                                 Yes
    Round to infinity                             Yes
    IEEE754-2008 fused multiply-add               Yes
    Support is emulated in software               No
    Correctly-rounded divide and sqrt operations  Yes
  Double-precision Floating-point support         (cl_khr_fp64)
    Denormals                                     Yes
    Infinity and NANs                             Yes
    Round to nearest                              Yes
    Round to zero                                 Yes
    Round to infinity                             Yes
    IEEE754-2008 fused multiply-add               Yes
    Support is emulated in software               No
    Correctly-rounded divide and sqrt operations  No
  Address bits                                    32, Little-Endian
  Global memory size                              2140311552 (1.993GiB)
  Global free memory (AMD)                        <printDeviceInfo:68: get number of CL_DEVICE_GLOBAL_FREE_MEMORY_AMD : error -33>
  Global memory channels (AMD)                    2
  Global memory banks per channel (AMD)           8
  Global memory bank width (AMD)                  256 bytes
  Error Correction support                        No
  Max memory allocation                           1591773593 (1.482GiB)
  Unified memory for Host and Device              No
  Minimum alignment for any data type             128 bytes
  Alignment of base address                       2048 bits (256 bytes)
  Global Memory cache type                        Read/Write
  Global Memory cache size                        16384
  Global Memory cache line                        64 bytes
  Image support                                   Yes
    Max number of samplers per kernel             16
    Max size for 1D images from buffer            134217728 pixels
    Max 1D or 2D image array size                 2048 images
    Base address alignment for 2D image buffers   256 bytes
    Pitch alignment for 2D image buffers          256 bytes
    Max 2D image size                             16384x16384 pixels
    Max 3D image size                             2048x2048x2048 pixels
    Max number of read image args                 128
    Max number of write image args                8
  Local memory type                               Local
  Local memory size                               32768 (32KiB)
  Local memory syze per CU (AMD)                  65536 (64KiB)
  Local memory banks (AMD)                        32
  Max constant buffer size                        65536 (64KiB)
  Max number of constant args                     8
  Max size of kernel argument                     1024
  Queue properties                                
    Out-of-order execution                        No
    Profiling                                     Yes
  Prefer user sync for interop                    Yes
  Profiling timer resolution                      1ns
  Profiling timer offset since Epoch (AMD)        1512650783761772748ns (Thu Dec  7 20:46:23 2017)
  Execution capabilities                          
    Run OpenCL kernels                            Yes
    Run native kernels                            No
    Thread trace supported (AMD)                  No
    SPIR versions                                 1.2
  printf() buffer size                            1048576 (1024KiB)
  Built-in kernels                                
  Device Available                                Yes
  Compiler Available                              Yes
  Linker Available                                Yes
  Device Extensions                               cl_khr_fp64 cl_amd_fp64 cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_gl_sharing cl_amd_device_attribute_query cl_amd_vec3 cl_amd_printf cl_amd_media_ops cl_amd_media_ops2 cl_amd_popcnt cl_khr_image2d_from_buffer cl_khr_spir cl_khr_gl_event 

NULL platform behavior
  clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...)  AMD Accelerated Parallel Processing
  clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...)   Success [AMD]
  clCreateContext(NULL, ...) [default]            Success [AMD]
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU)  Success (1)
    Platform Name                                 AMD Accelerated Parallel Processing
    Device Name                                   Hainan
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL)  Success (1)
    Platform Name                                 AMD Accelerated Parallel Processing
    Device Name                                   Hainan

ICD loader properties
  ICD loader Name                                 OpenCL ICD Loader
  ICD loader Vendor                               OCL Icd free software
  ICD loader Version                              2.2.8
  ICD loader Profile                              OpenCL 1.2
	NOTE:	your OpenCL library declares to support OpenCL 1.2,
		but it seems to support up to OpenCL 2.1 too.

### Describe the problem
I want Compile With sycl config  with the command 
bazel build -c opt --config=sycl //tensorflow/tools/pip_package:build_pip_package
 but  unfortunately get the error 

Illegal ambiguous match on configurable attribute ""deps"" in @local_config_sycl//sycl:sycl:
@local_config_sycl//sycl:using_sycl_ccpp
@local_config_sycl//sycl:using_sycl_trisycl
Multiple matches are not allowed unless one is unambiguously more specialized.

I did not know what casue this compile error. Please Help

",0,,8,2017-12-08T06:34:19Z,NONE,2017-12-09T01:57:50Z
15203,DataLossError : Checksum does not match,"stat:awaiting response,type:support","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:windows 7 professional,64bit
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.3.0
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:N/A
- **GPU model and memory**:N/A
- **Exact command to reproduce**:N/A

### Describe the problem
I trained on Ubuntu16.04 to get the model, and then restore the model on  Windows7 Professional, but it occured such a mistake:

`DataLossError (see above for traceback): Checksum does not match: stored 1713499277 vs. calculated on the restored bytes 1894941567`

`[Node: save/RestoreV2_282 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""]]`

**Both machines have the same version of python and TensorFlow, and the model tested in another machine succeed, but this machine failed**, how to solve it? Thank you!

### Source code / logs

```
Traceback (most recent call last):
  File ""D:/tensorboxPy3/evaluate.py"", line 138, in <module>
    main()
  File ""D:/tensorboxPy3/evaluate.py"", line 117, in main
    get_results(args, H, os.path.dirname(args.datadir))
  File ""D:/tensorboxPy3/evaluate.py"", line 59, in get_results
    saver.restore(sess, args.weights)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1560, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 895, in run
    run_metadata_ptr)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.DataLossError: Checksum does not match: stored 1713499277 vs. calculated on the restored bytes 1894941567
	 [[Node: save/RestoreV2_282 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_282/tensor_names, save/RestoreV2_282/shape_and_slices)]]

Caused by op 'save/RestoreV2_282', defined at:
  File ""D:/tensorboxPy3/evaluate.py"", line 138, in <module>
    main()
  File ""D:/tensorboxPy3/evaluate.py"", line 117, in main
    get_results(args, H, os.path.dirname(args.datadir))
  File ""D:/tensorboxPy3/evaluate.py"", line 55, in get_results
    saver = tf.train.Saver()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1140, in __init__
    self.build()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1172, in build
    filename=self._filename)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 688, in build
    restore_sequentially, reshape)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 407, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 247, in restore_op
    [spec.tensor.dtype])[0])
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 663, in restore_v2
    dtypes=dtypes, name=name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

DataLossError (see above for traceback): Checksum does not match: stored 1713499277 vs. calculated on the restored bytes 1894941567
	 [[Node: save/RestoreV2_282 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_282/tensor_names, save/RestoreV2_282/shape_and_slices)]]
```


`Process finished with exit code 1`

",0,,7,2017-12-08T06:21:22Z,NONE,2017-12-08T20:52:27Z
15199,missing instruction for BahdanauAttention,"stat:contributions welcome,type:feature","This is just my opinion: 
when call BahdanauAttention instance , it create a variable scope with None name_or_scope variable.
While name_or_scope is None, and get_variable with the same name inside the variable scope repeatedly, it will automatically add '_N' to the name of the scope. And I think it is not compatible with some functions like stati_rnn, because there are explicit 'for loop' inside the function and every loop of 'for loop' will create different variables inside the variable scope but not creating once and sharing",0,,8,2017-12-08T02:55:28Z,NONE,2017-12-08T12:57:38Z
15198,Missing documentation for using the Dataset API in combination with image summaries,type:docs,"The `Dataset` API is now the recommended input pipeline, however I am missing some guidance on how to include summaries of my images.

```python
def get_data():
  dataset = FixedLengthRecordDataset(...)
  dataset = dataset.map(parse_dataset, ...)
  if is_training:
    dataset = dataset.map(preprocess_for_train, ...)
  # Do shuffling, batching...
  return dataset

def preprocess_for_train(image, label):
  # Do preprocessing...
  image = tf.image.random_flip_left_right(image)
  # Add summary
  tf.summary.image('preprocessed_image', tf.expand_dims(image, 0))
  return image, label
```

This is what I would do intuitively, but since `map()` uses a different thread and therefore a different `tf.Graph` instance (?), the summaries are lost.

What is the recommended way of adding image summaries when using the `Dataset` API? I would like to request a comment / example on that in the official docs. ",1,,7,2017-12-08T02:32:28Z,NONE,2017-12-09T01:56:05Z
15197,Fix tag in source_remote_test: no_mac --> nomac,cla: yes,,0,,1,2017-12-08T02:19:29Z,CONTRIBUTOR,2017-12-08T02:51:33Z
15196,[XLA] OSX tfcompile compile failure in ../llvm_ir/kernel_support_library.cc ,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.13.2 
- **TensorFlow installed from (source or binary)**: Source 
- **TensorFlow version (use command below)**: Top of Master (34bcd09c5fd4f6435517a499987b7e5044c8f2c0) 
- **Python version**: 
- **Bazel version (if compiling from source)**:   0.7.0
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 

bazel build tensorflow/compiler/aot/tfcompile

### Describe the problem
Compiler error when compiling tfcompile on OSX. This was introduced by @sanjoy  Commit:
https://github.com/tensorflow/tensorflow/commit/c572bc4fd7c73f4b8014ae43cdf9da5b99592f59

You will need this PR to be able to fix other compile issues on OSX. https://github.com/tensorflow/tensorflow/pull/14893


ERROR: /Users/tfninja/github/tensorflow/tensorflow/compiler/xla/service/llvm_ir/BUILD:171:1: C++ compilation of rule '//tensorflow/compiler/xla/service/llvm_ir:kernel_support_library' failed (Exit 1).
tensorflow/compiler/xla/service/llvm_ir/kernel_support_library.cc:99:5: error: no matching function for call to 'transform'
    std::transform(function->arg_begin(), function->arg_end(),
    ^~~~~~~~~~~~~~
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/algorithm:1922:1: note: candidate template ignored: couldn't infer template argument '_UnaryOperation'
transform(_InputIterator __first, _InputIterator __last, _OutputIterator __result, _UnaryOperation __op)
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/algorithm:1932:1: note: candidate function template not viable: requires 5 arguments, but 4 were provided
transform(_InputIterator1 __first1, _InputIterator1 __last1, _InputIterator2 __first2,
^
1 error generated.
Target //tensorflow/compiler/aot:tfcompile failed to build

### Source code / logs


The issue seems to be with this line of code in which works on Linux but fails on OSX/Clang tensorflow/compiler/xla/service/llvm_ir/kernel_support_library.cc

```
+    std::transform(function->arg_begin(), function->arg_end(),
+                   std::back_inserter(arg_values), std::addressof<llvm::Value>);
```
",0,,5,2017-12-08T01:14:47Z,CONTRIBUTOR,2017-12-08T19:43:40Z
15195,"[MSVC] Add tensorflow::ops prefix for {Read,Write}File","awaiting testing (then merge),cla: yes",`ReadFile` and `WriteFile` collide with the functions in `windows.h`. Tell MSVC we want Tensorflow's ones.,1,,2,2017-12-08T00:46:53Z,CONTRIBUTOR,2017-12-08T15:00:14Z
15192,Branch 178260923,cla: yes,,1,,5,2017-12-07T19:16:28Z,CONTRIBUTOR,2017-12-07T19:24:21Z
15190,S3 reads eventually fail with tensorflow's dataset API,,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CPU
- **GPU model and memory**: N/A
- **Exact command to reproduce**: https://gist.github.com/thunterdb/d5f86c79457eea0f1021117ea4bce0ba

### Describe the problem
The given script reads the MNIST data from S3 repeatedly from a public bucket, on an EC2 machine in the same region as the S3 bucket.

Running this script eventually leads to python crashing after a few hours, and the following error:

```
Finished step 31890, time: 05:45:28 12/07/17, result: 32
Finished step 31920, time: 05:46:01 12/07/17, result: 32
terminate called after throwing an instance of 'std::system_error'
  what():  Resource temporarily unavailable
Aborted
```

### Source code / logs
https://gist.github.com/thunterdb/d5f86c79457eea0f1021117ea4bce0ba

It is hard to say what is happening without debugging symbols. From the very generic error, I suspect this is an issue with the S3 SDK. As a workaround, it would be nice for tensorflow's S3 plugin to retry or be more resilient to networking issues.

It is currently an annoying issue when running large distributed training jobs, because they crash after a few hours of reading data. Is anyone having a similar experience with S3?",0,,14,2017-12-07T19:07:40Z,NONE,2017-12-07T19:11:37Z
15189, fix #15188 replaced isnan with std::isnan to avoid build error ,cla: yes,,1,,11,2017-12-07T18:13:22Z,CONTRIBUTOR,2017-12-07T18:15:43Z
15188,error: 'isnan' was not declared in this scope,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: Master branch commit: `4ad12049`
- **Python version**: N/A
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**: g++ 5.4.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: `bazel build //tensorflow:libtensorflow.so`

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I'm trying to compile using `bazel build //tensorflow:libtensorflow.so`.
I'm getting this error:
```
ERROR: /home/rdi/Workspace/Common/tensorflow/tensorflow/core/BUILD:577:1: C++ compilation of rule '//tensorflow/core:random_ops_op_lib' failed (Exit 1)
In file included from ./tensorflow/core/framework/allocator.h:23:0,
                 from ./tensorflow/core/framework/tensor.h:20,
                 from ./tensorflow/core/framework/attr_value_util.h:24,
                 from ./tensorflow/core/framework/node_def_util.h:23,
                 from ./tensorflow/core/framework/shape_inference.h:20,
                 from ./tensorflow/core/framework/common_shape_fns.h:20,
                 from tensorflow/core/ops/random_ops.cc:16:
./tensorflow/core/framework/numeric_types.h: In constructor 'tensorflow::bfloat16::bfloat16(float)':
./tensorflow/core/framework/numeric_types.h:49:16: error: 'isnan' was not declared in this scope
     if (isnan(v)) {
                ^
```
",0,,1,2017-12-07T18:08:14Z,CONTRIBUTOR,2017-12-07T18:09:35Z
15187,Cherrypicks for 1.4.1,cla: yes,,0,,1,2017-12-07T18:03:50Z,MEMBER,2017-12-07T20:04:26Z
15186,Fix assert_called error on Python3,cla: yes,by replacing it with assertTrue(....called),0,,1,2017-12-07T15:17:07Z,CONTRIBUTOR,2017-12-07T15:17:25Z
15185,[XLA] Add fast path cases for common scatter and gather operations,"awaiting testing (then merge),cla: yes","This change checks if the indices vector passed to a scatter or gather operation is a constant, and does a fast-path operation when it is filled with a zero-based incrementing set.

This is quite a common case because of tensor-array stack and unstack.

",1,,6,2017-12-07T15:12:51Z,CONTRIBUTOR,2017-12-07T15:14:42Z
15182,GPU: Add Complex kernel for tf.exp(),"awaiting testing (then merge),cla: yes","Fix #15103.

Because it's my first contribution for GPU kernel, the PR might be not good. Welcome to feedback, and any help will be appreciated. Thanks.

### How to test

+ [x] add test case.
+ [ ] pass all test.",2,,4,2017-12-07T11:58:42Z,CONTRIBUTOR,2017-12-14T20:13:01Z
15181,Add un-fed placeholder warning to negative-dimension error,"awaiting testing (then merge),cla: yes","Sometimes a user runs a tensor and forgets to initialize one of its placeholders in `feed_dict`. If that placeholder has a shape that includes `None`, the `None` gets converted to -1 and rejected at the C++ level, resulting in [confusing](https://stackoverflow.com/q/45059428/1979005) [error](https://stackoverflow.com/a/45480003/1979005) [messages](https://stackoverflow.com/q/44706840/1979005) (each word a separate link). This problem also appears in #11371.

This PR is a two-line change that briefly mentions this issue in the exception string. The hope is that it will help developers find the problem more quickly.",1,,2,2017-12-07T11:33:57Z,CONTRIBUTOR,2017-12-07T16:41:03Z
15180,More simpler examples of using dataflow_ops.StagingArea,stat:awaiting response,"### Describe the problem
Getting the below error when using StagingArea.
`ValueError: Fetch argument <tf.Operation 'group_deps' type=NoOp> cannot be interpreted as a Tensor. (Operation name: ""group_deps""
op: ""NoOp""`

The error happens only after completing a few 100 steps. It would be a great help if someone can place simpler examples of proper usage of StagingArea

### Source code / logs
`compute_stage_put_op = compute_stage.put(iterator.get_next())
  if compute_stage_put_op.type == 'Stage':
         compute_stage_ops.append(compute_stage_put_op)`
",0,,6,2017-12-07T09:29:46Z,NONE,2017-12-07T19:37:26Z
15179,Failure in LMDBReaderTest while reading testdata,"stat:awaiting tensorflower,stat:community support","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: s390x Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.4.0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: gcc 5.4.0
- **CUDA/cuDNN version**: No GPU
- **GPU model and memory**: No GPU
- **Exact command to reproduce**: bazel test -c opt  //tensorflow/python/kernel_tests:reader_ops_test

### Describe the problem
While executing `reader_ops_test`, came across failure in `LMDBReaderTest`. 
Since I am running it on a big endian system, the failure could be because the testdata(data.mdb) is platform specific and hence gets interpreted wrongly. 
@bowang, @jhseu , Is my understanding correct? How can I generate the above testdata for s390x(big endian)?

### Source code / logs
```
F tensorflow/core/kernels/lmdb_reader_op.cc:49] Check failed: mdb_env_open(mdb_env_, current_work().c_str(), flags, 0664) == 0 (-30793 vs. 0)Invalid argument
012
234
012
234
012
234
012
234
012
234
012
234
Aborted (core dumped)
```
",0,,4,2017-12-07T08:44:16Z,CONTRIBUTOR,2017-12-07T18:28:45Z
15178,Dataset.from_generator doesn't play nice with feature_column.categorical_column_with_vocabulary_list,type:bug/performance,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux 4.4.0-98-generic #121~14.04.1-Ubuntu SMP Wed Oct 11 11:54:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.4.0-rc1-11-g130a514', '1.4.0')
- **Python version**: Python 2.7.6
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**:  -
- **GPU model and memory**: -
- **Exact command to reproduce**: curl -L -o predict_not_working.py 'https://drive.google.com/uc?authuser=0&id=1KHCNOfJOpEFSLzzJqjtP6oy0EvuLvETE&export=download' && python predict_not_working.py

### Describe the problem

I have the same code in two versions, [one using Dataset.from_generator](https://drive.google.com/open?id=1KHCNOfJOpEFSLzzJqjtP6oy0EvuLvETE) and [one using Dataset.from_tensor_slices](https://drive.google.com/open?id=1BFmIEnpuRWPeghUfeBD4225BXZqvLcmS). To me it looks like the created Datasets have the exact same content, but feature_column.categorical_column_with_vocabulary_list doesn't work with the `from_generator` one, which ought to be a bug, right?

### Source code / logs

Sources are in links above.",1,,5,2017-12-07T08:23:46Z,NONE,2017-12-07T18:45:59Z
15175,"Got ""Attempting to use uninitialized value"" error after variable initalization",,"I am working on windows 10, Python 3.6 and tensorflow 1.4.0. I tested the code on two laptops, one with gpu and another without, both of them had this problem.

This is my code:

`def network(self, net_input):
        dense1 = tf.layers.dense(net_input, 64)
        norm1 = tf.contrib.layers.batch_norm(dense1)
        relu1 = tf.nn.relu(norm1)
        dense2 = tf.layers.dense(relu1, 32)
        norm2 = tf.contrib.layers.batch_norm(dense2)
        relu2 = tf.nn.relu(norm2)
        out = tf.layers.dense(relu2, 1)
        return out`

`def train(self):
        init_global = tf.global_variables_initializer()
        init_local = tf.local_variables_initializer()
        sess = tf.InteractiveSession()
        #sess = tf.Session()
        sess.run(init_global)
        sess.run(init_local)
        data = tf.placeholder(tf.float32, [self.batch_size, 53])
        label = tf.placeholder(tf.float32, [self.batch_size, 1])
        prediction = self.network(data)
        loss = tf.reduce_mean(tf.reduce_sum(tf.square(
                label - prediction),reduction_indices=[1]))
        train_step = tf.train.GradientDescentOptimizer(1e-3).minimize(loss)
    
        for i in range(self.epoch):
            for j in range(20000 // self.batch_size):
                batch_data, batch_label = self.next_batch(self.train_data, self.train_label)
                #batch_label = self.next_batch(self.train_label)
                sess.run(train_step, feed_dict = {data : batch_data, label : batch_label})
                if j % 50 == 0:
                    start = time.time()
                    loss = sess.run(loss, feed_dict = {data : batch_data, label : batch_label})
                    print('Epoch: [%2d/%2d], Iter: [%4d/%4d], Loss: %8f, Time cost: %8f'%\
                          (i, self.epoch, j, 20000//self.batch_size, loss, time.time()-start))`

and this is the error message:

`Traceback (most recent call last):

  File ""<ipython-input-17-6e018316c758>"", line 1, in <module>
    runfile('C:/Users/System_Error/Downloads/report1/regression_tf.py', wdir='C:/Users/System_Error/Downloads/report1')

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 880, in runfile
    execfile(filename, namespace)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/System_Error/Downloads/report1/regression_tf.py"", line 99, in <module>
    main()

  File ""C:/Users/System_Error/Downloads/report1/regression_tf.py"", line 92, in main
    regression.train()

  File ""C:/Users/System_Error/Downloads/report1/regression_tf.py"", line 76, in train
    sess.run(train_step, feed_dict = {data : batch_data, label : batch_label})

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 889, in run
    run_metadata_ptr)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1317, in _do_run
    options, run_metadata)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)

`FailedPreconditionError:` Attempting to use uninitialized value dense_18/bias
	 [[Node: dense_18/bias/read = Identity[T=DT_FLOAT, _class=[""loc:@dense_18/bias""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](dense_18/bias)]]

Caused by op 'dense_18/bias/read', defined at:
  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\ipython\start_kernel.py"", line 231, in <module>
    main()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\ipython\start_kernel.py"", line 227, in main
    kernel.start()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\zmq\eventloop\ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tornado\ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2717, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2827, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-17-6e018316c758>"", line 1, in <module>
    runfile('C:/Users/System_Error/Downloads/report1/regression_tf.py', wdir='C:/Users/System_Error/Downloads/report1')
  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 880, in runfile
    execfile(filename, namespace)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)
  File ""C:/Users/System_Error/Downloads/report1/regression_tf.py"", line 99, in <module>
    main()
  File ""C:/Users/System_Error/Downloads/report1/regression_tf.py"", line 92, in main
    regression.train()
  File ""C:/Users/System_Error/Downloads/report1/regression_tf.py"", line 67, in train
    prediction = self.network(data)
  File ""C:/Users/System_Error/Downloads/report1/regression_tf.py"", line 26, in network
    dense1 = tf.layers.dense(net_input, 64)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\layers\core.py"", line 250, in dense
    return layer.apply(inputs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\layers\base.py"", line 671, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\layers\base.py"", line 559, in __call__
    self.build(input_shapes[0])
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\layers\core.py"", line 145, in build
    trainable=True)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\layers\base.py"", line 458, in add_variable
    trainable=trainable and self.trainable)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 1203, in get_variable
    constraint=constraint)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 1092, in get_variable
    constraint=constraint)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 425, in get_variable
    constraint=constraint)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 394, in _true_getter
    use_resource=use_resource, constraint=constraint)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 805, in _get_single_variable
    constraint=constraint)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\variables.py"", line 213, in __init__
    constraint=constraint)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\variables.py"", line 356, in _init_from_args
    self._snapshot = array_ops.identity(self._variable, name=""read"")
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 125, in identity
    return gen_array_ops.identity(input, name=name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 2070, in identity
    ""Identity"", input=input, name=name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

FailedPreconditionError (see above for traceback): Attempting to use uninitialized value dense_18/bias
	 [[Node: dense_18/bias/read = Identity[T=DT_FLOAT, _class=[""loc:@dense_18/bias""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](dense_18/bias)]]`


I searched stack overflow, most of the answers told me to add an global_variables_initializer, and I tried tf.Session and tf.InteractiveSession, global and local variables initializer, but still got this error. 






",0,,1,2017-12-07T04:04:51Z,NONE,2017-12-07T04:10:34Z
15174,Branch 178185697,cla: yes,,1,,5,2017-12-07T03:47:41Z,CONTRIBUTOR,2017-12-07T03:54:42Z
15173,Fix for #12537,"awaiting testing (then merge),cla: yes","I have built the fix on x86 linux no gpu and it's working.

There are some android specific code in logging that I ignored.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/logging.cc#L35-L75",1,,11,2017-12-07T02:17:27Z,CONTRIBUTOR,2017-12-07T02:19:06Z
15171,[FeatureRequest] Decorator for estimator input_fn,"stat:awaiting tensorflower,type:feature","In the documentation for [Passing input_fn Data to Your Model](https://www.tensorflow.org/get_started/input_fn#passing_input_fn_data_to_your_model) there are a few methods provided for using a function to construct input data and then wrapping that to get a function object. The documentation suggests `functools.partial` or `lambda`.

Would it be possible to provide a python decorator (maybe under the `tf.estimator` module)? 

Looking at suggested decorator and usage below, I think it makes the code cleaner when making these reusable input functions. 

## Proposed decorator
_n.b. name to be improved/aligned with TF_
```python
import functools

def tf_data_input_fn(old_func):
    def inside(*args, **kwargs):
        return functools.partial(old_func, *args, **kwargs)
    return inside
```

## Usage
```python
@tf_data_input_fn
def my_input_fn(data_set):
    # Construct dataset, repeat, maps etc
    features, labels = dataset.make_one_shot_iterator().get_next()
    return features, labels

train_input_fn = my_input_fn(training_set)
eval_input_fn = my_input_fn(test_set)

classifier.train(input_fn=train_input_fn, steps=2000)
classifier.evaluate(input_fn= eval_input_fn, steps=2000)
```

If this is favourable, I'd be happy to provide a contribution towards such an addition.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: v1.4.0-8-gbca50da6eb 1.4.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A",0,,10,2017-12-07T01:45:44Z,NONE,2017-12-07T12:30:03Z
15170,Go: Don't require -std=c99 for the cgo code.,cla: yes,"This should fix the error:
github.com/tensorflow/tensorflow/tensorflow/go/graph.go:31:3: error:
'for' loop initial declarations are only allowed in C99 mode
 //  for (int i = 0; i < num_shapes; i++) {
    ^

in some continuous builds like:
https://ci.tensorflow.org/job/tensorflow-master-cpu/3297/consoleFull

Alternative to #15169",1,,3,2017-12-06T22:30:54Z,MEMBER,2017-12-06T22:31:02Z
15169,DO NOT SUBMIT: Temporarily disable go:test,cla: yes,context: b/70154286,1,,3,2017-12-06T22:03:36Z,CONTRIBUTOR,2017-12-06T22:04:11Z
15167,Fix some Bash issues,"awaiting testing (then merge),cla: yes","Argument mixes string and array. Use `*` or separate argument.
Don't use `$` on the left side of assignments.",1,,2,2017-12-06T20:35:47Z,CONTRIBUTOR,2017-12-07T04:07:29Z
15166,[CMake] Add bazel tests for python file lists,"awaiting review,cla: yes","Progressing #10296 
",1,,25,2017-12-06T20:15:04Z,CONTRIBUTOR,2017-12-08T15:16:23Z
15163,Fix problem with camera on Android TV,"awaiting review,cla: yes","For a scenario of using a usb external camera, we should use Camera API 2 according to the definition:
Camera1 API is framework API that had been created to support HALv1.x
Camera2 API is a new API that is meant for HALv3.x.

However, **CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_FULL)** should return false as USB camera doesn' support all the necessary capabilities. 
https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#INFO_SUPPORTED_HARDWARE_LEVEL_FULL

This is a work around to use **facing == CameraCharacteristics.LENS_FACING_EXTERNAL** to detect usb external camera and use Camera2 API instead.",1,,4,2017-12-06T19:03:15Z,CONTRIBUTOR,2017-12-13T19:04:45Z
15162,"tf.data.Dataset.from_generator creates too many threads throwing ""thread constructor failed""",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Adapted an example from documentation
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Mac OS Siera (10.12.6)
- **TensorFlow installed from (source or binary)**:pip install tensorflow 
- **TensorFlow version (use command below)**: tensorflow-1.4.0-cp27-cp27m-macosx_10_11_x86_64.whl
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: CPU
- **GPU model and memory**: NA
- **Exact command to reproduce**:

```
import itertools
import tensorflow as tf

def gen():
  for i in itertools.count(1):
    yield (i, [1] * 5)

ds = tf.data.Dataset.from_generator(
    gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))
ds = ds.make_one_shot_iterator()

with tf.Session() as sess:
    while True:
        sess.run(ds.get_next())  # (1, array([1, 1, 1, 1, 1]))...
```

### Describe the problem
If you run the code above, which is adapted from tf.data.Dataset.from_generator docstring, the program will crash with an error: `libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: thread constructor failed: Resource temporarily unavailable`.

I see the number of threads increasing in the activity monitor of the mac OS and when it reaches ~3K threads the program crashes. It takes several seconds. 

Please let me know if this is not intended use of this API, it is OS releated issue or there is a bug involved.
",1,,2,2017-12-06T18:50:12Z,NONE,2017-12-06T19:09:56Z
15160,Installing Tensorflow from source,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Redhat
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.4.1
- **Python version**: 
2.7.14
- **Bazel version (if compiling from source)**:
0.7.0
- **GCC/Compiler version (if compiling from source)**:
gcc 4.8.5

- **CUDA/cuDNN version**:
8/6
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I am trying to install TF. Its the last part of building TF from source.

pip install /tmp/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27mu-linux_x86_64.whl 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

[amalik@node05 tensorflow]$ pip install /tmp/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27mu-linux_x86_64.whl 
Processing /tmp/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27mu-linux_x86_64.whl
Requirement already satisfied: enum34>=1.1.6 in /lfs1/software7/anaconda2/lib/python2.7/site-packages (from tensorflow==1.4.1)
Requirement already satisfied: backports.weakref>=1.0rc1 in /lfs1/software7/anaconda2/lib/python2.7/site-packages (from tensorflow==1.4.1)
Requirement already satisfied: wheel in /lfs1/software7/anaconda2/lib/python2.7/site-packages (from tensorflow==1.4.1)
Requirement already satisfied: mock>=2.0.0 in /lfs1/software7/anaconda2/lib/python2.7/site-packages (from tensorflow==1.4.1)
Collecting tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow==1.4.1)
  Retrying (Retry(total=4, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f384a2261d0>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-tensorboard/
  Retrying (Retry(total=3, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f384a226350>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-tensorboard/
  Retrying (Retry(total=2, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f384a2264d0>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-tensorboard/
  Retrying (Retry(total=1, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f384a226650>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-tensorboard/
  Retrying (Retry(total=0, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f384a2267d0>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-tensorboard/
  Could not find a version that satisfies the requirement tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow==1.4.1) (from versions: )
No matching distribution found for tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow==1.4.1)



",0,,1,2017-12-06T15:54:22Z,NONE,2017-12-06T17:08:40Z
15159,Unable access S3 using the S3 filesystem,stat:awaiting response,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  docker container based on centos7
- **TensorFlow installed from (source or binary)**: binary, from pip
- **TensorFlow version (use command below)**: ('v1.4.0-rc1-11-g130a514', '1.4.0')
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**:
```
from tensorflow.python.lib.io import file_io
file_io.stat('s3://datasets.elasticmapreduce/ngrams/books/20090715/eng-1M/1gram/data')
```


### Describe the problem
I'm unable to read files using the s3 filesystem. I believe my example above should work, please correct my usage of the api if not. I tried using both an object in a public bucket and one in a private bucket that I have credentials in my ~/.aws/config for. 

I'm not quite sure what the error is, or how to diagnose it further. Setting the log level to Debug had no effect. `tf.logging.set_verbosity(tf.logging.DEBUG)` 

### Source code / logs

Traceback from file_io.stat
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 540, in stat
    return file_statistics
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Object s3://datasets.elasticmapreduce/ngrams/books/20090715/eng-1M/1gram/data does not exist
```

Using the aws cli, you can verify the object exists 
```
aws s3 ls s3://datasets.elasticmapreduce/ngrams/books/20090715/eng-1M/1gram/data
```",0,,17,2017-12-06T15:37:22Z,NONE,2017-12-06T18:01:33Z
15158,"Feature: MonitoredSession should have run() method with 'hooks_to_trigger' argument: run(..., hooks_to_trigger=[hooks[1], hooks[3], ...])",,"At the moment the session_run_hooks are passed to the constructor of MonitoredSession(..., hooks=[...]) 
and then they get executed for EVERY session.run() call within the MonitoredSession block.

This is inefficient and problematic.

E.g., if I define a LoggingTensorHook, I want the logging output to be evaluated and printed at most ONCE per global step and not after some auxiliary session.run() calls that only evaluate the size of some queue or whatever else.

If you use feedable iterators, the current MonitoredSession implementation actually crashes the program, see https://github.com/tensorflow/tensorflow/issues/12859#issuecomment-348290076

The best solution in my opinion would be, to be able to specify which run() calls should actually trigger the before_run and after_run methods of the hooks, e.g. via some flag-argument in MonitoredSession.run(..., execute_hooks=True)
or alternatively pass a list of specific hooks whose before_run and after_run methods should be triggered by a run call, via MonitoredSession.run(..., hooks_to_trigger=[...])
",0,,8,2017-12-06T14:50:46Z,NONE,2017-12-06T17:05:57Z
15157,Feature: GANEstimator allow passing of namedtuples,,"In the current `GANEstimator` implementation in `train.py` `gan_model(..)` both `real_data` as well as `generator_inputs` is converted to tensors with either `_convert_tensor_or_l_or_d` or `ops.convert_to_tensor`. This prevents the user from using own data structures like `namedtuples` to pass information between the `generator` and `discriminator`.

In the current implementation when passing a `namedtuple` the result will be a `list` with all name information being lost.

I propose to either extend the tensor conversion to exclude namedtuples from them or to remove them entirely.

@joel-shor Do you think that is a good idea? I am currently passing logits as well as sample_id from a dynamic_decoding around and I would like to keep the meaning of these across loss_fn and discriminator_fn.

1. I could remove the conversions entirely. This would introduce breaking changes.
2. I could exclude namedtuples from the conversion.
3. Idk yet??

I would create a PR for any of them if we find a suitable solution. ",1,,4,2017-12-06T14:44:17Z,CONTRIBUTOR,2017-12-07T01:44:54Z
15155,Input too short to compute filterbank,,"Hi,
I am new to tensorflow and i am trying to train model with my own data but i am getting below error

2017-12-06 18:56:38.030081: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030095: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030105: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030162: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030203: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030243: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030256: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030267: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030305: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030347: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030359: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030370: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030408: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030446: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030458: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030469: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030481: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030492: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030534: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030547: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030558: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030569: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030632: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030645: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030656: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030668: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030679: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030720: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030732: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030743: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030754: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030790: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030851: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030865: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030877: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030937: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030954: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030967: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031028: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031043: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031054: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031066: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031078: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031118: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031132: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031144: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031155: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031211: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031227: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031238: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031249: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031309: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031325: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031338: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031349: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031408: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031425: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031437: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031474: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031512: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031525: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031537: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031573: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031608: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031620: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031631: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031643: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031687: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031699: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031710: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031769: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031799: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031812: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031823: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031834: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031965: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032014: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032024: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032036: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032046: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032057: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032069: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032081: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032128: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032141: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032153: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032165: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032176: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032236: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032248: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032260: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032272: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032313: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032326: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032336: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032347: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032360: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032399: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank

and i am using below command to train
bazel run tensorflow/examples/speech_commands:train -- \ --data_dir=sound --wanted_words=yes,no --data_url=
",0,,1,2017-12-06T13:31:40Z,NONE,2017-12-06T18:05:32Z
15152,Strange Dataset API behaviour,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
v1.4.0-rc1-11-g130a514 1.4.0
- **Python version**: 
sys.version_info(major=3, minor=4, micro=3, releaselevel='final', serial=0)
- **Bazel version (if compiling from source)**:
n/a
- **GCC/Compiler version (if compiling from source)**:
n/a
- **CUDA/cuDNN version**:
Cuda 8.0 
- **GPU model and memory**:
Titan XP 12Gb
- **Exact command to reproduce**:

### Describe the problem
I get very strange behaviour of image read function. See attached screenshot from tensorboard. This is NOT a tensorboard problem as I get images as they are from Dataset object.
![kodak_messed](https://user-images.githubusercontent.com/6204851/33659749-37586c4e-da79-11e7-8977-7d8f6da31c2d.png)

Source images are fine, I have attached a zip archive with source images:
[Kodak.zip](https://github.com/tensorflow/tensorflow/files/1534996/Kodak.zip)

```
import glob
import tensorflow as tf

def simply_read_image(image_path):
    image_string = tf.read_file(image_path)
    image_source = tf.image.decode_png(image_string, channels=3)
    image_source = tf.image.convert_image_dtype(image_source, dtype=tf.float32)
    return image_source

validation_files = glob.glob(os.path.join(validation_folder, '*.png'))

dataset = tf.data.Dataset.from_tensor_slices(validation_files)
dataset = dataset.map(simply_read_image).batch(len(validation_files)).repeat(30)

next_element = iterator.get_next()

    with tf.Session() as s:
        for i in range(30):
            im = s.run(next_element)
            for j in range(25):
                current_image = im[j]
                pass

```
Any ` current_image` with portrait orientation seems to be read incorrectly. 
### Source code / logs
There are no logs that could help",1,,2,2017-12-06T11:38:01Z,NONE,2017-12-06T17:51:57Z
15148,Using function defun and while loops,type:bug/performance,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: 
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8/6
- **GPU model and memory**: Tesla x Pascal 12gb
- **Exact command to reproduce**: run defun_while.py

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Function.defun when used alongside a while loop is throwing shape errors.

More explicitly: when using a concatenation operation of sliced tensors with the loop variable, the newly defined op throws no errors.  However, when the slices are added to the loop variable, it throws shape errors related to the while loop. 

It might be an issue with the fetch argument that fails to work in this case.

The code (along with a more detailed explanation) can be found on:
https://stackoverflow.com/questions/47646962/tensorflow-function-defun-with-a-a-while-loop-in-the-body-is-throwing-shape-err

 




### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,4,2017-12-06T08:38:54Z,NONE,2017-12-06T17:21:27Z
15147,summary_image_op_test_fixed_on_ppc64le,"awaiting review,cla: yes","Hi @gunan , 
As we discussed here https://github.com/tensorflow/tensorflow/issues/12325 , I have created this PR to fix summary_image_op_test on ppc64le.

Thanks!",1,,6,2017-12-06T08:34:16Z,CONTRIBUTOR,2017-12-07T06:07:34Z
15146,Allow keras applications to load weights from arbitrary path,"awaiting testing (then merge),cla: yes","This PR allows tensorflow.python.keras.applications to load pretrained weights from an arbitrary filepath (rather than only ~/.keras/models).  It is the parallel PR to https://github.com/fchollet/keras/pull/8637 which was merged by @fchollet on November 30.

This change allows useres to load models in environments with limited access to ~/.keras/models 

Kaggle notebooks are an example of this environment, and this PR will help us support Keras in TensorFlow. 

I have locally tested that I get the same predictions when loading a model with `weights='imagenet'` and with `weights` pointing to another location with the same pretrained model file.
",1,,2,2017-12-06T07:41:19Z,CONTRIBUTOR,2017-12-11T20:28:09Z
15141,MKL: Revving mkl-dnn to include all changes before 2017-11-20.,"awaiting testing (then merge),cla: yes","This commit will pull the latest changes from the mkl-dnn tree.

@jart the mirror.bazel.build URL doesn't exist: ""https://mirror.bazel.build/github.com/01org/mkl-dnn/archive/aab753280e83137ba955f8f19d72cb6aaba545ef.tar.gz"" can you create it?",1,,3,2017-12-05T22:30:35Z,CONTRIBUTOR,2017-12-06T14:38:40Z
15139,Support --config=monolithic in tf.sysconfig.get_link_flags(),"awaiting testing (then merge),cla: yes","Currently, `tf.sysconfig.get_link_flags()` always adds `-ltensorflow_framework`.  With this change, it would check whether TensorFlow was built with `--config=monolithic`.",1,,13,2017-12-05T20:22:48Z,CONTRIBUTOR,2017-12-05T20:24:57Z
15138,Document tf-coreml converter in lite/README.md,"cla: yes,comp:lite",,0,,2,2017-12-05T20:14:29Z,CONTRIBUTOR,2017-12-05T20:15:02Z
15137,Tensorflow broken by new Bazel versions,stat:awaiting tensorflower,"Simplest way to reproduce the issue, run:
`$ bazel build --config=opt --incompatible_load_argument_is_label --nobuild //tensorflow/tools/pip_package:build_pip_package`

Suggested fix to `tensorflow/third_party/sycl/sycl/BUILD.tpl`:
```
-load(""platform"", ""sycl_library_path"")
+load("":platform.bzl"", ""sycl_library_path"")

-load(""platform"", ""readlink_command"")
+load("":platform.bzl"", ""readlink_command"")
```

This should address the immediate need.
There are other issues to fix (although not as pressing). You can see them by building using `--all_incompatible_changes`.

Let me know if you need any help.
Thanks!",0,,5,2017-12-05T18:46:47Z,NONE,2017-12-05T18:56:14Z
15135,Solved for MNIST file downloading problem,"awaiting review,cla: yes","A lot of folks (#6742, #8126, #8134,#8116) were having trouble regarding this issue. Even I faced it today while writing the MNIST code as the mnist.py was unable to connect with the source url. I downloaded the files and pasted it in the folder containing the code and made some changes in the code which solves the need for connecting it to the website. Hope so this works out.",1,,9,2017-12-05T16:53:17Z,NONE,2017-12-07T15:15:00Z
15133,"Revert ""Speed up safe_strtod and safe_strtof functions by using doubl",cla: yes,"e-conversion library (#12102)""

This reverts commit 495bb7b9f6b55b0e431fc604ad9dbf5415016d90.",0,,3,2017-12-05T15:06:55Z,CONTRIBUTOR,2017-12-05T16:25:58Z
15132,Add decode_compressed support,"awaiting testing (then merge),cla: yes","This fix tries to address the issue raised in #14887 to add decode_compressed support.

The API will take a string Tensor (compressed with either ZLIB or GZIP) and output a string Tensor of the same shape with content uncompressed.

This fix fixes #14887.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,10,2017-12-05T15:04:15Z,MEMBER,2017-12-19T02:02:23Z
15129,build error: undefined reference to `clock_gettime',stat:community support,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux CentOS 6.9
- **TensorFlow installed from (source or binary)**:
source 
- **TensorFlow version (use command below)**:
master branch: the latest version
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.8.0
- **GCC/Compiler version (if compiling from source)**:
4.8.2
- **CUDA/cuDNN version**:
No
- **GPU model and memory**:
No
- **Exact command to reproduce**:
bazel build --linkopt=-lrt -c opt --verbose_failures //tensorflow:libtensorflow_cc.so

### Describe the problem
I tried to build the tensor flow c++ lib from the source code, but it failed. 

### Source code / logs
ERROR: /home/baigang/Projects/xylib/thirdparty/tenserflow/package/tensorflow/tensorflow/cc/BUILD:422:1: Linking of rule '//tensorflow/cc:ops/random_ops_gen_cc' failed (Exit 1): gcc failed: error executing command 
  (cd /home/baigang/.cache/bazel/_bazel_baigang/d3e5550086b82aa173767408d0f485e7/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/baigang/bin \
    PWD=/proc/self/cwd \
  /usr/bin/gcc -o bazel-out/host/bin/tensorflow/cc/ops/random_ops_gen_cc '-Wl,-rpath,$ORIGIN/../../../_solib_k8/_U_S_Stensorflow_Scc_Cops_Srandom_Uops_Ugen_Ucc___Utensorflow' -Lbazel-out/host/bin/_solib_k8/_U_S_Stensorflow_Scc_Cops_Srandom_Uops_Ugen_Ucc___Utensorflow '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../..' -pthread -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,-S -Wl,@bazel-out/host/bin/tensorflow/cc/ops/random_ops_gen_cc-2.params)
bazel-out/host/bin/_solib_k8/_U_S_Stensorflow_Scc_Cops_Srandom_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `clock_gettime'
collect2: error: ld returned 1 exit status
Target //tensorflow:libtensorflow_cc.so failed to build
INFO: Elapsed time: 418.738s, Critical Path: 35.11s
FAILED: Build did NOT complete successfully",0,,9,2017-12-05T14:19:20Z,NONE,2017-12-05T18:58:43Z
15128,GPU memory increases in multiples of batch_size 64 with allow_growth=True,type:support,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.4.0-14-gb5df90f', '1.4.1')
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0-6
- **CUDA/cuDNN version**: V9.0.176
- **GPU model and memory**: V100 AWS P3 8X large instance
- ** Exact command to reproduce**: Inception V3 training as mentioned in `models/research/slim`. `python train_image_classifier.py`


### Describe the problem
GPU memory is increasing in multiples of batch_size of 64. Using 8.9 GB of GPU memory for `batch_size=16` or `batch_size=32` or `batch_size=64`. And using 15.6 GB of GPU memory for `batch_size=96` and `batch_size=128`. I'd like to use `batch_size=96` so allocator won't throw warnings during global_step as it will have 2-3 GB unused.

Is this a feature? Can we change its functioning?",0,,3,2017-12-05T14:08:47Z,NONE,2017-12-06T17:16:19Z
