number,title,labels,body,assignees,milestone,comments,created_at,closed_at,author_association,address_date
16731,Fixed a couple of typos,cla: yes,Fixed a couple of typos,0,,3,2018-02-03T18:42:26Z,2018-02-03T23:22:23Z,CONTRIBUTOR,2018-02-03T18:48:31Z
16716,Linker Tools Error encountered when use StepStats,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10.0.16299
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.5 release
- **Python version**: 
3.5.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 



### Describe the problem

Encounter link error when build the program (source code attached). Build went through well with TF 1.4 release 

Error	LNK2001	unresolved external symbol ""class tensorflow::StepStatsDefaultTypeInternal tensorflow::_StepStats_default_instance_"" (?_StepStats_default_instance_@tensorflow@@3VStepStatsDefaultTypeInternal@1@A)	ReprBug	c:\Users\xx\documents\visual studio 2015\Projects\ReprBug\ReprBug\Source.obj


### Source code / logs

```cpp 
#include ""tensorflow/cc/saved_model/tag_constants.h""
#include ""tensorflow/core/public/session_options.h""
#include ""tensorflow/core/util/stat_summarizer.h""
#include ""tensorflow/contrib/session_bundle/bundle_shim.h""

class SynchronizedStatSummarizer
{
public:
	SynchronizedStatSummarizer(const tensorflow::StatSummarizerOptions& options)
		: m_statSummarizer{ options }, m_mutex{}
	{
	}

	void AddStepStats(const tensorflow::StepStats& stepStats)
	{
		std::lock_guard<std::mutex> guard{ m_mutex };
		m_statSummarizer.ProcessStepStats(stepStats);
	}

private:
	// The TF stat summarizer.
	tensorflow::StatSummarizer m_statSummarizer;

	// Synchronizes access to m_statSummarizer.
	mutable std::mutex m_mutex;
};

int main() {

	tensorflow::SessionOptions sessionOptions;
	tensorflow::RunOptions runOptions{};
	tensorflow::ConfigProto& config = sessionOptions.config;
	
	std::unique_ptr<tensorflow::SavedModelBundle> m_bundle (new tensorflow::SavedModelBundle());

	const std::string path = ""somepath"";
	tensorflow::Status status = tensorflow::serving::LoadSessionBundleOrSavedModelBundle(
		sessionOptions, runOptions, path, { tensorflow::kSavedModelTagServe }, m_bundle.get());

	std::vector<std::pair<std::string, tensorflow::Tensor>> modifiedInputs;
	std::vector<std::string> modifiedOutputNames;
	std::vector<tensorflow::Tensor> tensorOutputs;
	tensorflow::RunMetadata runMetadata{};

	tensorflow::Status run_status = m_bundle->session->Run(
		runOptions, modifiedInputs, modifiedOutputNames, {}, &tensorOutputs, &runMetadata);

	std::unique_ptr<SynchronizedStatSummarizer> m_runTracingStats;

	if (run_status.ok())
	{
		m_runTracingStats->AddStepStats(runMetadata.step_stats());
	}

}
```",0,,1,2018-02-03T01:07:58Z,2018-02-03T01:39:35Z,CONTRIBUTOR,2018-02-03T01:39:35Z
16707,Can't initialize an all zero SparseTensor,stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Kind of?
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.12.6 (not relevant)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.5.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem
It doesn't seem possible to initialize a `tf.SparseTensor` with all zero entries. 

A call doing this would look something like:

    tf.SparseTensor(indices=[], values=[], dense_shape=(10, 10))

However, attempting this initialization produces the error:

     ValueError: Shape (0,) must have rank 2


### Source code / logs
Current relevant section from `SparseTensor.__init__`:

    indices_shape = indices.get_shape().with_rank(2) # <--- .with_rank(2) is what causes the problem
    values_shape = values.get_shape().with_rank(1)
    dense_shape_shape = dense_shape.get_shape().with_rank(1)

    # Assert number of rows in indices match the number of elements in values.
    indices_shape[0].merge_with(values_shape[0])
    # Assert number of columns in indices matches the number of elements in
    # dense_shape.
    indices_shape[1].merge_with(dense_shape_shape[0])

Example solution:

    tf.cond(tf.equal(indices.get_shape()[0], 0),
            true_fn=lambda: None,
            false_fn=self._validate_input)

    def _validate_input(self):
        indices_shape = self._indices.get_shape().with_rank(2)
        values_shape = self._values.get_shape().with_rank(1)
        dense_shape_shape = self._dense_shape.get_shape().with_rank(1)

        # Assert number of rows in indices match the number of elements in values.
        indices_shape[0].merge_with(values_shape[0])
        # Assert number of columns in indices matches the number of elements in
        # dense_shape.
        indices_shape[1].merge_with(dense_shape_shape[0])

My only worry with the example solution is that `tf.cond` is too high level a function and there's some alternative that would be better. Is that the case? ",0,,3,2018-02-02T20:29:29Z,2018-02-03T00:14:37Z,NONE,2018-02-02T23:12:50Z
16695,Padding algo is not working as doc says,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linus centos 7
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: 0
- **GCC/Compiler version (if compiling from source)**:0
- **CUDA/cuDNN version**:0
- **GPU model and memory**:0
- **Exact command to reproduce**:

In the following situation, TF [doc](https://www.tensorflow.org/api_guides/python/nn#Convolution) is not correct.
- Input tensor shape : [1, 5, 2, 1]
- Kernel shape:           [1, 3, 1, 1]
- Stride :                      [1, 5, 5, 1]
- Padding =                  ""SAME""

According to the formula we can compute : 
out_h = 1
out_w = 1

```
if (in_height % strides[1] == 0):
  pad_along_height = max(filter_height - strides[1], 0)
else:
  pad_along_height = max(filter_height - (in_height % strides[1]), 0)
if (in_width % strides[2] == 0):
  pad_along_width = max(filter_width - strides[2], 0)
else:
  pad_along_width = max(filter_width - (in_width % strides[2]), 0)
```
gives :
pad_along_height = 0
pad_along_width = 1

then 
```
pad_top = pad_along_height // 2
pad_bottom = pad_along_height - pad_top
pad_left = pad_along_width // 2
pad_right = pad_along_width - pad_left
```

gives:

pad_top = 0
pad_bottom = 0
pad_left = 0
pad_right = 1

How tensorflow do a convolution with a kernel of height 1 on a image of height 5 and which gives output of height 1 (stride = 5) ??? How TF do this ? The doc can't explain the method used ... 

Doing retro engineering, I saw that TF apply the filter on the middle of the input tensor (pad_top = -2 and pad_bottom=-2).
I agree with this method, but the formulas of the Convolution doc is doing max(.., 0) so padding could never be negative (according to the doc).

Could someone explain me clearly what is the formula used in tensorflow ?
Could someone update the doc ?",0,,1,2018-02-02T15:19:12Z,2018-02-02T17:38:10Z,NONE,2018-02-02T17:38:10Z
16690,change to anchor link,"awaiting testing (then merge),cla: yes",Fixing markdown typo,0,,4,2018-02-02T10:10:12Z,2018-02-03T23:22:49Z,CONTRIBUTOR,2018-02-02T10:12:07Z
16688,how to assign the GPU device using C++?,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,1,2018-02-02T09:14:57Z,2018-02-02T18:52:11Z,NONE,2018-02-02T18:52:08Z
16684,The link for the  tutorial on Google's Tensorflow SyntaxNet  page  gives 404 error,"stat:awaiting tensorflower,type:docs","Go tot the page 
https://www.tensorflow.org/versions/r0.12/tutorials/syntaxnet/

and click the ""tutorial"" link. It gets a 404 error.

The target of the link is
https://github.com/tensorflow/models/tree/master/syntaxnet#installation
",1,,4,2018-02-02T03:58:02Z,2018-02-03T04:49:04Z,NONE,2018-02-02T18:44:40Z
16680,Branch 184220615,cla: yes,,0,,1,2018-02-02T01:59:52Z,2018-02-02T02:58:22Z,MEMBER,2018-02-02T02:02:15Z
16674,Fix sanity build,cla: yes,"- [x] Fix build error
- [x] Update test
",0,,2,2018-02-01T19:00:57Z,2018-02-02T06:51:34Z,MEMBER,2018-02-02T00:54:42Z
16672,Updating the version to 1.6.0-rc0.,cla: yes,,0,,1,2018-02-01T18:30:59Z,2018-02-02T08:35:04Z,MEMBER,2018-02-01T18:57:50Z
16662,"Current Bazel version is 0.10.0, expected at least 0.5.4",,"I get this error message when trying to build from source (r1.5) with the new bazel version published today.

Current Bazel version is 0.10.0, expected at least 0.5.4

I guess the version check is wrong. ",0,,9,2018-02-01T15:52:21Z,2018-02-01T21:40:12Z,NONE,2018-02-01T16:51:44Z
16661,"Fix ""Define the model"" link.",cla: yes,"The link syntax was inverted, that is, round brackets were coming before square brackets, but Markdown doesn't like it.",0,,4,2018-02-01T15:38:23Z,2018-02-02T20:16:50Z,CONTRIBUTOR,2018-02-01T15:39:39Z
16658,Runtime Error with Qt GUI Application,"stat:awaiting response,type:bug/performance","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from source**:
- **TensorFlow version use master**:
- **Python version 2.7**: 
- **Bazel version 0.9.0**:
- **GCC/Compiler version 5.4.0**:
- **Without CUDA/cuDNN**:
- **Without GPU**:

### Describe the problem
When I used QtCreator to build GUI Application, if include ""tensorflow/core/lib/core/refcount.h"", it will throw The program has unexpectedly finished.

.pro like
####
    SOURCES += \
        main.cpp \
        mainwindow.cpp
    HEADERS += \
         mainwindow.h
    FORMS += \
         mainwindow.ui
    
    #tensorflow
    INCLUDEPATH += /home/face/Desktop/tensorflow/bazel-genfiles`
    INCLUDEPATH += /home/face/Desktop/tensorflow`
    INCLUDEPATH += /home/face/Desktop/tensorflow/tensorflow/contrib/makefile/gen/protobuf/include`
    INCLUDEPATH += /home/face/Desktop/tensorflow/tensorflow/contrib/makefile/downloads/nsync/public`
    INCLUDEPATH += /home/face/Desktop/eigen-eigen-5a0156e40feb`
    LIBS += -L/home/face/Desktop/tensorflow/bazel-bin/tensorflow -ltensorflow_cc -ltensorflow_framework

main.cpp
####
    #include ""mainwindow.h""
    #include <QApplication>
    #include <tensorflow/core/platform/env.h>
    #include <tensorflow/core/public/session.h>

    int main(int argc, char *argv[])
    {
        QApplication a(argc, argv);
        MainWindow w;
        w.show();
        return a.exec();
    }

then if ""tensorflow/core/lib/core/refcount.h"" line 79
####
    inline RefCounted::~RefCounted() {
        DCHECK_EQ(ref_.load(), 0); 
    }
to
####
    inline RefCounted::~RefCounted() {
        //DCHECK_EQ(ref_.load(), 0); 
    }
it will work.

### Source code / logs
debug log like:
####
    1  google::protobuf::internal::Mutex::Lock()                                    0x7fffde0c3516 
    2  google::protobuf::internal::OnShutdown(void ( *)())                          0x7fffde0c3833 
    3  call_init                                                     dl-init.c  72  0x7ffff7de76ba 
    4  call_init                                                     dl-init.c  30  0x7ffff7de77cb 
    5  _dl_init                                                      dl-init.c  120 0x7ffff7de77cb 
    6  dl_open_worker                                                dl-open.c  575 0x7ffff7dec8e2 
    7  _dl_catch_error                                               dl-error.c 187 0x7ffff7de7564 
    8  _dl_open                                                      dl-open.c  660 0x7ffff7debda9 
    9  dlopen_doit                                                   dlopen.c   66  0x7ffff18f0f09 
    10 _dl_catch_error                                               dl-error.c 187 0x7ffff7de7564 
    11 _dlerror_run                                                  dlerror.c  163 0x7ffff18f1571 
    12 __dlopen                                                      dlopen.c   87  0x7ffff18f0fa1 
    13 ??                                                                           0x7ffff33100e5 
    14 ??                                                                           0x7ffff3309975 
    15 QFactoryLoader::instance(int) const                                          0x7ffff32ff07e 
    16 QPlatformThemeFactory::create(QString const&, QString const&)                0x7ffff0b30231 
    17 QGuiApplicationPrivate::createPlatformIntegration()                          0x7ffff0b3aaf8 
    18 QGuiApplicationPrivate::createEventDispatcher()                              0x7ffff0b3b4bd 
    19 QCoreApplicationPrivate::init()                                              0x7ffff331ab3b 
    20 QGuiApplicationPrivate::init()                                               0x7ffff0b3cf7b 
    21 QApplicationPrivate::init()                                                  0x7ffff392d3b9 
    22 main                                                          main.cpp   103 0x402e3e  

",0,,3,2018-02-01T13:04:29Z,2018-02-02T12:56:35Z,NONE,2018-02-02T03:49:17Z
16657,Tensorflow on banana-pi m64,,"How to install TF on banana m64? OS: Linux bpi-iot-ros-ai 3.10.105-BPI-M64-Kernel 

When i trying install i have an error

> tensorflow-1.5.0-cp34-none-any.whl is not a supported wheel on this platform.
",0,,1,2018-02-01T12:14:54Z,2018-02-01T18:01:59Z,NONE,2018-02-01T18:01:59Z
16654,Bazel version comparison fails with bazel 0.10.0,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, just commented 6 lines in the bzl files out
- **OS Platform and Distribution**: 16.04 on Jetson TX2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: TX2 GPU, 5GB (not sure)
- **Exact command to reproduce**: bazel build -c opt --local_resources 3072,4.0,1.0 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
I cannot build tensorflow using bazel 0.10.0. It seems like the version checks in repositories.bzl and wokspace.bzl fail. Commenting them out solves the issue, even though I know that is no persistent solution. I think it is simply that bazel thinks that 0.10.0 is smaller 0.5.4 due to its string comparison, but I am no bazel expert.

### Source code / logs
ERROR: /home/nvidia/git/tensorflow/WORKSPACE:15:1: Traceback (most recent call last):
	File ""/home/nvidia/git/tensorflow/WORKSPACE"", line 15
		closure_repositories()
	File ""/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/external/io_bazel_rules_closure/closure/repositories.bzl"", line 69, in closure_repositories
		_check_bazel_version(""Closure Rules"", ""0.4.5"")
	File ""/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/external/io_bazel_rules_closure/closure/repositories.bzl"", line 172, in _check_bazel_version
		fail((""%s requires Bazel >=%s but was...)))
Closure Rules requires Bazel >=0.4.5 but was 0.10.0- (@non-git)
ERROR: Error evaluating WORKSPACE file
ERROR: /home/nvidia/git/tensorflow/WORKSPACE:41:1: Traceback (most recent call last):
	File ""/home/nvidia/git/tensorflow/WORKSPACE"", line 41
		tf_workspace()
	File ""/home/nvidia/git/tensorflow/tensorflow/workspace.bzl"", line 48, in tf_workspace
		check_version(""0.5.4"")
	File ""/home/nvidia/git/tensorflow/tensorflow/workspace.bzl"", line 38, in check_version
		fail(""\nCurrent Bazel version is {}, ...))

Current Bazel version is 0.10.0- (@non-git), expected at least 0.5.4
ERROR: Error evaluating WORKSPACE file
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'external': Package 'external' contains errors
WARNING: Target pattern parsing failed.
ERROR: error loading package 'external': Package 'external' contains errors
INFO: Elapsed time: 3.145s
FAILED: Build did NOT complete successfully (0 packages loaded)
",0,,12,2018-02-01T10:38:27Z,2018-02-01T13:36:45Z,NONE,2018-02-01T11:53:15Z
16653,Fix docs,cla: yes,* Fix xcode path error,0,,3,2018-02-01T10:24:43Z,2018-02-03T00:39:18Z,CONTRIBUTOR,2018-02-01T10:31:45Z
16652,v1.3 batch_norm layer,,"I use the batch norm layer like this:
`def batch_norm_layer(x,train_phase,scope_bn):

	bn_train = batch_norm(x, decay=0.999, center=True, scale=True,
	is_training=True,
	reuse=None, # is this right?
	trainable=True,
	scope=scope_bn)
	bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,
	is_training=False,
	reuse=True, # is this right?
	trainable=True,
	scope=scope_bn)
	z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)
	return z`
I don't know in v1.3.0 is the code worked?
I saw the [issue1122](https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-232535426), someone said it would not work well.

thank you in advance.",0,,1,2018-02-01T10:02:01Z,2018-02-02T02:10:41Z,NONE,2018-02-02T02:10:41Z
16651,Temporarily remove three linter checks for now.,cla: yes,"  # C0330 bad-continuation
  # C0301 line-too-long
  # C0326 bad-whitespace
Will fix the following 25 error and add them back:
tensorflow/contrib/session_bundle/bundle_shim.py:85: [C0301(line-too-long), ] Line too long (83/80)

tensorflow/contrib/session_bundle/bundle_shim.py:94: [C0301(line-too-long), ] Line too long (89/80)

tensorflow/contrib/session_bundle/bundle_shim.py:135: [C0301(line-too-long), ] Line too long (81/80)

tensorflow/contrib/session_bundle/bundle_shim.py:136: [C0301(line-too-long), ] Line too long (81/80)

tensorflow/contrib/kafka/python/ops/kafka_dataset_ops.py:33: [C0301(line-too-long), ] Line too long (85/80)

tensorflow/contrib/tpu/profiler/pip_package/cloud_tpu_profiler/main.py:29: [C0330(bad-continuation), ] Wrong continued indentation (remove 3 spaces).

tensorflow/contrib/tpu/profiler/pip_package/cloud_tpu_profiler/main.py:31: [C0330(bad-continuation), ] Wrong continued indentation (remove 3 spaces).

tensorflow/contrib/tpu/profiler/pip_package/cloud_tpu_profiler/main.py:35: [C0330(bad-continuation), ] Wrong continued indentation (remove 3 spaces).

tensorflow/contrib/learn/python/learn/datasets/synthetic_test.py:139: [E0102(function-redefined), SyntheticTest.test_spirals] method already defined line 92

tensorflow/contrib/layers/python/layers/layers.py:63: [C0330(bad-continuation), ] Wrong hanging indentation (remove 7 spaces).

tensorflow/contrib/layers/python/layers/layers.py:1421: [C0301(line-too-long), ] Line too long (104/80)

tensorflow/contrib/py2tf/impl/api.py:89: [C0301(line-too-long), ] Line too long (81/80)

tensorflow/contrib/rnn/python/kernel_tests/core_rnn_cell_test.py:160: [C0326(bad-whitespace), ] Exactly one space required after comma

tensorflow/contrib/ndlstm/python/lstm1d.py:91: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).

tensorflow/contrib/rnn/python/kernel_tests/rnn_cell_test.py:1639: [E0102(function-redefined), WeightNormLSTMCellTest] class already defined line 1548

tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:209: [C0301(line-too-long), ] Line too long (98/80)

tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:209: [E1124(redundant-keyword-arg), get_graph_def_from_url_tarball] Argument 'filename' passed by position and keyword in function call

tensorflow/contrib/layers/python/layers/layers_test.py:1311: [C0301(line-too-long), ] Line too long (89/80)

tensorflow/python/kernel_tests/tensordot_op_test.py:108: [C0326(bad-whitespace), ] Exactly one space required after comma

tensorflow/python/data/util/nest.py:482: [C0301(line-too-long), ] Line too long (81/80)

tensorflow/python/data/ops/dataset_ops.py:909: [C0301(line-too-long), ] Line too long (88/80)

tensorflow/python/ops/image_ops_impl.py:1694: [C0301(line-too-long), ] Line too long (87/80)

tensorflow/python/ops/image_ops_impl.py:1720: [C0301(line-too-long), ] Line too long (87/80)

tensorflow/python/ops/image_ops_impl.py:1745: [C0301(line-too-long), ] Line too long (87/80)

tensorflow/python/ops/image_ops_impl.py:1771: [C0301(line-too-long), ] Line too long (87/80)",0,,1,2018-02-01T09:55:53Z,2018-02-01T18:03:32Z,MEMBER,2018-02-01T10:22:31Z
16646,can tf.estimator.Estimator's  parameters be modified by hand? ,stat:awaiting response,"TF's  high level API  is very convenient to defined a new model. 
However, many DNN Machine Learning task has to reuse some old model's parameter to fill a new model and then  fine-tune it in new tasks. 
I have read the tf.estimator.Estimator'API  carefully, but cann't find any API to set It's parameters. Hope  TF developer  add this function to the high level API. 
Thank very much!",1,,5,2018-02-01T06:34:16Z,2018-02-02T19:11:42Z,NONE,2018-02-02T02:08:28Z
16645,What's the difference between Univariate prediction and Multivariate prediction?,,My understanding is that paramenters of neurals are shared in Multivariate prediction and they can learn some correlations between series. There is less training time in Multivariate prediction. I wonder if that's right. Could you please explain any basic principles of Multivariate prediction with LSTM or recommend related papers to me? Thank you.,0,,1,2018-02-01T05:56:36Z,2018-02-01T18:02:23Z,NONE,2018-02-01T18:02:23Z
16636,Add relnote about bug in ptxas in CUDA 9 and 9.1.,cla: yes,,0,,3,2018-02-01T01:56:29Z,2018-02-01T03:15:45Z,MEMBER,2018-02-01T01:57:15Z
16631,Allow variable_overwrites on scope level,,"This is a request for allowing to pass a dict of `variable_overwrites` to variable scopes which to be returned when `tf.get_variable` is called instead of the usual procedure, if they are provided, otherwise do the usual procedure. A simple example of this beahviour is:
```
    import numpy as np
    with tf.variable_scope(""one""):
        a = tf.ones((5, 5), tf.float32)
        with tf.variable_scope(""two""):
            x1 = tf.get_variable(""x"", initializer=np.random.randn(5, 5).astype(""float32""))
            c1 = tf.sqrt(tf.abs(x1 + a))

    variables_overwrites = {x1._shared_name: c1}
    with tf.variable_scope(""one"", reuse=tf.AUTO_REUSE, variables_overwrites=variables_overwrites):
        a = tf.ones((5, 5), tf.float32)
        with tf.variable_scope(""two""):
           // x2 here is in fact the value of c1
            x2 = tf.get_variable(""x"", initializer=np.random.randn(5, 5).astype(""float32""))
            c2 = tf.sqrt(tf.abs(x2 + a))
```
This is particularly usefull for being able easily to bootstrap neural network parameters coming from inside the layers trough a standard function interface. My specific usage is for HMC for NN parameters. This is a question on whether you guys are interested in this so that I spend more time on doing this properly.",0,,5,2018-01-31T22:24:53Z,2018-02-01T00:27:35Z,NONE,2018-01-31T23:00:57Z
16628,Tensorflow switches to CPU when using Variable.assign,,"### System information
- **OS**:Windows 10
- **TensorFlow installed from**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.6.4
- **CUDA/cuDNN version**: 8.0 / 64
- **GPU model and memory**: GeForce GTX 1080 8 GB
- **Exact command to reproduce**: run the provided code below

### Describe the problem
I'm using a `tf.Variable` for the learning rate of a optimizer. If I change its value with `sess.run(var.assign(0.1))` the performance drops extremly and it seems tensorflow switches from GPU use to only CPU use (no workload on the GPU and more load on the CPU).

### Source code / logs

I did write a minimal working example (training a network with XOR). **To see the difference just comment the line `sess.run(learning_rate.assign(0.1))` out** and it will run much much faster using the GPU.

```
import tensorflow as tf


def XOR(x_, y_):
    Theta1 = tf.Variable(tf.random_uniform([2, 2], -1, 1), name=""Theta1"")
    Theta2 = tf.Variable(tf.random_uniform([2, 1], -1, 1), name=""Theta2"")

    Bias1 = tf.Variable(tf.zeros([2]), name=""Bias1"")
    Bias2 = tf.Variable(tf.zeros([1]), name=""Bias2"")

    with tf.name_scope(""layer2""):
        A2 = tf.sigmoid(tf.matmul(x_, Theta1) + Bias1)

    with tf.name_scope(""layer3""):
        Hypothesis = tf.sigmoid(tf.matmul(A2, Theta2) + Bias2)

    with tf.name_scope(""cost""):
        cost = tf.reduce_mean(((y_ * tf.log(Hypothesis)) +
                               ((1 - y_) * tf.log(1.0 - Hypothesis))) * -1)

    return cost


if __name__ == ""__main__"":
    x_ = tf.placeholder(dtype=tf.float32, shape=[4, 2], name='x-input')
    y_ = tf.placeholder(dtype=tf.float32, shape=[4, 1], name='y-input')
    xor_cost = XOR(x_, y_)

    learning_rate = tf.Variable(0.1, dtype=tf.float32)

    with tf.name_scope(""train""):
        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(xor_cost)

    XOR_X = [[0, 0], [0, 1], [1, 0], [1, 1]]
    XOR_Y = [[0], [1], [1], [0]]

    sess = tf.Session()
    init = tf.global_variables_initializer()
    sess.run(init)

    for i in range(40001):
        sess.run(learning_rate.assign(0.1))

        _, loss = sess.run(fetches=[train_step, xor_cost], feed_dict={x_: XOR_X, y_: XOR_Y})

        if i % 100 == 0:
            print('iteration: {0:5}, loss: {1:15.10f}'.format(i, loss))


```",0,,2,2018-01-31T18:17:24Z,2018-01-31T19:20:50Z,NONE,2018-01-31T19:05:22Z
16626,"example script multivariate.py throws ""UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape.  This may consume a large amount of memory.""",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I am using the example script `tensorflow/contrib/timeseries/examples/multivariate.py`
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS High Sierra (darwin)
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
v1.5.0-rc1-1781-g86c10063c8 1.5.0-rc1
- **Python version**: 
3.6.4
- **Bazel version (if compiling from source)**:
0.9.0-homebrew
- **GCC/Compiler version (if compiling from source)**:
Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **CUDA/cuDNN version**:
n/a compiled without CUDA support
- **GPU model and memory**:
n/a GPU not supported on Mac with SIP
- **Exact command to reproduce**:
$ python $GOPATH/src/github.com/tensorflow/tensorflow/tensorflow/contrib/timeseries/examples/multivariate.py

### Describe the problem
The script throws warning:
`UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.`

### Source code / logs
Here is the traceback of the warning on my system (I have tensorflow installed in a virtual environment called ""tf3"".:
```
  File ""multivariate.py"", line 59, in multivariate_train_and_sample
    estimator.train(input_fn=train_input_fn, steps=training_steps)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 352, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 809, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 790, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/timeseries/python/timeseries/head.py"", line 228, in create_estimator_spec
    return self._train_ops(features)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/timeseries/python/timeseries/head.py"", line 85, in _train_ops
    learning_rate=None)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py"", line 241, in optimize_loss
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 458, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 610, in gradients
    lambda: grad_fn(op, *out_grads))
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 376, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 610, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py"", line 589, in _PadGrad
    x_grad = array_ops.slice(grad, begin, sizes)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 640, in slice
    return gen_array_ops._slice(input_, begin, size, name=name)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 4591, in _slice
    ""Slice"", input=input, begin=begin, size=size, name=name)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1036, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 97, in _IndexedSlicesToTensor
    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
  File ""~/.pyenv/versions/3.6.4/lib/python3.6/warnings.py"", line 99, in _showwarnmsg
    msg.file, msg.line)
~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
```
I see discussion about this warning on SO:
https://stackoverflow.com/questions/35892412/tensorflow-dense-gradient-explanation/35893467
But the problem seems to happen in the estimator ""train"" method, not in any custom code I have written.",0,,1,2018-01-31T16:34:00Z,2018-01-31T19:24:53Z,NONE,2018-01-31T19:24:53Z
16625,[For Test; DO NOT MERGE] Add grpcio as a pip dependency of tensorflow,cla: yes,,0,,1,2018-01-31T15:22:47Z,2018-02-01T01:41:14Z,CONTRIBUTOR,2018-02-01T01:41:12Z
16621,Using tf.train.SyncReplicasOptimizer with multiple optimizers,stat:awaiting tensorflower,"I am trying to run the DeepLab Resnet (https://github.com/DrSleep/tensorflow-deeplab-resnet) in a distributed setup. I opted Synchronous Data parallel training approach similar to the one demonstrated in the Inception distributed training example.(https://github.com/tensorflow/models/tree/master/research/inception).

In the Inception example, a single RMS optimizer is used to reduce the loss. The tf.train.SyncReplicasOptimizer function wraps the optimizer and becomes responsible for synchronization, aggregation and application of gradients to various workers. Also, it takes care of updating the global_step variable. In my case, the DeepLab Resnet makes use of three optimizers each handling specific portions of the network. Following snippet explains the case:

    `#Three optimizers declared with different learning rates
     opt_conv = tf.train.MomentumOptimizer(learning_rate, args.momentum)
     opt_fc_w = tf.train.MomentumOptimizer(learning_rate * 10.0, args.momentum)
     opt_fc_b = tf.train.MomentumOptimizer(learning_rate * 20.0, args.momentum)`

    #Scope for every optimizer 
    grads = tf.gradients(reduced_loss, conv_trainable + fc_w_trainable + fc_b_trainable)
    grads_conv = grads[:len(conv_trainable)]
    grads_fc_w = grads[len(conv_trainable) : (len(conv_trainable) + len(fc_w_trainable))]
    grads_fc_b = grads[(len(conv_trainable) + len(fc_w_trainable)):]

    #Gradients applied to various portions of the network
    train_op_conv = opt_conv.apply_gradients(zip(grads_conv, conv_trainable))
    train_op_fc_w = opt_fc_w.apply_gradients(zip(grads_fc_w, fc_w_trainable))
    train_op_fc_b = opt_fc_b.apply_gradients(zip(grads_fc_b, fc_b_trainable))
 
    `#tf.group to combine all three operations
    train_op = tf.group(train_op_conv, train_op_fc_w, train_op_fc_b)`
   
I don't have any clue about using the tf.train.SyncReplicasOptimizer for multiple optimizers to achieve synchronous data parallel training. Also, I don't have an idea about updating the global_step variable and using the chief_queue_runner for this case. Please help me on this.


 


",0,,2,2018-01-31T11:59:09Z,2018-01-31T20:53:32Z,NONE,2018-01-31T18:19:06Z
16620,How to use model.summary() when using placeholder instead of Input(keras),,"Dear all, 
I follow  post in ""https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html""
The little modified code I use is:
----------------------------------------------------------------------------------------------------------------------------------
import tensorflow as tf
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.backend import categorical_crossentropy
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.python.keras.models import Model


sess = tf.Session()
img = tf.placeholder(tf.float32, shape=(None, 784))
x = Dense(128, activation='relu')(img)  # fully-connected layer with 128 units and ReLU activation
x = Dense(128, activation='relu')(x)
preds = Dense(10, activation='softmax')(x)  # output layer with 10 units and a softmax activation

labels = tf.placeholder(tf.float32, shape=(None, 10))
loss = tf.reduce_mean(categorical_crossentropy(labels, preds))
mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

init_op = tf.global_variables_initializer()
sess.run(init_op)
with sess.as_default():
    for i in range(100):
        batch = mnist_data.train.next_batch(50)
        train_step.run(feed_dict={img: batch[0],
                                  labels: batch[1]})

----------------------------------------------------------------------------------------------------------------------------------
It work fine until I use model.summary :  

model = Model(inputs=img, outputs=preds)

The error message show"" Input tensors to a Model must come from `tf.layers.Input`""
I can use tf.layers.Input to solve this problem.
But I really want to use tf.placeholder so I can feed data as I like.
Can anyone help me?  Thanks!!",1,,7,2018-01-31T10:23:42Z,2018-02-02T19:15:06Z,NONE,2018-02-02T05:15:49Z
16612,Simplify loader_impl.py logic around main Op Tensor.,"awaiting testing (then merge),cla: yes",,0,,1,2018-01-31T03:08:35Z,2018-01-31T21:08:14Z,CONTRIBUTOR,2018-01-31T15:25:25Z
16607,MKL: Pooling and AddN bug fixes,"awaiting testing (then merge),cla: yes",,0,,1,2018-01-31T00:18:08Z,2018-02-01T08:37:48Z,CONTRIBUTOR,2018-01-31T00:29:27Z
16605,Description in docs of one-hot vector for mnist deep example confusing and/or wrong,stat:awaiting tensorflower,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:n/a
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.4
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**:n/a
- **GCC/Compiler version (if compiling from source)**:n/a
- **CUDA/cuDNN version**:n/a
- **GPU model and memory**:n/a
- **Exact command to reproduce**:n/a

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Documentation found at https://www.tensorflow.org/versions/r1.4/get_started/mnist/pros describes one of the two parameters passed to the training process as a 2d tensor of ""one-hot"" 10-dimensional vectors specifying the classes of the samples in the other 2d tensor parameter. But clearly the placeholders define 2d and 1d tensors, not 2d and 2d. There is no ""one-hot"" representation used at all as far as I can tell by using print() statements - if the class if a sample is class 3, then the corresponding entry is simple 3, not the one-hot representation of it. If this class is subsequently converted into a one-hot representation, it does not happen in the mnist_deep.py source file. I'm too much of a beginner to say what the documentation should say, but it seems at best confusing, and at worst completely wrong.

### Source code / logs
n/a
",0,,4,2018-01-30T22:19:50Z,2018-01-30T23:11:10Z,NONE,2018-01-30T23:11:10Z
16604,Branch 183881907,cla: yes,,1,,2,2018-01-30T22:12:40Z,2018-01-30T23:33:14Z,MEMBER,2018-01-30T22:57:24Z
16595,Branch 183846994,cla: yes,,0,,3,2018-01-30T18:37:36Z,2018-01-30T23:43:41Z,MEMBER,2018-01-30T18:41:07Z
16591,Fix FutureWarning on issubdtype from float to np.floating,"awaiting testing (then merge),cla: yes","This is try to fix [#16587](https://github.com/tensorflow/tensorflow/issues/16587). 
```
FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated
```

Before fix:
```
>>> np.issubdtype(np.integer, np.float)
__main__:1: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
False
```
After fix:
```
>>> np.issubdtype(np.integer, np.floating)
False
>>>
```",0,,1,2018-01-30T17:34:37Z,2018-02-01T03:03:19Z,CONTRIBUTOR,2018-01-31T13:19:05Z
16587,Feature deprecated in h5py is used in TF1.5,stat:contributions welcome,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux, OS X
- **TensorFlow installed from (source or binary)**: source and binary
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: 9.0 - 7.0
- **GPU model and memory**: GTX1060, GTX 1050Ti 
- **Exact command to reproduce**:
`sudo pip3 install h5py`
run python3, from there, type:
`import tensorflow as tf`


### Describe the problem
A feature of h5py used in TF 1.5 is deprecated, in particular: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated.

### Source code / logs
Warning message: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
",0,,2,2018-01-30T15:48:47Z,2018-02-01T03:03:19Z,NONE,2018-01-30T17:35:55Z
16585,TensorFlow with CUDA or Python might rebuilds more than necessary instead of re-using bazel cache,stat:contributions welcome,"Context: for DeepSpeech, we perform tensorflow builds and then keep the cache in a tar (capturing the whole of the home directory of the build user). We then untar it and the deepspeech build through `bazel build` picks the proper cached items so it does not rebuild anything.

Recently, we started to have increased (2.5x) build time on CUDA-enabled builds. Debugging with Bazel showed that it was rebuilding because the `actionKey` computed for `stream_executor_impl` was different. Instrumenting Bazel to get more informations, I could get down to the reason of the different actionKey: the ordering of the CUDA includes was different. The list itself contained the exact same content, just a different ordering.

Those includes are symlinks, and they are generated from a genrule. This is all taken care of by https://github.com/tensorflow/tensorflow/blob/ba64f5334d4bba31d22c30e09a96f806ea0e2f7e/third_party/gpus/cuda_configure.bzl#L915-L1035 which generated shell script for the genrules, that actually do perform the symlinks. Checking those shell scripts revealed the exact same and different ordering.

Checking more carefully, one will see that the headers are discovered by `_read_dir` function: https://github.com/tensorflow/tensorflow/blob/ba64f5334d4bba31d22c30e09a96f806ea0e2f7e/third_party/gpus/cuda_configure.bzl#L891-L894, it does directly get the output of `find`. This is dependant on the ordering provided by `readdir` syscall.

In our case, the ordering on the filesystem before making the tar archive, and after untarring it would be different.

One simple fix for that is to force ordering the list of headers, this way we are sure the order is always the same and we are not dependant on what `readdir` is going to get us.

In the past, Bazel would force the ordering of the elements considered to compute the actionKey. This was removed with 0.3.0 but it might have make the issue hidden https://github.com/bazelbuild/bazel/commit/9dc32111d5b6c1c7c5eaf39efad5fef75327ee75",0,,2,2018-01-30T14:16:49Z,2018-02-02T02:03:32Z,CONTRIBUTOR,2018-01-30T18:31:46Z
16584,TensorFlow op to copy weights of Keras model,,"I am doing a distributed calibration of an LSTM model (keras 2.0 + TensorFlow 1.0)

    with tf.device(tf.train.replica_device_setter(...):
          model = ##create model by keras
          clone_model = ## create the same model by keras but now a stateful one

after calibration, I want my chief worker to use the clone_model, copy the weights the calibration reached in model, and make predictions on some test set, but simply calling

     clone_model.set_weights(model.get_weights())

does not work.
I understand I need to define this weight copy as an op and then call session(run) of that op

Can you please help with a TensorFlow op copying weights of a keras model to another (identical architecture) Keras model?

",1,,3,2018-01-30T13:00:44Z,2018-02-01T12:38:21Z,NONE,2018-01-30T23:47:49Z
16570,Fix typos.,cla: yes,,0,,3,2018-01-30T03:59:02Z,2018-02-01T03:00:52Z,CONTRIBUTOR,2018-01-30T04:06:40Z
16557,MKL: Fix for mkl_input conversion for MKL DNN. ,"awaiting testing (then merge),cla: yes",Fix also enables elementwise operations in MKL,0,,2,2018-01-29T23:07:55Z,2018-02-01T18:10:54Z,CONTRIBUTOR,2018-01-31T00:30:06Z
16556,tf.argmax appears to be functioning incorrectly on occasion,"stat:awaiting response,type:bug/performance","**EDIT**

[Link to script and input/label data as pickle files to reproduce the error.](https://drive.google.com/open?id=13Kice6p3IQvRVOKIlW0DSvD9wwJDL-YH)

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I have written my own code. My code base, data set and batch generating algorithm are quite large, so I am attempting to illustrate this as best as possible. If no mistake of mine can be seen in this post and the examples I have given, then I will provide further code/data. I have asked on StackOverflow, but have got not replies. If  I am missing something simple, then I'm sure it would have been pointed out on StackOverflow by now.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  
Manjaro 17.1.3 Kernel 4.14

- **TensorFlow installed from (source or binary)**:
python pip

- **TensorFlow version (use command below)**:
tensorflow-gpu 1.5.0

- **Python version**:
3.6.4

- **CUDA/cuDNN version**:
CUDA 9.0
cuDNN 7.0

- **GPU model and memory**: 
Nvidia GeForce GTX 1050 8GB

### Describe the problem
tf.argmax seems to be occasionally producing incorrect results when used on the last axis of a 3-dimensional tensor.

### Source code / logs
To debug this, I have printed out the following operations:

```
print(self.session.run(
    tf.equal(tf.argmax(self.predictions, axis=-1),
             tf.argmax(self.labelsUnrolled, axis=-1)),
    self.batchDict))
print("""")
print(self.session.run(self.predictions, self.batchDict))
print("""")
print(self.session.run(self.labelsUnrolled, self.batchDict))
print(""\n********\n"")
```

Which on two consecutive iterations output the following:

```
[[ True  True  True]
 [ True  True  True]]

[array([[0.06275553, 0.44493628, 0.42474008, 0.06756803],
        [0.06320112, 0.49631155, 0.4021484 , 0.03833894],
        [0.04378054, 0.59403986, 0.3236889 , 0.03849069]], dtype=float32), 
array([[8.1677590e-06, 9.9997127e-01, 2.0200867e-05, 3.6184446e-07],
        [4.3686719e-06, 9.9992716e-01, 6.6905603e-05, 1.5286902e-06],
        [1.3270236e-05, 9.9986196e-01, 1.1622251e-04, 8.5579613e-06]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32),
 array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]

********

[[False False  True]
 [ True  True  True]]

 [array([[0.0466171 , 0.53605616, 0.37778312, 0.03954368],
         [0.05007472, 0.4603508 , 0.44400516, 0.0455693 ],
         [0.06134444, 0.38073638, 0.504286  , 0.05363319]], dtype=float32),
 array([[9.6363285e-05, 9.9861979e-01, 1.2741127e-03, 9.7381862e-06],
         [1.6185455e-05, 9.9977034e-01, 2.0742408e-04, 6.0521238e-06],
         [2.9893983e-05, 9.9954152e-01, 4.2436572e-04, 4.2021661e-06]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.]], dtype=float32), 
 array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]

********
```

Isn't the very first `True` in the first iteration incorrect? There are many more examples being executed where these are wrong (although it is right the majority of the time). Am I going wrong somewhere with the `tf.argmax` function?

Printing out the same operations, but not inside the session gives the following shapes:

```
Tensor(""Equal_175:0"", shape=(2, ?), dtype=bool)

[<tf.Tensor 'unstack_2:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'unstack_2:1' shape=(?, 4) dtype=float32>]

[<tf.Tensor 'unstack_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'unstack_1:1' shape=(?, 4) dtype=float32>]
```

Is it a problem that the number associated with the tensor name ""Equal_XXX:0"" is incrementing each iteration?

I have also tried changing the axis argument in both argmax functions to `axis=2`, giving the ""Equal"" tensor a shape of (2, 3) again, but there are still similar errors.

Here is an example:

```
[[False False  True]
 [ True  True False]]

[array([[0.09075877, 0.41096467, 0.4460272 , 0.05224944],
        [0.04962843, 0.43777955, 0.46654516, 0.04604685],
        [0.07901238, 0.40768984, 0.46641603, 0.04688181]], dtype=float32),
 array([[0.04444276, 0.49195835, 0.42141557, 0.04218334],
        [0.02372498, 0.47147286, 0.4679979 , 0.03680426],
        [0.03707527, 0.435518  , 0.48937747, 0.03802926]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [1., 0., 0., 0.],
        [0., 0., 1., 0.]], dtype=float32),
 array([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.]], dtype=float32)]
```

I would expect this to be:

```
[[ True False  True]
 [ True False False]]
```

I thought this may have been a problem with parallel computations on the GPU, but I tried the same execution on just my CPU and got the following, similar miscalculation, too:

```
[[False  True False]
 [False False False]]

[array([[0.07774187, 0.40993363, 0.47022063, 0.04210386],
        [0.04910654, 0.44086066, 0.46013904, 0.04989377],
        [0.06700655, 0.37128285, 0.51324743, 0.04846317]], dtype=float32), 
array([[0.07584244, 0.3863555 , 0.5090046 , 0.02879751],
        [0.06959026, 0.3221606 , 0.5715027 , 0.03674646],
        [0.09042579, 0.32515866, 0.5385905 , 0.04582503]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.]], dtype=float32),
array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]
```

",0,,6,2018-01-29T22:34:01Z,2018-01-31T02:34:55Z,NONE,2018-01-30T02:51:17Z
16550,Add options to enable new features for cloud-tpu-profiler.,"awaiting review,cla: yes","Add options for the user to manually include dataset ops in trace collection, and to automatically recapture the traces when no trace event is collected.
Also change tf.flags to absl.flags since the former is going to be deprecated. ",0,,1,2018-01-29T17:28:23Z,2018-01-30T06:58:20Z,CONTRIBUTOR,2018-01-29T18:07:02Z
16548,AttributeError: module 'tensorflow.python.layers.layers' has no attribute 'conv_2d',stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.13.2
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 3.6.3
- **GPU model and memory**: no GPU
- **Exact command to reproduce**:

```python
class my_RNNCell(tf.nn.rnn_cell.RNNCell):
    def __init__(self):
        super(my_RNNCell, self).__init__()
        self._output_size = 2
        self._state_size = 2

    def __call__(self, tensor_in, state):

        output = tf.layers.conv_2d(tensor_in, 1, [1, 10])

        return output, output
    
    @property
    def output_size(self):
        return self._output_size
    @property
    def state_size(self):
        return self._state_size


tf.nn.dynamic_rnn(my_RNNCell(), inputs=tf.placeholder(shape=[None,2,100,3], dtype=tf.float32), initial_state= tf.placeholder(shape=[None,2], dtype=tf.float32))

>>>
AttributeError: module 'tensorflow.python.layers.layers' has no attribute 'conv_2d'
```
",0,,8,2018-01-29T17:01:12Z,2018-01-31T19:21:38Z,NONE,2018-01-30T02:42:37Z
16544,"deconv_output_length(input_length, filter_size, padding, stride)",,"deconv_output_length(input_length, filter_size, padding, stride) is defined [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/utils.py#L159)

When padding is valid, input_length += max(filter_size - stride, 0), why to use `max` function?
See more details on #2118

For conv2d:
```
output = (input - filter + stride) // stride  # VALID
output = (input + stride - 1) // stride  # SAME
```

For conv2d_transpose:
```
output = input * stride + filter - stride  # VALID
output = input * stride - stride + 1  # SAME 
```

Even when filter_size is less than stride, I think output is also `input * stride + filter - stride` rather `input * stride`, so why to use `max`?

 
",0,,1,2018-01-29T15:12:08Z,2018-01-30T02:37:56Z,NONE,2018-01-30T02:37:56Z
16537,added audio_ops.cc to tf_op_files.txt to fix the Op type not registered DecodeWav error ,cla: yes,- https://github.com/tensorflow/tensorflow/issues/15921,0,,5,2018-01-29T07:38:00Z,2018-02-01T22:31:05Z,CONTRIBUTOR,2018-01-29T07:39:22Z
16524,MKL: Reverting the switch to max_pool_v2 in python,cla: yes,"A prior commit https://github.com/tensorflow/tensorflow/pull/14983 changed python interface to call max_pool_v2 causing failure in MKL build. Currently MKL doesn't support max_pool_v2. Reverting  the commit  for now, will change it back when MKL implementation is complete.",1,,1,2018-01-28T20:54:41Z,2018-01-29T17:46:06Z,CONTRIBUTOR,2018-01-29T14:44:24Z
16517,ou must feed a value for placeholder tensor 'import/Placeholder when i test my frozen model,,"Hello , I trying create mobile app for object recognition for my own created model. I fallow this tutorial https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/#2
But when i even get a testing model from step 3 i get error 

```
InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'import/Placeholder' with dtype float
	 [[Node: import/Placeholder = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```

I'm aware that is wrong node problem, but i trying with other and always i get failure 

My code for model creation 
```

x = tf.placeholder(tf.float32,
                   shape=[None, cons.IMAGE_SIZE, cons.IMAGE_SIZE, 3], name=""x"")
y_ = tf.placeholder(tf.float32, shape=[None, cons.LABELS_NUMB], name=""labels"")

K = 4
L = 8
M = 12
N = 200

x_image = tf.reshape(x, [-1, cons.IMAGE_SIZE, cons.IMAGE_SIZE, 3])
tf.summary.image('input', x_image, 3)
print(""X image "")
print(tf.shape(x_image))

################## first ##############

W_conv1 = weight_variable([5, 5, 3, 32], ""weight1"")
b_conv1 = bias_variable([32], ""bias1"")

h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1)
print(""W_conv1 "")
print(tf.shape(W_conv1))
print(""b_conv1 "")
print(tf.shape(b_conv1))
print(""h_conv1 "")
print(tf.shape(h_conv1))
print(""h_pool1 "")
print(tf.shape(h_pool1))

################## second ##############

W_conv2 = weight_variable([5, 5, 32, 64], ""weight2"")
b_conv2 = bias_variable([64], ""bias2"")

h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
tf.summary.histogram(""activations"", h_conv2)


h_pool2 = max_pool_2x2(h_conv2)

print(""W_conv2 "")
print(tf.shape(W_conv2))
print(""b_conv2 "")
print(tf.shape(b_conv2))
print(""h_conv2 "")
print(tf.shape(h_conv2))
print(""h_pool2 "")
print(tf.shape(h_pool2))

################## fully connected 3 ##############

W_fc1 = weight_variable([8 * 8 * 64, 1024], ""Weight3"")
b_fc1 = bias_variable([1024], ""bias3"")

h_pool2_flat = tf.reshape(h_pool2, [-1, 8 * 8 * 64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

print(""W_fc1 "")
print(tf.shape(W_fc1))
print(""b_fc1 "")
print(tf.shape(b_fc1))
print(""h_pool2_flat "")
print(tf.shape(h_pool2_flat))
print(""h_fc1 "")
print(tf.shape(h_fc1))

################ dropout  4 #################

keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

################## fully connected 5 ##############

W_fc2 = weight_variable([1024, cons.LABELS_NUMB], ""weight5"")
b_fc2 = bias_variable([cons.LABELS_NUMB], ""bias5"")

Y = tf.matmul(h_fc1_drop, W_fc2) + b_fc2
tf.summary.histogram(""final"", Y)


print(""W_fc2 "")
print(tf.shape(W_fc2))
print(""b_fc2 "")
print(tf.shape(b_fc2))
print(""Y "")
print(tf.shape(Y))

with tf.name_scope(""cross_entropy""):
    cross_entropy = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=Y))
    tf.summary.scalar(""xent"", cross_entropy)

with tf.name_scope(""train_step""):
    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)

with tf.name_scope(""Acuracy""):
    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    tf.summary.scalar(""accuracy"", accuracy)

summ = tf.summary.merge_all()
saver = tf.train.Saver()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    writer = tf.summary.FileWriter(""/home/damian/api/mnist_demo/10"")
    writer.add_graph(sess.graph)
    for i in range(1000):
        img, lb = fileCreation.next_batch(100, images32, labels)
        if i % 5 == 0:
            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 1.0})
        writer.add_summary(s, i)
        if i % 100 == 0:
            saver.save(sess, '/home/damian/api/checkpoint/my_test_model', global_step=i)
            train_accuracy = accuracy.eval(feed_dict={
                x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 1.0})
            print('step %d, training accuracy %g' % (i, train_accuracy))
        train_step.run(feed_dict={x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 0.5})

    print('test accuracy %g' % accuracy.eval(feed_dict={x: test_images32,
                                                        y_: fileCreation.dense_to_one_hot(test_labels,
                                                                                          cons.LABELS_NUMB),
                                                        keep_prob: 0.1}))
```


I'm freeze this model with and i point output_node to 'final'
next i call script from tutorial


```
python -m scripts.label_image \
  --graph=tf_files/frozen_model3.pb  \
  --input_layer=x \
  --output_layer=final \
  --image=tf_files/stop.png  \
  --input_height=32 \
  --input_width=32 \
  --input_mean=16  \
  --input_std=16  
```









",0,,1,2018-01-28T16:15:22Z,2018-01-30T19:10:44Z,NONE,2018-01-30T19:10:44Z
16513,TF1.5.0 not working with CUDA 8.0,,"After upgrading to TF 1.5.0, when I import tensorflow, it raises:

```
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory
```

- System: Ubuntu 14.04.5 LTS (64 bit)
- Python: 2.7.6
- TensorFlow: tensorflow-gpu-1.5.0
- GPU: GeForce GTX TITAN
- CUDA: 8.0",0,,3,2018-01-28T10:24:08Z,2018-01-29T19:34:12Z,NONE,2018-01-29T19:34:12Z
16512,how to install ffmpeg in tensorflow 1.4 binary,,"hi
i want to install ffmpeg in tensorflow 1.4 binary , python 3.5 on ubuntu 16.04 , please help me how do i do ? 
the output type python -c ""from tensorflow.contrib import ffmpeg"" is ok dont have anly error , but i dont know why : 
from tensorflow.contrib import ffmpeg

i get error , 
>>>  from tensorflow.contrib import ffmpeg
  File ""<stdin>"", line 1
    from tensorflow.contrib import ffmpeg
    ^
IndentationError: unexpected indent
",0,,2,2018-01-28T07:56:25Z,2018-01-30T03:34:18Z,NONE,2018-01-28T14:16:58Z
16507,"ResourceExhaustedError, when running UNET",,"My computer has a gpu GeForce 940MX installed. It has the Memory bandwidth 16.02 GB/s. I'm trying to train LUNA dataset using UNET model using following code.

	from __future__ import print_function

	import numpy as np
	from keras.models import Model
	from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D
	from keras.layers import concatenate
	from keras.optimizers import Adam
	from keras.optimizers import SGD
	from keras.callbacks import ModelCheckpoint, LearningRateScheduler
	from keras import backend as K


	K.set_image_dim_ordering('th')  # Theano dimension ordering in this code

	img_rows = 512
	img_cols = 512

	smooth = 1.


	def dice_coef(y_true, y_pred):
		y_true_f = K.flatten(y_true)
		y_pred_f = K.flatten(y_pred)
		intersection = K.sum(y_true_f * y_pred_f)
		return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

	def dice_coef_np(y_true,y_pred):
		y_true_f = y_true.flatten()
		y_pred_f = y_pred.flatten()
		intersection = np.sum(y_true_f * y_pred_f)
		return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)

	def dice_coef_loss(y_true, y_pred):
		return -dice_coef(y_true, y_pred)


	def get_unet():
		inputs = Input((1,img_rows, img_cols))
		conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
		conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)
		pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

		conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)
		conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)
		pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

		conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)
		conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)
		pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

		conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)
		conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)
		pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

		conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)
		conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)

		#up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)
		up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=1)
		conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)
		conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)

		#up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)
		up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=1)
		conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)
		conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)

		#up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)
		up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=1)
		conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)
		conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)

		#up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)
		up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=1)
		conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)
		conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)

		conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)

		model = Model(inputs=inputs, outputs=conv10)

		model.compile(optimizer=Adam(lr=1.0e-5), loss=dice_coef_loss, metrics=[dice_coef])

		return model


	def train_and_predict(use_existing):
		print('-'*30)
		print('Loading and preprocessing train data...')
		print('-'*30)
		imgs_train = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""trainImages.npy"").astype(np.float32)
		imgs_mask_train = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""trainMasks.npy"").astype(np.float32)

		imgs_test = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""testImages.npy"").astype(np.float32)
		imgs_mask_test_true = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""testMasks.npy"").astype(np.float32)
		
		mean = np.mean(imgs_train)  # mean for data centering
		std = np.std(imgs_train)  # std for data normalization

		imgs_train -= mean  # images should already be standardized, but just in case
		imgs_train /= std

		print('-'*30)
		print('Creating and compiling model...')
		print('-'*30)
		model = get_unet()
		# Saving weights to unet.hdf5 at checkpoints
		model_checkpoint = ModelCheckpoint('unet.hdf5', monitor='loss', save_best_only=True)
		#
		# Should we load existing weights? 
		# Set argument for call to train_and_predict to true at end of script
		if use_existing:
			model.load_weights('./unet.hdf5')
			
		# 
		# The final results for this tutorial were produced using a multi-GPU
		# machine using TitanX's.
		# For a home GPU computation benchmark, on my home set up with a GTX970 
		# I was able to run 20 epochs with a training set size of 320 and 
		# batch size of 2 in about an hour. I started getting reseasonable masks 
		# after about 3 hours of training. 
		#
		print('-'*30)
		print('Fitting model...')
		print('-'*30)
		model.fit(imgs_train, imgs_mask_train, batch_size=50, epochs=10, verbose=1, shuffle=True,
				  callbacks=[model_checkpoint])

		# loading best weights from training session
		print('-'*30)
		print('Loading saved weights...')
		print('-'*30)
		model.load_weights('./unet.hdf5')

		print('-'*30)
		print('Predicting masks on test data...')
		print('-'*30)
		num_test = len(imgs_test)
		imgs_mask_test = np.ndarray([num_test,1,512,512],dtype=np.float32)
		for i in range(num_test):
			imgs_mask_test[i] = model.predict([imgs_test[i:i+1]], verbose=0)[0]
		np.save('masksTestPredicted.npy', imgs_mask_test)
		mean = 0.0
		for i in range(num_test):
			mean+=dice_coef_np(imgs_mask_test_true[i,0], imgs_mask_test[i,0])
		mean/=num_test
		print(""Mean Dice Coeff : "",mean)

	if __name__ == '__main__':
		train_and_predict(False)
		
But when running it using GPU I'm getting the following error.

	Warning (from warnings module):
	  File ""C:\Research\Python_installation\lib\site-packages\h5py\__init__.py"", line 36
		from ._conv import register_converters as _register_converters
	FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
	Using TensorFlow backend.
	------------------------------
	Loading and preprocessing train data...
	------------------------------
	------------------------------
	Creating and compiling model...
	------------------------------
	------------------------------
	Fitting model...
	------------------------------
	Epoch 1/10
	Traceback (most recent call last):
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1327, in _do_call
		return fn(*args)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1306, in _run_fn
		status, run_metadata)
	  File ""C:\Research\Python_installation\lib\contextlib.py"", line 66, in __exit__
		next(self.gen)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
		pywrap_tensorflow.TF_GetCode(status))
	tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]
		 [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]
		 [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3022_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

	During handling of the above exception, another exception occurred:

	Traceback (most recent call last):
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 150, in <module>
		train_and_predict(False)
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 127, in train_and_predict
		callbacks=[model_checkpoint])
	  File ""C:\Research\Python_installation\lib\site-packages\keras\engine\training.py"", line 1657, in fit
		validation_steps=validation_steps)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\engine\training.py"", line 1213, in _fit_loop
		outs = f(ins_batch)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\backend\tensorflow_backend.py"", line 2357, in __call__
		**self.session_kwargs)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 895, in run
		run_metadata_ptr)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1124, in _run
		feed_dict_tensor, options, run_metadata)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1321, in _do_run
		options, run_metadata)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1340, in _do_call
		raise type(e)(node_def, op, message)
	tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]
		 [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]
		 [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3022_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

	Caused by op 'conv2d_1/convolution', defined at:
	  File ""<string>"", line 1, in <module>
	  File ""C:\Research\Python_installation\lib\idlelib\run.py"", line 124, in main
		ret = method(*args, **kwargs)
	  File ""C:\Research\Python_installation\lib\idlelib\run.py"", line 351, in runcode
		exec(code, self.locals)
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 150, in <module>
		train_and_predict(False)
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 106, in train_and_predict
		model = get_unet()
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 39, in get_unet
		conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\engine\topology.py"", line 603, in __call__
		output = self.call(inputs, **kwargs)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\layers\convolutional.py"", line 164, in call
		dilation_rate=self.dilation_rate)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\backend\tensorflow_backend.py"", line 3195, in conv2d
		data_format=tf_data_format)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 672, in convolution
		op=op)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 338, in with_space_to_batch
		return op(input, num_spatial_dims, padding)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 664, in op
		name=name)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 131, in _non_atrous_convolution
		name=name)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 397, in conv2d
		data_format=data_format, name=name)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
		op_def=op_def)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
		original_op=self._default_original_op, op_def=op_def)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
		self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

	ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[50,32,512,512]
		 [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]
		 [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3022_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]


Can someone please kindly explain me the reason behind this error, ResourceExhaustedError. Is it because that the memory of GPU is not enough to load the dataset. This worked fine without GPU. But took around 6 hours to finish one epoch",0,,2,2018-01-28T01:43:47Z,2018-01-29T19:47:54Z,NONE,2018-01-29T19:47:54Z
16506,Feature request: Have Estimator display Loss and Metrics for Every Epoch and not Every Step,,"Most of the papers Ive read measure the time it takes to train a model with every epoch and not every step. If it isnt possible to display the loss only for every epoch, I think it would be nice to print when an epoch has passed.",0,,1,2018-01-28T00:03:46Z,2018-01-30T03:31:05Z,CONTRIBUTOR,2018-01-30T03:31:05Z
16500,contrib/learn: Typo in variable name x_exrta --> x_extra,"awaiting testing (then merge),cla: yes","flake8 testing of https://github.com/tensorflow/tensorflow

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./tensorflow/contrib/learn/python/learn/datasets/synthetic.py:156:32: F821 undefined name 'x_extra'
    spir_x = np.append(spir_x, x_extra)
                               ^
```",1,,4,2018-01-27T17:58:50Z,2018-01-29T21:50:40Z,CONTRIBUTOR,2018-01-27T23:04:23Z
16496,"Feature Suggestion: ""Float-bit-strings""",type:feature,"### System information (Not really relevant ...)
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 8 (?)
- **GPU model and memory**: GTX 1070
- **Exact command to reproduce**: NA

### Summary

This proposes the use of what I call ""float-bit-strings"" or ""float-bits"" instead of one-hot-encoded arrays so as to greatly reduce the memory and computational usage e.g. in language models.

I don't think this preliminary discussion belongs on StackOverflow so I hope it is OK to post it here. It is a new feature that could be added to TensorFlow. There's quite likely somebody on the TensorFlow dev-team or in the community who has already thought of this. But I have searched the internet and cannot find any mentioning of a similar idea.


### Background

I have started looking at language-models using e.g. LSTM and encoder-decoder architectures. There are some aspects that seem to be incredibly wasteful and limiting. Let me briefly describe this and please forgive me if I am ignorant, I have only spent a week or two on studying LSTM and language models so far :-)

For example in Machine Translation we typically have the text-data for the source- and target-languages as lists of integer-tokens, where each integer maps to a word in the vocabulary. There may be e.g. 100k different words so these integer-tokens can take on values between zero and 100k. This data cannot be input directly to a Neural Network so we use an embedding layer to convert these integers to n-dimensional vectors with values between zero and one, according to a mapping-function that may either be loaded from disk or trained along with the rest of the Neural Network; if I understand correctly.

For the decoder in a language model, we have a similar problem where we must somehow convert integer-tokens to data that the neural network can work on. A typical way of doing this seems to be a one-hot encoding; if I understand correctly. (This could also be done for the encoder-part, but it doesn't seem to be necessary).

I can't figure out what the max-size of one-hot encodings are in TensorFlow and whether it can even handle 100k one-hot encoded tensors. But it is obviously an extremely wasteful data-mapping. For example, for a vocab of 100k words we only need 17-bits (log2(100k)) to represent each integer-token - but for a one-hot encoding using 32-bit floats we need 32 x 100k bits!

I can't figure out what people normally do, but it seems like the common practice is to limit the vocab to a smaller number of words, e.g. 1k or 10k. It appears that Google Translate runs on multiple GPU's and maybe that's why they can handle extremely large vocabs with one-hot encoded tensors?


### Float-bit-strings

I thought it might be possible to use a bit-string-like representation inside a TensorFlow model. I have searched the internet and cannot find anyone who has proposed a similar idea.

The idea is to convert each integer-token to what I call a ""float-bit-string"" or ""float-bits"". For example, the number 123 has the bit-string 01111011. We can then make a corresponding tensor with floats [0., 1., 1., 1., 1., 0., 1., 1.] and input this to the TensorFlow model.

In a language model we would then have to input and output these ""float-bits"" instead of one-hot encoded arrays. This would dramatically reduce the memory and computational requirements of the models.


### Test

I have hacked together a little test using numpy and Keras / TensorFlow. The idea is to see if we can learn to map integers x with values between 0 and 10k to y = 123 * x using these ""float-bit"" encodings. And it works as you can see by running the code further below! That is perhaps not a surprise as neural networks are general function approximators, but it's not always that they work according to theory :-)

However, the network cannot learn the arithmetic mapping of e.g. y = 123 * x when x and y are ""float-bits"". This means it cannot generalize to data it hasn't seen during training in the arithmetic manner we might expect. But I don't think that is necessary for use in e.g. language models where we merely want to be able to map some tensor from e.g. an LSTM to an integer-token from the vocabulary.


### Loss Functions

I have tested this with both MSE and binary cross-entropy in Keras, which unfortunately isn't documented so I'm not completely sure what it does. But in both cases it works and the model trains to get the bit-wise mapping correct.

There might be cases where you are more concerned about the MSE between the actual integer-values instead of their ""float-bit-string"" representations, in which case we would need a TensorFlow method to convert ""float-bits"" to integers and then take the MSE of the resulting integer and the true integer from the data-set. This is not relevant for language models, because the proximity of integer-keys do not correspond to words that are necessarily similar in meaning. But it could be useful in other applications.


### TensorFlow Implementation

In order to make this work in TensorFlow it seems that we just need a couple of TensorFlow-methods for converting between integers and ""float-bit-strings"". I have hacked this together using numpy but I'm sure somebody on the dev-team can make a super-fast native TensorFlow implementation. Then we just need a wrapper in Keras and that might be enough to do e.g. language models with gigantic vocabs.


### Test-Code

    import numpy as np
    from tensorflow.python.keras.models import Sequential
    from tensorflow.python.keras.layers import InputLayer
    from tensorflow.python.keras.layers import Dense
    from tensorflow.python.keras.optimizers import RMSprop
    
    
    # Number of bits to use in our ""float-bit-strings"".
    num_bits = 32
    
    def int_to_floatbits(value):
        """"""
        Convert a single integer value to an array of 0.0 and 1.0 floats
        corresponding to the bit-string.
    
        Example: value==123 gives [0.  ... 0.  1.  1.  1.  1.  0.  1.  1.]
        """"""
    
        # Convert the integer value to a bit-string.
        # NOTE: This has been fixed to 32-bit length.
        bitstr = ""{0:032b}"".format(value)
    
        # Convert the bit-string to an array of equivalent float-values.
        floatbits = np.array([1.0 if bit == '1' else 0.0 for bit in bitstr])
    
        return floatbits
    
    
    def floatbits_to_strbits(floatbits):
        """"""
        Convert an array of floats to a bit-string.
        A float value greater than 0.5 results in 1.0
        and a float value less or equal to 0.5 results in 0.0
    
        Example: [0.1, 0.49, 0.51, 0.9, 1.1, -2.3] gives ""001110""
        """"""
    
        # Convert the float-array to a list of bit-characters '0' or '1'.
        charbits = ['1' if floatbit > 0.5 else '0' for floatbit in floatbits]
    
        # Convert the bit-characters to a string.
        strbits = """".join(charbits)
    
        return strbits
    
    def floatbits_to_int(floatbits):
        """"""
        Convert a float-array to an integer, assuming each element
        of the float-array corresponds to a bit.
        
        Example: [0.1, 0.49, 0.51, 0.9, 1.1, -2.3] corresponds to
        the bit-string ""001110"" which is the integer 14.
        """"""
    
        # Convert the float-array to a bit-string.
        strbits = floatbits_to_strbits(floatbits=floatbits)
    
        # Convert the bit-string to an integer value.
        value = int(strbits, base=2)
    
        return value
    
    
    # Various tests of the above functions.
    if True:
        foo = int_to_floatbits(123)
        print(foo)
        print(floatbits_to_strbits(foo))
        print(floatbits_to_int(foo))
    
        bar = [0.3,  0.9,  0.8,  0.51,  0.501,  0.4999,  0.999,  1.1]
        print(floatbits_to_strbits(bar))
        print(floatbits_to_int(bar))
    
        baz = [0.1, 0.49, 0.51, 0.9, 1.1, -2.3]
        print(floatbits_to_strbits(baz))
        print(floatbits_to_int(baz))
    
    # quit()
    
    # We will now train a TensorFlow / Keras model
    # that maps integers between 0 and 10000 to
    # the same numbers multiplied by 123.
    # If we were to use one-hot encoding then we would
    # need 10000 inputs to the Neural Network and
    # 1230000 outputs if using the full output range.
    # Using ""bit-strings"" encoded as floats, we only need
    # 14 bits for the input and 21 bits for the output.
    # We round it up to 32-bits.
    
    # The dataset as integers,
    # we want the Neural Network to map from x to y.
    x_int = np.arange(10000, dtype=int)
    y_int = 123 * x_int
    
    # Convert the dataset to ""float-bit-strings"" (aka. float-bits).
    x = np.array(list(map(int_to_floatbits, x_int)))
    y_true = np.array(list(map(int_to_floatbits, y_int)))
    
    # Check the mapping is correct. E.g. if the number of required bits
    # exceeds num_bits then these may not create numpy matrices correctly.
    if False:
        print(x.shape)
        print(y_true.shape)
        print(x[0:10])
        print(y_true[0:10])
    
    # Start construction of the Keras Sequential model.
    model = Sequential()
    
    # Add an input layer to the model.
    model.add(InputLayer(input_shape=(num_bits,)))
    
    # Fully-connected / dense layers with ReLU-activation.
    model.add(Dense(512, activation='relu'))
    model.add(Dense(512, activation='relu'))
    
    # Last fully-connected / dense layer with sigmoid-activation
    # so the output is between 0.0 and 1.0
    model.add(Dense(num_bits, activation='sigmoid'))
    
    optimizer = RMSprop(lr=1e-3)
    
    if True:
        # Loss is MSE.
        model.compile(optimizer=optimizer,
                      loss='mean_squared_error')
    else:
        # Loss is Binary Crossentropy, but also report MSE.
        model.compile(optimizer=optimizer,
                      loss='binary_crossentropy',
                      metrics=['mse'])
    
    epochs = 50
    
    if True:
        # Fit the model using the entire data-set.
        model.fit(x, y_true, epochs=epochs)
    else:
        # Fit the model using the data-set split into training and validation.
        # You will see that the validation-error is high so the model
        # has not learned the arithmetic function of the data-set.
        model.fit(x, y_true, epochs=epochs, validation_split=0.2)
    
    # Use the model to predict the output for a part of the data-set.
    y_pred = model.predict(x[0:10])
    
    # The true output for this part of the data-set.
    y_true_subset = y_true[0:10]
    
    # Map the ""float-bit-strings"" to integers.
    y_pred_int = list(map(floatbits_to_int, y_pred))
    y_true_int = list(map(floatbits_to_int, y_true_subset))
    
    # Print the predicted and true integers.
    print(*zip(y_pred_int, y_true_int))
    
    # Round the float-bit-strings to 2 decimals for pretty printing.
    def rounded(numbers):
        return np.array([[""{:.2f}"".format(x) for x in row] for row in numbers])
    y_pred_rounded = rounded(y_pred)
    y_true_rounded = rounded(y_true_subset)
    
    # Print the predicted and true float-bit-strings.
    # (I know it is bad to reuse the same variable-names here ...)
    for y_pred_int, y_true_int, y_pred_rounded, y_true_rounded \
        in zip(y_pred_int, y_true_int, y_pred_rounded, y_true_rounded):
    
        print(y_true_int, ""\t"", y_true_rounded)
        print(y_pred_int, ""\t"", y_pred_rounded)
        print()


### Output

True integer and true ""float-bit-string"" (note that the numbers are all exactly 0.00 or 1.00):

	738 	 ['0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '1.00' '0.00' '1.00' '1.00' '1.00' '0.00' '0.00' '0.00' '1.00' '0.00']

Predicted integer and predicted ""float-bit-string"" (note that the numbers a **not** all exactly 0.00 or 1.00):

	738 	 ['0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.03' '0.00' '0.00' '0.00' '0.01' '0.00' '0.00' '0.00' '0.00' '0.99' '0.00' '1.00' '1.00' '1.00' '0.00' '0.00' '0.00' '1.00' '0.00']
",0,,1,2018-01-27T15:23:29Z,2018-01-30T03:22:21Z,CONTRIBUTOR,2018-01-30T03:22:21Z
16486,Change RELEASE.md to specify CUDA 9.0,cla: yes,PR for https://github.com/tensorflow/tensorflow/issues/16348 (tinyest PR ever?),0,,3,2018-01-27T09:17:33Z,2018-02-01T03:22:55Z,CONTRIBUTOR,2018-01-27T09:19:21Z
16481,Container localhost does not exist.,"stat:awaiting tensorflower,type:bug/performance","Hi,

I upgraded from 1.5.0-rc1 to the current master branch and I started receiving the following error:

```
2018-01-27 02:48:38.928667: W tensorflow/core/framework/op_kernel.cc:1201] OP_REQUIRES failed at lookup_table_op.cc:656 : Not found: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
2018-01-27 02:48:38.928786: W tensorflow/core/framework/op_kernel.cc:1201] OP_REQUIRES failed at iterator_ops.cc:855 : Not found: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
	 [[Node: Lookup_1/LookupTableFind = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT64](lookup_1_placeholder, input_1, lookup_1_placeholder_1)]]
Exception in thread ""main"" org.platanios.tensorflow.jni.NotFoundException: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
	 [[Node: Lookup_1/LookupTableFind = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT64](lookup_1_placeholder, input_1, lookup_1_placeholder_1)]]
	 [[Node: Model/Model/Iterator/Next = IteratorGetNext[output_shapes=[[?,?], [?], [?,?], [?,?], [?]], output_types=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Model/Model/Iterator)]]
```

It's hard to reproduce this error but a summary of the context is that I have a lookup table op inside a dataset map operator and I get this error when I try to execute the corresponding iterator ""GetNext"" op. I'm looking for information in how to parse and debug this error. I never explicitly set any containers for my variables or lookup tables (i.e., leave them to the default value; an empty string). Were there any changes introduced recently that could result in this error? Note that this happens with my Scala API but not with the Python API and so it may be that I haven't updated something in my code. I just don't really know where to look for this.

Thanks!",1,,11,2018-01-27T07:58:18Z,2018-02-02T02:58:59Z,CONTRIBUTOR,2018-01-27T15:54:41Z
16479,Does 1.5.0 not suppurt CUDA 9.1? It worked with CUDA 9.0 but not 9.1,,"I installed 1.5.0, and tying to import tensorflow, but it said that 'cannot find cudart64_90.dll'. 
Then I installed the CUDA 9.0, and then everything works fine. 
So I want to make sure than does 1.5.0 not support CUDA 9.1 or I have something installed wrong? 
",0,,6,2018-01-27T02:22:43Z,2018-01-27T23:01:32Z,NONE,2018-01-27T03:21:39Z
16478,Failed install on Windows,type:bug/performance,"Python 3.6.4

There is a strange error when I install tensorflow 1.5.

```
Collecting tensorflow
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/34/96/11f048eca7b4d6da3084ca49c636b9e720e9dd1483c0c4e9ba3cf5037564/tensorflow-1.5.0-cp36-cp36m-win_amd64.whl
Requirement already up-to-date: wheel>=0.26 in d:\python\python36\lib\site-packages (from tensorflow)
Requirement already up-to-date: numpy>=1.12.1 in d:\python\python36\lib\site-packages (from tensorflow)
Collecting absl-py>=0.1.6 (from tensorflow)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/42/3c/1985d86a44bfe44fd060c02807336f840a509bfaa2d340860fba7d22da39/absl-py-0.1.9.tar.gz
Requirement already up-to-date: protobuf>=3.4.0 in d:\python\python36\lib\site-packages (from tensorflow)
Requirement already up-to-date: six>=1.10.0 in d:\python\python36\lib\site-packages (from tensorflow)
Collecting tensorflow-tensorboard<1.6.0,>=1.5.0 (from tensorflow)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/43/69/82e2a368076c94edbba3cd15804103bf1f31486d69e11551b71fa1d1f384/tensorflow_tensorboard-1.5.0-py3-none-any.whl
Requirement already up-to-date: setuptools in d:\python\python36\lib\site-packages (from protobuf>=3.4.0->tensorflow)
Requirement already up-to-date: bleach==1.5.0 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Requirement already up-to-date: markdown>=2.6.8 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Requirement already up-to-date: werkzeug>=0.11.10 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Requirement already up-to-date: html5lib==0.9999999 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Collecting futures>=3.1.1 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/1f/9e/7b2ff7e965fc654592269f2906ade1c7d705f1bf25b7d469fa153f7d19eb/futures-3.2.0.tar.gz
Unknown requires Python '>=2.6, <3' but the running Python is 3.6.4
```

Why the dependency is *futures*? It doesn't have a verion of Python 3.6.4.",0,,15,2018-01-27T02:08:44Z,2018-01-27T10:16:51Z,NONE,2018-01-27T03:28:37Z
16474,MKL: Making MKL-DNN default,"awaiting testing (then merge),cla: yes",Make Tensroflow use MKL DNN by default if --config=mkl is used when building,1,,10,2018-01-26T22:08:58Z,2018-01-26T23:43:30Z,CONTRIBUTOR,2018-01-26T22:14:09Z
16473,Fixing hard_sigmoid's documentation to match impl,cla: no,,1,,3,2018-01-26T21:25:42Z,2018-01-30T07:15:07Z,NONE,2018-01-26T21:34:42Z
16464,AssignAddVariableOp has no output,"stat:awaiting tensorflower,type:docs","
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0-rc1
- **Python version**: NA (Using Go bindings)
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 7.2.1
- **CUDA/cuDNN version**: 9.1 / 7.0
- **GPU model and memory**: GTX 1060 6GB
- **Exact command to reproduce**: See below


### Describe the problem
According to the docs, AssignAddVariableOp ""Outputs the incremented value, which can be used to totally order the increments to this variable."". Without this feature, I get non deterministic behavior when reading the value of the variable at the same time as I update it. However, at least in the Go bindings, it returns an operation which has no outputs. I can work around this problem by using two calls to `sess.Run()`, but this is inelegant.

### Source code / logs
```
package main

import (
	""fmt""

	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func main() {
	s := op.NewScope()
	value1 := op.Const(s.SubScope(""zero""), float32(0))
	value2 := op.Const(s, float32(3.1415))
	handle := op.VarHandleOp(s, tf.Float, tf.ScalarShape())
	init := op.AssignVariableOp(s, handle, value1)
	update := op.AssignAddVariableOp(s, handle, value2)
	fmt.Println(""NumOutputs:"", update.NumOutputs())
	graph, err := s.Finalize()
	if err != nil {
		panic(err)
	}
	sess, err := tf.NewSession(graph, nil)
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{init})
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, []tf.Output{update.Output(0)}, nil)
	if err != nil {
		panic(err)
	}
}
```
```
$ go run assign_demo.go 
NumOutputs: 0
panic: Tried to fetch data for 'AssignAddVariableOp:0', which produces no output.  To run to a node but not fetch any data, pass 'AssignAddVariableOp:0' as an argument to the 'target_node_names' argument of the Session::Run API.

goroutine 1 [running]:
main.main()
	/home/isaac/go/src/github.com/is8ac/gotf/assign_demo.go:32 +0x448
exit status 2
```",1,,4,2018-01-26T17:33:42Z,2018-02-01T06:41:53Z,NONE,2018-01-27T05:35:56Z
16463,Improve profiler error message when graph_path is not available.,"awaiting testing (then merge),cla: yes,kokoro:run","This fix tries to address the issue raised in #16451 to provide a better error message when graph_path is not available for profiler.

Previously if graph_path is not available, the process will crash
with not very imformative message and a core dump:
```
2018-01-26 01:43:29.458032: F tensorflow/core/profiler/profiler.cc:206] Non-OK-status: ReadProtoFile(Env::Default(), FLAGS_graph_path, graph.get(), false) status: Not found: ; No such file or directory
Aborted (core dumped)
```

With this fix, the error message is improved to:
```
Failed to read graph_path: Invalid argument: Cannot parse proto file.
```
and the process exit with 1.

This fix fixes #16451.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,1,2018-01-26T16:55:04Z,2018-01-26T18:11:51Z,MEMBER,2018-01-26T17:13:08Z
16462,How to create a model checkpoint based on each step rather than time interval.? using TensorFlow-Slim api.,"stat:awaiting tensorflower,type:support","slim.learning.train(
    train_op,
    logdir,
    number_of_steps=1000,
    **save_summaries_secs=300**,
    **save_interval_secs=600**):

The above api only supports to capture model checpoints periodically, but I need to checkpoint based on each step. How do achieve this using TesorFlow-Slim API.?

I am looking for parameters like this:

save_summaries_steps = 10,
save_interval_steps=10
    where the value 10 is the number of steps and that should be configurable.",0,,3,2018-01-26T16:48:53Z,2018-01-27T01:03:49Z,NONE,2018-01-27T01:03:49Z
16458,How to parse multivalve feature using tf.feature_column and tf.data API ??,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,2,2018-01-26T15:58:52Z,2018-01-27T01:02:00Z,NONE,2018-01-27T01:02:00Z
16454,ValueError: Dimensions 1069539296 and 13528529576648672 are not compatible,stat:awaiting response,"Traceback (most recent call last):
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 558, in merge_with
    new_dims.append(dim.merge_with(other[i]))
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 133, in merge_with
    self.assert_is_compatible_with(other)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 106, in assert_is_compatible_with
    other))
ValueError: Dimensions 1069539296 and 13528529576648672 are not compatible

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""Train.py"", line 117, in <module>
    train()
  File ""Train.py"", line 54, in train
    train_op=Evaluation.trainning(loss=loss1, learning_rate=0.0001)
  File ""/home/lihua/Documents/Projects/Project2018/trafficSignClassification/Evaluation.py"", line 36, in trainning
    train_op = optimizer.minimize(loss, global_step= global_step)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 315, in minimize
    grad_loss=grad_loss)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 386, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 560, in gradients
    in_grad.set_shape(t_in.get_shape())
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 443, in set_shape
    self._shape = self._shape.merge_with(shape)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 561, in merge_with
    raise ValueError(""Shapes %s and %s are not compatible"" % (self, other))
ValueError: Shapes (128, 4, 4, 1069539296) and (128, 4, 4, 13528529576648672) are not compatible


ubuntu16.04
tensorflow 1.4


",0,,1,2018-01-26T13:37:31Z,2018-01-27T02:52:08Z,NONE,2018-01-27T02:52:08Z
16451,Parameter parsing error messages,,"Parameter parsing error messages probably can be improved, e.g. 

`bazel-bin/tensorflow/core/profiler/profiler --profile_path=/tmp/for_tfprof/profile_20`

runs ok, but if cd to bazel-bin/tensorflow/core/profiler/ and

`./profiler --profile_path /tmp/for_tfprof/profile_20`

results in 

> ./profiler
> --profile_path
> /tmp/for_tfprof/profile_20
> Reading Files...
> Try to use a single --profile_path instead of graph_path,op_log_path,run_meta_path
> 2018-01-26 01:43:29.458032: F tensorflow/core/profiler/profiler.cc:206] Non-OK-status: ReadProtoFile(Env::Default(), FLAGS_graph_path, graph.get(), false) status: Not found: ; No such file or directory
> Aborted (core dumped)

",0,,1,2018-01-26T10:01:37Z,2018-01-26T18:11:51Z,CONTRIBUTOR,2018-01-26T16:55:34Z
16450,InvalidArgumentError (see above for traceback): sequence_length(0) <= 80 thrown by ctc_loss,stat:awaiting response,"I am encountering this error thrown by `ctc_loss` and I have no idea what it means nor how to resolve it.

```
InvalidArgumentError (see above for traceback): sequence_length(0) <= 80
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](transpose_1/_455, Where/_445, GatherNd, reshape_1/_457)]]
	 [[Node: CTCLoss/_459 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_7988_CTCLoss"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

```",0,,1,2018-01-26T09:49:04Z,2018-01-27T03:31:47Z,CONTRIBUTOR,2018-01-27T03:31:47Z
16448,embedding lookup table in tensorflow serving,,"Hi, I am trying to serve a NLP model in tensorflow serving. I am wondering how embedding matrix is being stored in tensorflow serving. If I deploy model to two servers, will the embedding matrix be a distributed table with sharding for looking up?",0,,2,2018-01-26T08:36:01Z,2018-01-27T03:32:39Z,NONE,2018-01-27T03:32:39Z
16446,use tflite bilinear op to resize input of label_image,"awaiting testing (then merge),cla: yes,comp:lite",replace previous naive `downsize()` function with a `resize()` using TF Lite RESIZE_BILINEAR operator,1,,1,2018-01-26T07:44:36Z,2018-01-31T19:22:18Z,CONTRIBUTOR,2018-01-30T18:02:50Z
16444,Documentation update,,"The mean squared error is described as following

mean_squared_error(
    labels,
    predictions,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)

The default reduction method is MEAN in v1.4",0,,6,2018-01-26T06:42:29Z,2018-01-26T12:08:15Z,NONE,2018-01-26T07:05:20Z
16443,Disable AWS S3 virtual addressing,"awaiting testing (then merge),cla: yes","The fix disables the virtual addressing of AWS S3, as was suggested in the comment https://github.com/tensorflow/tensorflow/issues/16397#issuecomment-360654674

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
",1,,4,2018-01-26T05:58:11Z,2018-01-29T16:24:14Z,MEMBER,2018-01-26T18:52:02Z
16438,xrange() was removed in Python 3,"awaiting testing (then merge),cla: yes",Each of these files contains at least one call to the Python 2-only builtin function __xrange()__ which was removed in Python 3 in favor of __range()__.  To each of these files we add the line [__from six.moves import xrange__](https://pythonhosted.org/six/#module-six.moves) for compatibility with both Python 2 and Python 3.,1,,1,2018-01-26T03:43:34Z,2018-01-26T18:10:56Z,CONTRIBUTOR,2018-01-26T06:41:59Z
16434, Imported lstm1d and lstm2d in ndlstm __init__.py.,"awaiting testing (then merge),cla: yes",Makes importing ndlstm modules easier.,1,,1,2018-01-26T02:58:34Z,2018-01-26T18:11:11Z,CONTRIBUTOR,2018-01-26T05:38:52Z
16433,Fix an imperfect implementation of tf.losses.mean_pairwise_squared_error,"awaiting testing (then merge),cla: yes","Here is a fix for the issue [Imperfect implementation of tf.losses.mean_pairwise_squared_error (#15968)](https://github.com/tensorflow/tensorflow/issues/15968)

RELNOTES: Fixed wrong normalization in tf.losses.mean_pairwise_squared_error to conform to the math and documentation. Numerical results will be different.",1,,5,2018-01-26T02:42:02Z,2018-02-01T18:32:37Z,CONTRIBUTOR,2018-02-01T01:27:39Z
16427,Fix build errors in contrib/mpi introduced by commit 6042b5d267f,"awaiting testing (then merge),cla: yes","The commit https://github.com/tensorflow/tensorflow/commit/6042b5d267f42d004087b44c29525951700579f9#diff-7c00d4a3caee74eedf5bb638bce23e5a 
* Introduced code to `tensorflow/contrib/mpi/mpi_rendezvous_mgr.h` to use the type `RecentRequestIds` without including the header `tensorflow/core/distributed_runtime/recent_request_ids.h`.
```
ERROR: /opt/tensorflow/tensorflow/contrib/mpi/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr' failed (Exit 1)
In file included from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:0:
./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:182:3: error: 'RecentRequestIds' does not name a type
   RecentRequestIds recv_tensor_recent_request_ids_;
   ^
```
* Probably a typo in `tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc : MPIRendezvousMgr::AddRequest()`. The variable `req` was probably meant to be `request` as per the commit message.
```
ERROR: /opt/tensorflow/tensorflow/contrib/mpi/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr' failed (Exit 1)
In file included from ./tensorflow/core/framework/variant.h:29:0,
                 from ./tensorflow/core/framework/allocator.h:26,
                 from ./tensorflow/core/framework/tensor.h:20,
                 from ./tensorflow/core/framework/device_base.h:23,
                 from ./tensorflow/core/framework/rendezvous.h:22,
                 from ./tensorflow/core/distributed_runtime/rendezvous_mgr_interface.h:22,
                 from ./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:22,
                 from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:35,
                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc: In member function 'void tensorflow::MPIRendezvousMgr::AddRequest(tensorflow::RecvTensorRequest, int)':
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:155:7: error: 'req' was not declared in this scope
       req.request_id(), ""RecvTensor (MPIRendezvousMgr)"", req));
       ^
```

I compiled with following commands:
```
echo ""deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8"" > /etc/apt/sources.list.d/bazel.list
curl https://bazel.build/bazel-release.pub.gpg | apt-key add -
git clone https://github.com/tensorflow/tensorflow .
export PYTHON_BIN_PATH=/path/to/python ## python 2.7.14
export USE_DEFAULT_PYTHON_LIB_PATH=1
export TF_NEED_JEMALLOC=1
export TF_NEED_GCP=0
export TF_NEED_HDFS=1
export TF_ENABLE_XLA=1
export TF_NEED_OPENCL=0
export TF_NEED_S3=0
export TF_NEED_GDR=0
export TF_NEED_VERBS=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=8.0
export CUDA_TOOLKIT_PATH=/path/to/cuda
export TF_CUDNN_VERSION=7
export CUDNN_INSTALL_PATH=/path/to/cudnn
export TF_CUDA_COMPUTE_CAPABILITIES=""3.5,5.2,6.0,6.1""
export TF_CUDA_CLANG=0
export GCC_HOST_COMPILER_PATH=/path/to/gcc
export TF_NEED_MPI=1
export MPI_HOME=/path/to/openmpi
export CC_OPT_FLAGS=""-march=native""
export TF_SET_ANDROID_WORKSPACE=0
./configure
bazel build --config=mkl --config=opt --config=cuda \
          //tensorflow/tools/pip_package:build_pip_package && \
bazel-bin/tensorflow/tools/pip_package/build_pip_package ./tensorflow_pkg
pip install -v ./tensorflow_pkg/tensorflow-*.whl
```",1,,6,2018-01-26T01:43:01Z,2018-01-26T19:28:34Z,CONTRIBUTOR,2018-01-26T02:07:18Z
16424,Dependency on old version of bleach (1.5),type:build/install,"Bleach 1.5 came out Nov 4th 2016 and this is old enough to cause dependency issues for projects that stayed up to date with Bleach.

In particular, this causes issues for Jupyter users.",1,,11,2018-01-25T23:20:59Z,2018-02-02T22:16:28Z,NONE,2018-01-27T06:01:01Z
16423,Windows 10 Cmake GPU nvcc.exe error,type:build/install,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
r1.5
- **Python version**: 
3.6
- **GCC/Compiler version (if compiling from source)**:
Visual Studio 2017
- **CUDA/cuDNN version**:
9.1
- **GPU model and memory**:
1080Ti
- **Exact command to reproduce**:

**Cmake Command:**
```
cmake -G ""Visual Studio 15 2017 Win64"" -T host=x64 -DCMAKE_BUILD_TYPE=""
Release"" -DSWIG_EXECUTABLE='C:\ProgramData\Chocolatey\bin\swig.exe' -Dtensorflow_ENABLE_GPU=ON -Dtensorflow_CUDA_VERSION
=9.1 -Dtensorflow_CUDNN_VERSION=7 -Dtensorflow_WIN_CPU_SIMD_OPTIONS=""/arch:AVX2"" -DCUDA_CUDART_LIBRARY=D:\NVIDIA\CUDA\v9
.1 -DCUDNN_HOME='D:\NVIDIA\CUDA\v9.1' ..
```

**Build command**
```
""C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\Bin\amd64\MSBuild.exe"" /m:4 /p:Configuration=Release .\tf_core_gpu_kernels.vcxproj
```
### Describe the problem
Cmake creates a bad command to send to nvcc.exe

In tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.Release.cmake:202L
there is an error with the resulting command.

There is issues with spacing, "";"" in between arguments and others.

**Command Ran**
`C:/NVIDIA/CUDA/v9.1/bin/nvcc.exe -M -D__CUDACC__ D:/tensorflow/tensorflow/core/kernels/adjust_contrast_op_gpu.cu.cc -o D:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.NVCC-depend -ccbin;C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin -m64;-DSQLITE_OMIT_LOAD_EXTENSION;-DEIGEN_AVOID_STL_ARRAY;-DNOMINMAX;-D_WIN32_WINNT=0x0A00;-DLANG_CXX11;-DCOMPILER_MSVC;-DWIN32;-DOS_WIN;-D_MBCS;-DWIN64;-DWIN32_LEAN_AND_MEAN;-DNOGDI;-DPLATFORM_WINDOWS;-DTENSORFLOW_USE_EIGEN_THREADPOOL;-DEIGEN_HAS_C99_MATH;-DTF_COMPILE_LIBRARY;-DGRPC_ARES=0;-DTF_USE_SNAPPY;-DGOOGLE_CUDA=1;-DTF_EXTRA_CUDA_CAPABILITIES=6.1 -Xcompiler;,""/DWIN32"",""/D_WINDOWS"",""/W3"",""/GR"",""/EHsc"",""/MP"",""/arch:AVX2"",""/MD"",""/O2"",""/Ob2"",""/DNDEBUG"",""/D_ITERATOR_DEBUG_LEVEL=0""  -gencode;arch=compute_61,code=""sm_61,compute_61"";--include-path;D:/tensorflow/tensorflow/contrib/cmake/build/Release;--expt-relaxed-constexpr;-ftz=true;  -DNVCC -IC:/NVIDIA/CUDA/v9.1/include ;-ID:/tensorflow ;-ID:/tensorflow/tensorflow/contrib/cmake/build ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/zlib_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/gif_archive/giflib-5.1.4 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/png_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/jpeg_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/lmdb ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/eigen_archive ;-ID:/tensorflow/third_party/eigen3 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/gemmlowp/src/gemmlowp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/src/jsoncpp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive/util ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/highwayhash ;-ID:/tensorflow/tensorflow/contrib/cmake/build/cub/src/cub ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/nsync/public ;-ID:/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src ;-ID:/tensorflow/tensorflow/contrib/cmake/build/re2/install/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/sqlite ;-ID:/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/snappy/src/snappy ;-IC:/NVIDIA/CUDA/v9.1 ;-IC:/NVIDIA/CUDA/v9.1/extras/CUPTI/include ;-ID:/tensorflow/third_party/gpus`

Multable invalid cmake varables

```
${CCBIN} = -ccbin;C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin 
${nvcc_flags} = -m64;-DSQLITE_OMIT_LOAD_EXTENSION;-DEIGEN_AVOID_STL_ARRAY;-DNOMINMAX;-D_WIN32_WINNT=0x0A00;-DLANG_CXX11;-DCOMPILER_MSVC;-DWIN32;-DOS_WIN;-D_MBCS;-DWIN64;-DWIN32_LEAN_AND_MEAN;-DNOGDI;-DPLATFORM_WINDOWS;-DTENSORFLOW_USE_EIGEN_THREADPOOL;-DEIGEN_HAS_C99_MATH;-DTF_COMPILE_LIBRARY;-DGRPC_ARES=0;-DTF_USE_SNAPPY;-DGOOGLE_CUDA=1;-DTF_EXTRA_CUDA_CAPABILITIES=6.1
${nvcc_host_compiler_flags} = -Xcompiler;,""/DWIN32"",""/D_WINDOWS"",""/W3"",""/GR"",""/EHsc"",""/MP"",""/arch:AVX2"",""/MD"",""/O2"",""/Ob2"",""/DNDEBUG"",""/D_ITERATOR_DEBUG_LEVEL=0""  
${depends_CUDA_NVCC_FLAGS} = -gencode;arch=compute_61,code=""sm_61,compute_61"";--include-path;D:/tensorflow/tensorflow/contrib/cmake/build/Release;--expt-relaxed-constexpr;-ftz=true
${CUDA_NVCC_INCLUDE_ARGS} = -IC:/NVIDIA/CUDA/v9.1/include ;-ID:/tensorflow ;-ID:/tensorflow/tensorflow/contrib/cmake/build ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/zlib_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/gif_archive/giflib-5.1.4 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/png_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/jpeg_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/lmdb ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/eigen_archive ;-ID:/tensorflow/third_party/eigen3 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/gemmlowp/src/gemmlowp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/src/jsoncpp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive/util ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/highwayhash ;-ID:/tensorflow/tensorflow/contrib/cmake/build/cub/src/cub ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/nsync/public ;-ID:/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src ;-ID:/tensorflow/tensorflow/contrib/cmake/build/re2/install/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/sqlite ;-ID:/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/snappy/src/snappy ;-IC:/NVIDIA/CUDA/v9.1 ;-IC:/NVIDIA/CUDA/v9.1/extras/CUPTI/include ;-ID:/tensorflow/third_party/gpus
```


Should be
```
${CCBIN} = -ccbin ""C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin""
${nvcc_flags} = -m64 -DSQLITE_OMIT_LOAD_EXTENSION -DEIGEN_AVOID_STL_ARRAY -DNOMINMAX -D_WIN32_WINNT=0x0A00 -DLANG_CXX11 -DCOMPILER_MSVC -DWIN32 -DOS_WIN -D_MBCS -DWIN64 -DWIN32_LEAN_AND_MEAN -DNOGDI -DPLATFORM_WINDOWS -DTENSORFLOW_USE_EIGEN_THREADPOOL -DEIGEN_HAS_C99_MATH -DTF_COMPILE_LIBRARY -DGRPC_ARES=0 -DTF_USE_SNAPPY -DGOOGLE_CUDA=1 -DTF_EXTRA_CUDA_CAPABILITIES=6.1 
${nvcc_host_compiler_flags} = -Xcompiler ""/DWIN32,/D_WINDOWS,/W3,/GR,/EHsc,/MP,/arch:AVX2,/MD,/O2,/Ob2,/DNDEBUG,/D_ITERATOR_DEBUG_LEVEL=0"" 
${depends_CUDA_NVCC_FLAGS} = -gencode arch=compute_61,code=\""sm_61,compute_61\"" --include-path D:/tensorflow/tensorflow/contrib/cmake/build/Release --expt-relaxed-constexpr -ftz=true
${CUDA_NVCC_INCLUDE_ARGS} = -IC:/NVIDIA/CUDA/v9.1/include -ID:/tensorflow -ID:/tensorflow/tensorflow/contrib/cmake/build -ID:/tensorflow/tensorflow/contrib/cmake/build/external/zlib_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/gif_archive/giflib-5.1.4 -ID:/tensorflow/tensorflow/contrib/cmake/build/external/png_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/jpeg_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/lmdb -ID:/tensorflow/tensorflow/contrib/cmake/build/external/eigen_archive -ID:/tensorflow/third_party/eigen3 -ID:/tensorflow/tensorflow/contrib/cmake/build/gemmlowp/src/gemmlowp -ID:/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/src/jsoncpp -ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive/util -ID:/tensorflow/tensorflow/contrib/cmake/build/external/highwayhash -ID:/tensorflow/tensorflow/contrib/cmake/build/cub/src/cub -ID:/tensorflow/tensorflow/contrib/cmake/build/external/nsync/public -ID:/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src -ID:/tensorflow/tensorflow/contrib/cmake/build/re2/install/include -ID:/tensorflow/tensorflow/contrib/cmake/build/external/sqlite -ID:/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/include -ID:/tensorflow/tensorflow/contrib/cmake/build/snappy/src/snappy -IC:/NVIDIA/CUDA/v9.1 -IC:/NVIDIA/CUDA/v9.1/extras/CUPTI/include -ID:/tensorflow/third_party/gpus

```",0,,1,2018-01-25T21:55:09Z,2018-01-26T01:19:15Z,NONE,2018-01-26T01:19:15Z
16419,R1.4,cla: no,want to test ,0,,2,2018-01-25T20:09:27Z,2018-01-25T21:33:32Z,NONE,2018-01-25T21:33:32Z
16417,Add missing library in Dockerfile,cla: yes,The local Dockerfile does not have all the dependencies for running the exercise notebooks in udacity assignments.,1,,5,2018-01-25T19:42:22Z,2018-01-25T23:31:26Z,CONTRIBUTOR,2018-01-25T19:44:00Z
16416,Making global constant.py file for ops,cla: no,Created a separate common `constants.py` which can be used globally.,0,,2,2018-01-25T19:39:27Z,2018-01-25T19:39:59Z,CONTRIBUTOR,2018-01-25T19:39:59Z
16413,Separate constant file for global variables,cla: no,"Created a separate common `constants.py` which can be used globally under `ops.`

**P.S:** I created the same pull request [#16401](https://github.com/tensorflow/tensorflow/pull/16401) as after pushing my latest changes I was facing CLA issues.",0,,4,2018-01-25T18:55:02Z,2018-01-25T19:29:35Z,CONTRIBUTOR,2018-01-25T19:04:36Z
16412,Documentation on build from source is unclear,type:support,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh


python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.4.1-7-gaa03bfc', '1.4.1')
built and installed from source with
git checkout r1.4
bazel build -c opt --copt=-march=""haswell"" --config=cuda --verbose_failures --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package >pip_package_build2.log 2>&1
note: incompatible path flag is required with R1.4 at this time per https://github.com/tensorflow/tensorflow/issues/15492
ubuntu 16.04
Cuda 9.1, cudnn 7.0.4
gcc --version
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
uname -r
4.4.0-104-generic
Bazel 0.9

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
It is unclear how to build and install the entire package purely from source
I will attempt to log what I have done so far
clone and build TF R1.4 for cuda
install wheel into local directory  (sudo pip install /tmp/tensorflow-pkg/tensorflow*.whl -t ~/mytf_r1.4_c9.1
export PYTHONPATH=~/mytf_r1.4_c9.1
move tensorflow directory

install common_voice files to ~/Common_voice

per native client build from source instructions: https://github.com/mozilla/DeepSpeech/blob/master/native_client/README.md
git clone tensorflow
cd tensorflow
git checkout r1.4
ln -s ../DeepSpeech/native_client ./
./configure
edit native_client/BUILD
comment out the following:
#    tfcompile_flags = select({
#        ""//tensorflow:rpi3"": str('--target_triple=""armv6-linux-gnueabihf"" --target_cpu=""cortex-a53"" --target_features=""+neon-fp-armv8""'),
#        ""//conditions:default"": str('')
#    }),
bazel build -c opt --copt=-O3 --incompatible_load_argument_is_label=false //tensorflow:libtensorflow_cc.so //tensorflow:libtensorflow_framework.so //native_client:deepspeech //native_client:deepspeech_utils //native_client:libctc_decoder_with_kenlm.so //native_client:generate_trie
at this point all the native client binaries are in
~/tensorflow/bazel-bin/native_client
levinth@zt-gpu-lin:~/DeepSpeech/native_client$ ls ~/tensorflow/bazel-bin/native_client/
generate_trie
generate_trie-2.params
generate_trie.runfiles
generate_trie.runfiles_manifest
libctc_decoder_with_kenlm.so
libctc_decoder_with_kenlm.so-2.params
libctc_decoder_with_kenlm.so.runfiles
libctc_decoder_with_kenlm.so.runfiles_manifest
libdeepspeech.a
libdeepspeech.a-2.params
libdeepspeech.pic.a
libdeepspeech.pic.a-2.params
libdeepspeech.so
libdeepspeech.so-2.params
libdeepspeech_utils.a
libdeepspeech_utils.a-2.params
libdeepspeech_utils.pic.a
libdeepspeech_utils.pic.a-2.params
libdeepspeech_utils.so
libdeepspeech_utils.so-2.params
_objs

cd ../Deepspeech/native_client
export TFDIR ~/tensorflow
make deepspeech


at this point however the native client shared objects are still in bazel-bin/native client and have not been installed. the invocation of Deepspeech.py fails as it cannot find the shared objects
python DeepSpeech.py --train_files ../Common_voice/cv-valid-train.csv,../Common_voice/cv-other-train.csv --dev_files ../Common_voice/cv-valid-dev.csv --test_files ../Common_voice/cv-valid-test.csv >deepspeech_1.log 2>&1
tensorflow.python.framework.errors_impl.NotFoundError: native_client/libctc_decoder_with_kenlm.so: cannot open shared object file: No such file or directory

the native_client/Makefile has sections for bindings and install..so try
sudo make install
and this still generates the error as install does not put
~/tensorflow/bazel-bin/native_client/libctc_decoder_with_kenlm.so
into /usr/local/lib 
though deepspeech.so and deepspeech_utils.so are installed there.

manually copy /tensorflow/bazel-bin/native_client/libctc_decoder_with_kenlm.so to ~/DeepSpeech/native_client and set permissions
at this point the invocation now starts running but complains about
------------------------------------------------------------------------
WARNING: libdeepspeech failed to load, resorting to deprecated code
         Refer to README.md for instructions on installing libdeepspeech
------------------------------------------------------------------------
even though /usr/local/lib is in the $LD_LIBRARY_PATH

invoking
python DeepSpeech.py --train_files ../Common_voice/cv-valid-train.csv,../Common_voice/cv-other-train.csv --dev_files ../Common_voice/cv-valid-dev.csv --test_files ../Common_voice/cv-valid-test.csv --display_step 1 --validation_step 10

------------------------------------------------------------------------
WARNING: libdeepspeech failed to load, resorting to deprecated code
         Refer to README.md for instructions on installing libdeepspeech
------------------------------------------------------------------------
I STARTING Optimization
Loading the LM will be faster if you build a binary file.
Reading data/lm/lm.binary
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
terminate called after throwing an instance of 'lm::FormatLoadException'
  what():  native_client/kenlm/lm/read_arpa.cc:65 in void lm::ReadARPACounts(util::FilePiece&, std::vector<long unsigned int>&) threw FormatLoadException.
first non-empty line was ""version https://git-lfs.github.com/spec/v1"" not \data\. Byte: 43

I clearly have not figured this out
:-)
 
",0,,3,2018-01-25T17:58:48Z,2018-01-25T19:45:03Z,NONE,2018-01-25T19:45:03Z
16409,Build libjpeg-turbo ALTIVEC SIMD,"awaiting review,cla: yes","The libjpeg-turbo package has ALTIVEC SIMD and this updates the
third_party build to build the ALTIVEC SIMD on the appropriate
platform.",1,,1,2018-01-25T16:12:29Z,2018-01-25T18:39:32Z,CONTRIBUTOR,2018-01-25T16:59:47Z
16407,Creating placeholder with `np.uint32` dtype fails,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes custom snippet below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 7
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
not applicable
- **GCC/Compiler version (if compiling from source)**:
not applicable
- **CUDA/cuDNN version**:
CUDA 8.5
- **GPU model and memory**:
Titan X
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
np.uint32 dtype is not supported while creating placeholder

### Source code / logs
```python
import tensorflow as tf
import numpy as np
print(tf.placeholder(np.int32, [None], 'ph1'))
print(tf.placeholder(np.uint32, [None], 'ph2'))
```
Line 3 works, line 4 fails.

Console Output:
```txt
Tensor(""ph1:0"", shape=(?,), dtype=int32)
Traceback (most recent call last):
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 126, in make_type
    v = dtypes.as_dtype(v).base_dtype
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py"", line 595, in as_dtype
    ""Cannot convert value %r to a TensorFlow DType."" % type_value)
TypeError: Cannot convert value <class 'numpy.uint32'> to a TensorFlow DType.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:/scaffold-ext/scaffold_ext/analysis/ann/bug.py"", line 4, in <module>
    print(tf.placeholder(np.uint32, [None], 'ph2'))
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1599, in placeholder
    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 3083, in _placeholder
    dtype = _execute.make_type(dtype, ""dtype"")
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 129, in make_type
    (arg_name, repr(v)))
TypeError: Expected DataType for argument 'dtype' not <class 'numpy.uint32'>.
```",0,,2,2018-01-25T14:50:18Z,2018-01-25T22:20:04Z,NONE,2018-01-25T22:14:40Z
16405,tf.contrib.framework.sort failing with <<...has no attribute 'sort'>> in TF windows ,,"### [Problem] : Can't use tf.contrib.framework.sort in my tensorflow code as it's failing to find the 'sort' attribute

Source code:
```
import tensorflow as tf

x = tf.placeholder(tf.float32, shape=(10, 10))
y = tf.contrib.framework.sort(x)

with tf.Session() as sess:
    rand_array = np.random.rand(10, 10)
    print(sess.run(y, feed_dict={x: rand_array})) 
```

Error log:
```
 <module>
    y = tf.contrib.framework.sort(x)
AttributeError: module 'tensorflow.contrib.framework' has no attribute 'sort'
```

### System information
- I've been trying to use the tf.contrib.framework.sort  within a custom loss function but issue being reproduced with a simple call to the tf.contrib.framework.sort
- Windows 10 64 bit
- TF installed with native pip3
- TF version: 1.4.0
- Python version: 3.6.2 
- TF with CPU support only
",0,,1,2018-01-25T14:17:01Z,2018-01-25T23:57:30Z,NONE,2018-01-25T23:57:30Z
16404,remove SRU num_units == x.shape[-1] restriction,"awaiting testing (then merge),cla: yes","Based on the [author's response](https://github.com/taolei87/sru/issues/12), the restriction is unnecessary. Simply add a linear transform to the input will solve the issue

#13094 ",1,,7,2018-01-25T13:06:58Z,2018-01-29T21:33:49Z,CONTRIBUTOR,2018-01-25T16:55:25Z
16403,1D Convolution in Tensorflow Serving,stat:awaiting response,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: tensorflow binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.6
- **CUDA/cuDNN version**: 9.0, 7.0
- **GPU model and memory**: GTX 1050

### Describe the problem
The Problem is a little bit hard to reproduce, I guess because so many steps are involved.
So, the basic scenario is, that I am using keras to train a model in python. Here is the model I am using:

`
           input = Input(shape=(200, 8))
            x = Conv1D(filters=128, kernel_size=7, activation=""relu"", padding=""same"")(input)
            x = Conv1D(filters=128, kernel_size=7, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=128, kernel_size=3, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=128, kernel_size=3, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=128, kernel_size=3, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=2, kernel_size=1, activation=""softmax"")(x)

`

Now, I extract the graph and I am saving graph and weights with the ModelBundleBuilder:

`
session = K.get_session()

        signature = tf.saved_model.signature_def_utils.build_signature_def(
            inputs={'input': tf.saved_model.utils.build_tensor_info(self._get_model().inputs[0])},
            outputs={'output': tf.saved_model.utils.build_tensor_info(self._get_model().outputs[0])},
            method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME
        )

        b = tf.saved_model.builder.SavedModelBuilder(filename)
        legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')
        b.add_meta_graph_and_variables(session,
                                       [tf.saved_model.tag_constants.SERVING],
                                       signature_def_map={
                                           tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature},
                                       legacy_init_op=legacy_init_op)
        b.save()
`

If I am loading the model via python, everything works as expected.

Now I am deploying the model into TF serving and using protobuf / gRPC to make the prediction via Java. I am converting a 3D float array to a TensorProto like this:

`
TensorShapeProto.Dim dim1 = TensorShapeProto.Dim.newBuilder()
                .setSize(data.length).build();

        TensorShapeProto.Dim dim2 = TensorShapeProto.Dim.newBuilder()
                .setSize(data[0].length).build();

        TensorShapeProto.Dim dim3 = TensorShapeProto.Dim.newBuilder()
                .setSize(data[0][0].length).build();

        TensorShapeProto shape = TensorShapeProto.newBuilder()
                .addDim(dim1).addDim(dim2).addDim(dim3).build();

        TensorProto.Builder builder = TensorProto.newBuilder()
                .setDtype(DataType.DT_FLOAT)
                .setTensorShape(shape);

        for(int i = 0; i < data.length; i++) {
            for(int j = 0; j < data[0].length; j++) {
                for(int k = 0; k < data[0][0].length; k++) {
                    builder.addFloatVal(data[k][j][i]);
                }
            }
        }

        return builder.build();
`

And do the predicition like this:

`
public class ModelClientImpl implements ModelClient {

    private String host;
    private Integer port;
    private ManagedChannel channel;
    private PredictionServiceGrpc.PredictionServiceBlockingStub stub;

    public void init() {
        channel = ManagedChannelBuilder
                .forAddress(getHost(), getPort())
                .usePlaintext(true)
                .build();

        stub = PredictionServiceGrpc.newBlockingStub(channel);
    }

    @Override
    public Map<String, TensorProto> predict(final String signatureName, Map<String, TensorProto> inputs) {
        final Predict.PredictResponse response = stub.predict(createRequest(signatureName, inputs));

        return response.getOutputsMap();
    }

    protected Predict.PredictRequest createRequest(final String signatureName, final Map<String, TensorProto> inputs) {
        final Model.ModelSpec modelSpec = Model.ModelSpec.newBuilder()
                .setName(signatureName)
                .setSignatureName(""serving_default"").build();

        final Predict.PredictRequest.Builder builder = Predict.PredictRequest.newBuilder()
                .setModelSpec(modelSpec)
                .putAllInputs(inputs);

        return builder.build();
    }

    public String getHost() {
        return host;
    }

    public void setHost(String host) {
        this.host = host;
    }

    public Integer getPort() {
        return port;
    }

    public void setPort(Integer port) {
        this.port = port;
    }

    @Override
    public void close() throws Exception {
        channel.shutdown().awaitTermination(5, TimeUnit.DAYS);
    }
}

`

But the prediction is totally different from python. Does anybody know if this is a bug or is something wromg with 1dconv?
",0,,2,2018-01-25T12:50:42Z,2018-01-26T01:32:27Z,NONE,2018-01-26T01:32:27Z
16402,Modified Implementation of ndlstm_base_dynamic.,"awaiting review,cla: yes",It now uses a `BasicLSTMCell` that has `state_is_tuple=True` to address the deprecation thrown by having `state_is_tuple=False`.,1,,1,2018-01-25T12:13:47Z,2018-01-25T18:41:31Z,CONTRIBUTOR,2018-01-25T16:40:02Z
16401,Separate constant file for global variables,cla: no,Created a separate common `constants.py` which can be used globally under `ops`.,1,,3,2018-01-25T11:36:25Z,2018-01-25T18:48:26Z,CONTRIBUTOR,2018-01-25T16:52:38Z
16400,"[doc] link to ""How to Use t-SNE Effectively"" from embeddings is broken",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)  NO (since Web page problem)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7**:
- **TensorFlow installed from (source or binary) binary**:
- **TensorFlow version (use command below) 1.5.0rc0 **:
- **Python version  3.5.1**: 
- **Bazel version (if compiling from source) NOT USED**:
- **GCC/Compiler version (if compiling from source) NOT USED**:
- **CUDA/cuDNN version NOT USED**:
- **GPU model and memory NOT USED **:
- **Exact command to reproduce DOC Problem. Just look https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/**:

### Describe the problem
- Link to to ""How to Use t-SNE Effectively"" is broken.
- The page link is follows (before junmping)
  - https://www.tensorflow.org/programmers_guide/embedding
  - 404 page is following URL 
     - https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/
- Original page should be follows. (the URL in embedding.md should rewrite to follows)
  -   https://distill.pub/2016/misread-tsne/

### Source code / logs
- The problem code is follows.
  - https://github.com/tensorflow/tensorflow/blame/v1.5.0-rc1/tensorflow/docs_src/programmers_guide/embedding.md#L123

",0,,1,2018-01-25T10:17:07Z,2018-01-25T16:06:01Z,NONE,2018-01-25T16:06:00Z
16399,Does TensorFlow 1.5 support CUDA 9.1?,,"My notebook has an MX150 display adapter, someone said that it's available with CUDA 9.1",0,,3,2018-01-25T08:28:58Z,2018-01-26T01:42:20Z,NONE,2018-01-25T10:52:35Z
16398,Compare_and_bitpack function for bool for big endian,"cla: yes,stat:awaiting response","Added condition for endianness check and related conversion for Big Endian.
Removed the note from file: 
`// NOTE(ebrevdo): This assumes memory is little-endian.`
Please let me know your feedback.",1,,1,2018-01-25T08:22:19Z,2018-01-26T16:48:27Z,CONTRIBUTOR,2018-01-25T16:51:11Z
16394,cmake gpu build improvement,"awaiting review,cla: no","cmake build pass with gpu enabled

python binding option can change to off now",1,,4,2018-01-25T06:23:27Z,2018-01-27T03:56:45Z,CONTRIBUTOR,2018-01-26T16:49:11Z
16389,cherrypick bfloat16 changes,cla: yes,,1,,2,2018-01-25T04:12:26Z,2018-01-25T18:20:31Z,NONE,2018-01-25T18:13:12Z
16386,Using keras layers within an Estimator either causes training where it shouldn't or corrupts weights,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 3.16.36
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9', '1.4.1')
- **Python version**: 2.7.9
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See gist

### Describe the problem

Hey there,

So I have a `tf.keras` model that for use on Amazon SageMaker I'm trying to convert into an Estimator. I know there's `tf.keras.estimator.model_to_estimator` but, I'm having separate [issues with that](https://github.com/tensorflow/tensorflow/issues/16385).

In the [easily run reproduction here](https://gist.github.com/zmjjmz/667d0d7e6b6c97c49b3aaf4d67b03d2c) I have (as a demonstration) a `tf.keras.layers.Embedding` which is initialized with all zeros and has `trainable=False`. Followed by that is a `Dense` layer with `use_bias=False` because I couldn't figure out how to get predictions out of an Estimator without training it first (and I can't train nothing apparently). Since all of the embeddings are zero however and can't be trained, the `Dense` layer should always produce a zero, even after training it. Instead, it produces garbage!

In fact, I've taken a few steps to ensure that no training takes place, although ideally I'd be able to just run the estimator without training:
1) I've set the loss to be 0 initially (l2_norm of what should start out as 0)
2) Optimize with SGD using a learning rate of 0
3) One training example that should have zero loss...

The output I actually get is very much non-zero. If I inspect the `embed/embeddings:0` tensor in `tfbdg`, I see this:
```
array([[ 0.14387012,  0.83495581,  0.44025695,  0.25154734,  0.7214781 ,  0.40229702,  0.82108581,  0.12210274,  0.43861651,  0.39615464],                
       [ 0.81636655,  0.48157215,  0.48987687,  0.48775947,  0.62187696,  0.25421095,  0.64555049,  0.97305572,  0.53352964,  0.34286666],                
       [ 0.82881641,  0.80365777,  0.4596678 ,  0.21614265,  0.22256434,  0.07986271,  0.92880177,  0.64946997,  0.89239001,  0.13793337],                
       [ 0.98491704,  0.15281868,  0.77106941,  0.30048406,  0.86042607,  0.88010466,  0.64362776,  0.70185173,  0.49912012,  0.61521161]], dtype=float32)
```

Even though it shouldn't have budged from all zeroes! 

So, something about how I'm doing this is fundamentally broken. I suspect that the issue is in line `61` where I start a new Session -- however this appears to be necessary, since I need to ensure that the `keras` backend is using the same `Graph` as `tensorflow.get_default_graph()` due to the peculiarities of how the Estimator calls the `model_fn`.

Notably those values hold between:
1) Runs of the estimator
2) Successive runs with the same `tf.set_random_seed` value

The latter makes me think that somehow the `Embedding` layer is receiving *a* gradient despite my best efforts, although it's hard to test this versus some sort of memory corruption. 

If you need me to provide any more information let me know -- I'm sure I've left something out.

",0,,5,2018-01-25T03:06:57Z,2018-01-25T17:23:21Z,NONE,2018-01-25T09:48:41Z
16384,fix typos,cla: yes,fix typos,1,,1,2018-01-25T01:28:59Z,2018-01-25T18:52:07Z,CONTRIBUTOR,2018-01-25T18:52:17Z
16382,Disable bfloat16 for sparse_matmul for 1.5.0,cla: yes,"I'm using this PR to test out simple workarounds for the sparse_matmul problem.
It doesn't need a reviewer yet.

Note: it looks like we're going to try to release 1.5.0 with the fix anyway. I will keep this PR available until that's finished.",1,,3,2018-01-25T00:00:18Z,2018-01-25T19:17:06Z,MEMBER,2018-01-25T01:29:34Z
16379,No module named 'tensorflow'. Anaconda+windows10+tensorflow-gpu+cuda8+cudnn6,type:build/install,"I had anaconda on my windows 10.
I installed CUDA 8.0 with cuDNN 6 and then followed [http://blog.nitishmutha.com/tensorflow/2017/01/22/TensorFlow-with-gpu-for-windows.html](url) to activate tensorflow-gpu environment. Now when I import tensorflow in the console, it works but with jupyter notebook opened right in this environment it throws the error. I even upgraded setuptools as mentioned in a previous issue.
![capture5](https://user-images.githubusercontent.com/10391022/35360420-a5cd8f98-0183-11e8-919a-53da56df5ee8.JPG)
![capture6](https://user-images.githubusercontent.com/10391022/35360450-c27e44e8-0183-11e8-9c0a-5aaaa05000c4.JPG)
![capture7](https://user-images.githubusercontent.com/10391022/35360455-c7e24a42-0183-11e8-964b-7186fb233078.JPG)
![capture8](https://user-images.githubusercontent.com/10391022/35360549-174292c2-0184-11e8-9d6a-93ba6139a275.JPG)


",0,,3,2018-01-24T22:27:17Z,2018-01-25T12:25:09Z,NONE,2018-01-24T23:43:04Z
16377,R1.4,cla: no,,0,,2,2018-01-24T21:03:18Z,2018-01-24T22:17:26Z,NONE,2018-01-24T22:17:26Z
16375,Branch 183115307,cla: yes,,0,,4,2018-01-24T20:39:34Z,2018-01-24T22:47:19Z,CONTRIBUTOR,2018-01-24T21:21:17Z
16371,Tegra Nvidia Jetson TX2 build python 2.7 new CUDA and CUDNN,"stat:awaiting response,type:build/install","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux4Tegra 28.2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5-rc1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.9
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: Denver2 8GB
- **Exact command to reproduce**: import tensorflow

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Clean installation with the new CUDA 9 and cudnn 7 from nvidia
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
Collecting system information...
Traceback (most recent call last):
  File ""/tmp/check_tf.py"", line 1, in <module>
    import tensorflow as tf;
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws4Time9LocalTimeEP2tml


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Wrote environment to tf_env.txt. You can review the contents of that file.
and use it to populate the fields in the github issue template.

cat tf_env.txt

```
",0,,6,2018-01-24T17:58:38Z,2018-01-26T15:45:08Z,NONE,2018-01-26T02:09:31Z
16369,Unittesting Models with Tensorflow - How to clear the existing graph ?,stat:awaiting response,"Hello dear tensorflowers,

I have already asked the question of [StackOverflow](https://stackoverflow.com/questions/48421308/tensorflow-and-unittests-layer-already-defined), however it seams like nobody can answer my question.

So I hope you will forgive me about reposting it here:

I am developing unittests for a product I implemented with TF.

Each part of the model is tested separately then all together in different conditions.

Let's take the example of a simple GAN, I have the following tests:

 - **GeneratorTest** Class: With all tests concerning G inside
 - **DiscriminatorTest** Class: With all tests concerning D inside
 - **GAN_Train_Test** Class: G and D connected all together: 1 training step is tested.
 - **GAN_Inference_Test** Class: G and D connecteed all together: 1 inference run is tested.

------------

When the files are executed independently, everything is working nicely and fine. Tests are all fine.

Problems start occuring when I try to create one file to launch them all from one master file.

**master_test_launcher.py:**

```python
import unittest
import time
    
import tensorflow as tf
    
from tests.test_generator import GeneratorTest
from tests.test_discriminator import DiscriminatorTest
from tests.test_anovae_model import GAN_Train_Test
from tests.test_inference import GAN_Inference_Test
    
runner = unittest.TextTestRunner(verbosity=2)
    
if __name__ == '__main__':
   tf.logging.set_verbosity(tf.logging.DEBUG)
    
    for test in [GeneratorTest, DiscriminatorTest, GAN_Train_Test, GAN_Inference_Test]:
        tf.logging.debug(""Running tests for: %s ..."" % test.__str__())
    
        tf.reset_default_graph()
    
        time.sleep(2)
    
        test_suite = unittest.TestSuite()
        test_suite.addTest(unittest.makeSuite(test))
    
        runner.run(test_suite)
```

I repeatedly obtain the same error when I run the tests related to G and D connected together: 

```python
Exception: Layer 'encoder/input' already exists, please choice other 'name' or reuse this layer
Hint : Use different name for different 'Layer' (The name is used to control parameter sharing)
```

The error is quite simple to understand, each test file is independant and thus create its own session and graph. While testing only G or D, there is no problem because they have different name_scope/variable_scope. However, when testing the whole model *Layers* already have been defined by previous tests and thus leading to issue.

I would like to find a way to completely drop the graph and reset the whole TF state as **brand new and clean**. However, everything I try seem to  fail.

I would like to avoid creating a new graph for each test, leaving the old one in memory (could lead to very high amount of memory waste after a few tests).


So my question is easy: ** How can I reset the whole TF state and internal vars as ""clean"" as if you relaunch a new python shell ? By some black-magic I can't find any way doing it (after looking for it for hours).

For information here are the things I tried and which failed:
- tf.reset_default_graph()
- cleaning everything in graph collections
- creating a new graph + new session before executing each Test File: A graph is still built somewhere containing my Layers and I can't manage to find it.
- reading the TF code and trying to find any __exit__ or close function which I didn't find

Thanks a lot,

Jonathan D.",0,,1,2018-01-24T15:28:36Z,2018-01-25T07:03:15Z,CONTRIBUTOR,2018-01-25T07:03:13Z
16366,I have an issue with http://projector.tensorflow.org/ always getting stuck,,"Can anyone help me with this issue? I start up the _**Visualizing High-Dimensional Space**_ webstie and it loads until it says something about metadata and never does anything from there. Plese, help. I'm so confused?",0,,1,2018-01-24T14:35:43Z,2018-01-24T17:41:16Z,NONE,2018-01-24T17:41:16Z
16362,`tf.foldl` should have more robust input handling (like `tf.scan`),"stat:contributions welcome,type:feature","### System information
- Windows 10 x64
- Installed from binary
- TensorFlow 1.4.0 (Cpu version)
- Python 3.6.1

### Bug Description
`tf.foldl` (and `tf.foldr`) are conceptually very very similar to `tf.scan`. Therefore the implementations are also very similar. However, `tf.scan` accepts initializer lists or tuples with varying type arguments, while `tf.foldl` does not. I think this is a simple oversight, and it seems that cutting and pasting some code from `tf.scan` to `tf.foldl` fixes this problem. Specifically. the master `tf.foldl `code is (after removing the docstring):

```
def foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,
          swap_memory=False, name=None):
  if not callable(fn):
    raise TypeError(""fn must be callable."")

  with ops.name_scope(name, ""foldl"", [elems]):
    # Any get_variable calls in fn will cache the first call locally
    # and not issue repeated network I/O requests for each iteration.
    varscope = vs.get_variable_scope()
    varscope_caching_device_was_none = False
    if varscope.caching_device is None:
      # TODO(ebrevdo): Change to using colocate_with here and in other methods.
      varscope.set_caching_device(lambda op: op.device)
      varscope_caching_device_was_none = True

    # Convert elems to tensor array.
    elems = ops.convert_to_tensor(elems, name=""elems"")
    n = array_ops.shape(elems)[0]
    elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,
                                            dynamic_size=False,
                                            infer_shape=True)
    elems_ta = elems_ta.unstack(elems)

    if initializer is None:
      a = elems_ta.read(0)
      i = constant_op.constant(1)
    else:
      a = ops.convert_to_tensor(initializer)
      i = constant_op.constant(0)

    def compute(i, a):
      a = fn(a, elems_ta.read(i))
      return [i + 1, a]
    _, r_a = control_flow_ops.while_loop(
        lambda i, a: i < n, compute, [i, a],
        parallel_iterations=parallel_iterations,
        back_prop=back_prop,
        swap_memory=swap_memory)

    if varscope_caching_device_was_none:
      varscope.set_caching_device(None)
    return r_a

```

Modifying the code in the following manner seems to allow non-homgoenous initializer lists (tuples do not work for some reason). Note that you can toggle the mofidication with the ""useModifications"" flag:

```
def foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,
          swap_memory=False, name=None):
    if not callable(fn):
        raise TypeError(""fn must be callable."")

    with ops.name_scope(name, ""foldl"", [elems]):
        # Any get_variable calls in fn will cache the first call locally
        # and not issue repeated network I/O requests for each iteration.
        varscope = vs.get_variable_scope()
        varscope_caching_device_was_none = False
        if varscope.caching_device is None:
            # TODO(ebrevdo): Change to using colocate_with here and in other methods.
            varscope.set_caching_device(lambda op: op.device)
            varscope_caching_device_was_none = True

        # Convert elems to tensor array.
        elems = ops.convert_to_tensor(elems, name=""elems"")
        n = array_ops.shape(elems)[0]
        elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,
                                                dynamic_size=False,
                                                infer_shape=True)
        elems_ta = elems_ta.unstack(elems)

        if initializer is None:
            a = elems_ta.read(0)
            i = constant_op.constant(1)
        else:
            useModifications = True
            if useModifications:
                output_is_sequence = nest.is_sequence(initializer)
                output_flatten = lambda x: nest.flatten(x) if output_is_sequence else [x]
                initializer_flat = output_flatten(initializer)
                a = [ops.convert_to_tensor(init) for init in initializer_flat]
            else:
                a = ops.convert_to_tensor(initializer)

            i = constant_op.constant(0)

        def compute(i, a):
            a = fn(a, elems_ta.read(i))
            return [i + 1, a]

        _, r_a = control_flow_ops.while_loop(
            lambda i, a: i < n, compute, (i, a),
            parallel_iterations=parallel_iterations,
            back_prop=back_prop,
            swap_memory=swap_memory)

        if varscope_caching_device_was_none:
            varscope.set_caching_device(None)
        return r_a
```

Here is a MWE:

```
import tensorflow as tf

a = tf.constant( 1, dtype = tf.float32 )
b = tf.constant( 2, dtype = tf.int64   )

useTuple = False

def body( ab, i ):
    a = ab[0]
    b = ab[1]
    if useTuple:
        return (a,b)
    else:
        return [a,b]

N = 3
with tf.Session() as sess:
    if useTuple:
        ab = (a,b)
    else:
        ab = [a,b]
    print( ""new foldl :"", sess.run(   foldl(  body, tf.range(N), ab ) ) )  
    print( ""tf.scan   :"", sess.run( tf.scan(  body, tf.range(N), ab ) ) )
    print( ""tf.foldl  :"", sess.run( tf.foldl( body, tf.range(N), ab ) ) )
```

with useTuple = False, this returns 

```
new foldl : [1.0, 2]
tf.scan   : [array([1., 1., 1.], dtype=float32), array([2, 2, 2], dtype=int64)]
# Crash for tf.foldl with error: 
TypeError: Cannot convert a list containing a tensor of dtype <dtype: 'int64'> to <dtype: 'float32'> (Tensor is: <tf.Tensor 'Const_5:0' shape=() dtype=int64>)

```",0,,1,2018-01-24T12:58:43Z,2018-01-25T00:37:32Z,NONE,2018-01-25T00:37:32Z
16361,Fix typo,"awaiting testing (then merge),cla: yes",fix typo,1,,1,2018-01-24T12:25:33Z,2018-01-24T18:29:36Z,CONTRIBUTOR,2018-01-24T17:26:08Z
16360,"Python: Make an alias for ""tf.variable"" (with a lower ""v"") so the naming of it is consistent with ""tf.placeholder""/""tf.constant""",type:support,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**:  N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
Many developers learn the naming standards of the software so they can write code faster. It does not make any sense to have to things ""tf.placeholder"" and ""tf.Variable"" named using different schema. Constant, Placeholder and Variable are similar entities and can be used interchangeably. They should be named in same style even if tf.Variable is a class. 

https://www.tensorflow.org/api_docs/python/tf/placeholder
https://www.tensorflow.org/api_docs/python/tf/Variable
https://www.tensorflow.org/api_docs/python/tf/constant

### Source code / logs
N/A
",0,,1,2018-01-24T12:20:59Z,2018-01-25T01:01:53Z,NONE,2018-01-25T01:01:53Z
16359,Return type annotation,"awaiting testing (then merge),cla: yes","Added type annotations in the docstring to the return types of dataset functions.
Presented like this, they can automatically be read by tools (I have tested that this works in PyCharm) to improve auto-completion when coding.

This is really useful in case of datasets, because they often result in long chained calls (something like `Dataset.generate...(...).map(...).repeat(...).batch(...)`). With this patch, code completion works after every `.` (again, I have only tested PyCharm).",1,,1,2018-01-24T11:42:09Z,2018-01-24T19:58:28Z,CONTRIBUTOR,2018-01-24T17:25:35Z
16358,Request for updating keras/datasets files to r1.5,type:bug/performance,"### System information
- **executes Keras sample code imdb_fasttext.py https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py**:
- **Windows 7**:
- **TensorFlow installed from binary**:
- **TensorFlow version 1.5.0rc0**:
- **Python version 3.5.1**: 

### Describe the problem
Keras sample program does not work.
 There is a bug for numpy arange method wrong usage.
   (Need to fix from arrange to arange) 
This issue is already solved on master branch. (not in 1.5.0rc1)
Would you update these source codes?

### Source code / logs
Error messages are follows
===
C:\Users\sakaia\work\tensorflow\keras>python imdb_fasttext.py
Loading data...
Traceback (most recent call last):
  File ""imdb_fasttext.py"", line 75, in <module>
    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features
)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\keras\_imp
l\keras\datasets\imdb.py"", line 77, in load_data
    indices = np.arrange(len(x_train))
AttributeError: module 'numpy' has no attribute 'arrange'
===

Following are just checking np.arrange (not np.arange)
>git branch r1.5
>grep -rn np.arrange *
tensorflow/python/keras/_impl/keras/datasets/boston_housing.py:51:  indices = np.arrange(len(x))
tensorflow/python/keras/_impl/keras/datasets/reuters.py:76:  indices = np.arrange(len(xs))
tensorflow/python/keras/_impl/keras/datasets/imdb.py:77:  indices = np.arrange(len(x_train))
tensorflow/python/keras/_impl/keras/datasets/imdb.py:82:  indices = np.arrange(len(x_test))
>git branch -
>grep -rn np.arrange *
(This line is intentionally blank)",0,,5,2018-01-24T10:39:52Z,2018-01-27T06:18:20Z,NONE,2018-01-24T19:56:43Z
16357,Increase tolerance in `losses_impl_test.py`. fixes #16238,cla: no,,1,,2,2018-01-24T07:53:22Z,2018-01-31T15:24:35Z,CONTRIBUTOR,2018-01-24T17:26:57Z
16355,minor spelling tweaks for eager execution docs,"awaiting testing (then merge),cla: yes",,1,,1,2018-01-24T06:35:52Z,2018-01-24T18:45:52Z,CONTRIBUTOR,2018-01-24T17:24:04Z
16351,TfLiteCameraDemo failed to work with NNAPI after commit e6ff665dbe4888aa5fdff8f34c44405acca2ddd1,comp:lite,"I am testing NNAPI by forcing TfLiteCameraDemo to invoking libneuralnetworks.so. It worked correctly though slower. But since commit e6ff665dbe4888aa5fdff8f34c44405acca2ddd1, TfLiteCameraDemo crashes with error message like,

	01-24 03:39:36.393 19136 19153 E AndroidRuntime: FATAL EXCEPTION: CameraBackground
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: Process: com.example.android.tflitecamerademo, PID: 19136
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: java.lang.IllegalArgumentException: Failed to run on the given Interpreter: NNAPI was requested, but dependent sized tensors being used.
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:95)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:123)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.run(Interpreter.java:104)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:114)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:663)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment.access$900(Camera2BasicFragment.java:69)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment$5.run(Camera2BasicFragment.java:558)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Handler.handleCallback(Handler.java:790)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Handler.dispatchMessage(Handler.java:99)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Looper.loop(Looper.java:164)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.HandlerThread.run(HandlerThread.java:65)
	01-24 03:39:36.396   626   871 W ActivityManager:   Force finishing activity com.example.android.tflitecamerademo/.CameraActivity

Here is my patch

	 index e44c5ae..1ed88eb 100644
	 ---a/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java
	 +++b/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java
	 @@ -91,7 +91,7 @@ public class ImageClassifier {
		
	   /** Initializes an {@code ImageClassifier}. */
	   ImageClassifier(Activity activity) throws IOException {
	-    tflite = new Interpreter(loadModelFile(activity));
	+    tflite = new Interpreter(loadModelFile(activity), true);
	     labelList = loadLabelList(activity);
	     imgData =
	         ByteBuffer.allocateDirect(
	diff --git a/tensorflow/contrib/lite/java/demo/build.gradle b/tensorflow/contrib/lite/java/demo/build.gradle
	index dd883d6..9361c71 100644
	--- a/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java
	+++ b/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java
	@@ -66,6 +66,13 @@ public final class Interpreter implements AutoCloseable {
	     }
	     wrapper = new NativeInterpreterWrapper(modelFile.getAbsolutePath());
	   }
	+  public Interpreter(@NotNull File modelFile, boolean nn) {
	+    if (modelFile == null) {
	+      return;
	+    }
	+    wrapper = new NativeInterpreterWrapper(modelFile.getAbsolutePath());
	+    wrapper.setUseNNAPI(nn);
	+  }
	
	   /**
	    * Initializes a {@code Interpreter} with a {@code MappedByteBuffer} to the model file.
	@@ -76,6 +83,10 @@ public final class Interpreter implements AutoCloseable {
	   public Interpreter(@NotNull MappedByteBuffer mappedByteBuffer) {
	     wrapper = new NativeInterpreterWrapper(mappedByteBuffer);
	   }
	+  public Interpreter(@NotNull MappedByteBuffer mappedByteBuffer,  boolean nn) {
	+    wrapper = new NativeInterpreterWrapper(mappedByteBuffer);
	+    wrapper.setUseNNAPI(nn);
	+  }
	 
	   /**
	    * Runs model inference if the model takes only one input, and provides only one output.
	   /**
	    * Runs model inference if the model takes only one input, and provides only one output.
",0,,4,2018-01-24T03:55:51Z,2018-01-26T02:12:56Z,NONE,2018-01-24T12:14:24Z
16347,Golang API to serialize data into tf.Example protos(tfrecords),stat:awaiting response,"The Golang api [WriteContentsTo](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go#Tensor.WriteContentsTo) can be used to writes the serialized contents of a tensor to io.Writer, where the tensor is built from golang scalars, slices, and arrays. 

Yet there's not a Golang API to serialize data into tf.Example protos(tfrecords).

For example, when i want serialize a libsvm into tf.Example protos, i can do this by:
```
def libsvm2tfrecords(data_source, target_dir, delimiter='\t'):
    """"""
    a single file should not contain lines more than 1000,000, or we should use libsvm2proto_par
    :param data_source: libsvm file path
    :param target_dir: dir to storage the serialize proto file
    :param delimiter: delimiter for csv reader
    :return:
    """"""
    if not os.path.isfile(data_source):
        raise ValueError('data file passed do not exist or not a file')

    file_name = os.path.join(target_dir, os.path.splitext(
        os.path.basename(data_source))[0] + '.tfrecords')
    writer = tf.python_io.TFRecordWriter(file_name)
    start = datetime.now()
    line_c = 0
    with open(data_source, 'rb') as rf:
        f_reader = csv.reader(rf, delimiter=delimiter, quotechar='|')
        for row in f_reader:
            line_c += 1
            feature = dict()
            indexes = []
            values = []
            feature.update({'label': _float_feature([float(row[0])])})
            for e in row[1:]:
                index, value = e.split(':')
                indexes.append(int(index))
                values.append(float(value))
                feature.update({'index': _int64_feature(indexes)})
                feature.update({'value': _float_feature(values)})

            example = tf.train.Example(features=tf.train.Features(feature=feature))
            writer.write(example.SerializeToString())

        writer.close()
        end = datetime.now()

        print(""- consumed time: %ds for %s"" % ((end-start).seconds, data_source))
```
BUT Golang api does not seem to be able to achieve this. ",0,,2,2018-01-24T02:53:13Z,2018-01-25T06:51:38Z,NONE,2018-01-24T19:57:06Z
16341,"[Bazel/Windows] Don't use -Wl, -lpthread and -lm on Windows","awaiting testing (then merge),cla: yes",,1,,1,2018-01-24T00:48:57Z,2018-01-24T18:37:25Z,CONTRIBUTOR,2018-01-24T17:29:07Z
16339,Remove path_to_str from the public API,"awaiting testing (then merge),cla: yes",@martinwicke fyi,0,,3,2018-01-23T23:48:50Z,2018-01-24T17:20:46Z,OWNER,2018-01-24T17:20:42Z
16337,Fix a bug that capture_tpu_profile only takes absolute logdir path.,"awaiting testing (then merge),cla: yes",Also removed package dependancy on tensorflow for better compatibility.,1,,1,2018-01-23T21:23:02Z,2018-01-23T22:42:42Z,CONTRIBUTOR,2018-01-23T22:32:09Z
16336,Apply final cherry-picks for 1.5.0 release.,"awaiting review,cla: yes",,1,,5,2018-01-23T21:10:12Z,2018-01-24T18:47:16Z,MEMBER,2018-01-23T22:59:59Z
16332,Fixes #16314,"awaiting testing (then merge),cla: yes",Fixes #16314.,1,,3,2018-01-23T18:47:02Z,2018-01-23T19:50:27Z,CONTRIBUTOR,2018-01-23T19:26:44Z
16331,Compilation failure with gcc-6.4 (gcc-7.2 and clang-4) in ubuntu 17.10,stat:awaiting response,"When compiling master in ubuntu 17.10, compilation fails due to 'too perfect forwarding' in variant_op_registry
`
std::unordered_map<std::tuple<VariantXOp, StringPiece, StringPiece>, 
                     VariantXOpFn, TupleHash>
`
A workaround, by replacing tuple with a struct, provided in PR #16309 for your review.

Thanks,
Sami
",0,,2,2018-01-23T17:17:44Z,2018-01-24T16:40:37Z,CONTRIBUTOR,2018-01-24T13:08:45Z
16328,Gradient computation across multi-GPU,,"
------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7.6
- **CUDA/cuDNN version**: 8.0/6.0

I am trying to compute global mean and global variance for batch normalization layer across GPUs, both forward and backward should be considered. With `\sigma^2 = mean(x^2) - mean(x)^2`, the gradient w.r.t. each `x` can be computed independently in the GPU that `x` is attached to. 

However, when computing the gradients, I met a problem: without specifying GPU device, `tf.gradient` will use the `\gpu:0`. I cannot specify each operation of gradient computation because the gradients are computed automatically by the `optimizer` and only gradients of parameters are computed.

My question is that if a node is explicitly attached to a GPU device, why the gradient can not be attached to the same GPU device?

I tried this code and get two timeline files [timelines.zip](https://github.com/tensorflow/tensorflow/files/1649923/timelines.zip) and two snapshots bellow.

    import tensorflow as tf
    import numpy as np
    from tensorflow.python.client import timeline
    
    N_SAMPLES = 100000000
    
    
    def all_reduce(gpu_num):
        means = []
        x2s = []
        axs = []
        for i in range(gpu_num):
            with tf.device('/cpu:0'):
                x = tf.placeholder(dtype=tf.float32, shape=[N_SAMPLES], name='local_input_%d' % i)
            with tf.device('/gpu:%d'%i):
                ax = tf.multiply(10.0, x, name='local_multiply_%d'%i)
                mean = tf.reduce_mean(ax, name='local_mean_%d'%i)
                x2 = tf.square(ax, name='local_square_%d'%i)
                axs.append(ax)
                means.append(mean)
                x2s.append(x2)
    
        with tf.device('/gpu:0'):
            global_mean = tf.reduce_mean(means, name='global_mean')
            global_var = tf.subtract(tf.reduce_mean(x2s, name='global_x2'),
                                     tf.square(global_mean, name='global_mean_square'),
                                     name='global_sub')
            print global_var.get_shape()
    
        gs = []
        # manually
        # for i in range(gpu_num):
        #     with tf.device('/gpu:%d'%i):
        #         gradient_wrt_mean = tf.gradients(global_mean, axs[i])
        #         gradient_wrt_var = tf.gradients(global_var, axs[i])
        #         gs.append(gradient_wrt_mean)
        #         gs.append(gradient_wrt_var)
    
        # auto by tf
        gradient_wrt_mean = tf.gradients(global_mean, axs)
        gradient_wrt_var = tf.gradients(global_var, axs)
        gs.append(gradient_wrt_var)
        gs.append(gradient_wrt_mean)
    
        for n in tf.get_default_graph().as_graph_def().node:
            print [n.name, n.device]
    
        return global_mean, global_var, axs, gs
    
    
    def main(_):
        gpu_num = 2
        mean_op, var_op, xs, gs = all_reduce(gpu_num)
        x = np.random.randn(N_SAMPLES*gpu_num)
        print np.mean(x), np.var(x)
        feed_dict = dict()
        for i in range(gpu_num):
            feed_dict[xs[i]] = x[i*N_SAMPLES:(i+1)*N_SAMPLES]
    
        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
        run_metadata = tf.RunMetadata()
        gpu_options = tf.GPUOptions(allow_growth=False)
        config = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options)
        sess = tf.Session(config=config)
    
        # mean, var, g = sess.run([
        #     mean_op, var_op, gs
        # ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)
        # print mean, var
    
        g = sess.run([
            gs
        ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)
    
        # Create the Timeline object, and write it to a json
        tl = timeline.Timeline(run_metadata.step_stats)
        ctf = tl.generate_chrome_trace_format()
        with open('timeline.json', 'w') as f:
            f.write(ctf)
    
    
    if __name__ == '__main__':
        tf.app.run()

Two figures:
auto, without specifying GPU device.
![image](https://user-images.githubusercontent.com/13829174/35196526-76c1fa20-fed3-11e7-995f-6c63813acc83.png)

manually specifying GPU device.
![image](https://user-images.githubusercontent.com/13829174/35196537-93757eee-fed3-11e7-8df5-986a90a8c0f8.png)

If using `tf.gradient` without specifying GPU devices, only a `tf.reduce_mean` operation is done in `/gpu:1`. So is there some easy way that the operations of gradient computation can be assigned automatically to the corresponded GPU device?


",0,,2,2018-01-23T15:30:55Z,2018-01-24T14:15:40Z,NONE,2018-01-23T19:17:33Z
16327,iOS app does not output predictions using resnet50,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.1
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**: 0.9.0-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A 


### Steps I followed:

- I have trained resnet_v2_50 using slim.

- I created a script just to run inference, so the input image is a placeholder with name ""input_1"" and the output is the softmax with name ""softmax"". 

- I exported the .pb graph, then I ran `python python/tools/freeze_graph.py --input_graph=resnet_v2_50.pb --input_checkpoint=model.ckpt-1 --output_graph=frozen_resnet_v2_50.pb --input_binary=True --output_node_names=""softmax""` to freeze my graph using my checkpoint.

- I ran `bazel-bin/tensorflow/tools/graph_transforms/transform_graph  --inputs=input_1 --in_graph=frozen_resnet_v2_50.pb --outputs=softmax --out_graph=quantized_resnet_v2_50.pb --transforms='add_default_attributes strip_unused_nodes(type=float, shape=""1,180,180,3"") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) quantize_weights strip_unused_nodes sort_by_execution_order'`
 to quantize it. Here it's worth mentioning that when I add fold_batch_norms and fold_old_batch_norms at transforms I am getting this error: `E tensorflow/tools/graph_transforms/transform_graph.cc:210] Beta input to batch norm has bad shape: [64]`

- I imported the graph and also my labels file into `tensorflow/examples/ios/camera/data` and I ran the model on my iphone 5s (ios 10.3) using xcode 9.2. 

### Problem:
The app is running on my iphone but I am not getting any labels and probabilities while the `tensorflow/examples/label_image/label_image.py` script gives me results. If I use my model inside `tensorflow/examples/ios/simple` with iphone 8 simulator, the model is loaded but the predictions are empty. The result is the same if I use the frozen version before quantization. The result is also the same if I don't use slim.batch_norm as normalizer_fn in slim.conv2d and add fold_batch_norms and fold_old_batch_norms at transforms. Is there any bug? The message running on the simple example is the following (no predictions):
`I/Users/christos/tensorflow/tensorflow/examples/ios/my_simple/RunModelViewController.mm:246] Predictions: ` 

EDIT: The above process is working for a dummy small network that I made with the following nodes and ops:
input_1=>Placeholder
v/tower_0/trivial/Flatten/flatten/Reshape/shape/_0__cf__0=>Const
v/tower_0/trivial/Flatten/flatten/Reshape=>Reshape
v/trivial/fc3/weights_quantized_max=>Const
v/trivial/fc3/weights_quantized_min=>Const
v/trivial/fc3/weights_quantized_const=>Const
v/trivial/fc3/weights=>Dequantize
v/tower_0/trivial/fc3/MatMul=>MatMul
v/trivial/fc3/biases_quantized_max=>Const
v/trivial/fc3/biases_quantized_min=>Const
v/trivial/fc3/biases_quantized_const=>Const
v/trivial/fc3/biases=>Dequantize
v/tower_0/trivial/fc3/BiasAdd=>BiasAdd
v/tower_0/trivial/fc3/Relu=>Relu
v/trivial/fc4/weights_quantized_max=>Const
v/trivial/fc4/weights_quantized_min=>Const
v/trivial/fc4/weights_quantized_const=>Const
v/trivial/fc4/weights=>Dequantize
v/tower_0/trivial/fc4/MatMul=>MatMul
v/trivial/fc4/biases_quantized_max=>Const
v/trivial/fc4/biases_quantized_min=>Const
v/trivial/fc4/biases_quantized_const=>Const
v/trivial/fc4/biases=>Dequantize
v/tower_0/trivial/fc4/BiasAdd=>BiasAdd
softmax=>Softmax


You can see the node names and the operations of resnet 50 below (here the output node is v/tower_0/resnet_v2_50/predictions/Reshape_1):

input_1=>Placeholder
v/tower_0/Reshape/shape=>Const
v/tower_0/Reshape=>Reshape
v/tower_0/split/split_dim=>Const
v/tower_0/split=>Split
v/tower_0/sub/y=>Const
v/tower_0/sub=>Sub
v/tower_0/sub_1/y=>Const
v/tower_0/sub_1=>Sub
v/tower_0/sub_2/y=>Const
v/tower_0/sub_2=>Sub
v/tower_0/concat/axis=>Const
v/tower_0/concat=>ConcatV2
v/tower_0/Reshape_1/shape=>Const
v/tower_0/Reshape_1=>Reshape
v/tower_0/resnet_v2_50/Pad/paddings=>Const
v/tower_0/resnet_v2_50/Pad=>Pad
v/resnet_v2_50/conv1/weights_quantized_max=>Const
v/resnet_v2_50/conv1/weights_quantized_min=>Const
v/resnet_v2_50/conv1/weights_quantized_const=>Const
v/resnet_v2_50/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/conv1/Conv2D=>Conv2D
v/resnet_v2_50/conv1/biases=>Const
v/tower_0/resnet_v2_50/conv1/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/pool1/MaxPool=>MaxPool
v/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/biases=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/BiasAdd=>BiasAdd
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/add=>Add
v/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/add=>Add
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/shortcut/MaxPool=>MaxPool
v/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/Relu=>Relu
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/Pad/paddings=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/Pad=>Pad
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/add=>Add
v/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/biases=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/BiasAdd=>BiasAdd
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/add=>Add
v/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/add=>Add
v/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/add=>Add
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/shortcut/MaxPool=>MaxPool
v/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/Relu=>Relu
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/Pad/paddings=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/Pad=>Pad
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/add=>Add
v/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/BiasAdd=>BiasAdd
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/add=>Add
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/add=>Add
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/add=>Add
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/add=>Add
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/add=>Add
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/shortcut/MaxPool=>MaxPool
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/Relu=>Relu
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/Pad/paddings=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/Pad=>Pad
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/add=>Add
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/BiasAdd=>BiasAdd
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/add=>Add
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/add=>Add
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/add=>Add
v/resnet_v2_50/postnorm/gamma_quantized_max=>Const
v/resnet_v2_50/postnorm/gamma_quantized_min=>Const
v/resnet_v2_50/postnorm/gamma_quantized_const=>Const
v/resnet_v2_50/postnorm/gamma=>Dequantize
v/resnet_v2_50/postnorm/beta_quantized_max=>Const
v/resnet_v2_50/postnorm/beta_quantized_min=>Const
v/resnet_v2_50/postnorm/beta_quantized_const=>Const
v/resnet_v2_50/postnorm/beta=>Dequantize
v/tower_0/resnet_v2_50/postnorm/Const=>Const
v/tower_0/resnet_v2_50/postnorm/Const_1=>Const
v/tower_0/resnet_v2_50/postnorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/postnorm/Relu=>Relu
v/tower_0/resnet_v2_50/pool5/reduction_indices=>Const
v/tower_0/resnet_v2_50/pool5=>Mean
v/resnet_v2_50/logits/weights_quantized_max=>Const
v/resnet_v2_50/logits/weights_quantized_min=>Const
v/resnet_v2_50/logits/weights_quantized_const=>Const
v/resnet_v2_50/logits/weights=>Dequantize
v/tower_0/resnet_v2_50/logits/Conv2D=>Conv2D
v/resnet_v2_50/logits/biases_quantized_max=>Const
v/resnet_v2_50/logits/biases_quantized_min=>Const
v/resnet_v2_50/logits/biases_quantized_const=>Const
v/resnet_v2_50/logits/biases=>Dequantize
v/tower_0/resnet_v2_50/logits/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/SpatialSqueeze=>Squeeze
v/tower_0/resnet_v2_50/predictions/Reshape/shape=>Const
v/tower_0/resnet_v2_50/predictions/Reshape=>Reshape
v/tower_0/resnet_v2_50/predictions/Softmax=>Softmax
v/tower_0/resnet_v2_50/predictions/Shape=>Const
v/tower_0/resnet_v2_50/predictions/Reshape_1=>Reshape",0,,2,2018-01-23T15:26:45Z,2018-02-02T14:33:36Z,NONE,2018-01-25T07:13:12Z
16326,Making string values in constant,"awaiting testing (then merge),cla: yes",We already have a `constant.py` in session_bundle since adding these string values as a constant.,1,,3,2018-01-23T15:10:05Z,2018-01-24T19:24:31Z,CONTRIBUTOR,2018-01-24T17:45:49Z
16324,Using math_ops instead of defining separate mulop function,"awaiting review,cla: yes",,1,,1,2018-01-23T14:31:41Z,2018-01-23T18:10:16Z,CONTRIBUTOR,2018-01-23T18:10:31Z
16321,tpu contrib fix,"awaiting testing (then merge),cla: yes",fix the issue in https://github.com/tensorflow/tensorflow/issues/16262,1,,3,2018-01-23T10:33:16Z,2018-01-23T18:51:28Z,CONTRIBUTOR,2018-01-23T10:37:03Z
16318,import tensorflow as tf,cla: yes,These five files do not explicitly `import tensorflow as tf` yet they use they use __tf.__ methods or functions which drives linters like pylint and flake8 crazy unless special directives are put in place.,1,,4,2018-01-23T09:51:42Z,2018-01-26T00:31:42Z,CONTRIBUTOR,2018-01-25T17:11:48Z
16316,Lack of clarity in tf.while_loop documentation,type:docs,"I believe that the documentation for tf.while_loop is lacking usage clarity, and actually provides contradictory statements. 

Specifically, it seems that many people are using the tf.while_loop as a ""for loop"" ([see stackoverflow](https://stackoverflow.com/questions/35330117/how-can-i-run-a-loop-with-a-tensor-as-its-range-in-tensorflow)). However, the [tf.while_loop](https://www.tensorflow.org/versions/r0.12/api_docs/python/control_flow_ops/control_flow_operations#while_loop) docs state:

> For correct programs, while_loop should return the same result for any parallel_iterations > 0.

A loop counter inside of the ""while loop"" body, seems to violate this constraint despite the fact that this is given as an example usage in the docs:

> python i = tf.constant(0) c = lambda i: tf.less(i, 10) b = lambda i: tf.add(i, 1) r = tf.while_loop(c, b, [i])

So it seems that there are two bad outcomes here:

1. If this is indeed the canonical way of creating a ""for loop"", then the example explicitly creates a dependency between iterations, meaning that the ""while loop"" iterations cannot be run in parallel. 

1. The example is incorrect? 

It seems like the while_loop docs should have an example which better illustrates how to use it as a ""for loop"", if such usage is indeed intended, or a warning on the implications of the provided example.  
",0,,3,2018-01-23T08:07:11Z,2018-01-27T00:00:02Z,NONE,2018-01-24T01:26:10Z
16315,Remove Variables from a TF Server (e.g.),,"I have a cluster of long-lived TensorFlow servers  (//tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server).
My problem is how to reset variables on these server. 

There is a behavior in distributed TensorFlow in which a variable defined on a worker (e.g. PS) outlives the session which defines it. I understand this behavior is intentional to support between graph model-replica.

However, In my use case this behavior causes unexpected problem. I have not found a mechanism to override this. It there is, I believe it is helpful to better reflect it in the documentation, if there is not, I hope I can make a case to motivate its existence.

In my use case different training jobs are ran _sequentially_ (i.e. one training job at a time) on this cluster, each using one client (which connects to only one master). 

The problem I have is if a variable is defined in two training job with a same name but different shape sizes, the latter client gets the following error on ""Session"" creation:
```
InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [100] rhs shape= [200]%0A%09 [[Node: a/Assign = Assign[T=DT_FLOAT, _class=[""loc:@a""], use_locking=true, validate_shape=true, _device=""/job:worker/replica:0/task:0/device:GPU:0""](a, a/Initializer/random_uniform)]]
```

`tf.reset_default_graph` does not help. The solution to this problem could be a mechanism similar `tf.reset_default_graph` that resets variables in all the workers.

To replicate this problem let say we have two workers: (one PS, and on Worker)

Worker 1:
```bash
#worker 1
./bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server --cluster_spec=""ps|localhost:2222,worker|localhost:2223"" --job_name=worker --task_id=0 &
#worker 2
./bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server --cluster_spec=""ps|localhost:2222,worker|localhost:2223"" --job_name=worker --task_id=0
```
(Same result with `tf.train.Server` workers.)

Then run the simple code:
```python
import tensorflow as tf
var = tf.get_variable(""A"", shape=(100,))
with tf.train.MonitoredTrainingSession(master=""localhost:2223"") as sess:
   pass
```
It should work just fine.
Then when this code (which is identical except the variable shape) is ran:
```python
import tensorflow as tf
var = tf.get_variable(""A"", shape=(5000,))
with tf.train.MonitoredTrainingSession(master=""localhost:2223"") as sess:
   pass
```
This example fails.
",0,,2,2018-01-23T07:33:05Z,2018-01-23T21:29:30Z,CONTRIBUTOR,2018-01-23T07:33:40Z
16314,Published libtensorflow_framework.so binaries ABI Problem,,"The distributed `libtensorflow_framework.so` included in the JAR files published to Maven are built using the C++ 11 ABI, in contrast to the main TF build. I think that affects all continuous integration builds of the shared objects. I believe the `-D_GLIBCXX_USE_CXX11_ABI=0` compiler flag should be used for the CI builds as is done for the main build. An example of its use is shown in commit 550df413158b32645ca5df4dcaabc67f1a48964d. This causes some trouble when using these shared objects and developing custom ops, as those are required to be built using that compiler flag. It would be great if the use of the flag was consistent and all binaries were built using the same ABI.

Thanks! ",0,,1,2018-01-23T06:57:28Z,2018-01-23T19:50:27Z,CONTRIBUTOR,2018-01-23T18:47:30Z
16313,Bug of tf.data.TFRecordDataset? Couldn't use tf.reshape after the operations of tf.data.TFRecordDataset,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: Nvidia GeForce GTX TITAN X 12GB
- **Exact command to reproduce**:


### Describe the problem
I want to use  function **tf.profiler.ProfileOptionBuilder.float_operation** to show the flops of the model. But it need a certain input shape while the the output shape of **tf.data.TFrecordDataset** is like (?, 32,32,3). When I want to use tf.reshape to reshape the output of **tf.data.TFrecordDataset**, it generates an error ""Input to reshape is a tensor with 64512 values, but the requested shape has 98304"".  

### Source code 

  def dataset_input(self, dataset_type):
    with tf.variable_scope(""batch_"" + dataset_type):
        def parser(record):
            features = tf.parse_single_example(
                record,
                features={
                    'image': tf.FixedLenFeature([], tf.string),
                    'label': tf.FixedLenFeature([], tf.int64)
                })
            image, label = features['image'], features['label']
            height, width, channels = self.input_size, self.input_size, self.input_dim
            image = tf.decode_raw(image, tf.uint8)
            image = tf.reshape(image, [height, width, channels])
            return image, label
        dataset = tf.data.TFRecordDataset([self.dataset_dir[dataset_type]])
        dataset = dataset.map(parser)
        dataset = dataset.shuffle(buffer_size=50000)
        dataset = dataset.batch(self.batch_size)
        dataset = dataset.repeat()
        iterator = dataset.make_one_shot_iterator()
        features, labels = iterator.get_next()
        features = tf.reshape(features, [self.batch_size, self.input_size, self.input_size, self.input_dim])
        return features, labels
  ",0,,4,2018-01-23T06:26:49Z,2018-01-23T22:45:02Z,NONE,2018-01-23T22:45:02Z
16312,Allow step callback for scipy SLSQP,"awaiting testing (then merge),cla: yes,stat:awaiting response",This simple fix allows `SLSQP` method of scipy optimizer to use step callback as reported in issue [#16294](https://github.com/tensorflow/tensorflow/issues/16294). ,1,,2,2018-01-23T03:56:13Z,2018-01-29T21:36:18Z,CONTRIBUTOR,2018-01-28T20:23:04Z
16309,Workaround 'too perfect forwarding' issue in variant_op_registry,"awaiting testing (then merge),cla: yes","In variant_op_registry.h 
```
std::unordered_map<std::tuple<VariantXOp, StringPiece, StringPiece>, 
                     VariantXOpFn, TupleHash>
```
seems to be falling victim to 'too perfect forwarding' issue ( [SO link](https://stackoverflow.com/questions/44475317/variadic-template-issue), [Andrzej's blog](https://akrzemi1.wordpress.com/2013/10/10/too-perfect-forwarding/)) with gcc-6.4, gcc-7.2 and clang-4 in ubuntu-17.10 (and possibly others). This PR works around the issue by replacing std::tuple with a simple struct.",1,,3,2018-01-23T03:08:35Z,2018-01-23T23:13:15Z,CONTRIBUTOR,2018-01-23T17:46:51Z
16307,Fix Conv3DTranspose in tf.keras,"awaiting testing (then merge),cla: yes",,1,,6,2018-01-23T02:53:57Z,2018-01-25T17:01:38Z,CONTRIBUTOR,2018-01-23T02:59:05Z
16303,Don't load libcupti.so from regular path on Android,"awaiting testing (then merge),cla: yes,kokoro:run","Open to alternatives, but as other methods in this file work similarly this doesn't seem too bad.",1,,11,2018-01-23T00:52:20Z,2018-01-25T19:56:23Z,MEMBER,2018-01-23T21:44:33Z
16298,Bug of tf.data.TFRecordDataset? or my codes wrong?,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Windows 10
- **TensorFlow installed from (source or binary)**:  binary
- **TensorFlow version (use command below)**:  1.4
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: Nvidia Quadro K4000
- **Exact command to reproduce**: 


I tested to write dynamic numbers of variables into tfrecord. But when I use the tf.data.TFRecordDataset to read VarLenFeature, the program crashes. However, if I do not use dataset, but just tf.python_io.tf_record_iterator. The program works without problem. I wonder whether this is a bug of tf.data.TFRecordDataset, or there is something wrong in my codes?

My writing codes are 

    def test_write():
      writer = tf.python_io.TFRecordWriter('test.tfrecord')

      for i in range(3):
        val_list = []
        for j in range(i+1):
          val_list.append(i+j)
        feature_dict = {
          'val': tf.train.Feature(int64_list=tf.train.Int64List(value=val_list)),
        }
    
        example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
        writer.write(example.SerializeToString())

      writer.close()

The reading codes using tf.data.TFRecordDataset and causing error are

	def parse_test(example):
	  features = {
		'val': tf.VarLenFeature(dtype=tf.int64)
	  }
	  parsed_features = tf.parse_single_example(example, features)

	  return parsed_features

	def test_read():
	  dataset = tf.data.TFRecordDataset(['test.tfrecord'])
	  dataset = dataset.map(parse_test)
	  dataset = dataset.batch(1)

	  iterator = dataset.make_one_shot_iterator()
	  feature_dict =  iterator.get_next()

	  with tf.Session() as sess:
		for _ in range(3):
		  curr_dict = sess.run(feature_dict)
		  print([curr_dict['val']])

The error message is:

	TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(""ParseSingleExample/Slice_Indices_val:0"", shape=(?, 1), dtype=int64), values=Tensor(""ParseSingleExample/ParseExample/ParseExample:1"", shape=(?,), dtype=int64), dense_shape=Tensor(""ParseSingleExample/Squeeze_Shape_val:0"", shape=(1,), dtype=int64)). Consider casting elements to a supported type.


 The successful reading codes without using tf.data.TFRecordDataset are as below

	def test_read2():
	  with tf.Session() as sess:
		for serialized_example in tf.python_io.tf_record_iterator('test.tfrecord'):
		  features = tf.parse_single_example(serialized_example,
			features={
			  'val': tf.VarLenFeature(dtype=tf.int64),
			}
		  )

		  temp = features['val']

		  values = sess.run(temp)
		  print(values)

This code successfully print out

	SparseTensorValue(indices=array([[0]], dtype=int64), values=array([0], dtype=int64), dense_shape=array([1], dtype=int64))
	SparseTensorValue(indices=array([[0],
		   [1]], dtype=int64), values=array([1, 2], dtype=int64), dense_shape=array([2], dtype=int64))
	SparseTensorValue(indices=array([[0],
		   [1],
		   [2]], dtype=int64), values=array([2, 3, 4], dtype=int64), dense_shape=array([3], dtype=int64))

However, I am still hoping to use the dataset structure to deal with the VarLenFeature. Is there anything wrong with my reading codes or there is a bug in tf.data.TFRecordDataset? Thank you.

",0,,1,2018-01-22T19:48:46Z,2018-01-23T22:48:02Z,NONE,2018-01-23T22:48:02Z
16294,ScipyOptimizer SLSQP supporting callback,"stat:contributions welcome,type:bug/performance","The callback is deprecated when `SLSQP` method in scipy optimizer is selected (see [here](https://github.com/tensorflow/tensorflow/blob/04b5c75aae4bdbdac7c713714a369f9b360daf70/tensorflow/contrib/opt/python/training/external_optimizer.py#L400)). Actually, `SLSQP` does support callback, so 
```python 
if method == 'SLSQP':
  # SLSQP doesn't support step callbacks. Obviate associated warning
  # message.
  del minimize_kwargs['callback']
```
in the above linked file could be removed. 

The following example shows that `SLSQP` do support callback. 

```python 
from scipy.optimize import minimize, rosen, rosen_der

def callback(xk, step=[0]):
  print step[0], xk[0]
  step[0] += 1
  
x0 = [1.3, 0.7, 0.8, 1.9, 1.2]
res = minimize(rosen, x0, callback=callback, method='SLSQP',
    options={'ftol': 1e-6, 'disp': True})
 
print res.x[0]
```






",0,,3,2018-01-22T18:49:22Z,2018-01-23T05:37:22Z,CONTRIBUTOR,2018-01-23T02:15:44Z
16288,Tensorflow works in command prompt but not in Spyder,type:build/install,"Hello.
I'm new to Python so maybe I've missed something but anyway, here is my problem.
I've installed tensorflow in Anaconda prompt by using 
```
C:\WINDOWS\system32> conda create -n tensorflow python=3.6
C:\WINDOWS\system32> activate tensorflow
(tensorflow)C:\WINDOWS\system32> pip install --ignore-installed --upgrade tensorflow
```
Instalation was succesful, then I opened python and tried to import tensorflow to verify installation
```
(base) C:\WINDOWS\system32>python
Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow'
```

After activating tensorflow, it works.
```
(tensorflow) C:\WINDOWS\system32>python
Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
>>>
```
But I cannot find a way to make it work in Spyder IDE. I always get error:
```ModuleNotFoundError: No module named 'tensorflow'```",0,,5,2018-01-22T14:47:27Z,2018-01-25T19:40:16Z,NONE,2018-01-22T17:36:48Z
16287,[Bug] LuongMonotonicAttention in contrib/seq2seq/python/ops/attention_wrapper.py,stat:awaiting tensorflower,"`LuongMonotonicAttention.__init__(...)` calls its parent `_BaseAttentionMechanism` with `query_layer` as follows:
```
        query_layer=layers_core.Dense(
            num_units, name=""query_layer"", use_bias=False),
```
But, it doesn't apply it on query in `LuongMonotonicAttention.__call__(...)`.
```
  def __call__(self, query, previous_alignments):
    """"""...
    """"""
    with variable_scope.variable_scope(None, ""luong_monotonic_attention"",
                                       [query]):
      score = _luong_score(query, self._keys, self._scale)
      score_bias = variable_scope.get_variable(
          ""attention_score_bias"", dtype=query.dtype,
          initializer=self._score_bias_init)
      score += score_bias
    alignments = self._probability_fn(score, previous_alignments)
    return alignments
```
Guessing from the way `LuongAttention` works, there should be `query_layer=None` in `LuongMonotonicAttention.__init__(...)`.",0,,3,2018-01-22T13:50:55Z,2018-02-01T03:00:34Z,NONE,2018-01-26T22:10:26Z
16286,Removes redundant variable assignment,"awaiting testing (then merge),cla: yes","Addresses alert raised by lgtm.com:
https://lgtm.com/projects/g/tensorflow/tensorflow/snapshot/e6183fbeecf069148371be83988e8e5db2b14185/files/tensorflow/python/framework/constant_op.py#xb77a2f6647d782be:1

It doesn't seem like assigning `attr_tshape = attr_tshape` does anything, so there's no need to keep it in.",1,,5,2018-01-22T13:26:57Z,2018-01-22T20:55:02Z,CONTRIBUTOR,2018-01-22T13:28:34Z
16281,"After Working with Tensorflow cpu version for 2 days, it gave me an error on installation today","stat:awaiting response,type:build/install","I had installed and used Tensorflow successfully but today when I opened my computer it gave me this error message:
Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/asus/PycharmProjects/untitled3/CNNCIFARTFNEW.py"", line 2, in <module>
    import tensorflow as tf
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.


Please help. This is urgent.",0,,2,2018-01-22T07:38:03Z,2018-01-23T04:58:10Z,NONE,2018-01-22T19:38:09Z
16280,Repair compilation error of tensorflow built with MKL-DNN,"cla: yes,stat:awaiting response","When we compile tensorflow with Intel **MKL-DNN**, it will meet a failure:

`bazel build --copt -O3 --copt=-DINTEL_MKL_DNN --config=mkl -c opt //tensorflow/tools/pip_package:build_pip_package`

**error: 'mkldnn::algorithm' is not a namespace
  using mkldnn::algorithm::lrn_across_channels;**

 Removing the 'algorithm' field in _tensorflow/core/kernels/mkl_lrn_op.cc_ can solve this problem and lead to successful compilation.",1,,7,2018-01-22T07:27:10Z,2018-01-23T07:02:18Z,CONTRIBUTOR,2018-01-22T07:56:56Z
16278,__init__() got multiple values for argument 'strides',type:support,"    model = Sequential()
    model.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height, 3)))
    print(model.output_shape)
    model.add(Convolution2D(64, 3, 3, strides=(
        1, 1), activation='relu', name='conv1_1'))
above is my code, I got error:
__init__() got multiple values for argument 'strides'
If i don't use 'strides', it's fine. but the stride is 3. How should I set strides?",0,,2,2018-01-22T06:57:46Z,2018-01-25T01:13:49Z,NONE,2018-01-25T01:13:49Z
16277,Control dependency does not ensure write observed by read,,"TF version 1.3.0

```python
def sleep(t):
    '''TF sleep'''
    import time
    def f(t):
        time.sleep(t)
        return np.array([], dtype=np.float32)
    return tf.py_func(f, [t], [tf.float32])[0]

with tf.device('gpu'):
    x = tf.Variable(0.)
with tf.control_dependencies([tf.identity(sleep(0.1))]):
    with tf.device('gpu'):
        mod = tf.assign(x, 100.)
with tf.device('cpu'):
    a = x+1.
    with tf.control_dependencies([tf.identity(mod)]):
        b = x+2.
        with tf.device('gpu'):
            c = x+3.

x.initializer.run()
sess.run([a, b, c])
# [1.0, 2.0, 103.0]
```

When a variable is read on another device, TF seems to copy once regardless of dependencies. I understand this is how TF works, but I think it would be nice to have dependencies ensure memory access order.",0,,2,2018-01-22T06:22:40Z,2018-01-26T21:40:10Z,CONTRIBUTOR,2018-01-23T19:21:05Z
16274,R1.4,cla: no,,0,,2,2018-01-22T01:06:46Z,2018-01-23T01:02:40Z,NONE,2018-01-23T01:03:04Z
16272,Add a rnn example on mnist dataset using tf library,"awaiting review,cla: yes,stat:awaiting tensorflower","A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.
This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)
Links:
    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)
    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).

Training and evaluation log:
Extracting /tmp/mnist_data/train-images-idx3-ubyte.gz
Extracting /tmp/mnist_data/train-labels-idx1-ubyte.gz
Extracting /tmp/mnist_data/t10k-images-idx3-ubyte.gz
Extracting /tmp/mnist_data/t10k-labels-idx1-ubyte.gz
Step 1, Minibatch Loss= 2.5586, Training Accuracy= 0.258
Step 200, Minibatch Loss= 0.2419, Training Accuracy= 0.930
Step 400, Minibatch Loss= 0.1863, Training Accuracy= 0.938
Step 600, Minibatch Loss= 0.1000, Training Accuracy= 0.969
Step 800, Minibatch Loss= 0.0935, Training Accuracy= 0.977
Step 1000, Minibatch Loss= 0.0773, Training Accuracy= 0.969
Step 1200, Minibatch Loss= 0.0500, Training Accuracy= 0.984
Step 1400, Minibatch Loss= 0.0550, Training Accuracy= 0.977
Step 1600, Minibatch Loss= 0.0615, Training Accuracy= 0.984
Step 1800, Minibatch Loss= 0.0635, Training Accuracy= 0.977
Step 2000, Minibatch Loss= 0.0550, Training Accuracy= 0.992
Training Finished!
Testing Accuracy: 0.992188",1,,8,2018-01-21T18:23:28Z,2018-01-28T01:41:04Z,CONTRIBUTOR,2018-01-22T18:23:04Z
16271,"tf.pow(x, y) edge case with negative x (Bug)",type:support,"I am using tf.pow for my project, but my losses are 'nan', so I setup the test cases as shown below.
I found that whenever x is negative, tf.pow seems to output nan instead of the correct answer.

>>> r = tf.pow(0.4,0.4)
>>> r2 = tf.pow(-0.4,-0.4)
>>> r3 = tf.pow(0.4,-0.4)
>>> r4 = tf.pow(-0.4,0.4)
>>> sess.run(r)
0.69314486
>>> sess.run(r2)
nan
>>> sess.run(r3)
1.4426999
>>> sess.run(r4)
nan

I appreciate for anyone of the community who can address this issue.

Respectfully,",0,,3,2018-01-21T18:03:16Z,2018-01-22T03:45:00Z,NONE,2018-01-22T03:44:57Z
16269,ContentTooShortError: <urlopen error retrieval incomplete: got only 246506328 out of 247336696 bytes>,,"I have started UDACITY deep learning course.
I was copying the assignment 1 codes then I got the this error during downloading the notMNIST_large.tar.gz
Here are some screenshots of the code.
I am doing this on jupyter python3 notebook in ubuntu.

![1](https://user-images.githubusercontent.com/25321783/35194207-ca56cdc6-fed5-11e7-9fe7-3232c3741689.png)
![2](https://user-images.githubusercontent.com/25321783/35194208-cacbbc6c-fed5-11e7-9ff0-600527e4990a.png)
![error](https://user-images.githubusercontent.com/25321783/35194209-cb3fbd6a-fed5-11e7-8162-9b999b6240bb.png)
",0,,1,2018-01-21T12:36:47Z,2018-01-22T04:12:24Z,NONE,2018-01-22T04:12:24Z
16265,ImportError: cannot import name tf,"stat:awaiting response,type:support","### System information
- **Os version**: Linux Ubuntu 14.04
- **TensorFlow installed from**: binary (sudo pip  install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.1-cp27-none-linux_x86_64.whl
)
- **TensorFlow version** :1.4.1
- **Python version**: 2.7

### Describe the problem
After Install when i run the following command it throws error;
`from tensorflow import tf`

It throws
`Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name tf
`
",0,,2,2018-01-21T06:37:52Z,2018-01-22T03:52:51Z,NONE,2018-01-22T03:46:25Z
16262,"Cannot opened include file ""tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h"": no such file or directory",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: source 
- **TensorFlow version (use command below)**: current git master branch, should be v1.4.1 or v1.5.0rc1?
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: MSVC2015
- **CUDA/cuDNN version**: CPU build only, gpu function is off
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Trying a minimal build with cmake, with only snappy support and optimize for native arch turned on

### Describe the problem
Build failing due to missing header files ""tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h"".

Everything build succesful except for tpu project. I don't really know how to generate the pb.h file from protoc manually. I trying to fix the problem by chaning .cmake files, but not sure which one is for tpu.

### Source code / logs
133>D:\MSVC-source\tensorflow\tensorflow\contrib\tpu\ops\tpu_embedding_ops.cc(16): fatal error C1083: Cannot open include file: 'tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h': No such file or directory

",0,,6,2018-01-21T02:28:38Z,2018-01-23T21:28:01Z,CONTRIBUTOR,2018-01-22T00:27:42Z
16255,tf.scatter_update Error ,,"Hi, 

I use tf.scatter_update to update non-trainable variables AS and AO in a code. As I found, when one uses scatter_update, the gradient misses, so that there is no gradient. 
Because of that I set both AL and AO as non-trainable variables (actually they are non-trainable), called the optimizer: tf.train.AdamOptimizer(config.actor_lr0,0.9,0.999,1e-8).minimize(actor_loss), and I thought everything should be fine. 
However, I am getting error:
LookupError: No gradient defined for operation 'actor/encoder/beer_game_flow_8/next_scat_j_2' (op type: ScatterUpdate). 
Here are the lines of the code that I update AO and AS that gives the error:

self.players[k-1].AS = tf.scatter_update(self.players[k-1].AS, 
                    self.curTime + leadTimeIn, 
                    tf.add(self.players[k-1].AS[self.curTime + leadTimeIn], possible_shipment), name='next_scat_j' )                   

self.players[k+1].AO = tf.scatter_update(self.players[k+1].AO, 
                    self.curTime + leadTime, tf.add(self.players[k+1].AO[self.curTime + leadTime], 
                    self.players[k].actionValue(self.curTime, self.playType))
                    , name='handle_scat_j')

Since both AS and AO are non-trainable, I do not need their gradient, and AS and AO are the only variable in this op. So, I was wondering why TensorFlow want to obtain the gradient, since there is no trainable variable here? 
Is it something that you can fix it, or is there any reason behind this behavior? 

BTW, I use python 2.7 with tf 1.4.0 on Debian 8.7 with a K80 with 12GB of memory. 

Thanks, 
Afshin
",0,,3,2018-01-19T23:39:34Z,2018-01-26T18:31:42Z,NONE,2018-01-24T13:08:14Z
16254,adding placeholder_with_default in order to feed both via dataset and placeholders produces error on GPU,,"
Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:1.4.0
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:9.1.85
- **GPU model and memory**:Titan V, 12G
- **Exact command to reproduce**: See below

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The issue is when you want to train a classifier that takes both placeholder and dataset via queue to feed input. The reason one may want to do that is to run inference via placeholders. 

I added the following line under define the model section of train_image_classifier.py of slim library

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


_images = tf.placeholder_with_default(image, shape=[...], name='input)
 I get an error of the following kind when running on GPU:
Cannot assign a device for operation 'input': Could not satisfy explicit device specification '/device:GPU:0' 


",1,,5,2018-01-19T23:37:05Z,2018-01-22T00:02:36Z,NONE,2018-01-22T00:02:36Z
16249,Branch 182554969,cla: yes,,0,,1,2018-01-19T19:00:38Z,2018-01-19T19:27:04Z,MEMBER,2018-01-19T19:01:11Z
16246,Failed to build error: mismatched argument pack lenghts...,,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 4.14.13-1-ARCH
- **TensorFlow installed from (source or binary)**: git
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9.0-1
- **GCC/Compiler version (if compiling from source)**: 6.4.1
- **CUDA/cuDNN version**:  9.1.85-1 / 7.0.5-2
- **Exact command to reproduce**:
./configure
bazel build --config=opt --config=cuda --jobs 12 //tensorflow/tools/pip_package:build_pip_package


### Describe the problem
failed to build

### Source code / logs

> /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'
>        return __and_<is_convertible<_UElements&&, _Elements>...>::value;
>                                                                  ^~~~~
> /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>}; bool <anonymous> = true; _Elements = {tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece}]' not a return-statement
>      }
>  ^
> ERROR: /home/user/dev/git/tensorflow/tensorflow/core/kernels/BUILD:1884:1: output 'tensorflow/core/kernels/_objs/list_kernels_gpu/tensorflow/core/kernels/list_kernels.cu.pic.o' was not created
> ERROR: /home/user/dev/git/tensorflow/tensorflow/core/kernels/BUILD:1884:1: not all outputs were created or valid
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 29.727s, Critical Path: 28.35s
> FAILED: Build did NOT complete successfully
> ",1,,9,2018-01-19T17:10:53Z,2018-01-30T14:32:42Z,NONE,2018-01-30T18:18:56Z
16245,Branch 182511847,cla: yes,,0,,2,2018-01-19T16:35:22Z,2018-01-19T18:50:20Z,MEMBER,2018-01-19T17:55:52Z
16244,Benchmarking GPU ops in Tensorflow Graphs,,"I tried using the tool for benchmarking Tensorflow Graphs at
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark

Seems it only gives the memory profile for RAM per ops. How can I get the GPU memory and utilization profile per ops using this tool?

",0,,1,2018-01-19T16:03:21Z,2018-01-19T21:20:25Z,NONE,2018-01-19T21:20:25Z
16240,graph_metrics.py does not work well,,"I want to use function in graph_metrics.py, I run corresponding test file graph_metrics_test.py, but I get the following assertion error for ""weight_parameters"" metric
```

line 32, in testGraphMetrics
    self.assertEqual(expected[statistic_type], current_stats.value)
AssertionError: 100 != None
```
has anyone encounter this problem?",0,,2,2018-01-19T12:35:24Z,2018-01-20T01:50:26Z,NONE,2018-01-19T21:11:21Z
16238,//tensorflow/contrib/gan:losses_impl_test fails with AssertionError ,type:bug/performance,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04  s390x
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: No GPU
- **GPU model and memory**: NA
- **Exact command to reproduce**: bazel test -c opt //tensorflow/contrib/gan:losses_impl_test

### Describe the problem
One of the sub-test `test_stable_global_norm_unchanged` fails on s390x with 
`AssertionError: 110.709068 != 110.709084 +/- 0.000010`

Seems like a minor difference, so I tried changing the tolerance slightly as below:
```
-        self.assertNear(gnorm_np, precond_gnorm_np, 1e-5)
+        self.assertNear(gnorm_np, precond_gnorm_np, 2e-5)
```
with this the test is passing.

Is it ok to create a PR with this change? Could you please share your thoughts on this.

### Source code / logs
```
.......................F..................................................................................
======================================================================
FAIL: test_stable_global_norm_unchanged (__main__.CombineAdversarialLossTest)
Test that preconditioning doesn't change global norm value.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/gan/losses_impl_test.runfiles/org_tensorflow/tensorflow/contrib/gan/python/losses/python/losses_impl_test.py"", line 602, in test_stable_global_norm_unchanged
    self.assertNear(gnorm_np, precond_gnorm_np, 1e-5)
  File ""/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/gan/losses_impl_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 879, in assertNear
    if msg is not None else """"))
AssertionError: 110.709068 != 110.709084 +/- 0.000010

----------------------------------------------------------------------
Ran 106 tests in 9.119s

FAILED (failures=1)
```

",2,,2,2018-01-19T10:25:54Z,2018-01-26T05:31:42Z,CONTRIBUTOR,2018-01-19T21:00:09Z
16237,support preconditioner for `conjugated_gradient()` in `linear_equations.py`,"awaiting testing (then merge),cla: yes","1. support preconditioner for `conjugated_gradient()` in `tensorflow\tensorflow\contrib\solvers\python\ops\linear_equations.py`
2. add identity_operator() in `util.py` as default preconditioner
3. edit unit test files(`util_test.py`, `linear_equations_test.py`) to validate preconditioner",1,,4,2018-01-19T07:12:01Z,2018-01-24T18:39:00Z,CONTRIBUTOR,2018-01-19T07:13:03Z
16236,Branch 182474037,cla: yes,,0,,3,2018-01-19T06:47:58Z,2018-01-19T09:01:12Z,MEMBER,2018-01-19T07:29:26Z
16235,Feature Request: Make NDLSTM use state_is_tuple=True,type:feature,"I have successfully used [NDLSTM](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ndlstm) (specifically lstm2d.separable_lstm) in my own project but whenever I use it, I enocunter this warning: `""Using a concatenated state is slower and will soon be deprecated. use state_is_tuple=true.""`

The warning is caused by  `ndlstm_base_dynamic` in lstm1d.py. Specifically, this line: `lstm_cell = rnn_cell.BasicLSTMCell(noutput, state_is_tuple=False)`

I modified the code such that the deprecation warning won't appear:

```
with variable_scope.variable_scope(scope, ""SeqLstm"", [inputs]):
    lstm_cell = rnn_cell.BasicLSTMCell(noutput)
    if reverse:
      inputs = array_ops.reverse_v2(inputs, [0])
    outputs, _ = rnn.dynamic_rnn(
        lstm_cell, inputs, time_major=True, dtype=inputs.dtype)
    if reverse:
      outputs = array_ops.reverse_v2(outputs, [0])
    return outputs
```

Before I make any pull requests, is there a reason why the `state_is_tuple` argument is set to `False` in the code?",0,,5,2018-01-19T02:36:34Z,2018-01-25T19:27:43Z,CONTRIBUTOR,2018-01-19T02:45:25Z
16234,The recognized result is not correct when converting the frozen graph to tflite for android device use ,comp:lite,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: mac High Sierra
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.5.0
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:c++/4.2.1
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

The detailed system information, you can check the url:
https://drive.google.com/file/d/19oKikJ0PcGHx9daauub28IYb8J3hA-rw/view?usp=sharing

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Hi, I covert the frozen graph:mobilenet_v1_224 to tflite, and put it in the tflitecamerademo app, but the regonization result is not correct. If I use the tflite file which download from https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip

The regonization result is correct, I don't know what steps is not correct when I covert the frozeon graph to tflite file, could you help me to review what steps is the wrong?

I put the frozen graph, coverting tflite file and the regonized picture in the https://drive.google.com/drive/folders/12h9O2AtcnDuQZXogAdmexRSjxIQVc1Ej?usp=sharing

The correct result should be ""malamute"", but I use the my coverting tflite file, the result is ""shower curtain""

I use the command to do the covert
bazel run --config=opt //tensorflow/contrib/lite/toco:toco -- '--input_file=/tmp/mobilenet_frozen_graph.pb' '--output_file=/tmp/mobilenet_quant_20180117.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=QUANTIZED_UINT8' '--inference_input_type=QUANTIZED_UINT8' '--input_shapes=1,224,224,3' '--input_arrays=input' '--output_arrays=MobilenetV1/Predictions/Reshape_1' '--mean_values=128' '--std_values=128' '--default_ranges_min=0' '--default_ranges_max=6'

Thanks

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,4,2018-01-19T02:26:39Z,2018-01-31T20:13:48Z,NONE,2018-01-31T20:13:48Z
16232,Singleton S3Client,"awaiting testing (then merge),cla: yes","Fixes #16230 .

This drastically speeds up performance of interactions with S3, and eliminates a lot of spurious log warnings when interacting with S3 files.

The filesystem unit tests went from taking ~40 seconds to taking ~4 seconds with this change, a 10X performance improvement.

Some items of note:
- I updated the delete test to work on a bucket that had tests run previously. Without this change, a manual wipe of the file in quest was required after each run.
- I moved the request timeout to a central location, instead of being local to the `Sync` operation.
- I eliminated the increased connection timeout for `Sync`, which shouldn't be needed.
- Configuration is no longer a static variable protected by a mutex, but instead created as-needed. This should be non-functional, given that config is only created once during normal operation now.",1,,7,2018-01-18T23:54:26Z,2018-01-23T22:53:51Z,CONTRIBUTOR,2018-01-22T21:26:47Z
16231,x86_64 compilation failed,"stat:awaiting tensorflower,type:build/install","### System information

- **MacOS High Sierra 10.13.2**:
- **Python 3.6.3**:
- **TensorFlow Latest Pull from 1/17/18**:

### Describe the problem
I am following Pete Warden's TensorFlow for Mobile Poets guide and seem to have a found an error. When I run ""tensorflow/contrib/makefile/build_all_ios.sh"" after about 20 minutes it returns an error. 

I have tried running lipo -info /Users/ryan/Downloads/tensorflow2/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a

and this returns: 

Architectures in the fat file:
/Users/ryan/Downloads/tensorflow2/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a are: i386 

I have the entire error script here:
https://drive.google.com/file/d/1JovTMGBJKbqzRPBzXy3cIQ-hbz76n0ab/view?usp=sharing

### Source code / logs
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see 
invocation)
make: *** [/Users/ryan/Desktop/tensorflow-
master/tensorflow/contrib/makefile/gen/bin/ios_X86_64/benchmark] Error 1
+ '[' 2 -ne 0 ']'
+ echo 'x86_64 compilation failed.'
x86_64 compilation failed.
+ exit 1
",1,,2,2018-01-18T23:51:42Z,2018-01-31T19:42:50Z,NONE,2018-01-22T04:02:19Z
16230,A new S3Client is created with all file operations.,,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS X 10.12.6
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: commit 4595f1cff635ce024e875f0f3d480172731b0b22
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.5.4-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A

### Describe the problem

The [S3 filesystem](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc) creates a new `Aws::S3::S3Client` object with all interactions with S3. This is a heavyweight object, and takes relatively large amount of time to create and destroy.

This should be a singleton associated with the filesystem object.

Fix shortly.",0,,4,2018-01-18T23:39:21Z,2018-01-23T22:53:51Z,CONTRIBUTOR,2018-01-19T07:03:18Z
16225,maxout lose the number of features in the shape of its output,,"In tf.contrib.layers.maxout(), when the shape of ""inputs"" is not completely specified, the shape of its output will be completely unknown, such as [None, None, None] in the 3d case.
Since ""num_units"" has specified the final number of features in the maxout axis, the output should set its shape accordingly:
https://github.com/tensorflow/tensorflow/pull/16114",1,,3,2018-01-18T19:19:05Z,2018-02-01T05:58:39Z,NONE,2018-01-19T07:03:04Z
16224,Suppress AWS curl init warning,"awaiting testing (then merge),cla: yes","This shows up each time we run the TensorBoard command, even if we're
not using anything AWS related.

```sh
jart@compy:~/tmp/aws-sdk-cpp-1.3.15$ grep -R ""Initializing Curl library"" .
./aws-cpp-sdk-core/source/http/curl/CurlHttpClient.cpp:        AWS_LOGSTREAM_INFO(CURL_HTTP_CLIENT_TAG, ""Initializing Curl library"");
```",1,,1,2018-01-18T18:56:07Z,2018-01-19T09:01:54Z,MEMBER,2018-01-18T19:44:23Z
16221,Meaning of report_tensor_allocations_upon_oom output,"stat:awaiting response,type:support","Python: 3.6.2
OS: Ubuntu 16.04
Tensorflow: 1.5.0rc1

When running a session with `tf.RunOptions` and `report_tensor_allocations_upon_oom=True` I get the following output at the end of my log.

1. I am wondering why some entries occur multiple times? How can a single node have multiple allocations? Why are they not summed?
2. Does `Remaining 1252 nodes with 98.80MiB` mean that all 1252 nodes together use 98.80MiB or each single one uses that amount?
3. When summing up all values I get `10.607822265625GiB` but my free GPU space when starting my program is `11.92GiB` so shouldn't there still be enough space??

```
Current usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc
  250.78MiB from network/convolutions/conv2d_5/Conv2D
  217.34MiB from network/convolutions/conv2d_5/Conv2D
  203.75MiB from network/convolutions/conv2d_5/Conv2D
  192.91MiB from network/convolutions/conv2d_11/Conv2D
  168.05MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_11/Conv2D
  163.84MiB from network/convolutions/conv2d_7/Conv2D
  160.50MiB from network/convolutions/conv2d_7/Conv2D
  140.99MiB from network/convolutions/conv2d_12/Conv2D
  133.75MiB from network/convolutions/conv2d_6/Conv2D
  133.75MiB from network/convolutions/conv2d_7/Conv2D
  133.75MiB from network/convolutions/conv2d_11/Conv2D
  133.75MiB from network/convolutions/conv2d_11/Conv2D
  133.75MiB from network/convolutions/conv2d_11/Conv2D
  133.75MiB from network/convolutions/conv2d_12/Conv2D
  103.66MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  Remaining 1252 nodes with 98.80MiB
```",0,,2,2018-01-18T14:39:30Z,2018-01-19T08:41:16Z,CONTRIBUTOR,2018-01-18T17:01:36Z
16220,Fix result shape of tf.tensordot unknown when axes is an integer number,"awaiting testing (then merge),cla: yes","#8452 add the function ""Tensordot partial shape inference"", solves the problem #6682.
However, the shape of result of `tensordot` is still `<unknown>` when `axes` is an integer N, which is in common use.
For example, 
```
a = tf.placeholder('float32', shape=[None, 100])
b = tf.placeholder('float32', shape=[100, 300])
```
set `axes=1`,
```
result_tensordot = tf.tensordot(a, b, axes=1)
result_tensordot.get_shape()  # TensorShape(None)
result_tensordot.get_shape().as_list()  # Error
```
The equivalent `axes=[[1], [0]]` behaves correctly,
```
result_tensordot = tf.tensordot(a, b, axes=[[1], [0]])
result_tensordot.get_shape()  # TensorShape([Dimension(None), Dimension(300)])
result_tensordot.get_shape().as_list()  # [None, 300]
```
The simplified is more common and the partial shape should be inferred correctly.
This PR solves the problem.

",1,,5,2018-01-18T12:10:26Z,2018-01-24T18:29:16Z,CONTRIBUTOR,2018-01-23T02:11:01Z
16217,Windows: Add missing dependencies in lib_proto_parsing,cla: yes,"Fix http://ci.tensorflow.org/job/tf-master-win-bzl/2275/console
Culprit: https://github.com/tensorflow/tensorflow/commit/ccbd14b741e6efbe51769f0f1b9cb3719c42c23b
@gunan @panyx0718 ",0,,2,2018-01-18T09:11:23Z,2018-01-19T09:07:14Z,MEMBER,2018-01-18T09:11:42Z
16215,Tensorflow doesn't delete previous checkpoints,"stat:awaiting response,type:support","### System information
- Linux Ubuntu 16.04:
- Tensorflow version 1.4.1*:
- Python 3.5.2: 

### Describe the problem

A brief summary is that, if I run multiple times my training script tensorflow doesn't delete the checkpoints created in previous runs of the script.

I am preparing a automatic script that every X days runs and train with the new data collected. But I am facing a problem, even that I have configured the saver to keep the 2 last checkpoints, it doesn't work as I expected. 

Example:
I configure to run 100.000 iterations and each 10.000 to save the checkpoint. The system works and starts saving 10.000, 20.000, ... And when get to 30.000 starts deleting the firsts checkpoints. When the script ends I have the 2 last checkpoints(90.000 and 100.000). 

Then when I train again the system starts from the last checkpoint, in this example the 100.000, and do the same as the previous, 110.000, 120.000,.. and when gets to the 130.000 starts to delete the 100.000 and so on. But the 2 checkpoints from the previous run(90.000 and 100.000) remain there even that in the checkpoint txt are not listed there.

This will be repeated in every run of the script, creating files that I don't need anymore and growing during the time.

This is an intended behavior(expecting to the user to delete or manage manually) or it is really a problem?
It exist any workaround?

Thank you for your time and amazing work. 
 ",0,,2,2018-01-18T08:40:20Z,2018-01-18T23:24:11Z,NONE,2018-01-18T19:04:23Z
16208,using string_input_producer with train dataset and validate dataset,stat:awaiting response,"I have two datasets(files), for train and validate respectively. I can successfully load training set thru tf.train.string_input_producer, set num_epochs=5. Then I can iteratively get batch of data to optimize my model.
But, I got stuck when trying to load my validation set by the same way, the program keeps saying ""OutOfRange Error"" even I didn't set num_epochs in string_input_producer.
Can you supply an example that using string_input_producer  with two or more dataset?
same as the question on stackoverflow: [here](https://stackoverflow.com/questions/37068324/read-big-train-validation-test-datasets-in-tensorflow)
Please help me solve the problem. Thank you very much.
",0,,2,2018-01-18T03:21:26Z,2018-01-26T02:45:16Z,NONE,2018-01-18T13:27:03Z
16206,make label_image for tflite build again,"awaiting testing (then merge),cla: yes","1. add namespace to label_image.h to make label_image for tflite build again
2. add --config monolithic and mention NDK settings in label_image.md
3. fix a typo in display_usage()",1,,2,2018-01-18T02:33:26Z,2018-01-25T22:24:23Z,CONTRIBUTOR,2018-01-25T22:24:20Z
16200,Accepts `PathLike` objects for `model_dir`,"awaiting testing (then merge),cla: yes","* Retrieves the file system path representation if `PathLike` object is passed to `Estimator` or `RunConfig` for `model_dir`, instead of `str`.
* Closes #15784",0,,6,2018-01-17T23:27:52Z,2018-01-20T23:36:15Z,CONTRIBUTOR,2018-01-17T23:37:01Z
16198,Unable to build Tensorflow Benchmark model for Android,,"I've been trying to benchmark the model on mobile but I'm not able to build the model for Android. For desktop, I have been able to build and run the benchmark model.

The machine I'm using is a MacBook pro 15 inch with High Sierra and tensorflow v.1.4

I've been following the directions given at the following links:

(https://www.tensorflow.org/mobile/optimizing#how_to_profile_your_model
https://github.com/tensorflow/tensorflow/tree/r1.4/tensorflow/tools/benchmark)

Edit: Updated answer to the issue template

Have I written custom code

- No custom code was written

OS Platform and Distribution

- Mac OS High Sierra

TensorFlow installed from

- Tensorflow installed from source

TensorFlow version

- 1.4

Bazel version

- Bazel version 0.9.0

CUDA/cuDNN version

- N/A

GPU model and memory

- N/A

Exact command to reproduce

```
bazel build -c opt --cpu=armeabi-v7a \
  --crosstool_top=//external:android/crosstool \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  tensorflow/tools/benchmark:benchmark_model
```

Error message received

```
ERROR: /private/var/tmp/_bazel_abhisheksehgal/486c675b3107dc770b2281b905f670fe/external/highwayhash/BUILD:8:1: C++ compilation of rule '@highwayhash//:sip_hash' failed (Exit 1)
In file included from external/highwayhash/highwayhash/sip_hash.cc:15:
In file included from external/highwayhash/highwayhash/sip_hash.h:25:
external/highwayhash/highwayhash/state_helpers.h:76:3: error: use of undeclared identifier 'static_assert'; did you mean 'static_cast'?
  static_assert((kPacketSize & (kPacketSize - 1)) == 0, ""Size must be 2^i."");
  ^
In file included from external/highwayhash/highwayhash/sip_hash.cc:15:
external/highwayhash/highwayhash/sip_hash.h:33:15: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
  using Key = HH_U64[2];
              ^
external/highwayhash/highwayhash/sip_hash.h:104:22: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using SipHashState = SipHashStateT<2, 4>;
                     ^
external/highwayhash/highwayhash/sip_hash.h:105:24: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using SipHash13State = SipHashStateT<1, 3>;
                       ^
external/highwayhash/highwayhash/sip_hash.cc:20:13: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using Key = highwayhash::SipHashState::Key;
            ^
external/highwayhash/highwayhash/sip_hash.cc:21:15: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using Key13 = highwayhash::SipHash13State::Key;
              ^
5 warnings and 1 error generated.
Target //tensorflow/tools/benchmark:benchmark_model failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2.060s, Critical Path: 1.60s
FAILED: Build did NOT complete successfully
```",0,,5,2018-01-17T22:45:21Z,2018-01-19T20:32:12Z,NONE,2018-01-18T07:03:03Z
16196,Fix issue of branch switching not working with bazel,"awaiting review,cla: yes","This fix tries to address the issue raised in #15957 where bazel stops working after switching git branch, and reconfigure with `./configure` will not work as well.

This fix adds a quick fix as was suggested, by having `export TF_CONFIG_TIME=""$(date)"" `in configure.py and add it to the environ list in git_configure.bzl.

This fix fixes #15957.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,2,2018-01-17T19:00:24Z,2018-01-19T21:40:07Z,MEMBER,2018-01-19T18:27:46Z
16193,Performance issues with TF1.5 on CPU,type:bug/performance,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0-rc1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:

Hello,
I'm facing performance issues with the last releases of TF using a CPU.
I'm using the [benchmark tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/benchmark_model.cc) to calculate mean inference time of a model.

For example, in order to evaluate mobilenet (trained on a custom dataset), I'm using this command :
`bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=""path to mobilenet graph"" --input_layer=""input"" --input_layer_shape=""1,224,224,3"" --input_layer_type=""float"" --output_layer=""MobilenetV1/Predictions/Reshape_1""`
After setting CUDA_VISIBLE_DEVICES to """" in order to run on CPU.

With TF 1.4.1, I obtain a mean inference time equals to 26ms (13ms if I compile with optimization flags).
Using tf 1.5.*, I obtain a mean inference time equals to 51ms (45ms if I compile with optimization flags).

The loss is very important, so I'm wondering if it's a known issue and how I can improve this.

I tried with tags/v1.5.0-rc0, tags/v1.5.0-rc1 and master, and the problem is the same.

Thank you",1,,8,2018-01-17T13:39:19Z,2018-01-22T09:01:17Z,NONE,2018-01-17T21:15:04Z
16192,how to set ignore_label in tensorflow?,,"when i set the label=-1, there is an error: Received a label value of -1 which is outside the valid range of [0, 8)",0,,4,2018-01-17T13:31:38Z,2018-01-24T13:24:20Z,NONE,2018-01-18T01:24:34Z
16191,Fix docstring typo of losses_impl.py,cla: yes,"Add missing ""`"" to the docstring.",0,,3,2018-01-17T12:58:14Z,2018-01-17T14:58:11Z,CONTRIBUTOR,2018-01-17T13:02:57Z
16188,benchmark_model tool not build successfully for android version,,"Hello,

I try to build the benchmark_model for the android, but I encounter some errors.
Please help, is any setting not correct?

The configuration of the SDK and NDK in the WORKSPACE is
android_sdk_repository(
    name = ""androidsdk"",
    api_level = 23,
    build_tools_version = ""26.0.1"",
    path = ""/home/kk/android_sdk/android-sdk-linux"",
)

android_ndk_repository(
    name=""androidndk"",
    path=""/home/kk/android_sdk/ndk/android-ndk-r14"",
    api_level=14)

Use the command to build:
bazel build --cxxopt='--std=c++11' -c opt 
--crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain tensorflow/tools/benchmark:benchmark_model

There are three errors
1. external/gif_archive/lib/openbsd-reallocarray.c:33:19: error: use of undeclared identifier 'SIZE_MAX'
2. tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:172:53: error: no member named 'nanf' in namespace 'std'; did you mean simply 'nanf'?
3. external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: cannot find -lpthread

Thanks

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04.4 LTS
- **TensorFlow installed from (source or binary)**:use the pip install
- **TensorFlow version (use command below)**:1.4.0
- **Python version**: Python 2.7.6
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:(Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
- **CUDA/cuDNN version**:NA
- **GPU model and memory**:NA
- **Exact command to reproduce**:NA

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,4,2018-01-17T09:46:35Z,2018-01-19T07:00:17Z,NONE,2018-01-18T16:15:45Z
16186,A bug when applying MultiRNNCell?,,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: This code is very similar to an official example
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip install tensorflow
- **TensorFlow version (use command below)**: b'unknown' 1.4.0
- **Python version**: Python 3.5.2 :: Anaconda 4.2.0 (64-bit)
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
tf.nn.MultiRNNCell sometimes doesn't work.

It raises an issue like this:
ValueError: Dimensions must be equal, but are 64 and 96 for 'lstm/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [128,64], [96,128].

### Source code / logs
  import tensorflow as tf
  import numpy as np

  hidden_layer_size = 32
  embed = tf.zeros((128, 6, 64), dtype=tf.float32)

  num_LSTM_layers = 2
  with tf.variable_scope(""lstm""):
    
    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size, forget_bias=1.0)
    cell = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell]*num_LSTM_layers, state_is_tuple=True)
    outputs, states = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)
   
Error:
ValueError: Dimensions must be equal, but are 64 and 96 for 'lstm/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [128,64], [96,128].

",0,,3,2018-01-17T09:23:15Z,2018-01-30T23:31:09Z,NONE,2018-01-27T15:17:54Z
16183,`AssignVariableOp` supports `DT_BFLOAT16`,cla: yes,Fix #16103,0,,2,2018-01-17T08:06:26Z,2018-01-20T03:15:37Z,CONTRIBUTOR,2018-01-20T03:15:37Z
16180,Tensorboard is down after upgrading the tensorflow?,stat:awaiting response,"Hello everyone:

I meet a issue about tensorboard after upgrading the tensorflow. It runs nicely before, but I want maintain some Python2.7 codes in Python3.4. That is why I install tensorflow .whl file of Python 3.4 and modify some grammer from Python2.7 to Python3.4. Then codes still run fine, but tensorboard is donw. The error message as following:

![image](https://user-images.githubusercontent.com/12611573/35029640-d1981942-fb96-11e7-9b89-c3c14ffaa54b.png)

OS Platform: Ubuntu 14.04
TensorFlow installed from: pip instll .whl file
TensorFlow version: tensorflow 1.2.1 for Python2, but can not check the version for Python 3

What should I do for this issue? degrade tensorflow or upgrade CUDA?
Can anybody give me any help? Thank you!
",0,,15,2018-01-17T07:00:41Z,2018-01-31T19:25:03Z,NONE,2018-01-17T19:12:04Z
16178,Crash in TF lite demo android app when using preprocessing layer,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: v1.4.0-rc0-21-g1e25994 1.4.0-rc1
- **Python version**: Python 3.6.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8/6
- **GPU model and memory**: Titan X (Pascal), 12 GB
- **Exact command to reproduce**:


### Describe the problem
I have a problem adding preprocessing layers to MobileNetV1 model that is quantized afterward. As preprocessing method I would like to use inception preprocessing, but TF lite does not support several operations (sub, div, broadcasting, ...), so I modified following preprocessing

```python
images = tf.divide(images, tf.constant(255.0))
images = tf.subtract(images, tf.constant(0.5))
images = tf.multiply(images, tf.constant(2.0))
```

to

```python
shape = images.get_shape()
c1 = tf.constant(1.0/255.0, shape=shape)
c1 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)
c2 = tf.constant(-0.5, shape=shape)
c2 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)
c3 = tf.constant(2.0, shape=shape)
c3 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)

images = tf.multiply(images, c1)
images = tf.fake_quant_with_min_max_args(images, min=0, max=1)
images = tf.add(images, c2)
images = tf.fake_quant_with_min_max_args(images, min=-0.5, max=0.5)
images = tf.multiply(images, c3)
images = tf.fake_quant_with_min_max_args(images, min=-1.0, max=1.0)
```

Quantization is performed with
```python
fold_batch_norms.FoldBatchNorms(graph)
quantize.Quantize(graph, is_training=is_training)
```
 and can be trained and evaluated.

Further, graph is frozen.
```bash
bazel-bin/tensorflow/python/tools/freeze_graph \
  --input_graph=MobileNetV1-4.pbtxt \
  --input_checkpoint=MobileNetV1-4.ckpt \
  --output_node_names=output/softmax \
  --output_graph=MobileNetV1-4-frozen.pb
```

Finally, frozen graph is converted to TF lite model using command.
```bash
bazel-bin/tensorflow/contrib/lite/toco/toco \
 --input_file=MobileNetV1-4-frozen.pb \
 --input_format=TENSORFLOW_GRAPHDEF \
 --output_format=TFLITE \
 --output_file=model.tflite \
 --inference_type=QUANTIZED_UINT8 \
 --inference_input_type=QUANTIZED_UINT8 \
 --input_array=input/image \
 --output_array=output/softmax \
 --input_shape=1,224,224,3
```

During conversion no error occurs.
```
2018-01-17 11:25:52.905034: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 604 operators, 896 arrays (0 quantized)
2018-01-17 11:25:53.301108: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 66 operators, 127 arrays (1 quantized)
2018-01-17 11:25:53.302502: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 66 operators, 127 arrays (1 quantized)
2018-01-17 11:25:53.303020: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 35 operators, 96 arrays (1 quantized)
2018-01-17 11:25:53.303601: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 35 operators, 96 arrays (1 quantized)
2018-01-17 11:25:53.326761: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 1: 34 operators, 95 arrays (94 quantized)
2018-01-17 11:25:53.327269: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 2: 34 operators, 95 arrays (94 quantized)
2018-01-17 11:25:53.327854: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 1756160 bytes, theoretical optimal value: 1204224 bytes.
2018-01-17 11:25:53.328080: I tensorflow/contrib/lite/toco/toco_tooling.cc:269] Estimated count of arithmetic ops: 1.14175 billion (note that a multiply-add is counted as 2 ops).
```

When I upload generated model to TF lite demo application, app crashes logcat prints this error.
```
01-17 11:56:52.190 10923-10923/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
01-17 11:56:52.190 10923-10923/? A/DEBUG: Build fingerprint: 'samsung/dreamlteks/dreamlteks:7.0/NRD90M/G950NKSU1AQL3:user/release-keys'
01-17 11:56:52.191 10923-10923/? A/DEBUG: Revision: '11'
01-17 11:56:52.191 10923-10923/? A/DEBUG: ABI: 'arm64'
01-17 11:56:52.191 10923-10923/? A/DEBUG: pid: 10865, tid: 10881, name: CameraBackgroun  >>> android.example.com.tflitecamerademo <<<
01-17 11:56:52.191 10923-10923/? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x0   0000000000000000  x1   0000000000002a81  x2   0000000000000006  x3   0000000000000008
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x4   0000007e81515040  x5   0000007e815166c0  x6   0000ffffffffffff  x7   ffffffffffffffff
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x8   0000000000000083  x9   ffffffffffffffdf  x10  0000000000000000  x11  ffffffffffffffff
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x12  0000000000000000  x13  ffffffffffff0000  x14  00000000000002e0  x15  000000000000044d
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x16  0000007e9533aed0  x17  0000007e952e29f4  x18  0000000000000001  x19  0000007e817524f8
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x20  0000000000000006  x21  0000007e81752450  x22  000000000000000b  x23  0000007e858340f0
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x24  0000007e817524e8  x25  0000000000000000  x26  0000000000000080  x27  0000007e81516740
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x28  0000000000000001  x29  0000007e81750730  x30  0000007e952dfd14
01-17 11:56:52.191 10923-10923/? A/DEBUG:     sp   0000007e81750710  pc   0000007e952e29fc  pstate 0000000060000000
01-17 11:56:52.198 10923-10923/? A/DEBUG: backtrace:
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #00 pc 000000000006f9fc  /system/lib64/libc.so (tgkill+8)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #01 pc 000000000006cd10  /system/lib64/libc.so (pthread_kill+64)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #02 pc 0000000000025078  /system/lib64/libc.so (raise+24)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #03 pc 000000000001cc04  /system/lib64/libc.so (abort+52)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #04 pc 00000000000881a0  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #05 pc 0000000000071ce4  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #06 pc 00000000000707fc  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #07 pc 000000000007f99c  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #08 pc 0000000000011c5c  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+1628)
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #09 pc 0000000000384abc  /data/app/android.example.com.tflitecamerademo-1/oat/arm64/base.odex (offset 0x329000)
```

This error doesn't seem to be related to added preprocessing layer, but without adding preprocessing layer, no error occurs and app can run.
",0,,2,2018-01-17T05:09:08Z,2018-01-17T08:15:46Z,NONE,2018-01-17T06:53:47Z
16177,"RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6   return f(*args, **kwds)",stat:awaiting response,"### System information
- **OS Platform and Distribution :**  Linux Ubuntu 17.10
- **TensorFlow installed from :**  Anaconda [followed this tutorial](https://www.tensorflow.org/install/install_linux#InstallingAnaconda)
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: Python 3.6.4 :: Anaconda, Inc.
- **CUDA/cuDNN version**: not using GPU version
- **GPU model and memory**: 2GB GT720

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**result :** 
/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
v1.4.0-19-ga52c8d9 1.4.1

### Describe the problem
Followed Official tensorflow documentation to install tensorflow on Ubuntu 17.10, python3 (python 3.6) and with CPU support. Used conda environment. Followed [this](https://www.tensorflow.org/install/install_linux#InstallingAnaconda) and in the 4th step this is the command I used `pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.1-cp36-cp36m-linux_x86_64.whl`
it installed successfully. But when I try to import tensorflow in python I'm getting this error.

```
/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)

```

### Source code / logs
Activate Conda environment
`source activate tensorflow`

**command :** `python`
**log :** 
```
Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) 
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
```

**command :** `import tensorflow`
**log :**
```
/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
```


Why can't I use the tensorflow for python 3.6 in python 3.6 . How to fix this ?",0,,2,2018-01-17T05:05:21Z,2018-01-18T16:20:50Z,NONE,2018-01-17T12:59:46Z
16176,4d55397500 patch 1,"awaiting testing (then merge),cla: yes","Provide a practical meaning for the `pos_weights` parameter.

The current  weighted_cross_entropy_with_logits docs don't explain practically the relationship of
`pos_weights > 1`,  `pos_weights < 1` to precision, recall, and class imbalance.
",1,,2,2018-01-17T04:52:42Z,2018-01-22T21:30:33Z,CONTRIBUTOR,2018-01-22T21:30:49Z
16173,Add C++ toolchain for portable Linux builds,"awaiting review,cla: yes,kokoro:run",See #15777,1,,12,2018-01-17T02:10:01Z,2018-01-23T17:51:19Z,MEMBER,2018-01-17T22:35:41Z
16172,Merge changes from r1.5 into master,"awaiting testing (then merge),cla: yes","This change picks up the commits exclusive to the r1.5 branch and puts them back into master.

There were a bunch of merge conflicts here. I favored master in most cases except those to do with obvious versioning differences.

I'm not sure if I did the merge correctly, considering there are a great many CLs presented here.",0,,3,2018-01-17T00:46:21Z,2018-01-19T09:07:54Z,MEMBER,2018-01-18T01:00:26Z
16167,Documentation Method Templates Improvement,type:docs,"### System information
N/A

### Describe the problem
The method/class templates in documentation should include a full, functioning path to the method instead of just truncating to the method's name.

I.e. this is what we have at present (bad): 
<img width=""399"" alt=""screen shot 2018-01-16 at 2 23 08 pm"" src=""https://user-images.githubusercontent.com/9597721/35007940-0511d55c-fac9-11e7-9d0c-4be2db021533.png"">

This is a more practical and copy/paste-friendly version:
<img width=""426"" alt=""screen shot 2018-01-16 at 2 22 49 pm"" src=""https://user-images.githubusercontent.com/9597721/35007976-2970cdc2-fac9-11e7-80b8-0ec1e2334734.png"">

I'm constantly just grabbing method templates, pasting to my text editor and then coming back to docs to copy/paste the package path which is now the header of the page; which is an awful workflow.

### Source code / logs
N/A",1,,5,2018-01-16T19:28:06Z,2018-02-02T02:58:59Z,NONE,2018-01-16T22:44:48Z
16165,Error when building from source Fedora 27 CUDA 9.1,type:build/install,"### System information
- OS Platform and Distribution: Fedora 27
- TensorFlow installed from (source or binary): binary
- TensorFlow version: r1.4
- Python version: 3.6.3
- Bazel version: 0.8.1
- GCC/Compiler version: 7.2.1
- CUDA/cuDNN version: CUDA 9.1 cuDNN 7.0.5
- **GPU model and memory**: NVidia Geforce GTX 960 4GB
- Exact command to reproduce: bazel build -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
So I'm attempting to build tensorflow from source on fedora with the version of CUDA and cuDNN I already had installed to avoid have to also install an older version. The build however errors with the following message:

```
ERROR: .cache/bazel/_bazel_xd009642/f9f5dea1a139b69420e1045d339dda45/external/nccl_archive/BUILD:33:1: error while parsing .d file: /home/xd009642/.cache/bazel/_bazel_xd009642/f9f5dea1a139b69420e1045d339dda45/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/external/nccl_archive/_objs/nccl/external/nccl_archive/src/all_reduce.cu.pic.d (No such file or directory)
<command-line>:0:15: warning: ISO C++11 requires whitespace after the macro name
<command-line>:0:1: error: macro names must be identifiers
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 0.392s, Critical Path: 0.12s
FAILED: Build did NOT complete successfully
```
I also tried the command `bazel build --config=opt --config=cuda --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package` from [here](http://www.python36.com/install-tensorflow141-gpu/) with the same end result.

Any guidance is appreciated as this is my first time using bazel (and also trying to compile tensorflow).",0,,11,2018-01-16T18:21:58Z,2018-01-20T18:26:12Z,NONE,2018-01-16T22:49:02Z
16164,"Accidentally cancelled inceptionV3 during install, now can't install at all",,"Hello,
i was setting up tensorflow for image classification, and after i ran : 

python -m scripts.retrain \
  --bottleneck_dir=tf_files/bottlenecks \
  --model_dir=tf_files/models/""${ARCHITECTURE}"" \
  --summaries_dir=tf_files/training_summaries/""${ARCHITECTURE}"" \
  --output_graph=tf_files/retrained_graph.pb \
  --output_labels=tf_files/retrained_labels.txt \
  --architecture=""${ARCHITECTURE}"" \
  --image_dir=tf_files/flower_photos

It automatically started installing inception, i realized that i needed to change some options so i cancelled the install of inception.
Now i believe that i have a half install that doesn't let me install the full package or use the half package.

I may be wrong, but any suggestions would be appreciated.
FYI: i've run :
pip install inception, to which i receive a ""python setup.py egg_info"" failed with error code 1 in {my local/temp dir}

I also just tried running the scripts.retrain again, to which i receive a ""EOFError: compressed file ended before the end-of-stream marker was reached""

Running on Windows 7",1,,5,2018-01-16T17:28:24Z,2018-01-31T19:38:29Z,NONE,2018-01-17T01:01:27Z
16161,tf.case raising IllegalArgumentError 'None of the conditions evaluated as True' when used with Dataset,"stat:awaiting tensorflower,type:bug/performance","### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 64bit
- **TensorFlow installed from (source or binary)**:
via pip
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
3.5

When I use tf.case within a tf.data.Dataset, I get an IllegalArgumentError 'None of the conditions evaluated as True'. However, when I use the same code without the Dataset, it works fine. Furthermore, if I understand the error message correctly, it already tells me that one condition evaluated to true (see the end of the first line):

```
InvalidArgumentError (see above for traceback): assertion failed: [None of the conditions evaluated as True. Conditions: (Equal_3:0, Equal_4:0, Equal_5:0), Values:] [1 0 0]
	 [[Node: case/If_0/Assert_1/AssertGuard/Assert = Assert[T=[DT_STRING, DT_BOOL], summarize=3](case/If_0/Assert_1/AssertGuard/Assert/Switch, case/If_0/Assert_1/AssertGuard/Assert/data_0, case/If_0/Assert_1/AssertGuard/Assert/Switch_1)]]
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](OneShotIterator)]]
```

The following code reproduces the issue:
```
import tensorflow as tf


def random_map(i):
	random_int = tf.random_uniform([], minval=0, maxval=3, dtype=tf.int32)
	random_int = tf.Print(random_int, [random_int, tf.equal(random_int, 0), tf.equal(random_int, 1), tf.equal(random_int, 2)], 'random_int')

	result = tf.case([
		(tf.equal(random_int, 0), lambda: i * 10000),
		(tf.equal(random_int, 1), lambda: i * 20000),
		(tf.equal(random_int, 2), lambda: i * 30000)
	], exclusive=True)

	return result


print('working =========================================================================')
with tf.Session() as sess:
	input_pl = tf.placeholder(dtype=tf.int32)
	result = random_map(input_pl)
	for i in range(5):
		result_value = sess.run(result, feed_dict={input_pl: i})
		print(result_value)

print('not working =====================================================================')
with tf.Session() as sess:
	dataset = tf.data.Dataset.from_tensor_slices(tf.range(5))
	dataset = dataset.map(random_map)
	iterator = dataset.make_one_shot_iterator()
	next_result = iterator.get_next()

	for i in range(5):
		result_value = sess.run(next_result)
		print(result_value)
```

I also found [this question]( http://www.programfaqs.com/faq/tensorflow-case-error-invalid-argument-assertion-failed-none-of-the-conditions-evaluated-as-true/), which seems to be the same problem.",0,,3,2018-01-16T14:01:26Z,2018-01-17T19:01:54Z,CONTRIBUTOR,2018-01-16T22:56:02Z
16160,tf.contrib.lookup.HashTable(kv_initializer) does not work in eager mode.,"comp:eager,stat:awaiting tensorflower,type:bug/performance","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------
```
== cat /etc/issue ===============================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64
Mac OS X 10.11.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 7.3.0 (clang-703.0.31)
Target: x86_64-apple-darwin15.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.11.2)
protobuf (3.4.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named tensorflow

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib:

== nvidia-smi ===================================================
/Users/xiaoyun/tf14py3/bin/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

== cat /etc/issue ===============================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64
Mac OS X 10.11.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 7.3.0 (clang-703.0.31)
Target: x86_64-apple-darwin15.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.11.2)
protobuf (3.4.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.5.0-rc1
tf.GIT_VERSION = v1.5.0-rc0-9-gf9472619f6
tf.COMPILER_VERSION = v1.5.0-rc0-9-gf9472619f6
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib:

== nvidia-smi ===================================================
/Users/xiaoyun/tf14py3/bin/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.5.0-rc0-9-gf9472619f6 1.5.0-rc1

### Describe the problem
Can not use hashtable in eager mode.

### Source code / logs
```
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

keyfile = ""./key_dict""
kv_initializer = tf.contrib.lookup.TextFileInitializer(
    keyfile, tf.string, 0, tf.int64, 1, delimiter=""\t"")
table = tf.contrib.lookup.HashTable(kv_initializer, 0)
table.init.run()

filenames = [""./data1""]
dataset = tf.data.TextLineDataset(filenames)
#dataset = dataset.map(lambda tkns:table.lookup(tkns))
for x in tfe.Iterator(dataset):
    print(x)

Traceback (most recent call last):
  File ""torch/textline.py"", line 13, in <module>
    table = tf.contrib.lookup.HashTable(kv_initializer, 0)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py"", line 282, in __init__
    super(HashTable, self).__init__(table_ref, default_value, initializer)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py"", line 168, in __init__
    self._init = initializer.initialize(self)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py"", line 531, in initialize
    if constant_op.is_constant(filename):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 224, in is_constant
    op = tensor_or_op.op
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 825, in op
    raise AttributeError(""op not supported for Eager Tensors."")
AttributeError: op not supported for Eager Tensors.
```",1,,5,2018-01-16T13:11:57Z,2018-01-17T23:40:09Z,NONE,2018-01-16T18:06:37Z
16158,GAN model: move generated and real operations under discriminator namespace,"awaiting testing (then merge),cla: yes","Hi everybody,

`gan_model` runs the discriminator on both the generated and real data. This PR changes/fixes the namespace of the generated graph.

* Current: The network variables and operations on generated data are in the `Discriminator` namespace but the operations on real data are in the `Discriminator_1` namespace.
* PR: The network variables stay in the `Discriminator` namespace. Operations on generated data are in `Discriminator/generated` and operations on real data are in `Discriminator/real`.

`gan_model` only searches the `Discriminator` namespace for regularization. Presumably, if you were running activity regularization in your discriminator, only the part on generated data would be picked up. Plus, the graph looks much better visually this way and you can tell which discriminator is which.

Cheers",1,,2,2018-01-16T11:36:00Z,2018-01-22T21:31:35Z,CONTRIBUTOR,2018-01-16T11:40:07Z
16157,Update rules_closure to fix bazel version check,cla: yes,Related https://github.com/bazelbuild/bazel/issues/4425#issuecomment-357681237,0,,2,2018-01-16T11:31:14Z,2018-01-17T04:47:51Z,MEMBER,2018-01-16T11:43:16Z
16153,New features: tf.alphas and tf.alphas_like - Related to #16128,"API review,awaiting review,cla: yes","This PR is related to the issue: #16128

#### I send my work here for peer reviewing and discussion. Please do not merge now.

### A few interrogations before merging

1. Are the names I have chosen fine with everyone or you would like it to be changed to something else ?
2. Do my implementations seem fine ?
3. What kind of tests should I implement ? Where shall I put them ?
4. Is it a good idea to replace the function body of tf.ones/tf.zeros and tf.ones_like/tf.zeros_like by a function call to tf.alphas and tf.alphas_like ? Not doing it would lead to code duplication, however I would understand that you might be reluctant, these functions are at the core of the library.

### Why I created these functions ?

I oftenly need to create similar tensors with a non-zero/one value. A simple example would be cost functions in GANs with *label smoothing* applied. 

As stated by @facaiy in #16128, I could use : 
```python
b1 = tf.ones_like(a, dtype=tf.float32) * 0.9 # Tensor full of 0.9
b2 = tf.ones_like(a, dtype=tf.int32) * 2 # Tensor full of 2
b4 = tf.ones_like(a, dtype=tf.bool) # Tensor full of True
```
 However, as shown in my later comments in the issue, the method implemented in this PR is almost twice as fast.

In a wider view, I think that using a single function more *generic* is always a good thing whenever it is possible.

### How does the function API work ?

In a very similar manner than the existing ones: 

```python
import tensorflow as tf

a = tf.constant([
    [
        [4, 5, 6],
        [1, 2, 3]
    ],
    [
        [4, 5, 6],
        [1, 2, 3]
    ]
])

b1 = tf.alphas_like(a, 0.5431)
b2 = tf.alphas_like(a, 5)
b3 = tf.alphas_like(a, -5)
b4 = tf.alphas_like(a, True)

with tf.Session() as sess:
    _b1, _b2, _b3, _b4 = sess.run([b1, b2, b3, b4])
    
print(""b1:"", _b1)
print(""b2:"", _b2)
print(""b3:"", _b3)
print(""b4:"", _b4)

############### OUTPUTS ###############

>>> b1: [
  [
    [ 0.5431  0.5431  0.5431]
    [ 0.5431  0.5431  0.5431]
  ]
  [
    [ 0.5431  0.5431  0.5431]
    [ 0.5431  0.5431  0.5431]
  ]
]

>>> b2: [
  [
    [5 5 5]
    [5 5 5]
  ]
  [
    [5 5 5]
    [5 5 5]
  ]
]

>>> b3: [
  [
    [-5 -5 -5]
    [-5 -5 -5]
  ]
  [
    [-5 -5 -5]
    [-5 -5 -5]
  ]
]

>>> b4: [
  [
    [ True  True  True]
    [ True  True  True]
  ]
  [
    [ True  True  True]
    [ True  True  True]
  ]
]
```

---------------------------

I'm of course free for discussion over video-calls. It's the first time I try to make a change at the core of TF, and I'm quite afraid of breaking everything ;) Thanks for your help btw.

All the best,

Jonathan DEKHTIAR",1,,6,2018-01-16T10:12:45Z,2018-01-22T22:47:28Z,CONTRIBUTOR,2018-01-22T09:52:58Z
16151,ValueError: Labels are incompatible with given information. ,,"Have I written custom code: yes
OS: Windows 8.1
Tensorflow installed from: conda
Tensorflow version: 1.4

I am having problems in adding validation monitors to `Estimator.fit`. With this code I have:

```
def main(_):
    image_paths, labels = dataset_utils.read_dataset_list('../test/dummy_labels_file.txt')
    data_dir = ""../test/dummy_data/""
    images = dataset_utils.read_images(data_dir=data_dir, image_paths=image_paths, image_extension='png')
    print('Done reading images')
    images = dataset_utils.resize(images, (1596, 48))
    images = dataset_utils.transpose(images)
    labels = dataset_utils.encode(labels)
    x_train, x_test, y_train, y_test = dataset_utils.split(features=images, test_size=0.5, labels=labels)
    print(x_test)
    x_train_seq_lens = dataset_utils.get_seq_lens(x_train)
    x_test_seq_lens = dataset_utils.get_seq_lens(x_test)

    train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""x"": np.array(x_train),
           ""seq_lens"": np.array(x_train_seq_lens)},
        y=np.array(y_train),
        num_epochs=1,
        shuffle=True,
        batch_size=1
    )

    validation_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""x"": np.array(x_test),
           ""seq_lens"": np.array(x_test_seq_lens)},
        y=np.array(y_test),
        shuffle=True
    )

    validation_monitor = learn.monitors.ValidationMonitor(
        input_fn=validation_input_fn,
        every_n_steps=1
    )

    model = GridRNNModelFn(num_time_steps=1596, num_features=48, num_hidden_units=128, num_classes=80,
                           learning_rate=0.001, optimizer=Optimizers.MOMENTUM)

    classifier = learn.Estimator(model_fn=model.model_fn, params=model.params, model_dir=""/tmp/grid_rnn_ocr_model"")
    classifier.fit(input_fn=train_input_fn, monitors=[validation_monitor])


if __name__ == '__main__':
    tf.app.run(main=main)
```

It throws this error:

`ValueError: Labels are incompatible with given information. Given labels: Tensor(""random_shuffle_queue_DequeueUpTo:3"", shape=(?, 37), dtype=int32), required signatures: TensorSignature(dtype=tf.int32, shape=TensorShape([Dimension(None), Dimension(33)]), is_sparse=False).`

Which leads me to think that the dynamic label lengths are not accepted. To reproduce this, simply clone this [repository](https://github.com/selcouthlyBlue/simplified_bi_lstm_ocr) and run the script specified in the readme.",0,,1,2018-01-16T08:43:19Z,2018-01-16T09:31:29Z,CONTRIBUTOR,2018-01-16T09:31:29Z
16148,non_max_suppression is on CPU?,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
     Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
      Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
      binary(By pip)
- **TensorFlow version (use command below)**:
      1.4.1
- **Python version**: 
      3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
    8.0.61/6.0.21
- **GPU model and memory**:
    GTX 1080 Ti, 11172MiB
- **Exact command to reproduce**:
     python main.py

### Describe the problem
    
I train my RFCN by tensorflow. My project need very high speed. So I use the profile and I find that non_max_suppression is on CPU? Is there a GPU version?I think if you calculate all pairs of boxes IOU first, then just for-loop once will ultimately boost speed, there have some trick in it, just see the source code in [https://github.com/rbgirshick/py-faster-rcnn/tree/master/lib/nms](https://github.com/rbgirshick/py-faster-rcnn/tree/master/lib/nms). I think cuda version of NMS is faster than CPU version.
",0,,2,2018-01-16T08:21:02Z,2018-01-16T19:33:57Z,NONE,2018-01-16T13:30:51Z
16145,Decoding contents of BMP file on big endian,"awaiting testing (then merge),cla: yes","As the BMP file contents are encoded in little endian format, added byte swapping for reading the various header components correctly on big endian.",1,,2,2018-01-16T06:29:25Z,2018-01-26T16:40:05Z,CONTRIBUTOR,2018-01-25T19:04:02Z
16144,Is it possible to train CNN model by using tensorflow JAVA API?,,"Hello, TF.
I have plane to train my CNN model by using tensorflow JAVA API.
I got success on simple model( with a simple matmul operation between weights and bias)
BUT I failed to train CNN model.
",0,,2,2018-01-16T06:15:29Z,2018-01-16T07:20:47Z,NONE,2018-01-16T07:20:46Z
16143,"Undefined symbol ""_ZN3Aws8Security14SecureMemClearEPhj""",stat:awaiting response,"compiled tensorflow r.15 from source , when import tensorflow in python got following error:
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/_pywrap_tensorflow_internal.so: Undefined symbol ""_ZN3Aws8Security14SecureMemClearEPhj""


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace

thanks in advance !!!",0,,2,2018-01-16T06:04:50Z,2018-01-28T13:49:48Z,NONE,2018-01-16T19:01:24Z
16142,fix typo,"awaiting testing (then merge),cla: yes",,2,,4,2018-01-16T05:36:10Z,2018-01-23T17:35:39Z,CONTRIBUTOR,2018-01-16T05:41:07Z
16135,Distributed Tensorflow  using MPI,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

I have tried stackflow and Google group discussion forum but could  get any reply or comment

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Redhat 7.4

- **TensorFlow installed from (source or binary)**:
from source with MPI
- **TensorFlow version (use command below)**:
1.41
- **Python version**: 
2.7.14
- **Bazel version (if compiling from source)**:

- **GCC/Compiler version (if compiling from source)**:
GCC 6.0
- **CUDA/cuDNN version**:
8.0/6.5
- **GPU model and memory**:
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K20Xm         Off  | 00000000:08:00.0 Off |                    0 |
| N/A   34C    P0    61W / 235W |      0MiB /  5699MiB |     72%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I am using the following  script to launch distributed computing.


#! /bin/bash

module load openmpi/3.0.0-gnu

host=$(hostname -s)
if [[ $host == ""node06"" ]]; then
        echo ""statring Node 6""
        python tf_dis_2.py --job_name=""ps"" --task_index=0
elif [[ $host == ""node07"" ]]; then
        echo ""starting Node 7 as worker""
        python tf_dis_2.py --job_name=""worker"" --task_index=0
elif [[ $host == ""node08"" ]]; then
        echo ""starting Node 8 as worker""
        python tf_dis_2.py --job_name=""worker"" --task_index=1
fi

-----

I am running it on slurm  with three nodes.

srun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh

I am using MPI instead of GPRC.

I am getting the following message:

---------------------------------------------------
srun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh
statring Node 6
starting Node 8 as worker
starting Node 7 as worker
2018-01-15 11:34:59.961617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732
pciBusID: 0000:08:00.0
totalMemory: 5.57GiB freeMemory: 5.49GiB
2018-01-15 11:34:59.961674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
E0115 11:35:00.020327488   36133 ev_epoll1_linux.c:1051]     grpc epoll fd: 22
2018-01-15 11:35:00.026716: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> node06:2222}
2018-01-15 11:35:00.026760: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> node07:2223, 1 -> localhost:2224}
2018-01-15 11:35:00.029261: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2224
2018-01-15 11:35:00.439045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732
pciBusID: 0000:08:00.0
totalMemory: 5.57GiB freeMemory: 5.49GiB
2018-01-15 11:35:00.439124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
E0115 11:35:00.497022377   13701 ev_epoll1_linux.c:1051]     grpc epoll fd: 22
2018-01-15 11:35:00.503585: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}
2018-01-15 11:35:00.503622: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> node07:2223, 1 -> node08:2224}
2018-01-15 11:35:00.505803: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222
2018-01-15 11:33:39.681311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732
pciBusID: 0000:08:00.0
totalMemory: 5.57GiB freeMemory: 5.49GiB
2018-01-15 11:33:39.681375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
E0115 11:33:39.739196190   46236 ev_epoll1_linux.c:1051]     grpc epoll fd: 22
2018-01-15 11:33:39.745655: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> node06:2222}
2018-01-15 11:33:39.745697: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> node08:2224}
2018-01-15 11:33:39.747692: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223
Abid Malik
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
Variables initialized ...
Traceback (most recent call last):
  File ""tf_dis_2.py"", line 102, in <module>
    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=""/tmp/train_logs"",global_step=global_step,init_op=init_op)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 336, in __init__
    self._verify_setup()
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 885, in _verify_setup
    ""their device set: %s"" % op)
ValueError: When using replicas, all Variables must have their device set: name: ""weights/Variable""
op: ""VariableV2""
attr {
  key: ""container""
  value {
    s: """"
  }
}
attr {
  key: ""dtype""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""shape""
  value {
    shape {
      dim {
        size: 784
      }
      dim {
        size: 100
      }
    }
  }
}
attr {
  key: ""shared_name""
  value {
    s: """"
  }
}

2018-01-15 11:33:41.719083: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: Endpoint read failed
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
Variables initialized ...
Traceback (most recent call last):
  File ""tf_dis_2.py"", line 114, in <module>
    with sv.prepare_or_wait_for_session(server.target) as sess:
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 708, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 273, in prepare_session
    config=config)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 205, in _restore_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1666, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnavailableError: Endpoint read failed
srun: error: node08: task 2: Exited with exit code 1
srun: error: node07: task 1: Exited with exit code 1
---------------------------------------------------------------------------------

Why is it crashing? I have been trying to solve this for the last three weeks by putting it on different forums and groups. However, could not get any reply. I would be grateful if someone can guide me. I apologize in advance if this is not the right forum.





### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

``
from __future__ import print_function

import tensorflow as tf
import sys
import time


print(""Abid Malik"")


parameter_servers = [""node06:2222""]
workers = [""node07:2223"",""node08:2224""]
cluster = tf.train.ClusterSpec({""ps"":parameter_servers, ""worker"":workers})



tf.app.flags.DEFINE_string(""job_name"", """", ""Either 'ps' or 'worker'"")
tf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")
FLAGS = tf.app.flags.FLAGS





server = tf.train.Server(
    cluster,
    job_name=FLAGS.job_name,
    task_index=FLAGS.task_index)


batch_size = 100
learning_rate = 0.0005
training_epochs = 20
logs_path = ""/tmp/mnist/1""


from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

if FLAGS.job_name == ""ps"":
    server.join()
elif FLAGS.job_name == ""worker"":

        with tf.device(tf.train.replica_device_setter(worker_device=""/job:worker/task:%d"" % FLAGS.task_index,cluster=cluster)):
              
                global_step = tf.get_variable('global_step',[],initializer = tf.constant_initializer(0), trainable = False)

              
        with tf.name_scope('input'):
              
                  x = tf.placeholder(tf.float32, shape=[None, 784], name=""x-input"")
               
                  y_ = tf.placeholder(tf.float32, shape=[None, 10], name=""y-input"")

                
        tf.set_random_seed(1)
        with tf.name_scope(""weights""):
                        W1 = tf.Variable(tf.random_normal([784, 100]))
                        W2 = tf.Variable(tf.random_normal([100, 10]))

               
        with tf.name_scope(""biases""):
                        b1 = tf.Variable(tf.zeros([100]))
                        b2 = tf.Variable(tf.zeros([10]))

               
        with tf.name_scope(""softmax""):
                        # y is our prediction
                        z2 = tf.add(tf.matmul(x,W1),b1)
                        a2 = tf.nn.sigmoid(z2)
                        z3 = tf.add(tf.matmul(a2,W2),b2)
                        y  = tf.nn.softmax(z3)

               
        with tf.name_scope('cross_entropy'):
                        # this is our cost
                        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))

             
        with tf.name_scope('train'):
                       
                                                                                                                                                                                                                                                                

grad_op = tf.train.GradientDescentOptimizer(learning_rate)
                        train_op = grad_op.minimize(cross_entropy, global_step=global_step)


        with tf.name_scope('Accuracy'):
                        # accuracy
                        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
                        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

   
        tf.summary.scalar(""cost"", cross_entropy)
        tf.summary.scalar(""accuracy"", accuracy)

        saver = tf.train.Saver()
       
        summary_op = tf.summary.merge_all()
        init_op = tf.global_variables_initializer()
        print(""Variables initialized ..."")

        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=""/tmp/train_logs"",global_step=global_step,init_op=init_op)


        begin_time = time.time()
        frequency = 100
        with sv.prepare_or_wait_for_session(server.target) as sess:
                # create log writer object (this will log on every machine)
                writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())

                # perform training cycles
                start_time = time.time()
                for epoch in range(training_epochs):

                        # number of batches in one epoch
                        batch_count = int(mnist.train.num_examples/batch_size)

                        count = 0
                        for i in range(batch_count):
                                batch_x, batch_y = mnist.train.next_batch(batch_size)

                                # perform the operations we defined earlier on batch
                                _, cost, summary, step = sess.run([train_op, cross_entropy, summary_op, global_step], feed_dict={x: batch_x, y_: batch_y})
                                writer.add_summary(summary, step)

                                count += 1
                                if count % frequency == 0 or i+1 == batch_count:
                                        elapsed_time = time.time() - start_time
                                        start_time = time.time()
                                        print(""Step: %d,"" % (step+1),
                                                                "" Epoch: %2d,"" % (epoch+1),
                                                                "" Batch: %3d of %3d,"" % (i+1, batch_count),
                                                                "" Cost: %.4f,"" % cost,
                                                                "" AvgTime: %3.2fms"" % float(elapsed_time*1000/frequency))
                                        count = 0


                print(""Test-Accuracy: %2.2f"" % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
                print(""Total Time: %3.2fs"" % float(time.time() - begin_time))
                print(""Final Cost: %.4f"" % cost)

        sv.stop()
        print(""done"")
                                                                                                                                                                                                                                                                 

``",0,,4,2018-01-15T18:05:52Z,2018-01-23T20:26:01Z,NONE,2018-01-21T00:55:39Z
16132,Bug while printing parameters and gradients,type:support,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary (anaconda)
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: using CPU
- **GPU model and memory**: using CPU
- **Exact command to reproduce**: see below

### Describe the problem
The model is very simple, I do digits classification with MNIST. There is only one parameter matrix W, no bias and no non-linearities. The model show convergence since the loss is decreasing. I checked predictions and accuracy but I do not copy paste useless code here. If I print the parameters before and after training they are the same, however, it shouldn't be the case. Moreover, the gradient of the loss w.r.t. parameters are zero but again it shouldn't be the case since the model converges so there should be a non-zero gradient. I cannot explain why and my implementation seems correct, that's why I am posting my code here.

### Source code / logs
```
import numpy as np
import tensorflow as tf

tf.set_random_seed(42)

from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets('data/', one_hot=True)

x = tf.placeholder(tf.float32, shape=(None, 784))
y = tf.placeholder(tf.float32, shape=(None, 10))

W = tf.get_variable('W0', (784, 10))
pred = tf.matmul(x, W)
loss = tf.reduce_sum((y - pred) ** 2)
grads = tf.gradients(loss, W)
train_step = tf.train.AdamOptimizer().minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

print(sess.run(W))

>>> [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605
  -0.01042821]
 [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129
  -0.0796528 ]
 [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736
  -0.04312544]
 ...
 [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174
   0.06578781]
 [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339
  -0.02337921]
 [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324
  -0.02436799]]

for _ in range(1000):
    x_mb, y_mb = mnist.train.next_batch(32)
    loss_, _ = sess.run([loss, train_step], {x: x_mb, y: y_mb})
    print('loss: {:2.5}'.format(loss_))

>>> I won't print uselss log here but the loss is decreasing

print(sess.run(W))

>>> [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605
  -0.01042821]
 [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129
  -0.0796528 ]
 [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736
  -0.04312544]
 ...
 [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174
   0.06578781]
 [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339
  -0.02337921]
 [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324
  -0.02436799]]

print(sess.run(grads, {x: x_mb, y: y_mb}))

>>> [array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)]

```
",0,,1,2018-01-15T16:08:51Z,2018-01-19T22:26:56Z,NONE,2018-01-19T22:26:56Z
16131,Update contrib/HVX readme,"awaiting testing (then merge),cla: yes","I updated the README because of some imprecisions and to add clarifications of what I think will guide the users more appropriately.

First, the very simple ""quick start guide"" doesn't work, there's no ""-X"" option (at least publicly) and so you always need to have the SDK installed manually.

Apart from that, some clarifications and rewording were done to help the users understand what's happening.

/cc @satok16 ",1,,2,2018-01-15T15:59:50Z,2018-01-26T16:39:08Z,CONTRIBUTOR,2018-01-23T18:04:35Z
16130,Fix broken python3 build,"awaiting testing (then merge),cla: yes","Currently building tensorflow master branch with python3 fails with following error message.
```
ERROR: ${BAZEL_CACHE}/external/astor_archive/BUILD:8:1: Converting to Python 3: external/astor_archive/astor/code_gen.py failed (Exit 1).
```
It seems that the 3 newly added `third_party/*.BUILD` scripts from https://github.com/tensorflow/tensorflow/pull/15955/commits/4080654c8f03ec34f2822c14db5fd8b75f63d569 are missing `srcs_version = ""PY2AND3""` part, which all the other py_library modules have.

I'm using bazel 0.5.4 on linux ubuntu 16.04 to build the current master branch.",0,,2,2018-01-15T11:00:22Z,2018-01-15T17:30:36Z,CONTRIBUTOR,2018-01-15T16:09:15Z
16127,"fix default parameters for TimeFreqLSTMCell, fixes #16100",cla: yes,"Resolve #16100 

The default parameters for TimeFreqLSTMCell lead to a division by `None`, which throws an exception.",0,,5,2018-01-15T10:24:06Z,2018-01-22T20:27:14Z,CONTRIBUTOR,2018-01-15T10:26:15Z
16125,Disable stacktrace_handler_test becase stack trace isn't generated on Windows,cla: yes,Fix http://ci.tensorflow.org/job/tf-master-win-bzl/2259/console,0,,3,2018-01-15T08:16:44Z,2018-01-15T08:57:31Z,MEMBER,2018-01-15T08:18:41Z
16124,How can I batch images of arbitrary sizes in tensorflow?,stat:awaiting response,I want to realize arbitrary inputs that I can batch them in one batch.,0,,4,2018-01-15T07:02:24Z,2018-01-16T15:22:27Z,NONE,2018-01-15T18:59:25Z
16119,Created dense_to_sparse in contrib.layers,"awaiting testing (then merge),cla: yes",Added `dense_to_sparse`. This does the conversion of dense labels into sparse ones to be passed into the core ctc_loss function. Addresses feature request https://github.com/tensorflow/tensorflow/issues/15985,1,,7,2018-01-15T04:41:50Z,2018-01-25T01:21:52Z,CONTRIBUTOR,2018-01-23T00:14:50Z
16118,Minor improvements to TFRecord format docs,"awaiting testing (then merge),cla: yes","The TFRecord format documentation mentions that hashes are computed using a CRC32, but doesn't mention the polynomial used. I added that detail, so the documentation is now sufficient for a developer trying to write a parser / writer for (uncompressed) TFRecord files.",1,,3,2018-01-15T01:26:53Z,2018-01-23T18:29:39Z,CONTRIBUTOR,2018-01-22T23:12:57Z
16114,Update maxout.py,"awaiting testing (then merge),cla: yes,stat:awaiting response",Specify the final number of features in the maxout axis,1,,7,2018-01-14T22:37:39Z,2018-02-01T05:56:45Z,NONE,2018-01-29T21:51:45Z
16113,Propagate the error string of GIF processing for decode_gif,"cla: yes,stat:awaiting response","This fix tries to improve the error thrown by `decode_gif` to include the error string generated by GIF processing.

Previously, the error was not very indicative as the error string
returned by GIF processing was hidden:
```
..........
InvalidArgumentError (see above for traceback): Invalid GIF data, size 2091369
	 [[Node: DecodeGif = DecodeGif[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile)]]
```

This fix propagate the error string (`can't process optimized gif`) to be part of the `InvalidArgumentError`:
```
InvalidArgumentError (see above for traceback): Invalid GIF data (size 2091369), can't process optimized gif
         [[Node: DecodeGif = DecodeGif[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile)]]
```

This fix fixes #15838.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,1,2018-01-14T18:39:01Z,2018-01-23T18:01:19Z,MEMBER,2018-01-23T00:20:48Z
16108,No tf.metrics.true_negatives,,"**TensorFlow version**: 1.4.1

Is there any particular reason for why there is no `tf.metrics.true_negatives` method? I know it's simple to calculate from other confusion metrics that are available, but I was wondering why the developers chose to let this one method out.",1,,5,2018-01-14T09:47:59Z,2018-01-31T19:02:00Z,NONE,2018-01-14T18:55:05Z
16106,Eager: Invalid placement of vars/consts depending on their types and not the tf.device,,"Hi,
I'm currently testing eager execution on TF 1.5.0-rc1 (built it with XLA and CUDA enabled) and observe strange behavior: variables and constants get created either on GPU or CPU depending on their types, and not `with tf.device(...):` block. Moreover, on creation of int32 variable it fails completely. For example, when I run the following code

```
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

print('TensorFlow version:', tf.__version__)

with tf.device('/gpu:0'):
    A = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)
    print('Const A is placed on:', A.device)

    B = tf.constant([1, 2, 3], dtype=tf.int32)
    print('Const B is placed on:', B.device)

    C = tfe.Variable([1.0, 2.0, 3.0], dtype=tf.float32)
    print('Variable C is placed on:', C.device)

    D = tfe.Variable([1, 2, 3], dtype=tf.int32)
    print('Variable D is placed on:', D.device)
```

I get the following output:

```
TensorFlow version: 1.5.0-rc1
2018-01-14 01:16:06.385929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-14 01:16:06.386198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.91GiB freeMemory: 363.06MiB
2018-01-14 01:16:06.386223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Const A is placed on: /job:localhost/replica:0/task:0/device:GPU:0
Const B is placed on: CPU:0
Variable C is placed on: /job:localhost/replica:0/task:0/device:GPU:0
Traceback (most recent call last):
  File ""tf_bug.py"", line 18, in <module>
    D = tfe.Variable([1, 2, 3], dtype=tf.int32)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 277, in __init__
    constraint=constraint)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 422, in _init_from_args
    graph_mode=self._in_graph_mode)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 53, in _eager_safe_variable_handle
    container=container)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py"", line 396, in var_handle_op
    attrs=_attrs, ctx=_ctx, name=name)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'VarHandleOp' OpKernel for GPU devices compatible with node VarHandleOp = VarHandleOp[container=""eager-execution-2/"", dtype=DT_INT32, shape=[3], shared_name=""11""]()
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; dtype in [DT_VARIANT]
  device='GPU'; dtype in [DT_COMPLEX128]
  device='GPU'; dtype in [DT_COMPLEX64]
  device='GPU'; dtype in [DT_BOOL]
  device='GPU'; dtype in [DT_DOUBLE]
  device='GPU'; dtype in [DT_FLOAT]
  device='GPU'; dtype in [DT_HALF]
  device='CPU'
  device='XLA_GPU'
  device='XLA_CPU'
 [Op:VarHandleOp] name: Variable/
```

As you can see, the constants and variables get placed either on GPU:0 or CPU:0 despite all of them gathered inside the same `tf.device('/gpu:0')` block.",0,,14,2018-01-13T20:28:29Z,2018-01-25T00:10:50Z,NONE,2018-01-14T06:54:09Z
16103,No OpKernel was registered to support Op 'AssignVariableOp' with DT_BFLOAT16,"stat:awaiting response,type:support","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch linux
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.5.0-rc1
- **Python version**: NA (go bindings)
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 7.2.1
- **CUDA/cuDNN version**: na (CPU)
- **GPU model and memory**: na
- **Exact command to reproduce**: See below

### Describe the problem
`AssignVariableOp` does not appear to appear to have a kernel for `DT_BFLOAT16`.


### Source code / logs
```
package main

import (
	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func main() {
	s := op.NewScope()
	bfloat := op.Cast(s, op.Const(s, float32(0.1234)), tf.Bfloat16)
	variable := op.VarHandleOp(s, tf.Bfloat16, tf.ScalarShape())
	init := op.AssignVariableOp(s, variable, bfloat)

	graph, err := s.Finalize()
	if err != nil {
		panic(err)
	}
	sess, err := tf.NewSession(graph, nil)
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{init})
	if err != nil {
		panic(err)
	}
}
```
```
go run bfloat_demo.go 
panic: No OpKernel was registered to support Op 'AssignVariableOp' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; dtype in [DT_VARIANT]
  device='CPU'; dtype in [DT_QINT32]
  device='CPU'; dtype in [DT_QUINT8]
  device='CPU'; dtype in [DT_QINT8]
  device='CPU'; dtype in [DT_RESOURCE]
  device='CPU'; dtype in [DT_STRING]
  device='CPU'; dtype in [DT_BOOL]
  device='CPU'; dtype in [DT_COMPLEX128]
  device='CPU'; dtype in [DT_COMPLEX64]
  device='CPU'; dtype in [DT_DOUBLE]
  device='CPU'; dtype in [DT_FLOAT]
  device='CPU'; dtype in [DT_HALF]
  device='CPU'; dtype in [DT_INT8]
  device='CPU'; dtype in [DT_UINT8]
  device='CPU'; dtype in [DT_INT16]
  device='CPU'; dtype in [DT_UINT16]
  device='CPU'; dtype in [DT_INT32]
  device='CPU'; dtype in [DT_INT64]

	 [[Node: AssignVariableOp = AssignVariableOp[dtype=DT_BFLOAT16](VarHandleOp, Cast)]]

goroutine 1 [running]:
main.main()
	/home/isaac/go/src/github.com/is8ac/gotf/bfloat_demo.go:24 +0x250
exit status 2
```",0,,2,2018-01-13T17:45:32Z,2018-01-20T00:39:43Z,NONE,2018-01-19T22:34:16Z
16101,Add stream selection support for `tf.contrib.ffmpeg.decode_audio`,"awaiting review,cla: yes,stat:awaiting tensorflower","This fix tries to address the issue raised in #16073 where it was not possible to selectively decode a perticular stream with `tf.contrib.ffmpeg.decode_audio`.
This fix adds an additional attribute `stream` which could be used to specify the stream to decode (e.g., `stream=""1""`). By default `stream=""""` which leaves the decision to ffmpeg.

This fix fixes #16073.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,11,2018-01-13T15:50:52Z,2018-01-23T22:21:40Z,MEMBER,2018-01-13T16:34:12Z
16100,Exception when not providing optional parameter frequency_skip in TimeFreqLSTMCell,"stat:contributions welcome,type:bug/performance","### System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below
- OS Platform and Distribution: 
- TensorFlow installed from: `pip3 install --user tensorflow-gpu`
- TensorFlow version: 1.4.1
- Python version: 3.5.2
- CUDA: 8.0
- GPU: NVidia Titan X

### Describe the problem

Using a `TimeFreqLSTMCell` in a `dynamic_rnn` or `static_rcnn` without providing the optional parameter `frequency_skip` results in an exception:

```
TypeError: unsupported operand type(s) for /: 'int' and 'NoneType'
```

The line which throws this exception is https://github.com/tensorflow/tensorflow/blob/8b78c23c161c9d0bec462d5f4c73f0fca413bc8b/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L474-L475
`frequency_skip` has it's default value `None` here.

Maybe the default should be changed to `1`?

### Source code / logs

Sadly I am not allowed to share my full source code. However, this is how I create the RNN layers:

```
lstmcell = tf.contrib.rnn.TimeFreqLSTMCell(lstm_input.shape.as_list()[2], forget_bias = self.lstm_forget_bias, feature_size = lstm_input_rev.shape.as_list()[2])
                
stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstmcell] * self.layers_lstm)
                
lstm_output, lstm_state = tf.nn.dynamic_rnn(stacked_lstm, lstm_input_rev, dtype=""float32"", time_major=True)
```",1,,1,2018-01-13T12:20:41Z,2018-01-22T20:27:14Z,CONTRIBUTOR,2018-01-13T15:02:35Z
16099,Make srcd in variable,"awaiting testing (then merge),cla: yes",,0,,3,2018-01-13T10:00:11Z,2018-01-17T01:52:16Z,CONTRIBUTOR,2018-01-16T21:06:32Z
16096,Address bad merge in Java install instructions,cla: yes,,0,,2,2018-01-13T07:25:31Z,2018-01-13T18:36:04Z,MEMBER,2018-01-13T08:07:42Z
16094,Shape must be rank 1 but is rank 0 for 'CTCLoss' (op: 'CTCLoss'),stat:awaiting response,"Have I written custom code: yes
OS: Windows 8.1
Tensorflow installed from: conda
Tensorflow version: 1.4


I've successfully converted a Tensor into a SparseTensor with this code:

```
def dense_to_sparse(dense_tensor, out_type):
    indices = tf.where(tf.not_equal(dense_tensor, tf.constant(0, dense_tensor.dtype)
    values = tf.gather_nd(dense_tensor, indices)
    shape = tf.shape(dense_tensor, out_type=out_type)
    return tf.SparseTensor(indices, values, shape)
```

I want to try out using a `SparseTensor` converted from a dense one: 

```
input_layer = tf.placeholder(tf.float32, [None, 1596, 48])
dense_labels = tf.placeholder(tf.int32)
sparse_from_dense = dense_to_sparse(dense_lables, out_type=tf.int64)
cell_fw = grid_rnn.Grid2LSTMCell(num_units=128)
cell_bw = grid_rnn.Grid2LSTMCell(num_units=128)
bidirectional_grid_rnn = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, input_layer, dtype=tf.float32)
outputs = tf.reshape(bidirectional_grid_rnn[0], [-1, 256])

W = tf.Variable(tf.truncated_normal([256, 80], stddev=0.1, dtype=tf.float32), name='W')
b = tf.Variable(tf.constant(0., dtype=tf.float32, shape=[80], name='b'))

logits = tf.matmul(outputs, W) + b
logits = tf.reshape(logits, [tf.shape(input_layer)[0], -1, 80])
logits = tf.transpose(logits, (1, 0, 2))

loss = tf.nn.ctc_loss(inputs=logits, labels=sparse, sequence_length=320)
```

Unfortunately, when I do this, I encounter this error:

`Shape must be rank 1 but is rank 0 for 'CTCLoss' (op: 'CTCLoss') with input shapes: [?,?,80], [?,1], [?], [].`

",0,,3,2018-01-13T04:41:46Z,2018-01-13T23:20:07Z,CONTRIBUTOR,2018-01-13T12:53:28Z
16093,when will we have multi gpu support under eager mode? Pytorch has it.,type:feature,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,3,2018-01-13T03:22:07Z,2018-01-17T18:51:42Z,NONE,2018-01-17T18:50:12Z
16090,MKL-DNN:  fix batchnorm unit test failures,"awaiting testing (then merge),cla: yes","Fix failures of all 9 fuse batchnorm test cases.

handle corner case (empty input tensors)
handle inference case properly - bwd bug related to fwd primitive creation (as a hint)
refactor - moving output tensor allocation to separate methods - to avoid duplicated code",0,,2,2018-01-13T01:02:12Z,2018-01-17T05:49:23Z,CONTRIBUTOR,2018-01-16T16:50:19Z
16088,Disable keras data_utils_test as it's flaky.,"awaiting review,cla: yes",,1,,5,2018-01-12T23:44:10Z,2018-01-23T07:01:30Z,MEMBER,2018-01-12T23:49:57Z
16086,[Intel MKL-DNN] fixes for several MKLDNN unit tests.,"awaiting testing (then merge),cla: yes",Current MKLDNN element wise (add) results in several unit test failure. A temporary workaround is provided by comment out the MKLDNN element wise (add) optimization. ,0,,2,2018-01-12T22:49:26Z,2018-01-17T05:47:39Z,CONTRIBUTOR,2018-01-16T16:48:04Z
16084,Update download_dependencies.sh to prevent crash from 403,"awaiting testing (then merge),cla: yes","The eigen bitbucket seems to have changed causing the scrip to crash with a unrecognized archive error.
Changing to grep -v mirror.bazel seems to fix this because otherwise we get a 403 forbidden error.",1,,2,2018-01-12T22:20:03Z,2018-01-23T18:28:28Z,CONTRIBUTOR,2018-01-23T18:28:46Z
16081,MKL-DNN: fix concat issue related to negative input concat_dim,"awaiting testing (then merge),cla: yes","For a negative concat_dim input, the actual concat_dim should be N + concat_dim with N
being the dims of input tensors.

This PR fixes an issue of setting N properly.",0,,2,2018-01-12T20:40:28Z,2018-01-17T06:50:27Z,CONTRIBUTOR,2018-01-16T16:51:11Z
16079,Branch 181765083,cla: yes,,0,,2,2018-01-12T19:16:43Z,2018-01-12T21:26:35Z,MEMBER,2018-01-12T19:17:35Z
16075,optimize_for_inference_lib.fold_batch_norms() preserves data_format,"awaiting testing (then merge),cla: yes","`fold_batch_norms()` currently breaks graphs containing convolutions using NCHW data format. The function replaces a BiasAdd operation with a new one, while not preserving the data format of the original operation. As a result, the new operation always has NHWC data format, and the execution of the resulting graph fails because of mismatching dimensions.

The proposed resolution is to copy the `data_format` property from the original operation.

The patch fixes https://github.com/tensorflow/tensorflow/issues/15034.",1,,10,2018-01-12T17:07:04Z,2018-01-31T23:02:26Z,CONTRIBUTOR,2018-01-14T12:10:31Z
16073,Feature request: (optionally) return all audio streams in tf.contrib.ffmpeg.decode_audio,type:feature,"I'm trying to read [musdb18](https://sigsep.github.io/musdb.html) with `tf.data` and it comes in the form of mp4 files with multiple audio streams so `ffmpeg -map` is needed.

`tf.contrib.ffmpeg.decode_audio` cannot be configured to choose the audio stream. I wonder if we could have a `tf.contrib.ffmpeg.decode_audios` that returns every audio stream in the file, or if we could have a new argument in `tf.contrib.ffmpeg.decode_audio` for choosing streams.

Being able to choose the audio stream is also important for getting the right language in a movie file's audio, and similarly `tf.contrib.ffmpeg.decode_video` could need the same extension (though multiple video streams is not as common AFAIK).",0,,6,2018-01-12T16:28:55Z,2018-01-23T22:21:40Z,CONTRIBUTOR,2018-01-12T16:30:59Z
16071,fix comments and code matches,"awaiting review,cla: yes",,0,,2,2018-01-12T13:39:17Z,2018-01-15T16:06:45Z,CONTRIBUTOR,2018-01-14T02:23:16Z
16069,Key generator/encoder_8/conv/filter not found in checkpoint,type:support,"I'm using python 3.6.3 win 10 64bit and tensorflow 1.2.1 and now I'm working on https://github.com/datitran/face2face-demo project and **part 4.export model** I'm taking this error:NotFoundError (see above for traceback): Key generator/encoder_8/conv/filter not found in checkpoint 

how can I solve this problem ?

what I run
`C:\Users\hajum>python C:\Users\hajum\Desktop\face2face-demo-master\reduce_model.py --model-input C:\Users\hajum\Desktop\face2face-model --model-output C:\Users\hajum\Desktop\face2face-reduced-model`

same folder names with project but I have my own models

what it shows
```
2018-01-12 13:24:05.337267: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.337407: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.338476: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.338779: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339070: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339369: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339659: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339962: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.779044: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_8/conv/filter not found in checkpoint
2018-01-12 13:24:05.779482: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_2/deconv/filter not found in checkpoint
2018-01-12 13:24:05.779562: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_1/deconv/filter not found in checkpoint
2018-01-12 13:24:05.780339: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_3/deconv/filter not found in checkpoint
2018-01-12 13:24:05.780354: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_3/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.781063: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_2/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.781066: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_3/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.781015: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_2/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.784971: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_8/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.785703: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_4/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.785849: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_4/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.785928: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_4/deconv/filter not found in checkpoint
2018-01-12 13:24:05.787052: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_5/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.787160: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_6/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.787346: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_5/deconv/filter not found in checkpoint
2018-01-12 13:24:05.787687: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_5/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.791739: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_8/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.793255: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_6/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.793508: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_6/deconv/filter not found in checkpoint
2018-01-12 13:24:05.794303: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_7/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.795123: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_7/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.795823: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_8/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.796067: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_7/deconv/filter not found in checkpoint
2018-01-12 13:24:05.797352: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_8/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.798112: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_7/conv/filter not found in checkpoint
2018-01-12 13:24:05.800556: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_1/conv/filter not found in checkpoint
2018-01-12 13:24:05.801703: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_2/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.801868: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_8/deconv/filter not found in checkpoint
2018-01-12 13:24:05.801974: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_2/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.801977: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_2/conv/filter not found in checkpoint
2018-01-12 13:24:05.804154: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_3/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.805983: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_3/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.806160: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_7/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.807834: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_4/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.808628: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_3/conv/filter not found in checkpoint
2018-01-12 13:24:05.808808: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_5/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.809721: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_4/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.809836: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_4/conv/filter not found in checkpoint
2018-01-12 13:24:05.810842: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_5/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.811998: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_5/conv/filter not found in checkpoint
2018-01-12 13:24:05.812062: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_7/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.812846: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_6/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.812889: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_6/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.813195: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_6/conv/filter not found in checkpoint
Traceback (most recent call last):
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1139, in _do_call
    return fn(*args)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1121, in _run_fn
    status, run_metadata)
  File ""C:\Users\hajum\Anaconda3\lib\contextlib.py"", line 88, in __exit__
    next(self.gen)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Key generator/encoder_8/conv/filter not found in checkpoint
         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\hajum\Desktop\face2face-demo-master\reduce_model.py"", line 215, in <module>
    saver.restore(sess, checkpoint)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1548, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 789, in run
    run_metadata_ptr)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 997, in _run
    feed_dict_string, options, run_metadata)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1132, in _do_run
    target_list, options, run_metadata)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key generator/encoder_8/conv/filter not found in checkpoint
         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]

Caused by op 'save/RestoreV2_43', defined at:
  File ""C:\Users\hajum\Desktop\face2face-demo-master\reduce_model.py"", line 213, in <module>
    saver = tf.train.Saver()
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1139, in __init__
    self.build()
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1170, in build
    restore_sequentially=self._restore_sequentially)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 691, in build
    restore_sequentially, reshape)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 407, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 247, in restore_op
    [spec.tensor.dtype])[0])
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 640, in restore_v2
    dtypes=dtypes, name=name)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

NotFoundError (see above for traceback): Key generator/encoder_8/conv/filter not found in checkpoint
         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]
```",0,,4,2018-01-12T11:30:28Z,2018-01-16T23:01:50Z,NONE,2018-01-16T23:01:49Z
16067,[XLA] Separate out the dynamic slice wrapping tests,"awaiting testing (then merge),cla: yes","This is a set of changes to allow disabling of bfloat16 tests, for backends which don't support bfloat16.  Originally it was a change to the same set of tests to allow the wrapping behaviour tests to be disabled.  That change was made obsolete by some parallel work.

---
The original text was:

The XLA documentation says that the behaviour of dynamic slice and dynamic update slice is undefined when the indices wrap.

This separates out the tests which check for wrapping behaviour, so that they can be ignored for backends which don't exhibit the test's expected results.
",0,,5,2018-01-12T09:26:37Z,2018-01-20T22:46:05Z,CONTRIBUTOR,2018-01-15T13:50:52Z
16066,the loss is nan,,"when i training the facenet(build by myself) the loss is normal on the first iteration, but on the second and following iteration ,the loss became nan, i don't know what happened, please help me, Thanks!!!",0,,2,2018-01-12T05:50:20Z,2018-01-13T03:50:46Z,NONE,2018-01-13T03:50:46Z
16060,Branch 181679271,cla: yes,Merging internal changes,0,,1,2018-01-12T01:17:21Z,2018-01-12T04:00:51Z,CONTRIBUTOR,2018-01-12T03:18:38Z
16059,[Intel MKL] Fixes for various MKLDNN unit test failures,"awaiting testing (then merge),cla: yes","1. MklLayout pass changes

   Making workspace type uint8 for MaxPool; Handling duplicate control edge insertion

   1) Handles case of inserting duplicate control edge (fixing Mkl layout graph
   pass unit test)
   2) Enables uint8 as workspace tensor type (makes consistent with LRN workspace
   handling)

   Workspace tensor type change is also performed in MaxPool and MaxPoolGrad
   operators.

2. Handling MklReshape failing case

   MklReshape was failing on a unit test when Mkl layout and Tensorflow layout for
   input tensors were same, but shape of input tensor and output tensor was
   different. No reorder is required in such case, but reshape is needed. Before
   this fix, we were asserting that reorder is performed.

3. Adding support for empty input/filter tensors in Convolution backprop operators",0,,2,2018-01-12T00:47:26Z,2018-01-17T05:48:52Z,CONTRIBUTOR,2018-01-12T05:09:32Z
16058,How to initialize embeddings layer within Estimator API?,"stat:awaiting response,type:bug/performance","I'm trying to use existing embeddings within tensorflow model, the size of embedding is greater than 2Gb and this makes my original try of doing this unsuccessful:

```
embedding_var = tf.get_variable(
        ""embeddings"", 
        shape=GLOVE_MATRIX.shape, 
        initializer=tf.constant_initializer(np.array(GLOVE_MATRIX))
)
```
Which gave me this error:

` Cannot create a tensor proto whose content is larger than 2GB.`

I'm using AWS SageMaker, which based on the Estimator API, and the actual running of the graph in session happens behind the scene, so I'm not sure how to initialize some placeholders for embedding given that. Would be helpful if someone will be able to share the way how to do such initialization in term of EstimatorAPI.

--------------------------

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,3,2018-01-12T00:22:23Z,2018-01-17T21:04:57Z,CONTRIBUTOR,2018-01-12T21:24:07Z
16057,Make platform a proper module,cla: yes,"This fixes an issue where the nice error about importing tensorflow from the TF source directory is not displayed in Python 2.7

Fixes #16019 ",1,,6,2018-01-11T22:58:56Z,2018-01-23T20:00:35Z,NONE,2018-01-22T23:41:35Z
16056,Apply 1.5-rc1 cherry-picks.,cla: yes,,0,,4,2018-01-11T22:49:23Z,2018-01-12T00:38:14Z,MEMBER,2018-01-11T22:58:56Z
16055,MKL: Fixed 3 bugs picked up by the unit tests,cla: yes,"- There were 2 kinds of registrations for MatMul - with and without the 'eigen' label. Re-added the registrations with the 'eigen' label when MKL is used. 
- Removed the ifdef that removed the check for the label when MKL was used. The eigen op should be called when the eigen label is used.
- In the selective registration header test, unicode strings aren't handled correctly, so there's a ""u"" before the kernel class string that is compared to the hardcoded string. This has been fixed.
```
- [('BiasAdd', 'BiasOp<CPUDevice, float>'), 
+ [('BiasAdd', u'BiasOp<CPUDevice, float>'), 
```",0,,2,2018-01-11T22:10:37Z,2018-01-12T20:52:14Z,CONTRIBUTOR,2018-01-12T05:07:01Z
16047,Branch 181629980,cla: yes,,0,,2,2018-01-11T19:10:21Z,2018-01-12T02:45:09Z,MEMBER,2018-01-11T19:10:49Z
16046,Feature Request: clarify supported environments for official binaries.,,"As it stands now, binary release of TensorFlow 1.5 is set to drop compatibility with Ubuntu 14.04 ( https://github.com/tensorflow/tensorflow/issues/15777), and compatibility with Debian Linux distros, such as Amazon Linux AMI (`ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found`).

To avoid surprise, TensorFlow should either:
1. Follow other open-source projects like Ray/PyTorch and provide official binaries for these systems
or
2. Document that support is dropped, to encourage other players (ie, AWS) to take over the job of providing these binaries

@martinwicke",0,,11,2018-01-11T18:13:57Z,2018-01-31T04:22:09Z,CONTRIBUTOR,2018-01-11T18:58:25Z
16041,Code documentation for `confusion_matrix.py` misleading,stat:awaiting tensorflower,"### Describe the problem

The documentation for `confusion_matrix.py` says:

```
  Args:
    labels: 1-D `Tensor` of real labels for the classification task.
    predictions: 1-D `Tensor` of predictions for a given classification.
```

, however I found that those two arguments are simply python arrays and not Tensors. The following trial test code demonstrates this. As a TF/Python newbie, I'm wondering if this is actually a real issue, and if so I'll create a PR to correct it to prevent confusion to future programmers.

### Source code / logs

```
import tensorflow as tf

y_ = [0, 2, 2, 2]
y = [2, 1, 2, 2]

with tf.Session() as sess:
    confusion_matrix = tf.confusion_matrix(labels=y_, predictions=y, num_classes=4)
    confusion_matrix_to_Print = sess.run(confusion_matrix)
    print(confusion_matrix_to_Print)

```",0,,2,2018-01-11T14:25:22Z,2018-01-12T02:11:58Z,NONE,2018-01-12T02:04:25Z
16039,How TF-Detect draw a rectangular?,,"How TF-Detect draw a rectangular?
I can't find the corresponding code?
Is it calling OpenGL to draw a rectangular?",0,,4,2018-01-11T13:12:20Z,2018-01-29T22:16:53Z,NONE,2018-01-12T01:03:25Z
16036,"raise PiCameraMMALError(status, prefix) picamera.exc.PiCameraMMALError: Failed to enable connection: Out of resources",stat:awaiting response,,0,,4,2018-01-11T11:14:55Z,2018-01-12T06:55:18Z,NONE,2018-01-11T19:01:23Z
16031,tf.data.Dataset.padded_batch() doesn't work with dataset.map using tf.py_func,stat:awaiting response,"
------------------------

### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: CentOS Linux release 7.2.1511
- **TensorFlow installed from**: pip
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.5

### Describe the problem
It's quite common for NLP tasks to read variable-length sentences from text files, to map them and to padd them. But Dataset.padded_batch() doesn't work with tf.dataset which uses map (tf.py_func)

### Source code
```
import tensorflow as tf                                                                           
import numpy as np                                                                                
                                                                                                  
def convert(line):                                                                                
    tokens = line.split()                                                                 
    return np.array(tokens, dtype=np.int32)

# Simulating reading variable-length sentences from a file. Using TextLineDataset will have the same problem
dataset = tf.data.Dataset.from_tensor_slices([""1 2 3"", ""4 5""])         
                           
# Tokenize each sentence and convert it to list of int
dataset = dataset.map(lambda line: tf.py_func(convert, [line], [tf.int32]))     
                  
dataset = dataset.padded_batch(1, [None]) # This line doesn't work whatever the batch_size is
# dataset = dataset.batch(1) # This line works well                                              
 
iterator = dataset.make_one_shot_iterator()                                                       
batch_data = iterator.get_next()                                                                  
                                                                                                  
with tf.Session() as sess:                                                                        
    print sess.run(batch_data)                                                                                                 
```

### Log
```
    dataset = dataset.padded_batch(1, [None])
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 695, in padded_batch
    return PaddedBatchDataset(self, batch_size, padded_shapes, padding_values)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1292, in __init__
    input_dataset.output_shapes, _partial_shape_to_tensor, padded_shapes)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/util/nest.py"", line 512, in map_structure_up_to
    assert_shallow_structure(shallow_tree, input_tree)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/util/nest.py"", line 356, in assert_shallow_structure
    ""Input has type: %s."" % type(input_tree))
TypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: <type 'list'>.
```",0,,3,2018-01-11T05:44:19Z,2018-01-11T17:56:46Z,NONE,2018-01-11T12:57:18Z
16027,py2tf: add py2tf_internal BUILD rule to pip package,cla: yes,* to make pip tests pass,0,,1,2018-01-11T03:11:27Z,2018-01-11T04:56:14Z,CONTRIBUTOR,2018-01-11T03:38:01Z
16024,R1.4,cla: no,,0,,3,2018-01-11T00:17:30Z,2018-01-11T00:18:57Z,NONE,2018-01-11T00:18:57Z
16021,Update version strings.,cla: yes,,0,,1,2018-01-10T22:01:39Z,2018-01-10T22:02:37Z,MEMBER,2018-01-10T22:02:37Z
16019,tf-nightly and master - cannot import tensorflow,stat:contributions welcome,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: tf-nightly-gpu-1.6.0.dev20180110
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: V100 16GB
- **Exact command to reproduce**: ```import tensorflow```

### Describe the problem
On the current tf-nightly-gpu I cannot import tensorflow, the following error is produced. I am also seeing the same behavior on tf-nightly and also a build from source of master (SHA: 82b1e8eee8847730026379e3a5762c0e09d6fd36):
``` python
In [1]: import tensorflow
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

/home/ubuntu/tensorflow/tensorflow/__init__.py in <module>()
     22
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26

/home/ubuntu/tensorflow/tensorflow/python/__init__.py in <module>()
     47 import numpy as np
     48
---> 49 from tensorflow.python import pywrap_tensorflow
     50
     51 # Protocol buffers

/home/ubuntu/tensorflow/tensorflow/python/pywrap_tensorflow.py in <module>()
     23 import traceback
     24
---> 25 from tensorflow.python.platform import self_check
     26
     27

ImportError: No module named platform
```

### Source code / logs
N/A",0,,3,2018-01-10T20:50:09Z,2018-01-23T20:01:03Z,NONE,2018-01-11T01:44:18Z
16018,Branch 181499300,cla: yes,,0,,5,2018-01-10T20:38:55Z,2018-01-11T00:11:47Z,MEMBER,2018-01-10T20:39:22Z
16015,Modify `_parse_bazel_version` to return a tuple of ints,cla: yes,"Bazel is updating its version to 0.10.0, and this will break the version check. Applying suggested fix in https://github.com/bazelbuild/bazel/issues/4425.",0,,1,2018-01-10T19:17:41Z,2018-01-10T20:28:08Z,MEMBER,2018-01-10T19:17:59Z
16013,Disabling the interleave_op_test for now.,cla: yes,,0,,2,2018-01-10T18:43:39Z,2018-01-10T20:11:17Z,MEMBER,2018-01-10T20:02:05Z
16012,Fix a bug in ResolveConstantConcat,"awaiting testing (then merge),cla: yes",Changes to fix a bug in ResolveConstantConcat whereby shared tensors are removed without checking if they are used in other operators in the graph.,1,,14,2018-01-10T18:10:58Z,2018-01-25T01:23:11Z,CONTRIBUTOR,2018-01-10T23:23:08Z
16011,Tensorboard issue with the official docker image - 1.5.0-rc0-gpu-py3,"stat:awaiting tensorflower,type:build/install","Hello everyone,

I have the exact same issue as stated here: https://github.com/tensorflow/tensorflow/issues/14855
And on the official tensorboard repository: https://github.com/tensorflow/tensorboard/issues/812

The fact that I am using the official Docker Image and I didn't build anything from scratch.

`tensorflow/tensorflow   1.5.0-rc0-gpu-py3   9e770d59b136        6 days ago          2.85GB`

What I get when I try to launch Tensorboard as usual:

```
# tensorboard --logdir=log_directory
/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""/usr/local/bin/tensorboard"", line 7, in <module>
    from tensorboard.main import run_main
ImportError: cannot import name 'run_main'
```

**Resolution Idea:**
I noticed that by simply running the command: `pip install tensorboard` inside the container the problem is solved and I am able to normally launch Tensorboard.

Thanks a lot for the help,

All the best,

Jonathan
",1,,17,2018-01-10T16:29:58Z,2018-01-19T03:21:53Z,CONTRIBUTOR,2018-01-10T17:51:10Z
16010,lib_package does not bundle MKL-DNN,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.8.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: 

### Describe the problem
I wan't to build the tensorflow C-API from source with MKL-DNN support in order to use it in another project. The easiest solution (if not the only convenient one) I found for building the C-API is using the lib_package tool:

```bash
bazel build --config=mkl -c opt //tensorflow/tools/lib_package:libtensorflow
```
The build succeeds. However, the packaged library does not contain `libmklml_intel.so` and `libiomp5.so`. 

```bash
$ ldd libtensorflow_framework.so
	linux-vdso.so.1 =>  (0x00007ffec0f8a000)
	libmklml_intel.so => not found
	libiomp5.so => not found
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007feb07b2f000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007feb07826000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007feb07609000)
	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007feb07287000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007feb07071000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007feb06ca7000)

```
Is there a way to fix the Bazel build such that it outputs all necessary libs?
",0,,3,2018-01-10T14:47:46Z,2018-01-11T18:49:00Z,NONE,2018-01-11T06:15:12Z
16009,"bazel build ask for ANDROID_NDK_HOME, ANDROID_SDK_HOME -- no way to disable it",type:build/install,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
('v1.5.0-rc0-1-g793280a', '1.5.0-rc0')
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
```
Build label: 0.9.0
Build target: bazel-out/k8-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Dec 19 09:31:58 2017 (1513675918)
Build timestamp: 1513675918
Build timestamp as int: 1513675918
```
- **GCC/Compiler version (if compiling from source)**:
g++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
- **CUDA/cuDNN version**:
toolkit_9.0 and cudnn 7.0.5_for_9.0
- **GPU model and memory**:
different machines (irrelevant)
- **Exact command to reproduce**:
see [this gist](https://gist.github.com/PatWie/aef90e72dbeaf2f79fbcaa031d74baad) which is mainly

```bash
export TF_NEED_GCP=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=""$($CUDA_TOOLKIT_PATH/bin/nvcc --version | sed -n 's/^.*release \(.*\),.*/\1/p')""
export TF_CUDA_COMPUTE_CAPABILITIES=6.1,5.2,3.5
export TF_NEED_HDFS=0
export TF_NEED_OPENCL=0
export TF_NEED_JEMALLOC=1
export TF_ENABLE_XLA=0
export TF_NEED_VERBS=0
export TF_CUDA_CLANG=0
export TF_CUDNN_VERSION=7
export TF_NEED_MKL=0
export TF_DOWNLOAD_MKL=0
export TF_NEED_MPI=0
export TF_NEED_GDR=0
export TF_NEED_S3=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_COMPUTECPP=0
export GCC_HOST_COMPILER_PATH=$(which gcc)
export CC_OPT_FLAGS=""-march=native""

./configure

bazel build --config=opt --copt=-mfpmath=both --copt=-msse4.2 --copt=-O3 --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=1 ....
```
### Describe the problem
In the past, using exactly this scripted worked. However, there are now a few issues:
The build uses `AVX2` even I haven't specified it as `--copt` (which worked in the past)

### Source code / logs
depending on the machine it gives

```
Python 2.7.12 (default, Nov 20 2017, 18:23:56) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
2018-01-10 15:06:19.070740: F tensorflow/core/platform/cpu_feature_guard.cc:36] The TensorFlow library was compiled to use AVX2 instructions, but these aren't available on your machine.
zsh: abort      python
```
or
```
Python 2.7.12 (default, Nov 20 2017, 18:23:56) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
zsh: illegal hardware instruction  python
```

On machines with AVX2 everything is fine. Further, there is no way to skip to setup ANDROID_NDK_HOME, ANDROID_SDK_HOME (I manually uncommented this in `configure.py`).

*edit*
I am willing to provide a pull-request for `configure.py`, adding something like `TF_NEED_ANDROID`.",0,,5,2018-01-10T14:10:14Z,2018-01-30T19:31:57Z,NONE,2018-01-10T21:49:22Z
16008,"Java/JNI , Object Detection: Not big Difference with GPU or CPU? (Insignificant difference) ~300ms with and without GPU",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: 

**binary** by instructions https://www.tensorflow.org/versions/master/install/install_java
> 
> Install on Linux
> 
> Take the following steps to install TensorFlow for Java on Linux or macOS:
> 
> 1 Download libtensorflow.jar, which is the TensorFlow Java Archive (JAR).
> 2 Decide whether you will run TensorFlow for Java on CPU(s) only or with the help of GPU(s). To help you decide, read the section entitled ""Determine which TensorFlow to install"" in one of the following guides:
>  - Installing TensorFlow on Linux
>
> 3 Download and extract the appropriate Java Native Interface (JNI) file for your operating system and processor support by running the following shell commands:
> 
>  TF_TYPE=""gpu""
>  OS=$(uname -s | tr '[:upper:]' '[:lower:]')
>  mkdir -p ./jni
>  curl -L \
>    ""https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-${TF_TYPE}-${OS}-x86_64-1.4.0.tar.gz"" |
>    tar -xz -C ./jni

- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: n/a, not used here (Java instead)
- **Bazel version (if compiling from source)**: n/a, not used here
- **GCC/Compiler version (if compiling from source)**: n/a, not used here
- **CUDA/cuDNN version**: Cuda compilation tools, release 8.0, V8.0.61, cuDNN 6
- **GPU model and memory**: GeForce 940MX

### Source code / logs
```
Checking to see if TensorFlow native methods are already loaded
TensorFlow native methods not found, attempting to load via tensorflow_inference
Successfully loaded TensorFlow native methods (RunStats error may be ignored)
2018-01-10 15:51:41.115224: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-01-10 15:51:41.254497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-10 15:51:41.255183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 1.2415
pciBusID: 0000:01:00.0
totalMemory: 1,96GiB freeMemory: 1,51GiB
2018-01-10 15:51:41.255217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)
Model load took 313ms, TensorFlow version: 1.4.0
```
",0,,5,2018-01-10T14:06:41Z,2018-01-11T06:11:54Z,NONE,2018-01-11T06:11:54Z
16007,Fix inline if/else statement in CMAKE_CACHE_ARGS,"awaiting testing (then merge),cla: yes","An if/else statement was given inline as an argument to CMAKE_CACHE_ARGS for some CMake external projects as discussed in #15209. This resulted in the following init cache entries on some systems:

```
set(CMAKE_POSITION_INDEPENDENT_CODE ""ON;if(;tensorflow_ENABLE_POSITION_INDEPENDENT_CODE;);else;(;)""CACHE BOOL ""Initial cache"" FORCE)
set(CMAKE_POSITION_INDEPENDENT_CODE ""OFF;endif;(;)"" CACHEBOOL ""Initial cache"" FORCE)
```
This commit changes the inline if/else arguments to -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=${tensorflow_ENABLE_POSITION_INDEPENDENT_CODE} which is functionality equivalent.",0,,5,2018-01-10T13:05:09Z,2018-01-11T14:21:40Z,CONTRIBUTOR,2018-01-11T02:33:39Z
16006,Add property to get cell wrapped by DropoutWrapper ,"awaiting testing (then merge),cla: yes",Adding wrapped cell property as discussed in #15810.,1,,7,2018-01-10T12:20:19Z,2018-01-23T21:46:26Z,CONTRIBUTOR,2018-01-14T03:25:19Z
16005,Verbs w 0 copies,"awaiting testing (then merge),cla: yes","## Verbs implementation to use direct tensor writes (0 copies)

### Motivation:

Following HKUST research on the use of GPU direct, and their [GDR implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gdr/README.md), we wish to adopt the 0 copies approach and apply it to the current verbs implementation, while keeping the current implementation advantages, such as configurability and the use of RDMA for control messages.

### Performance:

Compared with the current GRPC, verbs and GDR implementation, the result implementation gave the best performance for every model, with any number of nodes. For VGG16 on 8 nodes with 4 P100 GPUs each, the prototype beat the second place by over 15%.

### Implementation requirements:

1. Tensor writes need to be done directly from the source Tensor to the destination Tensor, with no memory copies in between. This should be done for all DMAble tensors which are located either on CPU or on a RDMA compatible GPU device (GPU direct). 
2. Non DMAble tensors (CanMemCopy == false) will be serialized to proto on the sender side, RDMA written to a registered buffer on the receiver side, and then deserialized by the receiver.
3. Tensors which are located on a non-RDMA-compatible GPU, will be RDMA written to a registered CPU proxy buffer on the receiver side, and then copied to GPU by the receiver.

### Implementation constrains:

For best stability and proof of correctness, we will divide the implementation to two stages:
1. At first stage we will keep changes to the current implementation to the minimum possible. The expense will be that we may have unused or unnecessary code leftovers, which may also affect performance. 
2. At second stage, we will re-iterate over the code and remove irrelevant code parts.
The design of the solution aims that we will achieve both stages with relative ease. 

### Design guidelines:

1. Since we do not want to do any unnecessary memory copying, we will no longer allocate a fixed CPU buffer as the destination for the RDMA write. Instead we will do the writing directly to the result tensor, or if the result tensor is on a device which does not support RDMA, we will do the writing to a proxy CPU tensor and then copy its content to the result tensor.
2. The address of the destination Tensor needs to be sent to the sender side for writing, meaning that the result/proxy tensor should be pre-allocated on the receiver side, prior to sending the tensor request. In order to do that, we need to know its meta-data, i.e. shape and data-type for DMAble tensors, and proto-size for serialized tensors. Unfortunately, this information is only available on the sender side which complicates manners. In order to avoid sending extra messages for querying the meta-data on each step, we store a local meta-data cache per tensor. Based on the assumption that the meta-data of a tensor rarely changes between steps, we expect that on most times the cache will only be updated once. When the sender receives a request for a tensor, if it is the first time this tensor is requested, or in the rare case that the meta-data did change, the sender will first send a meta-data response, on which the receiver will update the local cache, and reallocate the result/proxy tensors if required. When the receiver sends the tensor request, it will contain also the meta-data currently stored in its local cache, so the sender can compare it to see if there was a change.
3. When the sender writes the tensor content to the result tensor, no additional data is being written with it. That means we need to reside on ibverbs immediate (uint32_t) to indicate which request we are responding to (in order to trigger the receive callback). The easiest and most elegant way is to key the recv callback with a unique request_index (uint32_t), instead of the current key_with_step_id (string). 
4. Since the sender no longer writes the tensor from/to fixed buffers, we no longer need to schedule the writes using the local/remote status. In addition we no longer rely on the RmdaTensorBuffer members as the source/destination addresses and rkey/lkey. Instead, each RdmaTensorBuffer will hold multiple ""Response"" objects (one per step-id), from which we derive destination address and rkey. The source address and lkey are always the ones of the source Tensor.
5. With the addition of tensor pre-allocation, we noticed there is a large code similarity between sending the first tensor request and re-sending the request in case of meta-data changes. After implementing a common method for tensor pre-allocation, it turned out that implementation becomes much simpler by encapsulating the process of request sending/re-sending, meta-data response callback and content response callback, all in a single ""Request"" class. The request class holds all the relevant request information, which reduces excessive parameter passing and lambda capturing. This decision is purely for elegance and code simplicity, and we decided to implement it in first stage because it makes the implementation much easier.
6. At phase 2, we adopt that approach for the sender side as well, by encapsulate all the send and resend logic in the ""Response"" class (and remove the RdmaTensorBuffer class completely). This should make our design easier to understand, and also hold common notions with the rest of the distributed implementations.

### New types/classes:

* **enum RdmaImmDataType** - Immediate types to distinguish between different RDMA writes on the remote side. Ack writes and control-message writes have a fixed immediate value. The rest of the writes are tensor writes and the immediate value is the relevant request index.
* **enum  RdmaWriteIDType**    - Types to distinguish between different RDMA write-complete events: Ack, control message, tensor DMA write and tensor proto write.
* **class RdmaWriteID**        - Context for RDMA write complete events. Holds the RdmaWriteIDType and additional data.
* **class RemoteAddressContext** - Remote address information (address + mr). Will be passed as write context for tensor proto writes.
* **class RdmaTensorMetaData** - Meta-data for a tensor (type, shape, is_dead, proto_size).
* **class RdmaMemoryMgr**      - Manages the meta-data cache, and the registered memory regions.
* **class RdmaTensorRequest**    - Holds and manages information for a single tensor request throughout the entire receive cycle. API:
	* **Start()**                - Start the request sequence.
		* Allocate the result tensor (and proxy tensor if required).
		* Send RDMA_MESSAGE_TENSOR_REQUEST to the remote side.
	* **RecvTensorMetaData()**   - Receive meta-data from the remote side.
		* Update the local meta-data cache.
		* Reallocate the result tensor (and proxy tensor if required).
		* Re-send the request to the remote side.
	* **RecvTensorContent()**    - Receive tensor content from the remote side (RDMA write was completed).
		* Decode proto if required and/or move to GPU if the content was not written to it directly (GPU direct is not avaliable).
		* Invoke the done callback.
* **class RdmaTensorResponse**   - Holds and manages information for a single tensor response throughout the entire send cycle. API:
	* **Start()**                - Start the response sequence. 
		* Find the tensor in the local tag-match table.
		* Compare the tensor's meta-data to the meta-data in the message (taken from the requester's local cache). 
			* If meta-data changed:
				* Clone the tensor to be sent later.
				* Send a meta-data update message and wait for re-request.
			* Else:
				* Send the tensor's content (using direct RDMA write).
	* **Resume()**               - Resume the response sequence after a re-request. Send the tensor's content that was cloned earlier.
	* **Destroy()**              - Destroy the response's resources and remove it form the pending list.

### Protocol changes:

The protocol messages themselves will remain mostly unchanged at the first stage, but will be used differently, as described below. The current messages structures already have most of the required fields for the new implementation. The only change is the ""buffer_size"" field which is no longer used since we are no longer sending additional information with the tensor, and thus it is now always equal to the ""tensor_bytes"" field. Instead, we use that field to pass the ""request_index"".

### Message structure:

| type | name_size | name | step_id | request_index | remote_addr | rkey | is_dead | data_type | tensor_shape | tensor_bytes |
|------|---------- |------|---------|---------------|-------------|------|---------|-----------|--------------|--------------|
|  1B  |    2B     | 512  |  8B     |      8B       |         8B  |   4B |      1B |     XB    |    XB        |    8B        |

* **RDMA_MESSAGE_TENSOR_REQUEST**  - (receiver ==> sender) The original tensor request. 
	* type - The message type.
	* name (name_size) - Name of the requested tensor.
	* step_id - Step ID.
	* request_index - Request index.
	* remote_addr/rkey - Address/rkey of the result/proxy tensor. Irrelevant for first-time request.
	* is_dead/data_type/tensor_shape/tensor_bytes - The current meta-data as stored in the receiver local cache. The sender will use that information to know if the receiver's cache requires updating.
* **RDMA_MESSAGE_BUFFER_REQUEST**  - (sender ==> receiver) The meta-data update message in case meta-data had changed (or if it is the first time the tensor is requested).
	* type - The message type.
	* request_index - Request index.
	* is_dead/data_type/tensor_shape/tensor_bytes - The up-to-date meta-data.

  **Note:** At phase 2 this message is renamed to - **RDMA_MESSAGE_META_DATA_UPDATE**.
* **RDMA_MESSAGE_BUFFER_RESPONSE** - (receiver ==> sender) Tensor re-requset after meta-data update and reallocation of result/proxy tensors.
	* type - The message type.
	* name (name_size) - Name of the requested tensor.
	* step_id - Step ID.
	* request_index - Request index.
	* remote_addr/rkey - Address/rkey of the reallocated result/proxy tensor.
	* is_dead/data_type/tensor_shape/tensor_bytes - The new meta-data. Will be removed in the next phase.

  **Note:** At phase 2 this message is renamed to - **RDMA_MESSAGE_TENSOR_RE_REQUEST**.
* **RDMA_MESSAGE_TENSOR_WRITE**    - (sender ==> receiver) No longer sent. There is only a direct write of the tensor content to the result/proxy tensor. Request index passed as the immediate value of the write.
* **RDMA_MESSAGE_TENSOR_IDLE**     - (receiver ==> sender) No longer sent.

### Phase 1:
![alt text](https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies_phase1_protocol.jpg ""Phase 1 transport protocol"")

### Phase 2:
![alt text](https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies.png ""Phase 2 transport protocol"")

### Second stage optimizations:
1. Remove unused code leftovers - Done.
2. Remove the ACK buffer completely, since we can rely completely on its immediate value - Done.

### Future optimizations:
1. Map the tensor names to indexes, to significantly reduce the request message size.
2. Understand the purpose of empty tensors and if we can skip remote fetching for them.
3. Consider concatenating multiple requests and/or using multiple message buffers.
4. Consider a no-request architecture.
  
 ",0,,8,2018-01-10T12:15:07Z,2018-01-20T22:41:29Z,CONTRIBUTOR,2018-01-10T12:18:11Z
16003,Adding meta_graph_be.pb testdata for big endian for framework_meta_graph_test,cla: yes,,1,,9,2018-01-10T09:11:05Z,2018-01-23T20:02:57Z,CONTRIBUTOR,2018-01-16T06:34:03Z
16002,fix a_ to allocator_,cla: yes,"```bash
[07:05:58]	[Step 1/1] In file included from tensorflow/core/common_runtime/threadpool_device.cc:32:0:
[07:05:58]	[Step 1/1] ./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'virtual void tensorflow::MklCPUAllocator::ClearStats()':
[07:05:58]	[Step 1/1] ./tensorflow/core/common_runtime/mkl_cpu_allocator.h:120:32: error: 'a_' was not declared in this scope
[07:05:58]	[Step 1/1]    void ClearStats() override { a_->ClearStats(); }
[07:05:58]	[Step 1/1]                                 ^
```

Please review this PR ASAP... @yuefengz ",0,,5,2018-01-10T07:55:45Z,2018-01-11T05:18:01Z,NONE,2018-01-11T05:18:01Z
15998,tensorflow input/output tensor reshape c++,stat:awaiting response,"currently , I am working on loading and testing a tensorflow model on android using c++, and the trained model is a full convolutional model, so the input need to be dynamically reshaped according to input image size.

I can make this done easily using python. but when turn to c++ , I can hardly find much examples and experience on this.

the trained model is converted to *.pb file , and the input and output tensor shape has been specified before conversion in python. and now I want to reshape the input and output in c++ before using the model.",0,,2,2018-01-10T05:53:51Z,2018-01-11T02:22:41Z,NONE,2018-01-10T13:17:17Z
15995,/home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/astor_archive/BUILD:8:1: Converting to Python 3: external/astor_archive/astor/source_repr.py failed (Exit 1). ,type:support,"

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:clone from git
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:0.5.4
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**:GTX1070ti  8GB
- **Exact command to reproduce**:


### Describe the problem
build error.when I finished ./configure and run bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package, met an error.

### Source code / logsExtracting Bazel installation...
..............
WARNING: /home/hp/Downloads/tensorflow/tensorflow/core/BUILD:1825:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/hp/Downloads/tensorflow/tensorflow/tensorflow.bzl:1152:30.
WARNING: /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/grpc/WORKSPACE:1: Workspace name in /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/grpc/WORKSPACE (@com_github_grpc_grpc) does not match the name given in the repository's definition (@grpc); this will cause a build error in future versions.
WARNING: /home/hp/Downloads/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/hp/Downloads/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Found 1 target...
ERROR: /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/astor_archive/BUILD:8:1: Converting to Python 3: external/astor_archive/astor/source_repr.py failed (Exit 1).

  ",0,,1,2018-01-10T03:41:02Z,2018-01-11T00:22:57Z,NONE,2018-01-11T00:22:55Z
15993,Fix typos,"awaiting testing (then merge),cla: yes","This PR fixes some typos: `refered`, `ouptuts`, `from from`, `suport`, `whithin`, `posibility`, and `then then`.",1,,2,2018-01-10T02:59:48Z,2018-01-11T03:07:31Z,CONTRIBUTOR,2018-01-11T02:15:41Z
15991,Hide MSVC workaround from Clang on Windows,"awaiting testing (then merge),cla: yes",#15990,0,,2,2018-01-10T01:51:56Z,2018-01-11T20:58:23Z,CONTRIBUTOR,2018-01-11T20:19:47Z
15989,Fix freeze_graph command line argument error.,"awaiting review,cla: yes",Fix TypeError: main() missing 1 required positional argument: 'unused_args' when using freeze_graph command line tool (pip console script entry point),1,,7,2018-01-10T00:12:56Z,2018-01-30T00:05:57Z,NONE,2018-01-29T19:53:17Z
15988,Add internal release notes that were previously missing.,cla: yes,"I wasn't sure about some of the Important/Other changes, so please double-check
that I haven't missed anything actually critical.",0,,1,2018-01-09T23:45:17Z,2018-01-10T19:29:28Z,MEMBER,2018-01-10T00:28:45Z
15987,"Documentation for placeholder does not explain when shape is (), [] or [None]",type:docs,"### System information

Not necessary.

### Describe the problem

The documentation for `placeholder` does not explain the case when its shape is `()`, `[]` or `[None]`.

### Possible solution

Add the explanations in [this SO  answer](https://stackoverflow.com/a/46941087/3924118) to the documentation of `placeholder`, including the example !!

  ",0,,2,2018-01-09T23:28:05Z,2018-01-10T01:01:55Z,NONE,2018-01-10T01:01:55Z
15986,Add new internal release notes that were missed in the previous iteration.,cla: no,,0,,2,2018-01-09T23:22:48Z,2018-01-09T23:23:49Z,MEMBER,2018-01-09T23:23:48Z
15985,Feature Request: Dense to Sparse and Dense to Sparse Tensor Ops,"stat:contributions welcome,type:feature","I think it would be helpful if there is a dense_to_sparse op in Tensorflow for ops like `ctc_loss` that requires sparse labels. I'm not really sure where else it can be used aside from that but in case only `ctc_loss` uses it, I think it would help if dense labels can be passed into `ctc_loss` and do the conversion within.",0,,24,2018-01-09T23:19:33Z,2018-01-15T04:43:23Z,CONTRIBUTOR,2018-01-10T00:56:54Z
15983,Feature request: Reduce learning rate on plateau,"stat:awaiting tensorflower,type:feature","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes. But applies to stock examples as well.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04.
- **TensorFlow installed from (source or binary)**:
Binary.
- **TensorFlow version (use command below)**:
v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 
Python 3.5.4
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
CUDA: V8.0.61
cuDNN: 6.0.21
- **GPU model and memory**:
GTX 1080Ti 11GB running driver version 384.98
- **Exact command to reproduce**:
N/A. However, I am using the Experiment, Estimator and Dataset APIs in order to do training.

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I would like to reduce the learning rate during training. However, I do not want to treat this as another tunable hyperparameter, so I would like this to be based on performance plateauing. In Keras, it is easy to implement learning rate reduction by monitoring the validation loss using the ReduceLROnPlateau callback function, but in TensorFlow this does not seem to be the case/easy. I certainly haven't found any implementation of this, so I propose this as a feature request.

Feel free to close this if this is not the correct forum.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,4,2018-01-09T22:49:36Z,2018-01-11T17:51:34Z,NONE,2018-01-11T00:48:57Z
15981,tf.Estimator creates loss and loss_1 for eval/train,type:support,"- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.12
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4
- **Python version**: 
2.7

### Describe the problem

When using the tf.Estimator, the summary files save out summaries for the loss variable evaluated every checkpoint.  The summary for the training, is saved as 'loss_1' .  I got this tensorboard by running the ciphar10_estimator code located: https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator/

This makes it difficult to compare the eval/train loss on the same graph in tensorboard.  What causes this naming issue and what can be done to fix it?

Thanks!

![screen shot 2018-01-09 at 12 02 53 pm](https://user-images.githubusercontent.com/22623388/34740703-311a2fc6-f535-11e7-9f88-ab16f65052ee.png)

  ",0,,4,2018-01-09T20:04:17Z,2018-01-11T01:24:05Z,NONE,2018-01-10T00:07:50Z
15980,While loop randomly doesn't evaluate tensors,stat:awaiting tensorflower,"Hello!
I believe to have found a bug in Tensorflow when running the code below. I am currently trying to build a neural transducer, and have stumbled across TF sometimes not returning any values for a tensor. I have not had the chance yet to test this out on another machine (no GPU, TF 1.4.1, Ubuntu 17.10). The code is redacted a bit to highlight only the parts that fail. [I've also posted to StackOverflow](https://stackoverflow.com/questions/48081063/tensorflow-non-deterministic-behaviour-with-large-model-using-while-loop) but didn't get any response there.

Notes:

- I believe the bug occurs around line 160, in the body of the while loop in the function run_full_transducer
- The session is returning [encoder_outputs, transducer_outputs]
- I do not use random functions
- As far as I can tell, if I remove the Print OP in line 164, the output is always 0

Example of a correct return value (more or less):
```
array([[[ 0.00811536, -0.00200322, -0.01177037,  0.03676344, -0.01909475,
             -0.03157664,  0.026092  ,  0.02367685, -0.01894805,  0.02832799,
              0.0377345 , -0.02583589, -0.02908566,  0.0299024 ,  0.00518877,
             -0.00064737,  0.01431572, -0.01053502, -0.01783628, -0.00382657,
              0.00076749, -0.02705991,  0.00112415, -0.0193013 ,  0.02346764,
              0.03014467,  0.02663364,  0.02503882,  0.03362656, -0.01877708,
              0.01859642,  0.02460729, -0.01395229, -0.03033791,  0.01177907,
             -0.03049169, -0.00389978,  0.02221515, -0.00073605,  0.01248251,
              0.00424051,  0.01070387,  0.02818898,  0.0321721 , -0.02462685,
              0.03495178, -0.02408989, -0.02742486,  0.00331823, -0.02311424,
             -0.01327039,  0.01095297,  0.02584363,  0.02083527, -0.01588045,
              0.02837921,  0.02100117,  0.00918638,  0.00109535, -0.02965789,
              0.01040822, -0.03240473,  0.00453057, -0.00603903]],
    
           [[ 0.01053647, -0.00457577, -0.01939731,  0.06317309, -0.03113565,
             -0.05525927,  0.04647589,  0.04213476, -0.03498235,  0.04962765,
              0.05989208, -0.04340284, -0.04777668,  0.05346756,  0.00395604,
             -0.0005207 ,  0.02079381, -0.01424338, -0.02584206, -0.00530154,
             -0.00031365, -0.04966826, -0.00091683, -0.03025239,  0.04526306,
              0.0595435 ,  0.0463665 ,  0.04578522,  0.05916505, -0.031725  ,
              0.03164144,  0.04257958, -0.02865831, -0.04795898,  0.01856991,
             -0.05512668, -0.00730711,  0.03953242,  0.00017992,  0.01710426,
              0.00754557,  0.01975578,  0.0469296 ,  0.05237873, -0.04435374,
              0.05924731, -0.04474678, -0.04605344,  0.00947831, -0.04284734,
             -0.01979787,  0.02003288,  0.04196753,  0.03900779, -0.02887472,
              0.05130195,  0.03419674,  0.0105699 ,  0.001114  , -0.0524303 ,
              0.01738651, -0.06084244,  0.01364262, -0.01153531]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```
Incorrect:
```
 [array([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],
    
           [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```

Code:
``` python
 import tensorflow as tf
    from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple
    from tensorflow.python.layers import core as layers_core
    import numpy as np
    # NOTE: Time major
    
    # Constants
    input_dimensions = 1
    vocab_size = 3
    input_embedding_size = 20
    encoder_hidden_units = 64
    inputs_embedded = True
    transducer_hidden_units = 64
    batch_size = 1
    GO_SYMBOL = vocab_size - 1  # TODO: Make these constants correct
    END_SYMBOL = vocab_size
    input_block_size = 2
    log_prob_init_value = 0
    
    
    # ---------------- Helper classes -----------------------
    
    
    # ----------------- Model -------------------------------
    embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)
    
    
    class Model(object):
        def __init__(self):
            self.encoder_inputs, self.encoder_inputs_length, self.encoder_hidden_state, \
            self.encoder_outputs, self.encoder_hidden_state_new = self.build_encoder_model()
            self.encoder_raw_outputs, self.trans_hidden_state, self.transducer_amount_outputs, \
            self.transducer_hidden_state_new, self.logits, self.decoder_prediction = self.build_transducer_model()
    
        def build_encoder_model(self):
            encoder_inputs = tf.Variable(tf.zeros(shape=(input_block_size, batch_size, input_dimensions)),
                                         dtype=tf.float32, name='encoder_inputs', trainable=False)
            encoder_inputs_length = tf.Variable([tf.shape(encoder_inputs)[0]], dtype=tf.int32,
                                                name='encoder_inputs_length', trainable=False)
            encoder_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, encoder_hidden_units)), dtype=tf.float32,
                                               name='encoder_hidden_state')  # Save the state as one tensor
    
            if inputs_embedded is True:
                encoder_inputs_embedded = encoder_inputs
            else:
                encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)
    
            # Build model
            encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)
    
            # Build previous state
            encoder_hidden_c, encoder_hidden_h = tf.split(encoder_hidden_state, num_or_size_splits=2, axis=0)
            encoder_hidden_c = tf.reshape(encoder_hidden_c, shape=[-1, encoder_hidden_units])
            encoder_hidden_h = tf.reshape(encoder_hidden_h, shape=[-1, encoder_hidden_units])
            encoder_hidden_state_t = LSTMStateTuple(encoder_hidden_c, encoder_hidden_h)
    
            #   encoder_outputs: [max_time, batch_size, num_units]
            encoder_outputs, encoder_hidden_state_new = tf.nn.dynamic_rnn(
                encoder_cell, encoder_inputs_embedded,
                sequence_length=encoder_inputs_length, time_major=True,
                dtype=tf.float32, initial_state=encoder_hidden_state_t)
    
            # Modify output of encoder_hidden_state_new so that it can be fed back in again without problems.
            encoder_hidden_state_new = tf.concat([encoder_hidden_state_new.c, encoder_hidden_state_new.h], axis=0)
            encoder_hidden_state_new = tf.reshape(encoder_hidden_state_new, shape=[2, -1, encoder_hidden_units])
    
            return encoder_inputs, encoder_inputs_length, encoder_hidden_state, encoder_outputs, encoder_hidden_state_new
    
        def build_transducer_model(self):
            encoder_raw_outputs = tf.Variable(tf.zeros(shape=(input_block_size, 1, encoder_hidden_units)),
                                              dtype=tf.float32,
                                              name='encoder_raw_outputs')
            trans_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, transducer_hidden_units)),
                                             dtype=tf.float32,
                                             name='trans_hidden_state')  # Save the state as one tensor
            transducer_amount_outputs = tf.Variable(0, dtype=tf.int32, name='transducer_amount_outputs',
                                                    trainable=False)
    
            # Model building
            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
                embedding=embeddings,
                start_tokens=tf.tile([GO_SYMBOL], [batch_size]),
                end_token=END_SYMBOL)
    
            attention_states = tf.transpose(encoder_raw_outputs,
                                            [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]
    
            attention_mechanism = tf.contrib.seq2seq.LuongAttention(
                encoder_hidden_units, attention_states)
    
            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
                tf.contrib.rnn.LSTMCell(transducer_hidden_units),
                attention_mechanism,
                attention_layer_size=transducer_hidden_units)
    
            projection_layer = layers_core.Dense(vocab_size, use_bias=False)
    
            # Build previous state
            trans_hidden_c, trans_hidden_h = tf.split(trans_hidden_state, num_or_size_splits=2, axis=0)
            trans_hidden_c = tf.reshape(trans_hidden_c, shape=[-1, transducer_hidden_units])
            trans_hidden_h = tf.reshape(trans_hidden_h, shape=[-1, transducer_hidden_units])
            trans_hidden_state_t = LSTMStateTuple(trans_hidden_c, trans_hidden_h)
    
            decoder = tf.contrib.seq2seq.BasicDecoder(
                decoder_cell, helper,
                decoder_cell.zero_state(1, tf.float32).clone(cell_state=trans_hidden_state_t),
                output_layer=projection_layer)
    
            outputs, transducer_hidden_state_new, _ = tf.contrib.seq2seq.dynamic_decode(decoder,
                                                                                        output_time_major=True,
                                                                                        maximum_iterations=transducer_amount_outputs)
            logits = outputs.rnn_output  # logits of shape [max_time,batch_size,vocab_size]
            decoder_prediction = outputs.sample_id  # For debugging
    
            # Modify output of transducer_hidden_state_new so that it can be fed back in again without problems.
            transducer_hidden_state_new = tf.concat(
                [transducer_hidden_state_new[0].c, transducer_hidden_state_new[0].h],
                axis=0)
            transducer_hidden_state_new = tf.reshape(transducer_hidden_state_new,
                                                     shape=[2, -1, transducer_hidden_units])
    
            return encoder_raw_outputs, trans_hidden_state, transducer_amount_outputs, transducer_hidden_state_new, \
                   logits, decoder_prediction
    
    
    model = Model()
    
    
    # ----------------- Alignment -------------------------
    
    # ----------------- Training --------------------------
    
    def run_full_transducer():
        # Inputs
        max_blocks = tf.placeholder(dtype=tf.int32, name='max_blocks')
        inputs_full_raw = tf.placeholder(shape=(None, batch_size, input_dimensions), dtype=tf.float32,
                                         name='inputs_full_raw')
        transducer_list_outputs = tf.placeholder(shape=(None,), dtype=tf.int32,
                                                 name='transducer_list_outputs')  # amount to output per block
    
        # Turn inputs into tensor which is easily readable
        inputs_full = tf.reshape(inputs_full_raw, shape=[max_blocks, input_block_size, batch_size, input_dimensions])
    
        # Outputs
        outputs_ta = tf.TensorArray(dtype=tf.float32, size=max_blocks)
    
        # Hidden states
        # TODO: make these correct
        encoder_hidden_init = tf.ones(shape=(2, 1, encoder_hidden_units))
        trans_hidden_init = tf.ones(shape=(2, 1, transducer_hidden_units))
    
        init_state = (0, outputs_ta, encoder_hidden_init, trans_hidden_init)
    
        def cond(current_block, outputs_int, encoder_hidden, trans_hidden):
            return current_block < max_blocks
    
        def body(current_block, outputs_int, encoder_hidden, trans_hidden):
            # Process encoder
            model.encoder_inputs = model.encoder_inputs.assign(inputs_full[current_block])
            model.encoder_inputs_length = model.encoder_inputs_length.assign([tf.shape(model.encoder_inputs)[0]])
            model.encoder_hidden_state = model.encoder_hidden_state.assign(encoder_hidden)
    
            # TODO: Error is SOMETIMES gone when using tf.Print
            current_block = tf.Print(current_block, [model.encoder_inputs], message='Enc in: ')
            #current_block = tf.Print(current_block, [model.encoder_outputs], message='Enc out: ')
    
            # Flow data from encoder to transducer
            model.encoder_raw_outputs = model.encoder_raw_outputs.assign(model.encoder_outputs)
            model.trans_hidden_state = model.trans_hidden_state.assign(trans_hidden)
            model.transducer_amount_outputs = model.transducer_amount_outputs.assign(transducer_list_outputs[current_block])
    
            # Note the outputs
            outputs_int = outputs_int.write(current_block, model.logits)
    
            return current_block + 1, outputs_int, model.encoder_hidden_state_new, model.transducer_hidden_state_new
    
        _, outputs_final, _, _ = tf.while_loop(cond, body, init_state)
    
        # Process outputs
        outputs = outputs_final.stack()  # Now the outputs are of shape [block, amount_of_trans_out, batch_size, vocab]
        outputs = tf.reshape(outputs, shape=(-1, 1, vocab_size))  # And now its [amount_outputs, batch_size, vocab]
    
        model.encoder_outputs = tf.Print(model.encoder_outputs, [model.encoder_outputs], message='Current block enc out: ')
    
        return max_blocks, inputs_full_raw, transducer_list_outputs, outputs, model.encoder_outputs
    
    # ---------------------- Testing -----------------------------
    
    
    # ---------------------- Management -----------------------------
    
    init = tf.global_variables_initializer()
    
    with tf.Session() as sess:
        sess.run(init)
    
        inp_max_blocks, inp_inputs_full_raw, inp_trans_list_out, out_outputs, enc_out = run_full_transducer()
    
        print sess.run([enc_out, out_outputs], feed_dict={
            inp_max_blocks: 3,
            inp_inputs_full_raw: np.ones(shape=(3 * input_block_size, 1, input_dimensions)),
            inp_trans_list_out: [1, 3, 2]
        })
```
System information:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10 (Artful Aardvark)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  1.4.1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Execute the code block as a python file a few times

Thanks!
Nikita
  
  
  ",0,,6,2018-01-09T19:58:14Z,2018-01-16T10:11:51Z,NONE,2018-01-11T03:02:39Z
15979,Branch 181341793,cla: yes,,0,,2,2018-01-09T19:16:46Z,2018-01-09T21:47:59Z,MEMBER,2018-01-09T19:16:57Z
15975,MKL: Fix for a compilation error caused by a previous commit,"awaiting testing (then merge),cla: yes",,1,,5,2018-01-09T13:05:01Z,2018-01-17T05:27:30Z,CONTRIBUTOR,2018-01-11T03:11:25Z
15974,Estimator.predict always loads model checkpoint preventing partially loading checkpoints,stat:awaiting tensorflower,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
pip
- **TensorFlow version (use command below)**:
1.4
- **Python version**: 
3.5

### Describe the problem
When using Tensorflow's Estimator to do predictions, the Estimator always loads the checkpoint in the model_dir. As this is done after the model_fn is called, there is no way to partially load a checkpoint for predictions. For training, I partially load the initial checkpoint in the model_fn which works fine.

I also tried not specifying the model_dir for the Estimator. As the [documentation states](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#__init__), this results in the Estimator using a temporary folder. However, as the temporary folder does not contain a checkpoint, I get the error `Could not find trained model in model_dir`.

It looks like there is no way to only partially load a checkpoint for prediction. If so, please provide a way to do this. For me, this is important, because I have a large model with several outputs. For different datasets, some of the outputs have different sizes. However, some of them are the same for all datasets. That's why I want to load them with the same code and only partially, because I don't need to load and run the whole network for this prediction. ",0,,2,2018-01-09T12:56:34Z,2018-01-10T00:21:25Z,CONTRIBUTOR,2018-01-10T00:21:20Z
15973,"How to change the model, without any change into android APK file",,"Hello,

I want to make an android app in this way, like we can change model file anytime in future, and it will not require any change into application code, means no need to generate new APK file of application, on any change into model.
In short I want to know, is there anyway to place model file other then assets folder. So that I can refer updated model file anytime from app.

Thanks,
Sumeet Guha.",0,,4,2018-01-09T10:46:45Z,2018-01-29T23:07:03Z,NONE,2018-01-09T19:01:48Z
15972,Maven Version of tensorflow Java API jar wrongly updated in Documentation,,"### System information
Have I written custom code : N/A
OS Platform and Distribution : N/A
TensorFlow installed from : N/A
TensorFlow version : N/A
Bazel version : N/A
CUDA/cuDNN version : N/A
GPU model and memory : N/A
Exact command to reproduce : N/A

### Describe the problem
https://www.tensorflow.org/install/install_java shows maven version as 1.4.1 

```
<dependency>
  <groupId>org.tensorflow</groupId>
  <artifactId>tensorflow</artifactId>
  <version>1.4.1</version>
</dependency>
```
However, this version is not available in public maven Repositories.
https://mvnrepository.com/artifact/org.tensorflow/tensorflow
Only versions  1.3.0 , 1.4.0, 1.4.0-rc0 and 1.5.0-rc0 are available.
Please correct documentation or release 1.4.1 Versions.

### Source code / logs
N/A
  ",1,,5,2018-01-09T09:44:50Z,2018-01-31T02:51:10Z,NONE,2018-01-09T19:01:44Z
15968,Imperfect implementation of tf.losses.mean_pairwise_squared_error,stat:awaiting response,"### System information
- **TensorFlow version**: 1.4.0, 1.4.1, and 1.5.0-rc0 (checked)
- **Have I written custom code**: N/A
- **OS Platform and Distribution**: N/A
- **TensorFlow installed from**: N/A
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
The implementation of `tf.losses.mean_pairwise_squared_error` looks imperfect.
For example, as explained in [the API reference of the function](https://www.tensorflow.org/api_docs/python/tf/losses/mean_pairwise_squared_error)
> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are three pairs of differences are summed to compute the loss: loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3

let me put the following data as `labels` and `predictions`:
```
labels = tf.constant([[0., 0.5, 1.]])
predictions = tf.constant([[1., 1., 1.]])
tf.losses.mean_pairwise_squared_error(labels, predictions)
```
In this case, the result should be `[(0-0.5)^2+(0-1)^2+(0.5-1)^2]/3=0.5`, but tensorflow returns different value 0.3333333134651184.

### Suggestion to fix the source code
[tensorflow/python/ops/losses/losses_impl.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/losses/losses_impl.py)

If the loss function `mean_pairwise_squared_error` measures the differences between pairs of corresponding elements of `predictions` and `labels` as explained in [the API reference of the function](https://www.tensorflow.org/api_docs/python/tf/losses/mean_pairwise_squared_error), here is a simple patch:
> (lines 520-521 need to be changed as)
> `term1 = 2.0 * _safe_div(sum_squares_diff_per_batch, num_present_per_batch-1)`
and
> (lines 525-526 need to be changed as)
> `term2 = 2.0 * _safe_div(math_ops.square(sum_diff), math_ops.multiply(num_present_per_batch, num_present_per_batch-1))`
  ",1,,2,2018-01-09T05:51:16Z,2018-02-02T13:37:01Z,CONTRIBUTOR,2018-01-09T13:00:03Z
15967,Make graph transform tool accessible via command line for pip install.,"awaiting review,cla: yes","RELNOTE: Make graph transform tool available from command line as
`transform_graph` for pip package.
Fix  #13287.",1,,1,2018-01-09T05:23:56Z,2018-01-24T07:43:36Z,MEMBER,2018-01-23T21:41:49Z
15964,DownloadfileTask Failed,stat:awaiting response,"try projrct as https://www.tensorflow.org/mobile/android_build#android_sample_apps,but downloadtask failed,  then solve it ,may be you shoule change the 

> download-models.gradle  classpath 'de.undercouch:gradle-download-task:3.2.0' to 3.3.0",0,,2,2018-01-09T03:37:07Z,2018-01-30T10:07:20Z,NONE,2018-01-09T12:59:53Z
15961,Adding cuda_config.h to the pip package.,cla: yes,,0,,2,2018-01-09T01:13:19Z,2018-01-11T07:14:13Z,MEMBER,2018-01-11T01:24:09Z
15960,Branch 181239691,cla: yes,,0,,2,2018-01-09T01:09:42Z,2018-01-09T19:10:06Z,MEMBER,2018-01-09T01:10:13Z
15959,Adding an install sources line for 1.5.0-rc0. Earlier we only updated,cla: yes, this for official.,0,,3,2018-01-08T22:21:15Z,2018-01-10T20:15:12Z,MEMBER,2018-01-08T23:36:06Z
15957,Switching branch and run ./configure does not regenerate spec.json,type:build/install,"When building from source with TensorFlow and switch to another branch, error returned even if I rerun `./configure`:

```
ubuntu@ubuntu:~/tensorflow$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
..........
ubuntu@ubuntu:~/tensorflow$ git checkout -b test
ubuntu@ubuntu:~/tensorflow$ ./configure
..........
ubuntu@ubuntu:~/tensorflow$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
..........
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded).
INFO: Found 1 target...
ERROR: /home/ubuntu/tensorflow/tensorflow/core/BUILD:1671:1: Executing genrule //tensorflow/core:version_info_gen failed (Exit 1)
Traceback (most recent call last):
  File ""tensorflow/tools/git/gen_git_source.py"", line 284, in <module>
    generate(args.generate)
  File ""tensorflow/tools/git/gen_git_source.py"", line 229, in generate
    (old_branch, new_branch))
RuntimeError: Run ./configure again, branch was 'refs/heads/master' but is now 'refs/heads/test'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 9.025s, Critical Path: 0.30s
FAILED: Build did NOT complete successfully
```


I think the issue is that `spec.json` is not updated when running `./configure`


```
ubuntu@ubuntu:~/tensorflow$ cat /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/local_config_git/gen/spec.json
{
  ""path"": ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/org_tensorflow/"", 
  ""git"": true, 
  ""branch"": ""refs/heads/master""
}
ubuntu@ubuntu:~/tensorflow$ 
```

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5) 
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```sh
git checkout -b test
./configure
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

  ",1,,3,2018-01-08T21:07:43Z,2018-01-19T21:40:07Z,MEMBER,2018-01-11T01:39:12Z
15955,Branch 181174976,cla: yes,,0,,4,2018-01-08T18:28:33Z,2018-01-09T01:00:25Z,MEMBER,2018-01-08T18:28:49Z
15951,[Build] Source build at HEAD generating XLA erros on Mac OS,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS High Sierra
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: HEAD@a770968
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 9.0.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Build procedures on doc optimized for native arch and XLA enabled.

### Describe the problem
Building TensorFlow on Mac OS with XLA enabled and configuration given above optimized for native arch and CPU only yields the following errors:
```
ERROR: /Users/adriano/MachineLearning/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:522:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:runtime_fft' failed (Exit 1)
In file included from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:21:
./tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h:42:30: error: implicit instantiation of undefined template 'std::__1::array<long long, 3>'
  const std::array<int64, 3> fft_shape = {
                             ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__tuple:222:64: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TEMPLATE_VIS array;
                                                               ^
In file included from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:21:
./tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h:65:30: error: implicit instantiation of undefined template 'std::__1::array<long long, 3>'
  const std::array<int64, 3> fft_shape = {
                             ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__tuple:222:64: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TEMPLATE_VIS array;
                                                               ^
In file included from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:21:
./tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h:106:30: error: implicit instantiation of undefined template 'std::__1::array<long long, 3>'
  const std::array<int64, 3> fft_shape = {
                             ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__tuple:222:64: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TEMPLATE_VIS array;
                                                               ^
3 errors generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1997.286s, Critical Path: 88.18s
FAILED: Build did NOT complete successfully

```
",1,,2,2018-01-08T17:04:44Z,2018-01-10T21:10:23Z,CONTRIBUTOR,2018-01-10T21:10:23Z
15949,Building TensorFlow on Windows: patch and rm,type:build/install,"### System information
- **Have I written custom code**: Yes, provided below.
- **OS Platform and Distribution**: Windows 10 1709, Build 16299.192
- **TensorFlow installed from (source or binary)**: Binary, Attempting source build of master
- **TensorFlow version**: 1.4.0
- **Python version**: 3.6
- **Bazel version**:  0.9.0
- **GCC/Compiler version**:  MSYS2 Shell, GCC unknown.
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A, CPU is i7-8550U, 8 GB memory
- **Exact command to reproduce**: Any `bazel build` on Windows. Please see Description.

### Describe the problem
Building TensorFlow on Windows has been a struggle with compatibility due to the fact that for many, MSYS will not run `patch` when installed from the MSYS2 shell. I have found a reliable way to resolve the issue: using Choco to install `patch`, moving patch.exe to a folder FOLDERNAME within its default directory, and then running %FOLDERNAME%/patch.exe with the flag `--binary` (to use CR LF line breaks) with a custom batch script compiled into a executable.

`bazel build` now completes `patch` commands without issue. But as it often is, another hurdle exists to the finish line. Bazel now attempts to recursively force remove a file using `rm -rf`, which obviously does not exist as a package in Choco as a bash command. MSYS will run it, but not from the command line.

Is there any way to get around the use of `rm`, or make a compatible solution for Windows using `del`?

I have ensured that #15829 has been installed. Still fails

If this is better left to the Bazel developers, please close this issue. 

### Source code

#### patch.bat
``` sh
start C:\ProgramData\chocolatey\lib\patch\tools\bin\%FOLDERNAME%\patch.exe --binary
exit
```

### Logs
``` sh
C:\tensorflow>bazel build --config=mkl --config=monolithic -c opt --copt=-march=native --copt=-mmmx --copt=-msse --copt=-msse2 --copt=-msse3 --copt=-mssse3 --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-mavx2 --copt=-maes --copt=-mfpmath=both //tensorflow/tools/pip_package:build_pip_package
#ERROR: C:/tensorflow/tensorflow/python/BUILD:4646:1: no such package '@cython//': Traceback (most recent call last):
        File ""C:/tensorflow/third_party/repo.bzl"", line 86
                _apply_delete(ctx, ctx.attr.delete)
        File ""C:/tensorflow/third_party/repo.bzl"", line 68, in _apply_delete
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/tensorflow/third_party/repo.bzl"", line 44, in _execute_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'C:\msys64\usr\bin\bash.exe -c rm -rf C:/users/eric/appdata/local/temp/_bazel_eric/x1e5egqw/external/cython/BUILD.bazel':
Stdout:
Stderr: /usr/bin/bash: rm: command not found
 and referenced by '//tensorflow/python:framework/fast_tensor_util.pyx_cython_translation'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed
INFO: Elapsed time: 61.324s
FAILED: Build did NOT complete successfully (92 packages loaded)
```

  ",1,,3,2018-01-08T15:32:59Z,2018-01-12T01:43:52Z,NONE,2018-01-10T19:08:57Z
15947,Windows: Override /DEIGEN_STRONG_INLINE=inline for //tensorflow/core/kernels:conv_ops,"awaiting testing (then merge),cla: yes","This change reduces the Windows building time by more than 15 minutes

Fix #10521",0,,7,2018-01-08T15:22:06Z,2018-01-13T04:52:14Z,MEMBER,2018-01-09T10:20:33Z
15946,Support for large number of classes when using tf.metrics.mean_per_class_accuracy(),"awaiting testing (then merge),cla: yes,kokoro:run","`tf.metrics.mean_per_class_accuracy()` uses a `num_classes x num_classes` matrix to keep track of accuracies for each class. This wastes a lot of memory and doesn't work well for large number of classes (e.g. matrix size for 500k classes is 500000^2*4 = 1 terabyte).

By switching to two 1-D variables of size `num_classes` instead, memory usage is reduced considerably. One variable keeps track of correct predictions for each class, while the other variable keeps track of the total number of predictions for each class.",1,,7,2018-01-08T14:49:46Z,2018-01-23T18:22:08Z,CONTRIBUTOR,2018-01-22T23:48:42Z
15941,Import Error: No module named '_pywrap_tensorflow',type:build/install,"On running the following command: import tensorflow I get an error:

`C:\Users\Neerav>python
Python 3.5.0 (v3.5.0:374f501f4567, Sep 13 2015, 02:27:37) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ImportError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.`


I have the following system features:

windows 64 bit
python 3.5.0 64 bit
Nvidia computing toolkit/CUDA/v8.0./(the cuDNN version 6.0)
all of them are added to my path location also which is: Python\Python35\Scripts
i have tensorflow in Python\Python35\Lib\site-packages\tensorflow
I even have a _pywrap_tensorflow.so file and pywrap_tensorflow.py",0,,2,2018-01-08T10:46:51Z,2018-01-10T01:16:38Z,NONE,2018-01-10T00:20:15Z
15939,Slim VGG losses increase gradually with default training configuration,,"Hi, when I try to train imagenet with slim vgg network with default configuration,
The loss increases gradually from ~0.1 to over 10000. 
I am not even able to debug this issue because, all tensors losses are encapsulated inside slim.
Is there any way to debug this issue? ",0,,3,2018-01-08T09:30:13Z,2018-01-29T23:13:45Z,NONE,2018-01-08T19:02:48Z
15938,An easy problem about tensorflow tf.reduce_mean op,,"I know... there might not be a suitable palce to ask this question, but I really hope someone cloud help me.

i want to use the ""tf.reduce_mean"" to obtain the mean of an array (ignore the zeros element)
eg:
    data = [[1,2,3],[4,5,6],[0,0,0]]   
    i want to obtain mean= [2.5, 3.5, 4.5]  
    but  tf.reduce_mean op gets the mean=[1.6, 2.3, 3]

Thank you very much!
  ",0,,1,2018-01-08T08:33:27Z,2018-01-08T15:55:30Z,NONE,2018-01-08T15:55:30Z
15930,Discontinuity at halfway point in graph output,stat:awaiting response,"- **I have written custom code (as opposed to using a stock example script provided in TensorFlow)**:
to reproduce the error:
1) convert HnH_gate.txt to HnH_gate.py
2) Edit mypath in out() method at end of file for your system.  Save 
3) in python: run HnH_gate.py
4) run out() to create csv files for the good and bad output
            i) out(""101"", new_probka_good)
            ii) out(""102"", new_probka_bad)
5) Plot data from hh_101.csv and hh_102.csv and verify the discontinuity at half way point in hh_102.csv
6) Two additional tests can be run:
          i) Edit parameter timepoints in main() to show error remaps to half way point.
          ii) My temporary correction is to create 2x points and throw half away.  this is done in p_update() setting cut_in_half = True

7) This same error was found running the code in Tensorflow 1.4 on MacOS Sierra.  My system info is:


== cat /etc/issue ===============================================
Linux PAULP-XPS15 4.4.0-43-Microsoft #1-Microsoft Wed Dec 31 14:42:53 PST 2014 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux PAULP-XPS15 4.4.0-43-Microsoft #1-Microsoft Wed Dec 31 14:42:53 PST 2014 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
numpydoc (0.7.0)
protobuf (3.4.1)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.5)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================


### Describe the problem
I am running a RNN for a Hodgkin and Huxley type gating of an ion channel protein
called HnH_gate.py.
The program takes a placeholder vmem and produces a timeseries output of the size
timepoints.
At the halfway point in the timeseries there is a discontinuity in the results
This only appears with some arrays fed to my tf.placeholder.  Others produce normal
results.  I can correct for the problem by doubling the number of timepoints requested
and throwing half away.

The array:
vmem_list_good = [[-100.0, -90.0, -80.0, -70.0, -60.0, -50.0, -41.0, -30.0, -20.0, -10.0, 0.0, 10.0, 20.0, 30.0, 40.0, 50.0],
                [-100.0, -90.0, -80.0, -70.0, -60.0, -50.0, -41.0, -30.0, -20.0, -10.0, 0.0, 10.0, 20.0, 30.0, 40.0, 50.0]]
appears to work perfectly
The array:
vmem_list_bad = [[80.0, 60.0, 40.0, 20.0, 00.0, -20.0, -41.0, -60.0, -80.0, -55.0, 0.0, 10.0, 20.0, 30.0, 40.0, 50.0],
            [70.0, 50.0, 30.0, 10.0, -10.0, -30.0, -50.0, -70.0, -90.0, -30.0, -10.0, 0.0, 10.0, 20.0, 30.0, 40.0]]

shows the error.

To see my temporary correction, edit the parameter in the HH.p_update() method
to: cut_in_half = True

I have written a short output routine to export the simulation to a csv file,
just edit the path and provide a string to make a unique filename:

out(""101"", new_probka_good)
out(""102"", new_probka_bad)

### Source code / logs
program file is: HnH_gate.py (provided as HnH_gate.txt)
HnH_gate.txt  (convert to HnH_gate.py)
[HnH_gate.txt](https://github.com/tensorflow/tensorflow/files/1610057/HnH_gate.txt)

System and Error Description: HnH_gate_bug_report.txt
[HnH_gate_bug_report.txt](https://github.com/tensorflow/tensorflow/files/1610056/HnH_gate_bug_report.txt)

Output example demonstrating problem: Artifact plotting new_probka_bad.py
[Artifact plotting new_probka_bad.pdf](https://github.com/tensorflow/tensorflow/files/1610058/Artifact.plotting.new_probka_bad.pdf)

Thanks for your help.
Paul
  
  ",0,,3,2018-01-07T18:17:27Z,2018-01-08T23:12:15Z,NONE,2018-01-08T06:54:37Z
15929,"C API, SIGABRT abort, Non-OK-status: RegisterAlreadyLocked, Invalid name.","stat:awaiting response,type:support","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Working with public C API.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
MacOS 10.13.2 (17C88)
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
- libtensorflow 1.4.1 (from brew package)
- **Python version**: 
non
- **Bazel version (if compiling from source)**:
non
- **GCC/Compiler version (if compiling from source)**:
Apple Swift version 4.0.3 (swiftlang-900.0.74.1 clang-900.0.39.2), lldb-900.0.64, Swift-4.0
- **CUDA/cuDNN version**:
non
- **GPU model and memory**:
non
- **Exact command to reproduce**:
Using swift code as example (https://github.com/Octadero/Example).

### Describe the problem
Dear TensorFlow community, 
It is really strange issue, from time to time at the same code, I have SIGABRT crash.
```
2018-01-05 21:03:55.627002: F tensorflow/core/framework/op.cc:165] Non-OK-status: RegisterAlreadyLocked(deferred_[i]) status: Invalid argument: Invalid name: {\242	
(lldb) bt
* thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGABRT
    frame #0: 0x00007fff528f7e3e libsystem_kernel.dylib`__pthread_kill + 10
    frame #1: 0x0000000108cd41b4 libsystem_pthread.dylib`pthread_kill + 333
    frame #2: 0x00007fff52854312 libsystem_c.dylib`abort + 127
    frame #3: 0x0000000108e600c0 libtensorflow_framework.so`tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 32
    frame #4: 0x0000000108e600d0 libtensorflow_framework.so`tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 16
    frame #5: 0x0000000108d28676 libtensorflow_framework.so`tensorflow::OpRegistry::MustCallDeferred() const + 406
    frame #6: 0x0000000108d2819d libtensorflow_framework.so`tensorflow::OpRegistry::LookUp(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::OpRegistrationData const**) const + 61
    frame #7: 0x0000000108d0c875 libtensorflow_framework.so`tensorflow::FunctionLibraryDefinition::LookUp(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::OpRegistrationData const**) const + 117
    frame #8: 0x0000000108d27b0a libtensorflow_framework.so`tensorflow::OpRegistryInterface::LookUpOpDef(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::OpDef const**) const + 42
    frame #9: 0x0000000108d81b65 libtensorflow_framework.so`tensorflow::Graph::AddNode(tensorflow::NodeDef const&, tensorflow::Status*) + 69
    frame #10: 0x0000000108d8177a libtensorflow_framework.so`tensorflow::Graph::Graph(tensorflow::OpRegistryInterface const*) + 458
    frame #11: 0x00000001004f7d73 libtensorflow.so`TF_NewGraph + 51
  * frame #12: 0x00000001066b739d CAPI`newGraph() at Graph.swift:26
    frame #13: 0x00000001063021b3 TensorFlowKit`Graph.init() at Graph.swift:36
    frame #14: 0x000000010630214a TensorFlowKit`Graph.__allocating_init() at Graph.swift:0
    frame #15: 0x0000000106500f2d TensorFlowKit`static SavedModel.load(exportPath=""/Users/Volodymyr/Projects/Examples/03_Reinforcement/Resources/save/"", tags=1 value, options=0x00000001098e35d0, self=TensorFlowKit.SavedModel) at SavedModel.swift:221
    frame #16: 0x00000001000069f4 03_Reinforcement`static Network.loadGraph(self=_3_Reinforcement.Network) at Network.swift:146
    frame #17: 0x0000000100003428 03_Reinforcement`Network.init() at Network.swift:73
    frame #18: 0x0000000100002e4c 03_Reinforcement`Network.__allocating_init() at Network.swift:0
    frame #19: 0x0000000100008656 03_Reinforcement`main at main.swift:32
    frame #20: 0x00007fff527a8115 libdyld.dylib`start + 1
    frame #21: 0x00007fff527a8115 libdyld.dylib`start + 1
```
Sanitizer options can't help to resolve that issue. List of libs loaded in attached file.
[dyld_log.txt](https://github.com/tensorflow/tensorflow/files/1609977/dyld_log.txt)


### Source code / logs
Using C API I am alloc [new Graph by TF_NewGraph()](https://github.com/Octadero/TensorFlow/blob/34addfc80cb7f220a7d9afa310f8a9845dba0d36/Sources/CAPI/Graph.swift#L26)",0,,5,2018-01-07T17:15:42Z,2018-01-09T16:06:20Z,NONE,2018-01-09T08:26:15Z
15928,Utility classes for writing Java source code from a C++ process (part 2),"awaiting testing (then merge),cla: yes","Part 2 of pull request #14094 that has been splitted into several commits.

This part features only a language-agnostic writer that outputs generated source code into a file or in memory. Note that this class is being kept apart from the Java-specific code generators since it could be a good candidate to be moved in the core io library in the future (ref #13748).

cc: @asimshankar ",1,,2,2018-01-07T16:51:22Z,2018-01-11T14:52:03Z,CONTRIBUTOR,2018-01-11T14:51:37Z
15927,R1.5/verbs w 0 copies,cla: yes,"## Verbs implementation to use direct tensor writes (0 copies)

### Motivation:

Following HKUST research on the use of GPU direct, and their [GDR implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gdr/README.md), we wish to adopt the 0 copies approach and apply it to the current verbs implementation, while keeping the current implementation advantages, such as configurability and the use of RDMA for control messages.

### Performance:

Compared with the current GRPC, verbs and GDR implementation, the result implementation gave the best performance for every model, with any number of nodes. For VGG16 on 8 nodes with 4 P100 GPUs each, the prototype beat the second place by over 15%.

### Implementation requirements:

1. Tensor writes need to be done directly from the source Tensor to the destination Tensor, with no memory copies in between. This should be done for all DMAble tensors which are located either on CPU or on a RDMA compatible GPU device (GPU direct). 
2. Non DMAble tensors (CanMemCopy == false) will be serialized to proto on the sender side, RDMA written to a registered buffer on the receiver side, and then deserialized by the receiver.
3. Tensors which are located on a non-RDMA-compatible GPU, will be RDMA written to a registered CPU proxy buffer on the receiver side, and then copied to GPU by the receiver.

### Implementation constrains:

For best stability and proof of correctness, we will divide the implementation to two stages:
1. At first stage we will keep changes to the current implementation to the minimum possible. The expense will be that we may have unused or unnecessary code leftovers, which may also affect performance. 
2. At second stage, we will re-iterate over the code and remove irrelevant code parts.
The design of the solution aims that we will achieve both stages with relative ease. 

### Design guidelines:

1. Since we do not want to do any unnecessary memory copying, we will no longer allocate a fixed CPU buffer as the destination for the RDMA write. Instead we will do the writing directly to the result tensor, or if the result tensor is on a device which does not support RDMA, we will do the writing to a proxy CPU tensor and then copy its content to the result tensor.
2. The address of the destination Tensor needs to be sent to the sender side for writing, meaning that the result/proxy tensor should be pre-allocated on the receiver side, prior to sending the tensor request. In order to do that, we need to know its meta-data, i.e. shape and data-type for DMAble tensors, and proto-size for serialized tensors. Unfortunately, this information is only available on the sender side which complicates manners. In order to avoid sending extra messages for querying the meta-data on each step, we store a local meta-data cache per tensor. Based on the assumption that the meta-data of a tensor rarely changes between steps, we expect that on most times the cache will only be updated once. When the sender receives a request for a tensor, if it is the first time this tensor is requested, or in the rare case that the meta-data did change, the sender will first send a meta-data response, on which the receiver will update the local cache, and reallocate the result/proxy tensors if required. When the receiver sends the tensor request, it will contain also the meta-data currently stored in its local cache, so the sender can compare it to see if there was a change.
3. When the sender writes the tensor content to the result tensor, no additional data is being written with it. That means we need to reside on ibverbs immediate (uint32_t) to indicate which request we are responding to (in order to trigger the receive callback). The easiest and most elegant way is to key the recv callback with a unique request_index (uint32_t), instead of the current key_with_step_id (string). 
4. Since the sender no longer writes the tensor from/to fixed buffers, we no longer need to schedule the writes using the local/remote status. In addition we no longer rely on the RmdaTensorBuffer members as the source/destination addresses and rkey/lkey. Instead, each RdmaTensorBuffer will hold multiple ""Response"" objects (one per step-id), from which we derive destination address and rkey. The source address and lkey are always the ones of the source Tensor.
5. With the addition of tensor pre-allocation, we noticed there is a large code similarity between sending the first tensor request and re-sending the request in case of meta-data changes. After implementing a common method for tensor pre-allocation, it turned out that implementation becomes much simpler by encapsulating the process of request sending/re-sending, meta-data response callback and content response callback, all in a single ""Request"" class. The request class holds all the relevant request information, which reduces excessive parameter passing and lambda capturing. This decision is purely for elegance and code simplicity, and we decided to implement it in first stage because it makes the implementation much easier.

### New types/classes:

* **enum RdmaImmDataType** - Immediate types to distinguish between different RDMA writes on the remote side. Ack writes and control-message writes have a fixed immediate value. The rest of the writes are tensor writes and the immediate value is the relevant request index.
* **enum  RdmaWriteIDType**    - Types to distinguish between different RDMA write-complete events: Ack, control message, tensor DMA write and tensor proto write.
* **class RdmaWriteID**        - Context for RDMA write complete events. Holds the RdmaWriteIDType and additional data.
* **class RemoteAddressContext** - Remote address information (address + mr). Will be passed as write context for tensor proto writes.
* **class RdmaTensorMetaData** - Meta-data for a tensor (type, shape, is_dead, proto_size).
* **class RdmaMemoryMgr**      - Manages the meta-data cache, and the registered memory regions.
* **class RdmaTensorRequest**  - Holds and manages information for a single tensor request throughout the entire receive cycle. API:
	* Start() - Start the request.
	* RecvTensorMetaData() - Receive meta-data from the remote side.
	* RecvTensorContent() - Receive tensor content from the remote side and invoke the done() callback. 
* **class RdmaTensorResponse** - Holds information for a single tensor response, such as destination address and rkey.

### Protocol changes:

The protocol messages themselves will remain mostly unchanged at the first stage, but will be used differently, as described below. The current messages structures already have most of the required fields for the new implementation. The only change is the ""buffer_size"" field which is no longer used since we are no longer sending additional information with the tensor, and thus it is now always equal to the ""tensor_bytes"" field. Instead, we use that field to pass the ""request_index"".

### Message structure:

| type | name_size | name | step_id | request_index | remote_addr | rkey | is_dead | data_type | tensor_shape | tensor_bytes |
|------|---------- |------|---------|---------------|-------------|------|---------|-----------|--------------|--------------|
|  1B  |    2B     | 512  |  8B     |      8B       |         8B  |   4B |      1B |     XB    |    XB        |    8B        |

* **RDMA_MESSAGE_TENSOR_REQUEST**  - (receiver ==> sender) The original tensor request. 
	* type - The message type.
	* name (name_size) - Name of the requested tensor.
	* step_id - Step ID.
	* request_index - Request index.
	* remote_addr/rkey - Address/rkey of the result/proxy tensor. Irrelevant for first-time request.
	* is_dead/data_type/tensor_shape/tensor_bytes - The current meta-data as stored in the receiver local cache. The sender will use that information to know if the receiver's cache requires updating.
* **RDMA_MESSAGE_BUFFER_REQUEST**  - (sender ==> receiver) The meta-data update message in case meta-data had changed (or if it is the first time the tensor is requested).
	* type - The message type.
	* request_index - Request index.
	* is_dead/data_type/tensor_shape/tensor_bytes - The up-to-date meta-data.
* **RDMA_MESSAGE_BUFFER_RESPONSE** - (receiver ==> sender) Tensor re-requset after meta-data update and reallocation of result/proxy tensors.
	* type - The message type.
	* name (name_size) - Name of the requested tensor.
	* step_id - Step ID.
	* request_index - Request index.
	* remote_addr/rkey - Address/rkey of the reallocated result/proxy tensor.
	* is_dead/data_type/tensor_shape/tensor_bytes - The new meta-data. Will be removed in the next phase.
* **RDMA_MESSAGE_TENSOR_WRITE**    - (sender ==> receiver) No longer sent. There is only a direct write of the tensor content to the result/proxy tensor. Request index passed as the immediate value of the write.
* **RDMA_MESSAGE_TENSOR_IDLE**     - (receiver ==> sender) No longer sent.

![alt text](https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies_phase1_protocol.jpg ""Phase 1 message protocol"")

### Second stage optimizations:
1. Remove unused code leftovers.
2. Remove the ACK buffer completely, since we can rely completely on its immediate value.

### Future optimizations:
1. Map the tensor names to indexes, to significantly reduce the request message size.
2. Understand the purpose of empty tensors and if we can skip remote fetching for them.
3. Consider concatenating multiple requests and/or using multiple message buffers.
4. Consider a no-request architecture.
  ",0,,9,2018-01-07T13:09:15Z,2018-01-10T12:18:57Z,CONTRIBUTOR,2018-01-07T14:42:51Z
15926,_Assert3DImage that adds a control dependency for the shape check,"awaiting testing (then merge),cla: yes","I noticed that the _Check3DImage was always used in exactly the same way and extracted this into its own convenience function.

The only remaining use of _Check3DImage was an import in gen_image_ops.py saying
""# TODO(drpng): remove these once internal use has discontinued."", so maybe it could be removed entirely.",1,,2,2018-01-07T12:32:26Z,2018-01-23T20:03:45Z,CONTRIBUTOR,2018-01-23T20:03:40Z
15925,cmake compile error C2678: binary '*': no operator found sparse_column_iterable.cc,,"`cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:/ProgramData/chocolatey/bin/swig.exe -DPYTHON_EXECUTABLE=C:/Python36/python.exe -DPYTHON_LIBRARIES=C:/Python36/libs/python36.lib -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 `

```
c:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.12.25827\include\algorithm(2417): error C2678: binary '*': no operator found which takes a left-hand operand of type 'const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' (or there is no acceptable conversion) (compiling source file D:\_working_dir\_ml\tensorflow\tensorflow\contrib\boosted_trees\lib\utils\sparse_column_iterable.cc) [D:\_working_dir\_ml\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  c:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.12.25827\include\algorithm(2417): error C2100: illegal indirection (compiling source file D:\_working_dir\_ml\tensorflow\tensorflow\contrib\boosted_trees\lib\utils\sparse_column_iterable.cc) [D:\_working_dir\_ml\tensorflow\tensorflow\con
trib\cmake\build\tf_core_kernels.vcxproj]
```
Windows 8.1 x64
cmake 3.10.1
swig 3.0.9
Visual Studio 2017 Community

the same problem asked
https://stackoverflow.com/questions/48058113/compiling-tensorflow-1-4-on-windows-10",0,,8,2018-01-07T12:22:32Z,2018-01-30T19:59:57Z,NONE,2018-01-08T00:58:11Z
15924,Tensorflow Optimize for Inference KeyError.,stat:awaiting response,"I got the following error when optimizing the graph for inference:
Traceback (most recent call last):
  File ""C:\Python35\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Python35\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Python35\lib\site-packages\tensorflow\python\tools\optimize_for_
inference.py"", line 146, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""C:\Python35\lib\site-packages\tensorflow\python\platform\app.py"", l
ine 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""C:\Python35\lib\site-packages\tensorflow\python\tools\optimize_for_
inference.py"", line 90, in main
    FLAGS.output_names.split("",""), FLAGS.placeholder_type_enum)
  File ""C:\Python35\lib\site-packages\tensorflow\python\tools\optimize_for_
inference_lib.py"", line 109, in optimize_for_inference
    placeholder_type_enum)
  File ""C:\Python35\lib\site-packages\tensorflow\python\tools\strip_unused_
lib.py"", line 83, in strip_unused
    raise KeyError(""The following input nodes were not found: %s\n"" % not_f
ound)
KeyError: ""The following input nodes were not found: {'input'}\n""

Please help me soon!",0,,2,2018-01-07T10:02:14Z,2018-01-08T05:09:43Z,NONE,2018-01-07T18:54:30Z
15923,Add clean_dep to copts macro.,"awaiting testing (then merge),cla: yes","Currently, copts macro has select statement with //tensorflow/...
This resulted in bazel error when any supermodule that uses tensorflow as a submoudle and uses copts macro.",0,,5,2018-01-07T04:38:49Z,2018-01-11T14:50:41Z,CONTRIBUTOR,2018-01-08T08:58:11Z
15922,Add clean_dep to a bazel macro.,"awaiting testing (then merge),cla: yes","Currently, copts macro has select statement with //tensorflow/...
This resulted in bazel error when any supermodule that uses tensorflow as a submoudle and uses copts macro.
",0,,5,2018-01-07T04:36:44Z,2018-01-11T14:48:42Z,CONTRIBUTOR,2018-01-08T08:57:45Z
15921,iOS: Op type not registered 'DecodeWav',type:build/install,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I am trying to run graph model from Simple Audio Recognition example on iOS.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13
- **TensorFlow installed from (source or binary)**: Branch r1.4
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: Build label: 0.9.0-homebrew
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

I am trying to run graph model from Simple Audio Recognition example on iOS. When I am calling `session->Create(tensorflow_graph)` with the graph I get the error: ""Could not create TensorFlow Graph: Not found: Op type not registered 'DecodeWav'..."".

My initial thought is that because I am using TensorFlow-experimental (1.1.1) from pods, it's possible that this Op type is not registered. So I tried building it myself, which builds without errors with command: `tensorflow/contrib/makefile/build_all_ios.sh`. I then remove TensorFlow-experimental (1.1.1) from the project and link my own build of tensorflow, but I get the same error. 

I also found the following PR - [[iOS] Add optional Selective Registration of Ops #14421](https://github.com/tensorflow/tensorflow/pull/14421)

I tried building from master with the above PR merged like so:
 
For iPhone 5:
`tensorflow/contrib/makefile/build_all_ios.sh -a armv7 -g /Users/anton/Development/tensorflow/tensorflow/examples/ios/simple/data/tensorflow_inception_graph_speech.pb`

For iPhone SE:
`tensorflow/contrib/makefile/build_all_ios.sh -a arm64 -g /Users/anton/Development/tensorflow/tensorflow/examples/ios/simple/data/tensorflow_inception_graph_speech.pb`

If I then go and check the file `/tensorflow/tensorflow/core/framework/ops_to_register.h` (auto-generated after above command) I can see that DecodeWav is listed among kernels and operations:

```
// This file was autogenerated by print_selective_registration_header.py
#ifndef OPS_TO_REGISTER
#define OPS_TO_REGISTER

    namespace {
      constexpr const char* skip(const char* x) {
        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;
      }

      constexpr bool isequal(const char* x, const char* y) {
        return (*skip(x) && *skip(y))
                   ? (*skip(x) == *skip(y) && isequal(skip(x) + 1, skip(y) + 1))
                   : (!*skip(x) && !*skip(y));
      }

      template<int N>
      struct find_in {
        static constexpr bool f(const char* x, const char* const y[N]) {
          return isequal(x, y[0]) || find_in<N - 1>::f(x, y + 1);
        }
      };

      template<>
      struct find_in<0> {
        static constexpr bool f(const char* x, const char* const y[]) {
          return false;
        }
      };
    }  // end namespace
    constexpr const char* kNecessaryOpKernelClasses[] = {
""BinaryOp< CPUDevice, functor::add<float>>"",
""AudioSpectrogramOp"",
""ConstantOp"",
""Conv2DOp<CPUDevice, float>"",
""DecodeWavOp"",
""IdentityOp"",
""MatMulOp<CPUDevice, float, false >"",
""MaxPoolingOp<CPUDevice, float>"",
""MfccOp"",
""NoOp"",
""PlaceholderOp"",
""ReluOp<CPUDevice, float>"",
""ReshapeOp"",
""SoftmaxOp<CPUDevice, float>"",
""RecvOp"",
""SendOp"",
};
#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))

constexpr inline bool ShouldRegisterOp(const char op[]) {
  return false
     || isequal(op, ""Add"")
     || isequal(op, ""AudioSpectrogram"")
     || isequal(op, ""Const"")
     || isequal(op, ""Conv2D"")
     || isequal(op, ""DecodeWav"")
     || isequal(op, ""Identity"")
     || isequal(op, ""MatMul"")
     || isequal(op, ""MaxPool"")
     || isequal(op, ""Mfcc"")
     || isequal(op, ""NoOp"")
     || isequal(op, ""Placeholder"")
     || isequal(op, ""Relu"")
     || isequal(op, ""Reshape"")
     || isequal(op, ""Softmax"")
     || isequal(op, ""_Recv"")
     || isequal(op, ""_Send"")
  ;
}
#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)

#define SHOULD_REGISTER_OP_GRADIENT false
#endif

```

But when I try to run the graph model I still get same error message. I have removed the pod version, and I am 100% sure I am running my own build version of tensorflow on iOS.

I can't tell if this is a bug or I am doing something wrong during the build process.

Has anyone tried running any graph that uses DecodeWav on iOS?

Thanks.

### Source code / logs

Error: Could not create TensorFlow Graph: Not found: Op type not registered 'DecodeWav' in binary running on Antons-iPhone. Make sure the Op and Kernel are registered in the binary running in this process.





",1,,10,2018-01-07T02:55:51Z,2018-01-31T19:36:01Z,NONE,2018-01-10T01:21:37Z
15918,Add pos_weights practical interpretation,cla: no,"The current  weighted_cross_entropy_with_logits docs don't explain practically the relationship of 
`pos_weights > 1`,  `pos_weights < 1` to precision, recall, and class imbalance.",0,,3,2018-01-06T20:24:18Z,2018-01-17T04:47:00Z,CONTRIBUTOR,2018-01-07T05:50:59Z
15917,Update docs for `concat` in case `axis < 0`,"awaiting testing (then merge),cla: yes","This fix tries to address the issue raised in #15905 where the documentation does not cover the case of `axis < 0` for `tf.concat`.

This fix fixes #15905.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,1,2018-01-06T19:37:24Z,2018-01-07T14:57:16Z,MEMBER,2018-01-07T14:50:52Z
15914,How to know my loss function does not have numerical problems?,stat:awaiting response,"I wrote the following loss function that I post below. How do I know that I don't have any kind of numerical issues with it? (because something tells me that I do)

```
def gather_cols(params, indices, name=None):
    """"""Gather columns of a 2D tensor.

    Args:
        params: A 2D tensor.
        indices: A 1D tensor. Must be one of the following types: ``int32``, ``int64``.
        name: A name for the operation (optional).

    Returns:
        A 2D Tensor. Has the same type as ``params``.
    """"""
    with tf.op_scope([params, indices], name, ""gather_cols"") as scope:
        # Check input
        params = tf.convert_to_tensor(params, name=""params"")
        indices = tf.convert_to_tensor(indices, name=""indices"")
        try:
            params.get_shape().assert_has_rank(2)
        except ValueError:
            raise ValueError('\'params\' must be 2D.')
        try:
            indices.get_shape().assert_has_rank(1)
        except ValueError:
            raise ValueError('\'params\' must be 1D.')

        # Define op
        p_shape = tf.shape(params)
        p_flat = tf.reshape(params, [-1])
        i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1],
                                       [-1, 1]) + indices, [-1])
        return tf.reshape(tf.gather(p_flat, i_flat),
                          [p_shape[0], -1])


def custom_binary_crossentropy(y_true, y_pred):
    # Assumes y_pred are probabilities and that y_true has actually 2 labels inside
    # Calculate: gain(y1, y2) * log(p) + gain(y2, y1) * log(1 - p)
    # gain(x1, x2) = (2 ^ x1 - 1) / ((2 ^ x1 - 1) + (2 ^ x2 - 1))

    # Gather y1 and y2 first
    y1 = gather_cols(y_true, [0])
    y2 = gather_cols(y_true, [1])

    # Get 2^y - 1
    y1_g = tf.subtract(tf.pow(tf.fill(tf.shape(y1), 2.0), y1), tf.fill(tf.shape(y1), 1.0))
    y2_g = tf.subtract(tf.pow(tf.fill(tf.shape(y2), 2.0), y1), tf.fill(tf.shape(y2), 1.0))

    # Get gains
    gain1 = tf.div(y1_g, tf.add(y1_g, y2_g))
    gain2 = tf.div(y2_g, tf.add(y1_g, y2_g))

    # Get logs
    log1 = tf.log(y_pred)
    log2 = tf.log(tf.subtract(tf.fill(tf.shape(y_pred), 1.0), y_pred))

    return -K.mean(tf.add(tf.multiply(gain1, log1), tf.multiply(gain2, log2)))
```",0,,1,2018-01-06T16:36:28Z,2018-01-07T01:05:33Z,NONE,2018-01-07T01:02:09Z
15909,Python Configuration Error,type:support,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**:  source 
- **TensorFlow version (use command below)**:the latest master branch
- **Python version**: 3.6.3 in anaconda ,python path is :C:/Users/huo_y/Anaconda3/python.exe
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:msvc 14
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**:
 bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
when I build tensorflow with bazel on windows by msys2 shell. I got this error
Python Configuration Error : --define PYTHON_BIN_PATH='C:/Users/huo_y/Anaconda3/python.exe' is not executable. Is it the python binary?

### Source code / logs
ERROR: C:/users/huo_y/tensorflow-master/util/python/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):
        File ""C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl"", line 291
                _create_local_python_repository(repository_ctx)
        File ""C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl"", line 251, in _create_local_python_repository
                _check_python_bin(repository_ctx, python_bin)
        File ""C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl"", line 204, in _check_python_bin
                _fail((""--define %s='%s' is not execut...)))
        File ""C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl"", line 27, in _fail
                fail((""%sPython Configuration Error:%...)))
Python Configuration Error: --define PYTHON_BIN_PATH='C:/Users/huo_y/Anaconda3/python.exe' is not executable. Is it the python binary?
 and referenced by '//util/python:python_headers'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed
INFO: Elapsed time: 16.677s
FAILED: Build did NOT complete successfully (63 packages loaded)
    currently loading: tensorflow/core/kernels
    Fetching http://ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz; 32,768b 4s

It may cause by python_configure.bzl I think,But I don't know how to correct it.

here is on function about the error  in python_config.bzl 
```
def _check_python_bin(repository_ctx, python_bin):
  """"""Checks the python bin path.""""""
  cmd =  '[[ -x ""%s"" ]] && [[ ! -d ""%s"" ]]' % (python_bin, python_bin)
  result = repository_ctx.execute([""bash"", ""-c"", cmd])
  if result.return_code == 1:
    _fail(""--define %s='%s' is not executable. Is it the python binary?"" % (
        _PYTHON_BIN_PATH, python_bin))
```
I find that when I run configure  the python path is windows format just like C:/Users/huo_y/Anaconda3/python.exe  but in msys2 the path may show /c/Users/huo_y/Anaconda3/python.exe .  I guess that when using bash -c ,it should need the path just like /c/Users/huo_y/Anaconda3/python.exe can get the correct return code, but it seems that the python_bin parameter is the windows path format .

can anyone help check it because I don't be farmiliar with bazel

",0,,1,2018-01-06T10:50:49Z,2018-01-09T22:28:30Z,NONE,2018-01-09T22:28:28Z
15907,nightly installed TF is the new 1.5 TF?,stat:awaiting response,"today I heard that there are a new version 1.5 TF which is with good support dynamic graph.

And I also find there is a new nightly installed method.


So this nightly installing method is install the new Version TF?


```
chg0901@ubuntu:~$ python3
Python 3.5.2 (default, Nov 23 2017, 16:37:01) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2018-01-06 16:49:32.242801: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing Curl library
>>> print(tf.__version__)
1.6.0-dev20180105
>>> x = [[2.]]
>>> m = tf.matmul(x,x)
>>> print(m)
Tensor(""MatMul:0"", shape=(1, 1), dtype=float32)
>>> print(tf.Session().run(m))
2018-01-06 16:51:25.418750: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX FMA
[[ 4.]]

```

  ",0,,2,2018-01-06T07:58:38Z,2018-01-18T16:34:47Z,NONE,2018-01-06T18:54:44Z
15906,Add additional argument to freeze_graph,"awaiting testing (then merge),cla: yes,stat:awaiting tensorflower",This PR fixes the failed testFreezeGraphV1 test for PR https://github.com/tensorflow/tensorflow/pull/14341,1,,13,2018-01-06T07:20:01Z,2018-01-25T19:05:09Z,CONTRIBUTOR,2018-01-23T01:18:11Z
15905,Documentation does not explain the utility of -1 as value for the axis parameter of the tf.concat method,stat:awaiting response,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X, version 10.13.2
- **TensorFlow installed from (source or binary)**: Binary (pip)
- **TensorFlow version (use command below)**: v1.3.0-rc1-5211-gab0fcac 1.5.0-dev20171126
- **Python version**: Python 3.5.0
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**:  N/A
- **CUDA/cuDNN version**:  N/A
- **GPU model and memory**:  N/A
- **Exact command to reproduce**: Just run a Python script with the code I am sharing with you

### Describe the problem

It apparently concatenates along the last axis. See the following example:  

```
import tensorflow as tf

t1 = [[[1, 2], [2, 3]], [[4, 4], [5, 3]]]
t2 = [[[7, 4], [8, 4]], [[2, 10], [15, 11]]]

with tf.Session() as sess:
    result = sess.run(tf.concat([t1, t2], -1))
    print(result)
```

which produces

```
[[[ 1  2  7  4]
  [ 2  3  8  4]]

 [[ 4  4  2 10]
  [ 5  3 15 11]]]
``` 

The following documention does not seem to explain this use case:

- https://www.tensorflow.org/api_docs/python/tf/concat
- https://www.tensorflow.org/versions/r1.5/api_docs/python/tf/concat
  
",0,,5,2018-01-06T03:41:22Z,2018-01-07T14:57:16Z,NONE,2018-01-06T12:54:19Z
15902," W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations. 2018-01-06 09:50:33.745519: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations. 2018-01-06 09:50:33.745553: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.",,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,2,2018-01-06T02:12:58Z,2018-01-06T05:39:30Z,NONE,2018-01-06T05:39:30Z
15901,Branch 180993147,cla: yes,,0,,4,2018-01-06T01:20:08Z,2018-01-06T03:46:31Z,CONTRIBUTOR,2018-01-06T01:21:28Z
15899,Addresses S3 timeout configurability discussed in #15868,"awaiting testing (then merge),cla: yes","This provides the ability to specify S3 timeouts via environment variables, as requested in #15868.
",1,,5,2018-01-06T00:54:44Z,2018-01-17T15:11:54Z,CONTRIBUTOR,2018-01-11T14:52:37Z
15897,Tensor Core support for NVIDIA Volta architecture,stat:awaiting tensorflower,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:
5386775e64aac0bb5020974122645da900bc312a
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:0.8.1
- **GCC/Compiler version (if comp6iling from source)**:5.4.0
- **CUDA/cuDNN version**:9.1 / 7.0.5
- **GPU model and memory**: Titan V
- **Exact command to reproduce**:

### Describe the problem
It is widely reported that using float16 on Nvidia Volta architecture comes only with x2 improvement instead of the expected x4 x8 improvement using Tensor Cores
https://github.com/tensorflow/benchmarks/issues/77
https://devblogs.nvidia.com/parallelforall/programming-tensor-cores-cuda-9/

I checked that Tensorflow master branch used 
cudnnGetConvolutionForwardAlgorithm
to get the best possible algorithm for the given GPU.
However I think either
cudnnGetConvolutionForwardAlgorithm_v7
or 
cudnnFindConvolutionForwardAlgorithmEx
should be used to fully utilize the Volta architecture.
Could you please check this issue with a Volta architecture GPU?
### Source code / logs
",1,,11,2018-01-05T23:48:20Z,2018-01-10T05:24:52Z,NONE,2018-01-09T22:25:59Z
15893,Transpose for high dimensional tensors using eigen,"awaiting testing (then merge),cla: yes","In our [library](https://github.com/Bihaqo/t3f) we rely heavily on fast transposes of high dimensional tensors, so we have added few extra cases to the CPU and GPU transpose functors. Previously it was done up to dimension 5 and we included dimensions 6, 7, 8 (similarly to the TENSORFLOW_USE_SYCL case).",1,,5,2018-01-05T22:41:55Z,2018-01-23T14:19:36Z,CONTRIBUTOR,2018-01-05T22:43:47Z
15892,Change GitHub repo URL from http://www.tensorflow.org to https://www.tensorflow.org,,Reasoning: HTTPS all the things,1,,7,2018-01-05T22:24:22Z,2018-01-31T00:52:25Z,CONTRIBUTOR,2018-01-06T07:00:07Z
15889,error while bazel build,stat:awaiting response,"## System information

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
**no**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**Ubuntu 16.04**
- TensorFlow installed from (source or binary):
 **Source**
- TensorFlow version (use command below):
 **r1.5**
- Python version: 
**3.6.3(Anaconda)**
- GCC/Compiler version (if compiling from source): 
**5.4**
- CUDA/cuDNN version:
 **9.0.176 / 7.0.5**
- GPU model and memory: 
**NVIDIA GTX 1080, Driver 385.111, 8G**
- Bazel version (if compiling from source):
**0.9**
- Exact command to reproduce:
`bazel build  --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --config=cuda //tensorflow/tools/pip_package:build_pip_package --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""`

##  Describe the problem

Use the commend line:
`bazel build  --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --config=cuda //tensorflow/tools/pip_package:build_pip_package --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""`
And got the error:

```
> ERROR: /home/xxh/tensorflow/tensorflow/contrib/boosted_trees/BUILD:559:1: Linking of rule '//tensorflow/contrib/boosted_trees:gen_gen_stats_accumulator_ops_py_wrap_py_wrappers_cc' failed (Exit 1)
> /usr/bin/ld: warning: libcublas.so.9.0, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> /usr/bin/ld: warning: libcudnn.so.7, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> /usr/bin/ld: warning: libcufft.so.9.0, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> /usr/bin/ld: warning: libcurand.so.9.0, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasGemmEx@libcublas.so.9.0'
> bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhpmv_v2@libcublas.so.9.0'
> bazel-out/host/
> ......
> ......
> bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsymm_v2@libcublas.so.9.0'
> bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateLRNDescriptor@libcudnn.so.7'
> collect2: error: ld returned 1 exit status
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 894.638s, Critical Path: 24.00s
> FAILED: Build did NOT complete successfully
```
When coppileing,I got a lot of warning like this.
`WARNING: /home/xxh/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.`

Cuda 9.0 test pass! And copied the cudnn.h and libcudnn* to cuda file.
It's all fine.
  
  ",0,,3,2018-01-05T20:05:06Z,2018-01-09T11:07:40Z,NONE,2018-01-06T07:00:03Z
15888,Cleanup CocoaPods dependency from TFLite iOS examples,"awaiting review,cla: yes",,1,,2,2018-01-05T19:47:50Z,2018-01-18T18:59:20Z,CONTRIBUTOR,2018-01-18T18:59:20Z
15886,"Successful Local Build of Tensorflow r1.5 GPU for Python 3.6, CUDA Toolkit 9.0, and CUDNN 7.0 on Windows 7 X64 SP1 using CMake in VS 2015 Update 3","stat:community support,type:build/install","I have spent a week trying to compile Tensorflow from source using Bazel on Windows with no success. In 2 days, I was able to compile it using CMake following the command output from a successful build I saw on Jenkins on 02-Jan-2018.

I wanted to provide details to spare others the pain of development in the future. The whole build took 6 hours to compile on my system.

I have an older system, which is why I was doing this. You will need to path variables in the attached scripts to work with the path variables for your system. For the most part, however, the scripts replicate what is mentioned on github here:

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 x64 SP1
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.5.0-rc0
- **Python version**: 3.6.4
- **CMake version (if compiling from source)**: CMake 3.10.1
- **GCC/Compiler version (if compiling from source)**: cl.exe (Visual Studio 2015 Update 3)
- **CUDA/cuDNN version**: 9.0.176 / 7.0.4
- **GPU model and memory**: NVIDIA Quadro K4000 P8, Driver 385.54, 3072 MiB
- **SWIG Version**: swigwin-3.0.12
- **Git Version**: Git for Windows 2.15.1 64-bit
- **MSBuild Version**: 14.0.25420.1
- **CPU**: Intel Xeon E5-2620 v2

- **Exact command to reproduce**:

After installing the above, I wrote a batch script to set and clean up system environment variables (please see attached script). Due to the 1024 character limit for PATH on windows, I manually edited the PATH in the registry editor to overcome this limitation.

I then wrote another script that set local variables, cloned tensorflow source and checked out version 1.5, then prepared the source with cmake and compiled with msbuild.

The final output was a python wheel, which I pip installed. I successfully ran the standard hello world script without error, i.e.

```
import tensorflow as tf
hello = tf.constant('Hello')
sess = tf.Session()
sess.run(hello)
```

as well as a small AlexNet network without issues.

I hope this helps future users and further emphasizes that it is possible to build Tensorflow 1.5 for GPU on Windows 7. I have not compiled this with AVX support, but that could be a next improvement (I'm not sure if this is possible. I only know MKL support is limited to Linux at the moment. However, Windows binaries for MKL and MPI can be downloaded from the Intel website).


[Scripts.zip](https://github.com/tensorflow/tensorflow/files/1607554/Scripts.zip)



",0,,8,2018-01-05T18:50:07Z,2018-01-31T00:41:51Z,NONE,2018-01-06T06:59:58Z
15885,Deletes unnecessary lines of code,"awaiting testing (then merge),cla: yes","The rest of this function changed sufficiently that these lines of code are not doing anything and can cause bugs.

Fixes issue #15852",1,,1,2018-01-05T18:41:14Z,2018-01-05T20:51:47Z,MEMBER,2018-01-05T18:41:30Z
15882,"tfdbg error ""Dump root directory does not exist"" with empty fetches",type:bug/performance,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: binary (pip install)
- **TensorFlow version**:
== tensorflow import ============================================
tf.VERSION = 1.4.1
tf.GIT_VERSION = v1.4.0-19-ga52c8d9
tf.COMPILER_VERSION = v1.4.0-19-ga52c8d9
Sanity check: array([1], dtype=int32)
- **Python version**: 2.7.12
- **CUDA/cuDNN version**: 
== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
- **GPU model and memory**: GeForce GTX 1080, 8114MiB
- **Exact command to reproduce**: see code below

### Describe the problem
`LocalCLIDebugWrapperSession.run()` does not behave like `tf.Session.run()` if there are no fetches. The dump directory will never be created and it crashes with an `IOError`. For me this issue occured in a situation like this:
```
      session.run([var.initializer for var in not_initialized_from_checkpoint])
```
where actually everything was restored from the checkpoint and `not_initialized_from_checkpoint` was empty. This code runs fine with an ordinary tf.Session but crashed with tfdbg. It took me some time to track down the issue. If it's not too hard to fix, it would be nice to keep other users from the same pain (maybe - just speculating - #13604 crashes for the same reason)

### Source code / logs
```
import tensorflow as tf
from tensorflow.python import debug as tf_debug

sess = tf.Session()
dbg_sess = tf_debug.LocalCLIDebugWrapperSession(tf.Session())

print sess.run([tf.constant(1.0)])     # [1.0]
print sess.run([])                     # []
print dbg_sess.run([tf.constant(1.0)]) # [1.0]
print dbg_sess.run([])                 # IOError: Dump root directory /tmp/tfdbg_ai_aWv does not exist
```
",1,,4,2018-01-05T15:34:48Z,2018-01-30T23:34:01Z,NONE,2018-01-06T01:26:18Z
15878,Windows: Release script for C library GPU builds on Windows,"awaiting testing (then merge),cla: yes","Fixed https://github.com/tensorflow/tensorflow/issues/11062

@gunan @asimshankar Can you also setup a job for GPU build using `tensorflow/tools/ci_build/windows/libtensorflow_gpu.sh` like http://ci.tensorflow.org/view/Nightly/job/nightly-libtensorflow-windows/
  ",1,,3,2018-01-05T13:19:12Z,2018-01-11T15:03:42Z,MEMBER,2018-01-09T08:52:05Z
15877,Documentation fix to contrib.signals,"awaiting testing (then merge),cla: yes","Fixed very confusing little typo.
The input `signals` is segmented into variable number of frames (with frame_length=256).
`frame_step` is the stride, not the size of a frame/window.",1,,2,2018-01-05T12:51:26Z,2018-01-24T21:31:51Z,CONTRIBUTOR,2018-01-23T20:05:10Z
15876,tfcompile with --config=monolithic and -fvisibility=hidden results in undefined reference __xla_cpu_runtime_EigenMatMulF32,stat:awaiting tensorflower,"Some background first. For DeepSpeech, I have been experimenting simplification of our set of dependencies, trying to do a `--config=monolithic` build. The root cause for that was being able to run a `SYCL`-enabled build on my system (Ubuntu 17.10). Using `OpenCL` on this would trigger dependency load-chain that in the end loads `libmirprotobuf`. This would clash with the `protobuf` symbols already built in our `libtensorflow_framework` / `libtensorflow_cc`. To avoid this, monolithic build and forcing `visibility=hidden` seemed to be the best solution.

This allows us to move from those libraries (non tfcompile build, tfcompile adds `libdeepspeech_model.so` and all the `XLA` dependencies):
 - `libdeepspeech.so`
 - `libdeepspeech_utils.so`
 - `libtensorflow_cc.so`
 - `libtensorflow_framework.so`

To just:
 - `libdeepspeech.so`
 - `libdeepspeech_utils.so`

This way, we have all needed TensorFlow bits within `libdeepspeech.so`, and those symbols are not re-exported thus avoiding any unwanted interaction. I could get `SYCL` build nearly working on Intel GPU.

Adding `tfcompile` in the equation, however, lead to linking issues. Symptom would be that build completes, but when one links binary against the model's `.so`, then it fails with:
```
undefined reference __xla_cpu_runtime_EigenMatMulF32
```

Checking with `objdump -t bazel-bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_matmul/tensorflow/compiler/xla/service/cpu/runtime_matmul*.o | grep EigenMatMul` would show that the symbol is properly built into `runtime_matmul`, but that it is hidden.

I would be able to solve that by exposing `__xla_cpu_runtime_EigenMatMulF32` and `__xla_cpu_runtime_EigenMatMulF64` through `TF_EXPORT`.",0,,16,2018-01-05T11:07:28Z,2018-01-16T18:03:20Z,CONTRIBUTOR,2018-01-05T11:08:29Z
15875,fix the comments which mistake x for y in gradient_checker,"awaiting testing (then merge),cla: yes",Fix the comments which mistake x for y in `gradient_checker.py`.,1,,3,2018-01-05T10:12:34Z,2018-01-05T19:12:01Z,CONTRIBUTOR,2018-01-05T18:05:00Z
15867,Branch 180856860,"awaiting testing (then merge),cla: yes",,0,,7,2018-01-05T00:24:19Z,2018-01-05T14:04:12Z,CONTRIBUTOR,2018-01-05T00:25:02Z
15866,TensorFlow fails to build with MPI,stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.4
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.5
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: 0.9.0 & 0.7.0
- **GCC/Compiler version (if compiling from source)**: 6.3.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
bazel build --copt -mfma --copt -mavx2 --copt -O3 --verbose_failures -s -c opt //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
TensorFlow fails to build with MPI with the following error:
```
ERROR: /disk/public_tf/tensorflow/tensorflow/contrib/mpi_collectives/BUILD:40:1: undeclared inclusion(s) in rule '//tensorflow/contrib/mpi_collectives:python/ops/_mpi_ops.so':
this rule is missing dependency declarations for the following files included by 'tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc':
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/lib/statusor.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/platform/port.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/lib/error.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/lib/status.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/lib/stringpiece.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/platform/logging.h'
tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:128:6: warning: 'bool tensorflow::contrib::mpi_collectives::{anonymous}::IsGPUDevice() [with T = Eigen::GpuDevice]' defined but not used [-Wunused-function]
 bool IsGPUDevice<GPUDevice>() {
      ^~~~~~~~~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 379.332s, Critical Path: 76.80s
FAILED: Build did NOT complete successfully
```
This error persists even when Bazel 0.7.0 is used to build TensorFlow.

  
  ",1,,7,2018-01-04T22:51:23Z,2018-01-17T22:32:45Z,CONTRIBUTOR,2018-01-08T15:47:41Z
15864,Fix docs,cla: yes,,0,,2,2018-01-04T22:24:25Z,2018-01-05T01:35:54Z,MEMBER,2018-01-05T00:46:43Z
15860,Branch 180746153,cla: no,,0,,5,2018-01-04T21:06:41Z,2018-01-05T01:03:06Z,CONTRIBUTOR,2018-01-04T21:08:26Z
15857,Update documentation for gather_nd/gather to specify behaviors for out-of-bound indices,"awaiting testing (then merge),cla: yes","This fix updates documentation for gather_nd/gather/scatter_nd to specify behaviors for out-of-bound indices. Basically, on CPU an error will be returned and on GPU 0 value will be filled to the expected positions of the output.

This fix closes #13687. This fix closes #12608.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,2,2018-01-04T20:02:37Z,2018-01-05T20:53:32Z,MEMBER,2018-01-05T06:06:07Z
15856,MKL DNN: Implementing MKL DNN version of Softmax,"awaiting testing (then merge),cla: yes",New MKL DNN implementation of Softmax is added. ,0,,2,2018-01-04T18:45:43Z,2018-01-17T05:47:51Z,CONTRIBUTOR,2018-01-16T16:51:55Z
15854,Enable tilde expansion in debug wrappers,"awaiting testing (then merge),cla: yes","This commit allows paths beginning with '~' to be used when specifying where
debug files should be dumped. The tilde will now be expanded to the user's
home directory.",1,,3,2018-01-04T17:50:11Z,2018-01-04T19:10:26Z,CONTRIBUTOR,2018-01-04T17:56:24Z
15853,Fixes and formatting to configure.py,"awaiting testing (then merge),cla: yes","* Fix a bug in error generation regarding true/false string parsing
* Some minor style fixes",1,,1,2018-01-04T16:05:37Z,2018-01-04T20:54:55Z,CONTRIBUTOR,2018-01-04T19:24:30Z
15852,Eager: crashed when using embedding_lookup in tfe.defun in tfe.GradientTape,"comp:eager,stat:awaiting tensorflower","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.5.0dev20171230
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**:pascal
- **Exact command to reproduce**:N/A
### Describe the problem
When I train a seq2seq model in eager, backward will raise a error:
```Python
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in get_attr(self, name)
   2162           with errors.raise_exception_on_not_ok_status() as status:
-> 2163             c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf, status)
   2164           data = c_api.TF_GetBuffer(buf)

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    472             compat.as_text(c_api.TF_Message(self.status.status)),
--> 473             c_api.TF_GetCode(self.status.status))
    474     # Delete the underlying status object from memory otherwise it stays alive

InvalidArgumentError: Operation 'embedding_lookup' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
    369     try:
--> 370       xla_compile = op.get_attr(""_XlaCompile"")
    371       xla_separate_compiled_gradients = op.get_attr(

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in get_attr(self, name)
   2166         # Convert to ValueError for backwards compatibility.
-> 2167         raise ValueError(str(e))
   2168       x = attr_value_pb2.AttrValue()

ValueError: Operation 'embedding_lookup' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

IndexError                                Traceback (most recent call last)
<ipython-input-1-ed6ea9045e3f> in <module>()
     12         embed = tfe.Variable(np.ones((10, 100)).astype(np.float32))
     13         toy_data = np.ones((1, 10)).astype(np.int64)
---> 14         embedding_crash(toy_data, embed)

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in decorated(*args, **kwds)
    639       arguments_to_functions[cache_key] = _defun_internal(
    640           name, func, args, kwds)
--> 641     return arguments_to_functions[cache_key](*args)
    642 
    643   return decorated

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in __call__(self, *args)
    461         self._extra_inputs):
    462       if not self._has_backprop:
--> 463         self._compute_backprop()
    464       return self._backprop_call(tensor_inputs)
    465 

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in _compute_backprop(self)
    359             filtered_outputs,
    360             self._input_placeholders,
--> 361             grad_ys=self._out_grad_placeholders)
    362         shapes = tuple(x.shape for x in in_gradients if x is not None)
    363     captures = list(sorted(c.captured_tensors, key=lambda x: x.name))

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)
    607                 # functions.
    608                 in_grads = _MaybeCompile(
--> 609                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
    610               else:
    611                 # For function call ops, we add a 'SymbolicGradient'

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
    373       xla_scope = op.get_attr(""_XlaScope"").decode()
    374     except ValueError:
--> 375       return grad_fn()  # Exit early
    376 
    377   if not xla_compile:

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py in <lambda>()
    607                 # functions.
    608                 in_grads = _MaybeCompile(
--> 609                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
    610               else:
    611                 # For function call ops, we add a 'SymbolicGradient'

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py in _GatherGrad(op, grad)
    895     # TODO(apassos): implement this for EAGER mode.
    896     while handle.op.type != ""VarHandleOp"":
--> 897       handle = handle.op.inputs[0]
    898   params_shape = gen_resource_variable_ops.variable_shape(handle)
    899   size = array_ops.expand_dims(array_ops.size(indices), 0)

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in __getitem__(self, i)
   1992 
   1993     def __getitem__(self, i):
-> 1994       return self._inputs[i]
   1995 
   1996 # pylint: enable=protected-access

IndexError: list index out of range
```
No problem when remove @tfe.defun, but eager is very slow without defun so I need to compile model with tfe.defun.
code to reproduce error:
```Python
import tensorflow as tf
import tensorflow.contrib.eager as tfe
import numpy as np
tfe.enable_eager_execution()
@tfe.defun
def embedding_crash(x, embedding):
    return tf.nn.embedding_lookup(embedding, x)
with tf.device(""gpu:0""):
    with tfe.GradientTape() as g:
        embed = tfe.Variable(np.ones((10, 100)).astype(np.float32))
        toy_data = np.ones((1, 10)).astype(np.int64)
        embedding_crash(toy_data, embed)
```
  ",1,,3,2018-01-04T16:01:07Z,2018-01-24T16:50:20Z,NONE,2018-01-05T00:05:43Z
15851,Tensorflow: Non-deterministic behaviour with large model using while_loop,stat:awaiting response,"Hello!
I believe to have found a bug in Tensorflow when running the code below. I am currently trying to build a neural transducer, and have stumbled across TF sometimes not returning any values for a function. I have not had the chance yet to test this out on another machine (no GPU, TF 1.4.1, Ubuntu 17.10). I am not sure whether this is indeed a bug or not, so I'm first posting it here. The code is redacted a bit to highlight only the parts that fail. [I've also posted to StackOverflow](https://stackoverflow.com/questions/48081063/tensorflow-non-deterministic-behaviour-with-large-model-using-while-loop) considering it might be an error in my code, but haven't got any response yet.

Notes:

- I believe the bug occurs around line 160, in the body of the while loop in the function run_full_transducer
- The session is returning [encoder_outputs, transducer_outputs]
- I do not use random functions
- As far as I can tell, if I remove the Print OP in line 164, the output is always 0

Example of a correct return value (more or less):
```
array([[[ 0.00811536, -0.00200322, -0.01177037,  0.03676344, -0.01909475,
             -0.03157664,  0.026092  ,  0.02367685, -0.01894805,  0.02832799,
              0.0377345 , -0.02583589, -0.02908566,  0.0299024 ,  0.00518877,
             -0.00064737,  0.01431572, -0.01053502, -0.01783628, -0.00382657,
              0.00076749, -0.02705991,  0.00112415, -0.0193013 ,  0.02346764,
              0.03014467,  0.02663364,  0.02503882,  0.03362656, -0.01877708,
              0.01859642,  0.02460729, -0.01395229, -0.03033791,  0.01177907,
             -0.03049169, -0.00389978,  0.02221515, -0.00073605,  0.01248251,
              0.00424051,  0.01070387,  0.02818898,  0.0321721 , -0.02462685,
              0.03495178, -0.02408989, -0.02742486,  0.00331823, -0.02311424,
             -0.01327039,  0.01095297,  0.02584363,  0.02083527, -0.01588045,
              0.02837921,  0.02100117,  0.00918638,  0.00109535, -0.02965789,
              0.01040822, -0.03240473,  0.00453057, -0.00603903]],
    
           [[ 0.01053647, -0.00457577, -0.01939731,  0.06317309, -0.03113565,
             -0.05525927,  0.04647589,  0.04213476, -0.03498235,  0.04962765,
              0.05989208, -0.04340284, -0.04777668,  0.05346756,  0.00395604,
             -0.0005207 ,  0.02079381, -0.01424338, -0.02584206, -0.00530154,
             -0.00031365, -0.04966826, -0.00091683, -0.03025239,  0.04526306,
              0.0595435 ,  0.0463665 ,  0.04578522,  0.05916505, -0.031725  ,
              0.03164144,  0.04257958, -0.02865831, -0.04795898,  0.01856991,
             -0.05512668, -0.00730711,  0.03953242,  0.00017992,  0.01710426,
              0.00754557,  0.01975578,  0.0469296 ,  0.05237873, -0.04435374,
              0.05924731, -0.04474678, -0.04605344,  0.00947831, -0.04284734,
             -0.01979787,  0.02003288,  0.04196753,  0.03900779, -0.02887472,
              0.05130195,  0.03419674,  0.0105699 ,  0.001114  , -0.0524303 ,
              0.01738651, -0.06084244,  0.01364262, -0.01153531]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```
Incorrect:
```
 [array([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],
    
           [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```

Code:
``` python
 import tensorflow as tf
    from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple
    from tensorflow.python.layers import core as layers_core
    import numpy as np
    # NOTE: Time major
    
    # Constants
    input_dimensions = 1
    vocab_size = 3
    input_embedding_size = 20
    encoder_hidden_units = 64
    inputs_embedded = True
    transducer_hidden_units = 64
    batch_size = 1
    GO_SYMBOL = vocab_size - 1  # TODO: Make these constants correct
    END_SYMBOL = vocab_size
    input_block_size = 2
    log_prob_init_value = 0
    
    
    # ---------------- Helper classes -----------------------
    
    
    # ----------------- Model -------------------------------
    embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)
    
    
    class Model(object):
        def __init__(self):
            self.encoder_inputs, self.encoder_inputs_length, self.encoder_hidden_state, \
            self.encoder_outputs, self.encoder_hidden_state_new = self.build_encoder_model()
            self.encoder_raw_outputs, self.trans_hidden_state, self.transducer_amount_outputs, \
            self.transducer_hidden_state_new, self.logits, self.decoder_prediction = self.build_transducer_model()
    
        def build_encoder_model(self):
            encoder_inputs = tf.Variable(tf.zeros(shape=(input_block_size, batch_size, input_dimensions)),
                                         dtype=tf.float32, name='encoder_inputs', trainable=False)
            encoder_inputs_length = tf.Variable([tf.shape(encoder_inputs)[0]], dtype=tf.int32,
                                                name='encoder_inputs_length', trainable=False)
            encoder_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, encoder_hidden_units)), dtype=tf.float32,
                                               name='encoder_hidden_state')  # Save the state as one tensor
    
            if inputs_embedded is True:
                encoder_inputs_embedded = encoder_inputs
            else:
                encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)
    
            # Build model
            encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)
    
            # Build previous state
            encoder_hidden_c, encoder_hidden_h = tf.split(encoder_hidden_state, num_or_size_splits=2, axis=0)
            encoder_hidden_c = tf.reshape(encoder_hidden_c, shape=[-1, encoder_hidden_units])
            encoder_hidden_h = tf.reshape(encoder_hidden_h, shape=[-1, encoder_hidden_units])
            encoder_hidden_state_t = LSTMStateTuple(encoder_hidden_c, encoder_hidden_h)
    
            #   encoder_outputs: [max_time, batch_size, num_units]
            encoder_outputs, encoder_hidden_state_new = tf.nn.dynamic_rnn(
                encoder_cell, encoder_inputs_embedded,
                sequence_length=encoder_inputs_length, time_major=True,
                dtype=tf.float32, initial_state=encoder_hidden_state_t)
    
            # Modify output of encoder_hidden_state_new so that it can be fed back in again without problems.
            encoder_hidden_state_new = tf.concat([encoder_hidden_state_new.c, encoder_hidden_state_new.h], axis=0)
            encoder_hidden_state_new = tf.reshape(encoder_hidden_state_new, shape=[2, -1, encoder_hidden_units])
    
            return encoder_inputs, encoder_inputs_length, encoder_hidden_state, encoder_outputs, encoder_hidden_state_new
    
        def build_transducer_model(self):
            encoder_raw_outputs = tf.Variable(tf.zeros(shape=(input_block_size, 1, encoder_hidden_units)),
                                              dtype=tf.float32,
                                              name='encoder_raw_outputs')
            trans_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, transducer_hidden_units)),
                                             dtype=tf.float32,
                                             name='trans_hidden_state')  # Save the state as one tensor
            transducer_amount_outputs = tf.Variable(0, dtype=tf.int32, name='transducer_amount_outputs',
                                                    trainable=False)
    
            # Model building
            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
                embedding=embeddings,
                start_tokens=tf.tile([GO_SYMBOL], [batch_size]),
                end_token=END_SYMBOL)
    
            attention_states = tf.transpose(encoder_raw_outputs,
                                            [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]
    
            attention_mechanism = tf.contrib.seq2seq.LuongAttention(
                encoder_hidden_units, attention_states)
    
            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
                tf.contrib.rnn.LSTMCell(transducer_hidden_units),
                attention_mechanism,
                attention_layer_size=transducer_hidden_units)
    
            projection_layer = layers_core.Dense(vocab_size, use_bias=False)
    
            # Build previous state
            trans_hidden_c, trans_hidden_h = tf.split(trans_hidden_state, num_or_size_splits=2, axis=0)
            trans_hidden_c = tf.reshape(trans_hidden_c, shape=[-1, transducer_hidden_units])
            trans_hidden_h = tf.reshape(trans_hidden_h, shape=[-1, transducer_hidden_units])
            trans_hidden_state_t = LSTMStateTuple(trans_hidden_c, trans_hidden_h)
    
            decoder = tf.contrib.seq2seq.BasicDecoder(
                decoder_cell, helper,
                decoder_cell.zero_state(1, tf.float32).clone(cell_state=trans_hidden_state_t),
                output_layer=projection_layer)
    
            outputs, transducer_hidden_state_new, _ = tf.contrib.seq2seq.dynamic_decode(decoder,
                                                                                        output_time_major=True,
                                                                                        maximum_iterations=transducer_amount_outputs)
            logits = outputs.rnn_output  # logits of shape [max_time,batch_size,vocab_size]
            decoder_prediction = outputs.sample_id  # For debugging
    
            # Modify output of transducer_hidden_state_new so that it can be fed back in again without problems.
            transducer_hidden_state_new = tf.concat(
                [transducer_hidden_state_new[0].c, transducer_hidden_state_new[0].h],
                axis=0)
            transducer_hidden_state_new = tf.reshape(transducer_hidden_state_new,
                                                     shape=[2, -1, transducer_hidden_units])
    
            return encoder_raw_outputs, trans_hidden_state, transducer_amount_outputs, transducer_hidden_state_new, \
                   logits, decoder_prediction
    
    
    model = Model()
    
    
    # ----------------- Alignment -------------------------
    
    # ----------------- Training --------------------------
    
    def run_full_transducer():
        # Inputs
        max_blocks = tf.placeholder(dtype=tf.int32, name='max_blocks')
        inputs_full_raw = tf.placeholder(shape=(None, batch_size, input_dimensions), dtype=tf.float32,
                                         name='inputs_full_raw')
        transducer_list_outputs = tf.placeholder(shape=(None,), dtype=tf.int32,
                                                 name='transducer_list_outputs')  # amount to output per block
    
        # Turn inputs into tensor which is easily readable
        inputs_full = tf.reshape(inputs_full_raw, shape=[max_blocks, input_block_size, batch_size, input_dimensions])
    
        # Outputs
        outputs_ta = tf.TensorArray(dtype=tf.float32, size=max_blocks)
    
        # Hidden states
        # TODO: make these correct
        encoder_hidden_init = tf.ones(shape=(2, 1, encoder_hidden_units))
        trans_hidden_init = tf.ones(shape=(2, 1, transducer_hidden_units))
    
        init_state = (0, outputs_ta, encoder_hidden_init, trans_hidden_init)
    
        def cond(current_block, outputs_int, encoder_hidden, trans_hidden):
            return current_block < max_blocks
    
        def body(current_block, outputs_int, encoder_hidden, trans_hidden):
            # Process encoder
            model.encoder_inputs = model.encoder_inputs.assign(inputs_full[current_block])
            model.encoder_inputs_length = model.encoder_inputs_length.assign([tf.shape(model.encoder_inputs)[0]])
            model.encoder_hidden_state = model.encoder_hidden_state.assign(encoder_hidden)
    
            # TODO: Error is SOMETIMES gone when using tf.Print
            current_block = tf.Print(current_block, [model.encoder_inputs], message='Enc in: ')
            #current_block = tf.Print(current_block, [model.encoder_outputs], message='Enc out: ')
    
            # Flow data from encoder to transducer
            model.encoder_raw_outputs = model.encoder_raw_outputs.assign(model.encoder_outputs)
            model.trans_hidden_state = model.trans_hidden_state.assign(trans_hidden)
            model.transducer_amount_outputs = model.transducer_amount_outputs.assign(transducer_list_outputs[current_block])
    
            # Note the outputs
            outputs_int = outputs_int.write(current_block, model.logits)
    
            return current_block + 1, outputs_int, model.encoder_hidden_state_new, model.transducer_hidden_state_new
    
        _, outputs_final, _, _ = tf.while_loop(cond, body, init_state)
    
        # Process outputs
        outputs = outputs_final.stack()  # Now the outputs are of shape [block, amount_of_trans_out, batch_size, vocab]
        outputs = tf.reshape(outputs, shape=(-1, 1, vocab_size))  # And now its [amount_outputs, batch_size, vocab]
    
        model.encoder_outputs = tf.Print(model.encoder_outputs, [model.encoder_outputs], message='Current block enc out: ')
    
        return max_blocks, inputs_full_raw, transducer_list_outputs, outputs, model.encoder_outputs
    
    # ---------------------- Testing -----------------------------
    
    
    # ---------------------- Management -----------------------------
    
    init = tf.global_variables_initializer()
    
    with tf.Session() as sess:
        sess.run(init)
    
        inp_max_blocks, inp_inputs_full_raw, inp_trans_list_out, out_outputs, enc_out = run_full_transducer()
    
        print sess.run([enc_out, out_outputs], feed_dict={
            inp_max_blocks: 3,
            inp_inputs_full_raw: np.ones(shape=(3 * input_block_size, 1, input_dimensions)),
            inp_trans_list_out: [1, 3, 2]
        })
```

Info about machine:
```

== cat /etc/issue ===============================================
Linux nikita-coolboi 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""17.10 (Artful Aardvark)""
VERSION_ID=""17.10""
VERSION_CODENAME=artful

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 7.2.0-8ubuntu3) 7.2.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux nikita-coolboi 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.5.1)
tensorflow (1.4.1)
tensorflow-tensorboard (0.4.0rc3)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.4.1
tf.GIT_VERSION = v1.4.0-19-ga52c8d9
tf.COMPILER_VERSION = v1.4.0-19-ga52c8d9
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```
Have I written custom code Yes
OS Platform and Distribution Ubuntu 17.10 (Artful Aardvark)
TensorFlow installed from binary
TensorFlow version 1.4.1
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce Execute the code block as a python file a few times.

Thanks!
Nikita
  ",0,,2,2018-01-04T15:20:55Z,2018-01-09T19:53:38Z,NONE,2018-01-05T01:40:58Z
15850,unable to install from source with undefined external dependency target error,stat:awaiting response,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.4.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**:
`sudo bazel build --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package`
### Describe the problem
 I'm trying to install r1.4.0 from source with CPU version and follow the install guidelines on the official website.  But some errors occur. (The errors are shown in ""Source code / logs"" )
(1) It seems that the undefined external dependency target ""@local_config_sycl"" is referred in some files (eg. //tensorflow-1.4.0/ third_party/eigen3/build ).
(2) I am sure the openCL suppurt is disabled when configure.
(3) I tried to install version r1.0.1 from source, but the same errors occur.
(4) I tried to install version r1.4.0 from binary, and success.

Why this happens and how can I fix it? Thank you!

### Source code / logs
`ERROR: /home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):
	File ""/home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD"", line 27
		cc_library(name = ""syclrt"", srcs = [sycl_libr..."")], <3 more arguments>)
	File ""/home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD"", line 30, in cc_library
		sycl_library_path
name 'sycl_library_path' is not defined
ERROR: /home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD:39:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '@local_config_sycl//sycl:sycl'
ERROR: /home/tangdehong/OpenSourceCode/tensorflow-1.4.0/third_party/eigen3/BUILD:20:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//third_party/eigen3:eigen3'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed
INFO: Elapsed time: 0.745s
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/contrib/losses ... (17 packages)
`
Thank you!",0,,6,2018-01-04T15:16:57Z,2018-01-10T20:43:22Z,NONE,2018-01-08T15:46:49Z
15844,MonitoredTrainingSession aborts without error or exeption,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04.3
- **TensorFlow installed from (source or binary)**:
binary, pip install
- **TensorFlow version (use command below)**:
v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 
Python 3.6.3 :: Anaconda, Inc.
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
cuda_8.0.61.2, libcudnn.so.6.0.21
- **GPU model and memory**:
Titan-X, 32GB
- **Exact command to reproduce**:
During some training runs, this script just ends after few epochs with printing 'eval done'. It doesn't print any error nor an exception. In other runs, with the same setup in runs through. How could this happen that the for-loop stops even the epochs is smaller than 100?

Edit: I tried it also without the try-except block around the `MonitoredTrainingSession`, but it was the same: no exception, no error, epoch ~ 15 and printing ""eval done""


```python
    tf.logging.set_verbosity(tf.logging.INFO)
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'

    listener_eval = ExampleCheckpointSaverListener(fun_after_save=eval_model)
    listener_generate = ExampleCheckpointSaverListener(fun_after_save=generate)
    hooks_train = [
      tf.train.SummarySaverHook(output_dir=model_path, save_secs=60, scaffold=scaffold_train),
      tf.train.LoggingTensorHook(train_log_tensors, every_n_secs=30),
      tf.train.CheckpointSaverHook(checkpoint_dir=model_path, save_secs=600, scaffold=scaffold_train,
                                   listeners=[listener_eval, listener_generate])
    ]

    try:
      with tf.train.MonitoredTrainingSession(is_chief=True, checkpoint_dir=model_path, save_checkpoint_secs=None,
                                             hooks=hooks_train, save_summaries_secs=None, save_summaries_steps=None,
                                             config=sess_config, scaffold=scaffold_train, log_step_count_steps=1000, stop_grace_period_secs=20) as sess:

        for epochs in range(0, 100):
          print(epochs)
          try:
            while True:
              sess.run([train_op])
          except tf.errors.OutOfRangeError as e:
            print(""Epoch %d end."" % epochs)
            sess.run(train_reader.iterator.initializer)
    except:
      print(""MonitoredTrainingSession"")
      print(sys.exc_info())

    print(""eval done"")
```

[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1603159/tf_env.txt)


  ",0,,4,2018-01-04T11:29:10Z,2018-01-05T21:07:25Z,CONTRIBUTOR,2018-01-04T22:31:35Z
15843,WIP: LSTMBlockFusedCell supports Dropout,cla: yes,"Fix #13649.

The work has not been done yet. However I'm not sure whether the solution is approved, I mean, using DropoutWrapper for inner implementation. So I open the PR early to collect feedback. cc @ebrevdo.

I'll add test case later.
  ",0,,2,2018-01-04T11:08:15Z,2018-01-09T04:03:07Z,CONTRIBUTOR,2018-01-09T04:03:07Z
15842,Windows: Disable bfloat16_test and framework_dtypes_test,"awaiting review,cla: yes","Reenable after fixing
https://github.com/tensorflow/tensorflow/issues/15297

@gunan ",1,,3,2018-01-04T11:01:19Z,2018-01-05T19:13:04Z,MEMBER,2018-01-04T11:52:40Z
15838,Gif can't be decoded. InvalidArgumentError: Invalid GIF data,stat:awaiting response,"## System information
**Have I written custom code : yes
OS Platform and Distribution : Mac OS 10.12.3
TensorFlow installed from : pip3
TensorFlow version : 1.4
Bazel version : N/A
CUDA/cuDNN version : N/A
GPU model and memory : N/A**

## Describe the problem
![0071qvrrgy1fn3h6v55gag308w0adx6p](https://user-images.githubusercontent.com/8256827/34550343-04b28b42-f14b-11e7-9b8a-729a82ba4742.gif)

The gif can be decoded by PIL,but the error occurred when I used tf.image.decode_gif to decode.

```
def load_gif(image_path, sess):
    image = tf.read_file(image_path)
    image = tf.image.decode_gif(image)
    return sess.run(image)

load_gif('0071Qvrrgy1fn3h6v55gag308w0adx6p',sess))
```

The error is:
```
NotFoundError                             Traceback (most recent call last)
/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1322     try:
-> 1323       return fn(*args)
   1324     except errors.OpError as e:

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1301                                    feed_dict, fetch_list, target_list,
-> 1302                                    status, run_metadata)
   1303 

/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    472             compat.as_text(c_api.TF_Message(self.status.status)),
--> 473             c_api.TF_GetCode(self.status.status))
    474     # Delete the underlying status object from memory otherwise it stays alive

NotFoundError: /Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p; No such file or directory
	 [[Node: ReadFile_21 = ReadFile[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_21/filename)]]

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
<ipython-input-70-7b1d3fa2f712> in <module>()
      1 print(load_gif('/Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p',
----> 2                sess))

<ipython-input-55-fd8c43263043> in load_gif(image_path, sess)
      3     # image = tf.image.decode_png(image, channels=3)
      4     image = tf.image.decode_gif(image)
----> 5     return sess.run(image)

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    887     try:
    888       result = self._run(None, fetches, feed_dict, options_ptr,
--> 889                          run_metadata_ptr)
    890       if run_metadata:
    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1119       results = self._do_run(handle, final_targets, final_fetches,
-> 1120                              feed_dict_tensor, options, run_metadata)
   1121     else:
   1122       results = []

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1315     if handle is None:
   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1317                            options, run_metadata)
   1318     else:
   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1334         except KeyError:
   1335           pass
-> 1336       raise type(e)(node_def, op, message)
   1337 
   1338   def _extend_graph(self):

NotFoundError: /Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p; No such file or directory
	 [[Node: ReadFile_21 = ReadFile[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_21/filename)]]

Caused by op 'ReadFile_21', defined at:
  File ""/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/usr/local/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/usr/local/lib/python3.6/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/usr/local/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2698, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2808, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-70-7b1d3fa2f712>"", line 2, in <module>
    sess))
  File ""<ipython-input-55-fd8c43263043>"", line 2, in load_gif
    image = tf.read_file(image_path)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 376, in read_file
    ""ReadFile"", filename=filename, name=name)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): /Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p; No such file or directory
	 [[Node: ReadFile_21 = ReadFile[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_21/filename)]]```
  
  
  
  ",0,,4,2018-01-04T07:27:19Z,2018-01-23T18:01:19Z,NONE,2018-01-04T19:05:38Z
15836,Add axis support for `tf.nn.crelu`,"awaiting testing (then merge),cla: yes","This fix tries to address the issue raised in #15619 where it was not possible to specify an `axis` for
`tf.nn.crelu`. By default, `axis=-1` was used for concatenation implicitly.

This fix adds the support of `axis` for `tf.nn.crelu`, and adds test cases for it.

This fix fixes #15619.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,7,2018-01-04T06:21:58Z,2018-01-11T17:33:24Z,MEMBER,2018-01-05T15:42:22Z
15834,I think some bug in  tf.contrib.layers.l2_regularizer,,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:win10
- **TensorFlow installed from (source or binary)**:install
- **TensorFlow version (use command below)**:1.5
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
when I call the  tf.contrib.layers.l2_regularizer(0.5)(w), I was toll that ""Expected int64, got 0.5 of type 'float' instead."" But the doc says that it need a float clearly and as a weight it can't be an integer.

### Source code / logs
` l2 = 0
for w in tf.global_variables():
    l2 += tf.contrib.layers.l2_regularizer(0.5)(w)
loss = tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels,logits=result_vector)+l2`

  ",0,,1,2018-01-04T05:41:11Z,2018-01-08T15:45:58Z,NONE,2018-01-04T06:52:26Z
15832,Feature Request: saver.save mkdir if directory not exist,,"### Describe the problem
`saver.save(sess, 'my-model') `
returns error if directory not exist.
It would be nice if saver can automatically create the missing directory.
`Traceback (most recent call last):
  File ""tf_voice_recognition.py"", line 783, in <module>
    saver.save(sess, 'my-model')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1594, in save
    raise exc
ValueError: Parent directory of my-model doesn't exist, can't save.
`",0,,4,2018-01-04T04:06:02Z,2018-01-04T09:01:05Z,CONTRIBUTOR,2018-01-04T05:58:27Z
15830,tensorflow/contrib/lite/kernels/resize_bilinear.cc:42 NumInputs(node) != 1 (2 != 1),"comp:lite,stat:awaiting response","###System information
Have I written custom code: Yes, 
OS Platform and Distribution: Ubuntu14.04
TensorFlow installed from: source build w/ Bazel
TensorFlow version: 1.4
Python version: Anaconda 3.5.2
Bazel version: 0.9.0
GCC/Compiler version (if compiling from source): gcc version 4.8.4
CUDA/cuDNN version: Not relevant
GPU model and memory: Not  relevant
Exact command to reproduce: Not relevant


### Describe the problem

I construct a network with only bilinear_resize operation (I use the tf.image.resize_bilinear) and add operation, and  convert it to a tflite model successfully. However,  when I run the tflite mode, it comes to the errors as follows:

java.lang.NullPointerException: Can not allocate memory for the given inputs: tensorflow/contrib/lite/kernels/resize_bilinear.cc:42 NumInputs(node) != 1 (2 != 1)

I find the code line in resize_bilinear.cc:42 as follows:
TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);

Is it right to modify the code line to : 
TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2); 
   


### Source code / logs
def network():
      img = tf.placeholder(name='img', dtype=tf.float32, shape=(1,100,100,3))
      img = tf.layers.conv2d(img, 3, (3,3), padding='same', name='conv1')
      img = tf.image.resize_bilinear(img, [200,200])
      var = tf.get_variable('weights', dtype=tf.float32, shape=(1,200,200,3))
      val = img + var
      out = tf.identity(val, name='out')


  ",0,,3,2018-01-04T03:42:09Z,2018-01-30T22:09:50Z,NONE,2018-01-04T12:59:48Z
15829,[Bazel/Windows] Wrap rm -rf in Bash for Windows (and some refactoring),"awaiting review,cla: yes","`rm -rf` is not available in Windows command prompt. Run it in Bash like `patch -pl`.

Refactor the wrapping logic into a new macro `_wrap_bash_cmd`.",1,,3,2018-01-04T03:10:47Z,2018-01-05T01:33:43Z,CONTRIBUTOR,2018-01-04T15:56:59Z
15827,ci.tensorflow.org lacks a security certificate,type:support,"### The Problem
https://ci.tensorflow.org/ now lacks a security certificate (or it expired).
To repro, visit https://ci.tensorflow.org/. Chrome will note that the connection is not private.

This is breaking TensorBoard's tests that run on travis. The tests `pip install` nightly versions of TensorFlow from these URLs:

- https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp27-none-linux_x86_64.whl 
- https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp34-cp34m-linux_x86_64.whl

### Error Logs from a Failed Test Run

> Collecting tensorflow==1.head from https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp27-none-linux_x86_64.whl
> Exception:
> Traceback (most recent call last):
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/basecommand.py"", line 215, in main
>     status = self.run(options, args)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/commands/install.py"", line 335, in run
>     wb.build(autobuilding=True)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/wheel.py"", line 749, in build
>     self.requirement_set.prepare_files(self.finder)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/req/req_set.py"", line 380, in prepare_files
>     ignore_dependencies=self.ignore_dependencies))
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/req/req_set.py"", line 620, in _prepare_file
>     session=self.session, hashes=hashes)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 821, in unpack_url
>     hashes=hashes
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 659, in unpack_http_url
>     hashes)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 853, in _download_http_url
>     stream=True,
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 488, in get
>     return self.request('GET', url, **kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 386, in request
>     return super(PipSession, self).request(method, url, *args, **kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 475, in request
>     resp = self.send(prep, **send_kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 596, in send
>     r = adapter.send(request, **kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/cachecontrol/adapter.py"", line 47, in send
>     resp = super(CacheControlAdapter, self).send(request, **kw)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/adapters.py"", line 497, in send
>     raise SSLError(e, request=request)
> SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:661)

  ",1,,6,2018-01-04T00:12:28Z,2018-01-04T08:37:37Z,MEMBER,2018-01-04T00:17:14Z
15826,Remove :0 in final result argument,"awaiting review,cla: no","Small change to line 393 - in my recent testing of retrain and the label_image workflow as of today (TF master @  136697e) I had to remove :0 from the label_image output_layer argument.

Thank you.",0,,4,2018-01-04T00:00:28Z,2018-01-04T15:17:07Z,CONTRIBUTOR,2018-01-04T00:00:52Z
15825,Update advise.md,"awaiting testing (then merge),cla: yes",,0,,4,2018-01-03T21:42:09Z,2018-01-04T01:35:51Z,CONTRIBUTOR,2018-01-03T21:43:33Z
15823,Fix a pessimizing-move warning in GetDeviceLapackInfo(),"awaiting testing (then merge),cla: yes","clang reports:
<pre>
./tensorflow/core/kernels/cuda_solvers.h:430:10: warning: moving a local object in a return statement prevents copy elision [-Wpessimizing-move]
  return std::move(new_dev_info);
         ^
./tensorflow/core/kernels/cuda_solvers.h:430:10: note: remove std::move call here
  return std::move(new_dev_info);
         ^~~~~~~~~~            ~
</pre>",1,,3,2018-01-03T20:39:18Z,2018-01-23T14:25:15Z,CONTRIBUTOR,2018-01-03T20:49:20Z
15822,Updating error handling in normalize_tuple,"awaiting testing (then merge),cla: yes","In normalize_tuple we test to see if all values are an int (or able to be cast to an int) using `int()`

`ValueError` is thrown if `int()` is called with an input like 'asdf' - this is caught and gives a helpful error, using the 'name' param to provide more context.

**However**, when given an input other than a string or int, a `TypeError` is thrown. This is **not** caught - making error messages much more esoteric than the helpful one written out here, especially when coming from a very long stack trace.

For example, before this change I was getting an error:
```
TypeError: int() argument must be a string or a number, not 'tuple'
```

With this change, I get the more useful:
```
ValueError: The `kernel_size` argument must be a tuple of 2 integers. 
Received: ((0, 3), 50) including element (0, 3) of type <type 'tuple'>
```
  
  ",1,,2,2018-01-03T16:41:25Z,2018-01-26T05:31:40Z,CONTRIBUTOR,2018-01-24T13:14:27Z
15821,TFGAN not compatible with eager execution mode,comp:eager,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colaboratory Google Compute Engine backend (not sure about OS here)
- **TensorFlow installed from (source or binary)**: binary (non-GPU version)
- **TensorFlow version (use command below)**: 1.5.0-dev20180102
- **Python version**: 3

### Describe the problem
When enabling eager execution mode
```python
import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()
```
and running 
```python
noise_dims = 64
gan_model = tfgan.gan_model(
    generator_fn,
    discriminator_fn,
    real_data=images,
    generator_inputs=tf.random_normal([batch_size, noise_dims]))
```
from the [TFGAN tutorial](https://github.com/tensorflow/models/blob/master/research/gan/tutorial.ipynb) by @joel-shor, I get the following error:
```bash
ValueError: When Eager Execution is enabled, variable collections are not supported.
```
because of the following lines:
https://github.com/tensorflow/tensorflow/blob/8c2d6fc2b0202304885d5d6c3cba57eb2a1b3262/tensorflow/contrib/gan/python/train.py#L119-L121
.

Are there any plans to make TFGAN compatible with eager in the short term? Is there any help wanted in this regard? I'd be happy to contribute.",1,,12,2018-01-03T16:08:13Z,2018-01-13T00:53:50Z,NONE,2018-01-03T23:37:49Z
15820,"Revert ""Remove unneeded branch check (#13495)""","awaiting testing (then merge),cla: yes","This reverts commit 3aee5f1df97f44d9c14995505895f1877d7de8ae.

Fix build with Python3",0,,1,2018-01-03T15:36:42Z,2018-01-03T16:36:32Z,MEMBER,2018-01-03T15:46:57Z
15815,"When data become large,parition variables can not initialized successfully",,"#15216 
i have a issue, but nobody help me to solve it ",0,,6,2018-01-03T12:42:54Z,2018-01-30T00:17:14Z,NONE,2018-01-04T01:39:21Z
15812,"Closing input stream, runner session in TensorFlowInferenceInterface.java and fixing bit changes","cla: yes,stat:awaiting response",,1,,3,2018-01-03T11:19:08Z,2018-01-04T23:28:59Z,CONTRIBUTOR,2018-01-03T18:43:27Z
15807,Fix unstable test case for Select op,"awaiting testing (then merge),cla: yes","Fix #14862. CF: #15764

In the test case for Select op, the condition might switch to another value when `x1` is close to `x2`.  The PR is opened to resolve the unstable condition.

Test passed for 100 times.
```bash
[facai@h1077922 tensorflow]$ bazel test --runs_per_test=100 -c opt //tensorflow/cc:gradients_math_grad_test
HEAD is now at c642574... TST: modify unstable test case
INFO: Found 1 test target...
Target //tensorflow/cc:gradients_math_grad_test up-to-date:
  bazel-bin/tensorflow/cc/gradients_math_grad_test
INFO: Elapsed time: 49.959s, Critical Path: 21.29s
//tensorflow/cc:gradients_math_grad_test                                 PASSED in 21.3s
  Stats over 100 runs: max = 21.3s, min = 0.7s, avg = 11.2s, dev = 4.4s

Executed 1 out of 1 test: 1 test passes.
```

cc @gunan @drpngx 
  ",1,,4,2018-01-03T06:12:08Z,2018-01-04T02:13:18Z,CONTRIBUTOR,2018-01-03T20:57:59Z
15806,Can we install tensorflow with a package manager in Linux distro?,,"As we all know, Tensorflow is a major opensource deeplearning framework to the developers. 
BTW, How can we install tensorflow with a package manager such as apt-get (for *.deb), yum/zypper/dnf (for *.rpm)  in Linux distro?

* Reference - https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/builds

I could not find related scripts from the above web address. Does tensorflow support 1)  manual compilation with ./tools/ci_build/build/*.sh and 2) pre-built docker-based compilation only?

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.0 and Ubuntu 16.04.3
- **TensorFlow installed from (source or binary)**: Latest version of Tensorflow (form github)
- **TensorFlow version (use command below)**: Latest version of Tensorflow (form github)
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: with Cmake (w/o Bazel)
- **GCC/Compiler version (if compiling from source)**: GCC 5.0
- **CUDA/cuDNN version**:  None (w/ CPU only)
- **GPU model and memory**: None , DRAM 16GB
- **Exact command to reproduce**:   Nonthing.



### Describe the problem
No support

### Source code / logs
Omission. 

  ",0,,1,2018-01-03T06:03:26Z,2018-01-03T20:55:34Z,NONE,2018-01-03T20:55:34Z
15803,Memory allocation improvement for `decode_libsvm`,"awaiting testing (then merge),cla: yes","This fix is an improvement to #14330. Previously, string split was handled through `str_util::Split`, which may incur unnecessary memory allocations. This fix uses StringPiece instead.

See comment https://github.com/tensorflow/tensorflow/pull/14330#pullrequestreview-79877956 for reference.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,1,2018-01-03T02:34:18Z,2018-01-03T20:26:52Z,MEMBER,2018-01-03T20:26:39Z
15802,tf.stack eats memory over time,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**:  3.5
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: 1060 + 6GB
 

### Describe the problem
I use tf.stack to stack 2 images. But the memory used by this program increase over time. I use `memory_profiler` check it. it is caused by tf.stack, here is the minimal re-produce code:

```
import tensorflow as tf
import glob
import gc
from memory_profiler import profile


@profile
def function_mark():
  pass

@profile
def stack_images():
  image_file_list = glob.glob(""car_images/*.jpg"")
  sess = tf.Session()

  for _ in range(300):
    # read image
    image1 = tf.gfile.FastGFile(image_file_list[0], 'rb').read()
    image2 = tf.gfile.FastGFile(image_file_list[1], 'rb').read()
    # decode image
    image1_decode = tf.image.decode_image(image1, channels=3)
    image2_decode = tf.image.decode_image(image2, channels=3)
    # stack image
    image_stack = tf.stack([image1_decode, image2_decode])
    # run session
    r_image_stack = sess.run(image_stack)
    # mark function. so I can check the memory-usage of every loop.
    function_mark()
    # force garbage collection, so all the un-reference variable will be freed.
    del r_image_stack
    gc.collect()
```

 First, I profile it line by line, here is the result:
**you can see it very clearly that line 26 take a lot of memory. My image is 800*600 and I only stack 2 image each time, so 1.3G memory consumption is not normal.**

> 
> Line #    Mem usage    Increment   Line Contents
> ================================================
>     11    190.0 MiB    190.0 MiB   @profile
>     12                             def stack_images():
>     13    190.0 MiB      0.0 MiB     image_file_list = glob.glob(""car_images/*.jpg"")
>     14    421.6 MiB    231.7 MiB     sess = tf.Session()
>     15
>     16   1998.4 MiB      0.0 MiB     for _ in range(300):
> 
>     17                                 # read image
>     18   1992.5 MiB      1.5 MiB       image1 = tf.gfile.FastGFile(image_file_list[0], 'rb').read()
>     19   1992.5 MiB      0.0 MiB       image2 = tf.gfile.FastGFile(image_file_list[1], 'rb').read()
>     20                                 # decode image
>     21   1992.8 MiB     77.6 MiB       image1_decode = tf.image.decode_image(image1, channels=3)
>     22   1993.3 MiB    108.6 MiB       image2_decode = tf.image.decode_image(image2, channels=3)
>     23                                 # stack image
>     24   1993.3 MiB      0.7 MiB       image_stack = tf.stack([image1_decode, image2_decode])
>     25                                 # run session
>     26   1998.4 MiB   1350.4 MiB       r_image_stack = sess.run(image_stack)
>     27                                 # mark function. so I can check the memory-usage of every loop.
>     28   1998.4 MiB     29.0 MiB       function_mark()
>     29                                 # force garbage collection, so all the un-reference variable will be freed.
>     30   1998.4 MiB      0.0 MiB       del r_image_stack
>     31   1998.4 MiB      8.9 MiB       gc.collect()

Then **I profile it over time.** I use function `function_mark` to distinguish each loop. So we can see it very clearly that the memory usage of this program is increase over time.
![figure_1](https://user-images.githubusercontent.com/5325686/34505710-d25c80b0-f061-11e7-99c6-5db2e990d9aa.png)

My question is: How should I avoid this problem. because it cause a serious performance regression.
",0,,2,2018-01-03T02:15:52Z,2018-01-03T06:27:07Z,CONTRIBUTOR,2018-01-03T06:27:07Z
15801,Remove calculation of unnecessary matrix columns in SVD gradient,"awaiting testing (then merge),cla: yes","The SVD gradient calculation when `compute_uv=False` currently uses the orthogonal ""U"" and ""V"" matrices returned by the SVD operation with `full_matrices=True`, but it really requires only the `full_matrices=False` versions.  This pull request makes the calculation use the `full_matrices=False` versions.

@rmlarsen pointed out that this change could be made in https://github.com/tensorflow/tensorflow/pull/14259#discussion_r157067512.",1,,8,2018-01-03T01:41:42Z,2018-01-25T21:11:40Z,CONTRIBUTOR,2018-01-03T01:49:21Z
15800,[bug] tf.estimator.DNNClassifier setting n_classes has no effect,,"### Describe the problem
I was following the examples for a [tensorflow estimator](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html). I am setting n_classes but the label check in (_check_labels tensorflow/python/estimator/canned/head.py"", line 222) keeps kicking back the following error:

**ValueError: Mismatched label shape. Classifier configured with n_classes=1.  Received 4. Suggested Fix: check your n_classes argument to the estimator and/or the shape of your label.**

### Source code / logs
``` python
import tensorflow as tf
import numpy as np

trainX = np.array([1,0,2,3])
num_classes = 4
feature_names = ['f1']
feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]
classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns, 
                                            n_classes=num_classes, #setting number of classes here
                                            hidden_units=[10])

def input_fn():
    my_int_variable = tf.get_variable(""my_int_variable"", [1], dtype=tf.int32, initializer=tf.zeros_initializer)
    label = tf.one_hot(my_int_variable, num_classes) #using same number of classes
    return {'f1':trainX},label

classifier.train(input_fn=lambda: input_fn())
```

``` bash
Traceback (most recent call last):
  File ""test.py"", line 17, in <module>
    classifier.train(input_fn=lambda: input_fn())
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 334, in _model_fn
    config=config)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 203, in _dnn_model_fn
    logits=logits)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 493, in create_estimator_spec
    features=features, mode=mode, logits=logits, labels=labels)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 433, in create_loss
    label_ids = self._label_ids(_check_labels(_maybe_expand_dim(labels), 1))
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 222, in _check_labels
    (expected_labels_dimension, dim1))
ValueError: Mismatched label shape. Classifier configured with n_classes=1.  Received 4. Suggested Fix: check your n_classes argument to the estimator and/or the shape of your label.
```

------------------------

### System information
Mac OSX 10.12.6
Python 2.7
Tensorflow ('v1.4.0-19-ga52c8d9b01', '1.4.1')



",0,,3,2018-01-03T00:18:44Z,2018-01-03T15:52:39Z,NONE,2018-01-03T10:05:09Z
15797,Change dso_loader to look for libcupti.so instead of libcupti.so.8.0,"awaiting review,cla: yes","On Android, it is hard to package libcupti.so.8.0 with bazel to generate CUDA-enabled apk
The cc_library macro in bazel only looks for *.so, not *.so.*",1,,5,2018-01-02T22:58:58Z,2018-01-05T06:20:35Z,CONTRIBUTOR,2018-01-04T17:29:49Z
15796,Fix build issues with cuda 9.1 through updating eigen.,cla: yes,,0,,2,2018-01-02T21:55:13Z,2018-01-04T18:32:04Z,OWNER,2018-01-04T18:26:17Z
15795,configure.py environment variables,stat:awaiting response,"Have I written custom code: No
OS Platform and Distribution: Linux (Any?)
TensorFlow installed from: Source
TensorFlow version: 1.4 (Master Branch)
Bazel version: 0.6
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A


### System information
- Building from source
- Branch Master (currently d87a9fbbc5f49ec5ae8eb52c62628f0b1a0bf67f)



### Describe the problem
Line 1353 of configure.py needs to have int( ) wrapped around the function get_var, otherwise the string that is returned always flags positive and setting TF_SET_ANDROID_WORKSPACE=0 environment variable to avoid user interaction in building from source will never work.

  ",0,,2,2018-01-02T21:50:40Z,2018-01-03T16:35:40Z,NONE,2018-01-03T07:34:35Z
15790,order quantized table by value for ease of reading,cla: yes,,0,,2,2018-01-02T15:18:42Z,2018-01-02T17:40:27Z,CONTRIBUTOR,2018-01-02T17:40:24Z
15789,order quantized table by value for ease of reading,cla: yes,,0,,2,2018-01-02T15:18:10Z,2018-01-02T17:40:42Z,CONTRIBUTOR,2018-01-02T17:40:42Z
15787,order quantized table by value for ease of reading,cla: yes,,0,,2,2018-01-02T15:11:26Z,2018-01-02T19:29:58Z,CONTRIBUTOR,2018-01-02T17:40:52Z
15780,[configure] eagerly determine the truthfulness of environment variables,"awaiting testing (then merge),cla: yes","Eagerly determine the truthfulness of environment variables in `get_var` function.

This way we can skip checking, e.g. Android workspace setup if the user sets `TF_SET_ANDROID_WORKSPACE=0`

Tested manually. 
",1,,7,2018-01-02T05:59:18Z,2018-01-03T01:13:02Z,CONTRIBUTOR,2018-01-02T06:01:07Z
15775,R1.2,cla: no,,0,,3,2018-01-01T18:44:44Z,2018-01-02T01:20:34Z,NONE,2018-01-02T01:20:34Z
15774,Fix invalid Markdown in docstring,"awaiting testing (then merge),cla: yes","There is currently invalid markdown in the docstring, which is causing the docs site to render incorrectly.

https://www.tensorflow.org/api_docs/python/tf/contrib/gan/estimator/GANEstimator

<img width=""902"" alt=""screen shot 2018-01-01 at 10 17 59 am"" src=""https://user-images.githubusercontent.com/279498/34469987-1d2fbce6-eedd-11e7-896b-a43f65326fd0.png"">

This patch fixes the indentation on the MD code block, which should fix the issue.",0,,5,2018-01-01T18:19:03Z,2018-01-01T21:20:03Z,CONTRIBUTOR,2018-01-01T18:19:44Z
15773,How to define multiple loss function and train_op  in tf.estimator.EstimatorSpec,stat:awaiting tensorflower,"

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0
- **Python version**: 2.7.12
- **CUDA/cuDNN version**: 6.0
- **GPU model and memory**: 1080 Ti + 1080

### Describe the problem
I'm currently implementing a pose estimation system and I defined my network with 3 loss and train_op in each of degree, yaw, pitch and roll. And I'm current using your tf.estimator API which I think is pretty convenient to monitor the system, however I found that I may only be able to define one loss and train_op using this set of API. I would like to know if there is any solution to train and monitor the system with multiple loss and train_op at the same time. Thanks.

### Source code / logs
```
    return tf.estimator.EstimatorSpec(
        mode=mode,
        predictions=predictions,
        loss=[yaw_total_loss, pitch_total_loss, roll_total_loss],  # error
        train_op=[yaw_train_op, pitch_train_op, roll_train_op],  # error
        eval_metric_ops=None)
```
",1,,3,2018-01-01T18:14:02Z,2018-01-09T23:48:24Z,NONE,2018-01-09T23:13:30Z
15772,remove trailing semicolon at the end of line,"awaiting testing (then merge),cla: yes","removed trailing semicolon(;) in the statement

according to [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html?showone=Semicolons#Semicolons)
 _""Do not terminate your lines with semi-colons and do not use semi-colons to put two commands on the same line.""_ ",0,,5,2018-01-01T16:17:46Z,2018-01-01T19:03:28Z,CONTRIBUTOR,2018-01-01T16:28:55Z
15768,Training broke with ResourceExausted error,,"------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.5
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: TITAN X, 12207MiB

----------------------

Most Probably everyone will be asking about this is a question for StackOverflow. Here is the link to the StackOverflow question,  But please take a look at the problem. There could be some bug in tensorflow as the error occurs after 32 epoch.
https://stackoverflow.com/questions/48007984/training-broke-with-resourceexausted-error

Here is the code of the model, https://paste.ubuntu.com/26298336/
A short description of the model would be,

- Character level Embedding Vector -> Embedding lookup -> LSTM1
- Word level Embedding Vector->Embedding lookup -> LSTM2
- [LSTM1+LSTM2] -> single layer MLP-> softmax layer/CRF layer
- [LSTM1+LSTM2] -> Single layer MLP-> WGAN discriminator

While running the code it produces the following error output at the epoch 32,

`ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[24760,100] [[Node: chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split = Split[T=DT_FLOAT, num_split=4, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients_2/Add_3/y, chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] [[Node: bi-lstm/bidirectional_rnn/bw/bw/stack/_167 = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]]`

My question is if there is any error then it should occur in the first epoch why at 32 epoch?

I am using embedding_lookup in following way,

```
_word_embeddings = tf.Variable(
                        embeddings,
                        name=""_word_embeddings"",
                        dtype=tf.float32,
                        trainable=False)
            word_embeddings = tf.nn.embedding_lookup(_word_embeddings, self.word_ids, name=""word_embeddings"")

```

Where `embeddings` is a `(61698, 100)` sized vector. which is only 24 MB. However in the error message, it showed the error with, `(24760, 100)` sized vector which is only 10MB. It also produces warning while declaring optimizers for the model. it was suggested as below,

> gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.

",0,,1,2018-01-01T11:31:01Z,2018-01-03T02:13:59Z,NONE,2018-01-03T02:13:59Z
15766,tf.assert_equal raises incorrect traceback in Eager mode,"comp:eager,type:bug/performance","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.1 LTS
- **TensorFlow installed from (source or binary)**: pip binary
- **TensorFlow version (use command below)**: 1.5.0-dev20171227
- **Python version**: 3.5.0
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: python main.py


### Describe the problem

In eager mode, tf.assert_equal only shows `[]` in traceback message when two inputs are different. However, in graph mode, it does show different values in the message. 

### Source code / logs

```python
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

x = tf.constant([1,2,3])
y = tf.constant([3,2,1])

with tf.control_dependencies([tf.assert_equal(x, y)]):
    output = tf.reduce_sum(x)

```


Eager Mode Traceback:

```
Traceback (most recent call last):
  File ""/Users/matt/PycharmProjects/scratch/main.py"", line 9, in <module>
    with tf.control_dependencies([tf.assert_equal(x, y)]):
  File ""/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 376, in assert_equal
    summary_msg)))
tensorflow.python.framework.errors_impl.InvalidArgumentError: 
Condition x == y did not hold.
Indices of first 0 different values:
[]
Corresponding x values:
[]
Corresponding y values:
[]
```


Graph Mode Traceback:
```
Traceback (most recent call last):
  File ""/Users/matt/PycharmProjects/scratch/main.py"", line 9, in <module>
    with tf.control_dependencies([tf.assert_equal(x, y)]):
  File ""/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 391, in assert_equal
    _assert_static(condition_static, data)
  File ""/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 104, in _assert_static
    message='\n'.join(data_static))
tensorflow.python.framework.errors_impl.InvalidArgumentError: 
Condition x == y did not hold element-wise:
x (Const:0) = 
[1 2 3]
y (Const_1:0) = 
[3 2 1]
```
",1,,3,2018-01-01T09:43:43Z,2018-01-05T14:05:19Z,NONE,2018-01-02T18:51:31Z
15763,Branch 180441903,cla: yes,,0,,1,2018-01-01T00:05:18Z,2018-01-01T07:18:16Z,MEMBER,2018-01-01T06:30:37Z
15762,Make unused variable warning an error during TF builds.,"awaiting testing (then merge),cla: yes",We will need to eyeball build logs and see if this is really working as intended,1,,10,2017-12-31T23:44:09Z,2018-01-23T07:00:00Z,OWNER,2017-12-31T23:56:06Z
15761,"Revert ""add c++ gradient for op: Pow (#15245)""","awaiting testing (then merge),cla: yes","This reverts commit e1ded7fa7cfacaeea43a903e738dd3fe2baabc57.

CC @facaiy ",0,,3,2017-12-31T23:20:10Z,2018-01-01T05:37:03Z,OWNER,2018-01-01T00:19:35Z
15760,Custom gradient aggregation methods,,"I would like a way to apply some custom gradient aggregation ops. Probably the simplest thing to do is just allow `tf.gradients` (and `Optimizer.compute_gradients`) to return un-aggregated gradients so I could work with those?
Anyway, seems like an easy fix? I will do this myself in a month or so (cant now as on holiday), but if someone else wants to do it/has some thoughts, I am interested.
",0,,6,2017-12-31T20:41:42Z,2018-01-03T05:13:03Z,NONE,2018-01-01T22:04:44Z
15759,Update license year,"awaiting testing (then merge),cla: yes","TO DO:

- [x] Wait to the next year (100% done, depends on timezone)
- [ ] Merge!

:octocat:",0,,7,2017-12-31T20:26:04Z,2018-01-01T01:46:02Z,CONTRIBUTOR,2017-12-31T20:36:00Z
15757,Fixes #15736,cla: yes,"This will allow users to import keras submodules without typing `from tensorflow.python.keras...` but just directly from `tensorflow.keras`.
This change also does not create a duplicate of the module object but just assign it two names, one with `tensorflow.python.keras` keeping current functionality and `tensorflow.keras` allowing a much more consistent API use.

With this change the user is able to import keras objects directly, for example
```python
from tensorflow.keras.layers import Dense
```",0,,2,2017-12-31T14:05:33Z,2017-12-31T22:34:57Z,NONE,2017-12-31T22:34:57Z
15755,Tensorflow Dataset.from_generator blocks input?,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: both win7 and CentOS 7.2.1511
- **TensorFlow installed from (source or binary)**: pip3
- **TensorFlow version (use command below)**: 'v1.4.0-rc1-11-g130a514 1.4.0' and '1.5.0-dev20180102'
- **Python version**: 3.5.3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem

I submitted a problem on stackoverflow but nobody solved it. So I open it here. Does anybody can help to solve it?

Here is the problem:
[https://stackoverflow.com/questions/47917288/tensorflow-dataset-from-generator-blocks-input](https://stackoverflow.com/questions/47917288/tensorflow-dataset-from-generator-blocks-input)

I installed tensorflow 1.4.0 via pip, without gpu support. My python version is 3.5.3
  ",0,,7,2017-12-31T12:14:53Z,2018-01-03T06:41:38Z,NONE,2018-01-03T06:41:38Z
15752,Eager: Incompatible rnn model shapes inferred when using more than one CudnnGRU/LSTM,"comp:eager,stat:awaiting tensorflower","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.5.0dev20171230
- **Python version**: 3.6

### Describe the problem
When I use more than one CudnnGRU in eager, I got an error:
```Python
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-f618cd483215> in <module>()
     26 with tf.device(device):
     27     images = tf.constant(toy_data)
---> 28     logits = net(images)

~\Anaconda3\lib\site-packages\tensorflow\python\layers\base.py in __call__(self, inputs, *args, **kwargs)
    651 
    652         if not in_deferred_mode:
--> 653           outputs = self.call(inputs, *args, **kwargs)
    654           if outputs is None:
    655             raise ValueError('A layer\'s `call` method should return a Tensor '

<ipython-input-1-f618cd483215> in call(self, x)
     17         x = tf.transpose(x, [1, 0, 2])
     18         x, s = self.gru1(x)
---> 19         x, s = self.gru2(x)
     20         x = tf.transpose(x, [1, 0, 2])
     21         x = self.fc(x)

~\Anaconda3\lib\site-packages\tensorflow\python\layers\base.py in __call__(self, inputs, *args, **kwargs)
    651 
    652         if not in_deferred_mode:
--> 653           outputs = self.call(inputs, *args, **kwargs)
    654           if outputs is None:
    655             raise ValueError('A layer\'s `call` method should return a Tensor '

~\Anaconda3\lib\site-packages\tensorflow\contrib\cudnn_rnn\python\layers\cudnn_rnn.py in call(self, inputs, initial_state, training)
    400       c = array_ops.constant([], dtype=dtype)
    401     outputs, (output_h, output_c) = self._forward(inputs, h, c, self.kernel,
--> 402                                                   training)
    403     if self._rnn_mode == CUDNN_LSTM:
    404       return outputs, (output_h, output_c)

~\Anaconda3\lib\site-packages\tensorflow\contrib\cudnn_rnn\python\layers\cudnn_rnn.py in _forward(self, inputs, h, c, opaque_params, training)
    475         direction=self._direction,
    476         dropout=self._dropout,
--> 477         seed=self._seed)
    478     return output, (output_h, output_c)
    479 

~\Anaconda3\lib\site-packages\tensorflow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py in _cudnn_rnn(inputs, input_h, input_c, params, is_training, rnn_mode, input_mode, direction, dropout, seed, name)
    858       seed=seed,
    859       seed2=seed2,
--> 860       name=name)
    861   return (outputs, output_h, output_c)
    862 

~\Anaconda3\lib\site-packages\tensorflow\contrib\cudnn_rnn\ops\gen_cudnn_rnn_ops.py in cudnn_rnn(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name)
    120               ""seed2"", seed2, ""is_training"", is_training)
    121     _result = _execute.execute(b""CudnnRNN"", 4, inputs=_inputs_flat,
--> 122                                attrs=_attrs, ctx=_ctx, name=name)
    123   _execute.record_gradient(
    124       ""CudnnRNN"", _inputs_flat, _attrs, _result, name)

~\Anaconda3\lib\site-packages\tensorflow\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     64     else:
     65       message = e.message
---> 66     six.raise_from(core._status_to_exception(e.code, message), None)
     67   # pylint: enable=protected-access
     68   return tensors

~\Anaconda3\lib\site-packages\six.py in raise_from(value, from_value)

InvalidArgumentError: Incompatible rnn model shapes inferred: expecting [num_layers, input_size, num_units, dir_count]: [1, 28, 100, 1], getting [num_layers, input_size, num_units, dir_count]: [1, 100, 100, 1]. [Op:CudnnRNN]
```
If I use same network in graph mode, there is no problem.
reproduce error code:
```Python
import tensorflow as tf
import tensorflow.contrib.eager as tfe
import numpy as np
config = tf.ConfigProto()
config.gpu_options.allow_growth=True
tfe.enable_eager_execution(config=config)
class CudnnCrashNet(tfe.Network):
    def __init__(self, name):
        super(CudnnCrashNet, self).__init__(name)
        self.gru1 = tf.contrib.cudnn_rnn.CudnnGRU(1, 100)
        self.track_layer(self.gru1)
        self.gru2 = tf.contrib.cudnn_rnn.CudnnGRU(1, 100)
        self.track_layer(self.gru2)
        self.fc = self.track_layer(tf.layers.Dense(10))
    def call(self, x):
        x = tf.reshape(x, [-1, 28, 28])
        x = tf.transpose(x, [1, 0, 2])
        x, s = self.gru1(x)
        x, s = self.gru2(x)
        x = tf.transpose(x, [1, 0, 2])
        x = self.fc(x)
        return x
toy_data = np.ones((100, 784)).astype(np.float32)
device = ""gpu:0"" if tfe.num_gpus() else ""cpu:0""
net = CudnnCrashNet('net')
with tf.device(device):
    images = tf.constant(toy_data)
    logits = net(images)
```
normal graph-mode code:
```Python
import tensorflow as tf
import numpy as np
class CudnnNormalNet(tf.layers.Layer):
    def __init__(self, name):
        super(CudnnNormalNet, self).__init__(name)
        self.gru1 = tf.contrib.cudnn_rnn.CudnnGRU(1, 100)
        self.gru2 = tf.contrib.cudnn_rnn.CudnnGRU(1, 100)
        self.fc = tf.layers.Dense(10)
    def call(self, x):
        x = tf.reshape(x, [-1, 28, 28])
        x = tf.transpose(x, [1, 0, 2])
        x, s = self.gru1(x)
        x, s = self.gru2(x)
        x = tf.transpose(x, [1, 0, 2])
        x = self.fc(x)
        return x
config = tf.ConfigProto()
config.gpu_options.allow_growth=True
toy_data = np.ones((100, 784)).astype(np.float32)
with tf.Graph().as_default():
    net = CudnnNormalNet('net')
    x_p = tf.placeholder(tf.float32, [None, 784])
    logits = net(x_p)
    with tf.Session(config=config) as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(logits, feed_dict={x_p: toy_data})
```",1,,5,2017-12-31T10:29:19Z,2018-01-09T01:01:22Z,NONE,2018-01-03T02:08:50Z
15750,[XLA] Fix std::array initialization take 2,"awaiting testing (then merge),cla: yes","#15511 did not fix the issue on MSVC.

The actual root cause is due to the presence of `(` and `)` around const C array. For `std::array<int, 2> a ({1, 2, 3})`, MSVC seems to see `({1, 2, 3})` as a pointer, which triggers [C2100 compile error](https://msdn.microsoft.com/en-us/library/bzf3eha6.aspx). Removing `(` and `)` fixes the issue.

#15213",0,,2,2017-12-31T09:29:10Z,2018-01-01T00:40:59Z,CONTRIBUTOR,2017-12-31T22:31:11Z
15749,[XLA] Define TF_COMPILE_LIBRARY for two libraries,"awaiting testing (then merge),cla: yes","Both [`index_ops_kernel_argmax_float_1d.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/kernels/index_ops_kernel_argmax_float_1d.cc#L48) and [`index_ops_kernel_argmax_float_2d.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/kernels/index_ops_kernel_argmax_float_2d.cc#L50) use `TF_EXPORT` macro. We need to define `TF_COMPILE_LIBRARY` (comes from [`tf_copts`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L170)) to make sure `TF_EXPORT` is expanded into `__declspec(dllexport)`.

#15213",0,,2,2017-12-31T09:20:20Z,2018-01-01T00:06:53Z,CONTRIBUTOR,2017-12-31T22:29:15Z
15748,[XLA] Add missing win header deps to framework_lite,"awaiting testing (then merge),cla: yes","Continue from #15579. Most Tensorflow components depend on `framework` rather than `framework_lite`, so I did not notice this until I try to build XLA/tfcompile locally again.

#15213",0,,2,2017-12-31T09:14:28Z,2018-01-01T00:06:43Z,CONTRIBUTOR,2017-12-31T22:27:59Z
15746,CIFAR10 slows down every 100th step,"stat:community support,type:bug/performance","### System information
- **Have I written custom code**: No
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: Have tried both binary and source
- **TensorFlow version**: 1.4.0-19-ga52c8d9, 1.4.1
- **Python version**: 2.7.12
- **CUDA/cuDNN version**: 8.0.61
- **Hardware**: GPU: NVIDIA GeForce GTX 1080 Ti (11GB), RAM: 64GB, CPU: Intel i7-6850K
- **Exact command to reproduce**: python cifar10_train.py

### Describe the problem
The [CIFAR10 Tensorflow tutorial](https://www.tensorflow.org/tutorials/deep_cnn) seems to have a few odd patterns when it comes to the number of examples per second it can compute:
 - Step 0 is extremely slow 
 - Every 100th step is significantly slow 
 - The step after a really slow step is either a little slow or average
 - The next 10-30 steps after that are slightly boosted (faster than average)
 - The rest of the steps are average speed

I'm hoping for (in order of importance):
 - An explanation and fix for every 100th step being so slow
 - An explanation and instructions showing me how to make every step run at the boosted speed (the speed shortly after a slow step)
 - An explanation and fix for the slow 0th and 1st step

I can't find any additional logging or processing that happens on every 100th step. Could it be `tf.train.MonitoredSession`?

### Reproducible:
- when training on CPU rather than GPU
- independent of batch size
- on MacBook Pro (Retina, 13-inch, Mid 2014)

### Hardware utilization: 
1. Average:
- CPU: 82-84%
- GPU: 70-85%
- RAM: 3.7GB

2. Every 100th step:
- CPU: 9%
- GPU: 0%
- RAM: 3.7GB

3. Boosted after slow step: 
- GPU: 92%
- CPU: 82-84%
- RAM: 3.7GB

4. Idle:
- CPU: 1%
- GPU: 0%
- RAM: 1.6GB

*Overall CPU and RAM usage (clearly showing CPU trough every 100 steps)*:
![Overall CPU and RAM usage](https://user-images.githubusercontent.com/7654904/34998310-6b153646-fae7-11e7-8e0c-00ccf01349bf.png)

### Logs excerpt [(full logs)](https://github.com/tensorflow/tensorflow/files/1635342/terminalOutput.txt):

> step 0: (587.3 examples/sec; 6.974 sec/batch)
> step 1: (22630.6 examples/sec; 0.181 sec/batch)
> step 2: (36253.6 examples/sec; 0.113 sec/batch)
> step 3: (37966.0 examples/sec; 0.108 sec/batch)
> step 4: (38511.4 examples/sec; 0.106 sec/batch)
> step 5: (38554.6 examples/sec; 0.106 sec/batch)
> step 6: (32112.4 examples/sec; 0.128 sec/batch)
> step 7: (38912.4 examples/sec; 0.105 sec/batch)
> step 8: (39377.0 examples/sec; 0.104 sec/batch)
> step 9: (38206.2 examples/sec; 0.107 sec/batch)
> step 10: (38222.1 examples/sec; 0.107 sec/batch)
> step 11: (38757.5 examples/sec; 0.106 sec/batch)
> step 12: (38833.1 examples/sec; 0.105 sec/batch)
> step 13: (39774.8 examples/sec; 0.103 sec/batch)
> step 14: (39795.9 examples/sec; 0.103 sec/batch)
> step 15: (37850.5 examples/sec; 0.108 sec/batch)
> step 16: (38443.5 examples/sec; 0.107 sec/batch)
> step 17: (39194.6 examples/sec; 0.105 sec/batch)
> step 18: (39164.0 examples/sec; 0.105 sec/batch)
> step 19: (39057.5 examples/sec; 0.105 sec/batch)
> step 20: (33268.7 examples/sec; 0.123 sec/batch)
> step 21: (39459.7 examples/sec; 0.104 sec/batch)
> step 22: (39336.2 examples/sec; 0.104 sec/batch)
> step 23: (39207.1 examples/sec; 0.104 sec/batch)
> step 24: (39330.5 examples/sec; 0.104 sec/batch)
> step 25: (38783.9 examples/sec; 0.106 sec/batch)
> step 26: (39038.9 examples/sec; 0.105 sec/batch)
> step 27: (39214.2 examples/sec; 0.104 sec/batch)
> step 28: (39525.9 examples/sec; 0.104 sec/batch)
> step 29: (37209.0 examples/sec; 0.110 sec/batch)
> step 30: (38356.7 examples/sec; 0.107 sec/batch)
> step 31: (36077.0 examples/sec; 0.114 sec/batch)
> step 32: (37143.8 examples/sec; 0.110 sec/batch)
> step 33: (35961.1 examples/sec; 0.114 sec/batch)
> step 34: (33378.4 examples/sec; 0.123 sec/batch)
> step 35: (37830.3 examples/sec; 0.108 sec/batch)
> step 36: (36789.5 examples/sec; 0.111 sec/batch)
> step 37: (36638.2 examples/sec; 0.112 sec/batch)
> step 38: (36848.1 examples/sec; 0.111 sec/batch)
> step 39: (36041.4 examples/sec; 0.114 sec/batch)
> step 40: (36612.0 examples/sec; 0.112 sec/batch)
> step 41: (35623.9 examples/sec; 0.115 sec/batch)
> step 42: (37589.3 examples/sec; 0.109 sec/batch)
> step 43: (37462.9 examples/sec; 0.109 sec/batch)
> step 44: (35823.6 examples/sec; 0.114 sec/batch)
> step 45: (35911.8 examples/sec; 0.114 sec/batch)
> step 46: (36073.8 examples/sec; 0.114 sec/batch)
> step 47: (36930.2 examples/sec; 0.111 sec/batch)
> step 48: (36142.9 examples/sec; 0.113 sec/batch)
> ...
> step 99: (36434.8 examples/sec; 0.112 sec/batch)
> step 100: (1215.0 examples/sec; 3.371 sec/batch)
> step 101: (35952.9 examples/sec; 0.114 sec/batch)
> step 102: (38422.5 examples/sec; 0.107 sec/batch)
> step 103: (39315.8 examples/sec; 0.104 sec/batch)
> step 104: (38989.1 examples/sec; 0.105 sec/batch)
> step 105: (39091.4 examples/sec; 0.105 sec/batch)
> step 106: (39247.6 examples/sec; 0.104 sec/batch)
> step 107: (38009.7 examples/sec; 0.108 sec/batch)
> step 108: (38746.7 examples/sec; 0.106 sec/batch)
> step 109: (39505.4 examples/sec; 0.104 sec/batch)
> step 110: (39340.0 examples/sec; 0.104 sec/batch)
> step 111: (39065.0 examples/sec; 0.105 sec/batch)
> step 112: (38561.1 examples/sec; 0.106 sec/batch)
> step 113: (39109.0 examples/sec; 0.105 sec/batch)
> step 114: (39203.7 examples/sec; 0.104 sec/batch)
> step 115: (39144.4 examples/sec; 0.105 sec/batch)
> step 116: (38317.6 examples/sec; 0.107 sec/batch)
> step 117: (33757.5 examples/sec; 0.121 sec/batch)
> step 118: (34115.4 examples/sec; 0.120 sec/batch)
> step 119: (35671.4 examples/sec; 0.115 sec/batch)
> step 120: (35297.2 examples/sec; 0.116 sec/batch)
> step 121: (36152.8 examples/sec; 0.113 sec/batch)
> step 122: (35780.1 examples/sec; 0.114 sec/batch)
> step 123: (35847.1 examples/sec; 0.114 sec/batch)
> step 124: (36888.9 examples/sec; 0.111 sec/batch)
> step 125: (36946.2 examples/sec; 0.111 sec/batch)
> ...",0,,9,2017-12-31T09:02:50Z,2018-01-31T02:39:47Z,NONE,2018-01-03T02:07:55Z
15745,Eager: variable created in @tfe.defun is invalid and raise error when print,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.5.0dev20171230
- **Python version**: 3.6

### Describe the problem
I want to use defun to speed up static rnn compute in eager:
```Python
def _eager_dynamic_rnn(cell,
                       inputs,
                       sequence_length=None,
                       initial_state=None,
                       dtype=tf.float32,
                       parallel_iterations=None,
                       swap_memory=False,
                       time_major=False,
                       scope=None):
    time_axis = 0 if time_major else 1
    input_shape = inputs.shape.as_list()
    seq_len = input_shape[time_axis]
    if time_major:
        batch_size = input_shape[1]
    else:
        batch_size = input_shape[0]
    if initial_state is None:
        initial_state = cell.zero_state(batch_size, dtype)
    inputs = tf.unstack(inputs, num=seq_len, axis=time_axis)
    outputs = []
    for inp in inputs:
        output, initial_state = cell(inp, initial_state)
        outputs.append(output)
    outputs = tf.stack(outputs, axis=time_axis)
    return outputs, initial_state


@tfe.defun
def _eager_compiled_dynamic_rnn(cell,
                                inputs,
                                sequence_length=None,
                                initial_state=None,
                                dtype=tf.float32,
                                parallel_iterations=None,
                                swap_memory=False,
                                time_major=False,
                                scope=None):
    return _eager_dynamic_rnn(cell, inputs, sequence_length, initial_state,
                              dtype, None, False, time_major, scope)
```
If I directly use `_eager_compiled_dynamic_rnn` in forward, because of `tf.layers.Layer` create variables in its first __call__, then variables created in `_eager_compiled_dynamic_rnn` is invalid, if print it, get a error:
```Python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-7-308544234cc4> in <module>()
      1 for var in net.variables:
----> 2     print(var)

~\Anaconda3\lib\site-packages\tensorflow\python\ops\variables.py in __repr__(self)
    233       return ""<tf.Variable '%s' shape=%s dtype=%s, numpy=%s>"" % (
    234           self.name, self.get_shape(), self.dtype.name,
--> 235           ops.numpy_text(self.read_value(), is_repr=True))
    236     else:
    237       return ""<tf.Variable '%s' shape=%s dtype=%s>"" % (

~\Anaconda3\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py in read_value(self)
    679       # Ensure we read the variable in the same device as the handle.
    680       with ops.device(self._handle_device):
--> 681         value = self._read_variable_op()
    682     # Return an identity so it can get placed on whatever device the context
    683     # specifies instead of the device where the variable is.

~\Anaconda3\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py in _read_variable_op(self)
    657       tape.watch_variable(self)
    658     return gen_resource_variable_ops.read_variable_op(self._handle,
--> 659                                                       self._dtype)
    660 
    661   def read_value(self):

~\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_resource_variable_ops.py in read_variable_op(resource, dtype, name)
    209     _attrs = (""dtype"", dtype)
    210     _result = _execute.execute(b""ReadVariableOp"", 1, inputs=_inputs_flat,
--> 211                                attrs=_attrs, ctx=_ctx, name=name)
    212   _execute.record_gradient(
    213       ""ReadVariableOp"", _inputs_flat, _attrs, _result, name)

~\Anaconda3\lib\site-packages\tensorflow\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,
     59                                                op_name, inputs, attrs,
---> 60                                                num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

TypeError: provided list of inputs contains objects other than 'EagerTensor'
```
To solve this problem, I must call function which isn't decorated by tfe.defun in first forward, then switch to `_eager_compiled_dynamic_rnn`:
```Python
if self._cell.built is True:
    func = _eager_compiled_dynamic_rnn
else:
    func = _eager_dynamic_rnn
outputs, state = func(
    self._cell,
    inputs,
    seq_len,
    state,
    dtype=self._rnn_dtype,
    time_major=self._time_major, )
```
locate this error cost me much time. please consider to fix it.",1,,6,2017-12-31T08:13:11Z,2018-01-24T00:39:08Z,NONE,2017-12-31T08:32:09Z
15744,Summary op crashes when run multiple times,stat:awaiting response,"### System information
- **Windows 10 16299**:
- **pip install tensorflow-gpu**:
- **b'unknown' 1.4.0**:
- **3.5**: 
- **CUDA: V9.1.85, cuDNN version: 6**:
- **Geforce 930 MX, 2GB**:

### Describe the problem
Opening a session for the second time and trying to run a summary op causes crash for some reason. The error message is misleading as a placeholder is provided.

### Source code / logs
```
import tensorflow as tf

class MyModel:
    def __init__(self, sess, i):
        self.x = tf.placeholder(tf.float32, [2, 2])
        self.W = tf.Variable(tf.truncated_normal([2, 2]))
        self.y = tf.matmul(self.W, self.x)

        tf.summary.scalar('y',tf.reduce_sum(self.y))

        sess.run(tf.global_variables_initializer())

        self.summary = tf.summary.merge_all()
        self.writer = tf.summary.FileWriter('./logs/test' + str(i),sess.graph)

    def run(self, sess):        
        feed_dict = {self.x:[[0,0],[0,0]]}
        sess.run(self.y,feed_dict)
        print('inside1')
        s = sess.run(self.summary,feed_dict)
        print('inside2')
        self.writer.add_summary(s,0)

    def close(self):
        self.writer.close()        

# Swapping order of line 1 and 2 still causes crash
for i in range(3): # line 1
    with tf.Session() as sess: # line 2
        print('outside')
        M = MyModel(sess, i)
        M.run(sess)
        M.close()
    #sess.close() # No luck
```

OUTPUT:
```
outside
inside1
inside2
outside
inside1

Traceback (most recent call last):
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1323, in _do_call
    return fn(*args)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1302, in _run_fn
    status, run_metadata)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [2,2]
	 [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[2,2], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
	 [[Node: Sum_1/_7 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_7_Sum_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\TensorFlow\tensorflow-experiments\ravens-matrix-autoencoder\minimum.py"", line 31, in <module>
    M.run(sess)
  File ""D:\TensorFlow\tensorflow-experiments\ravens-matrix-autoencoder\minimum.py"", line 20, in run
    s = sess.run(self.summary,feed_dict)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 889, in run
    run_metadata_ptr)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [2,2]
	 [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[2,2], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
	 [[Node: Sum_1/_7 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_7_Sum_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'Placeholder', defined at:
  File ""<string>"", line 1, in <module>
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\idlelib\run.py"", line 124, in main
    ret = method(*args, **kwargs)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\idlelib\run.py"", line 351, in runcode
    exec(code, self.locals)
  File ""D:\TensorFlow\tensorflow-experiments\ravens-matrix-autoencoder\minimum.py"", line 30, in <module>
    M = MyModel(sess, i)
  File ""D:\TensorFlow\tensorflow-experiments\ravens-matrix-autoencoder\minimum.py"", line 5, in __init__
    self.x = tf.placeholder(tf.float32, [2, 2])
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1599, in placeholder
    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 3090, in _placeholder
    ""Placeholder"", dtype=dtype, shape=shape, name=name)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""C:\Users\windows\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [2,2]
	 [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[2,2], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
	 [[Node: Sum_1/_7 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_7_Sum_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```
",0,,2,2017-12-31T04:19:35Z,2018-01-03T02:06:26Z,NONE,2018-01-03T02:05:56Z
15743,Including common.h with NEON_2_SSE.h,"awaiting testing (then merge),cla: yes,comp:lite",Including common.h to make sure that USE_NEON is defined in case of NEON_2_SSE.h is used; otherwise USE_NEON will not be propagated to this file and `portable_tensor_utils.h` will be used,1,,25,2017-12-31T00:15:31Z,2018-01-26T02:53:00Z,CONTRIBUTOR,2017-12-31T00:16:45Z
15742,Use Eigen version of the `scalar_pow_op` for `pow` ops,"awaiting review,cla: yes","This fix use `scalar_pow_op` in Eigen to replace customerized scalar_binary_pow_op_google, as `scalar_pow_op` seems to be in place in Eigen.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,3,2017-12-30T23:47:11Z,2018-01-22T22:01:41Z,MEMBER,2017-12-31T05:50:49Z
15741,Python tensorflow module cannot be reloaded (bug),"stat:awaiting tensorflower,type:feature","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  confirmed on both Ubuntu 16.04 LTS in VirtualBox and OS X 10.12.6
- **TensorFlow installed from (source or binary)**:  installed via pip
- **TensorFlow version (use command below)**:  ('v1.4.0-19-ga52c8d9b01', '1.4.1')
- **Python version**: Python 2.7.13
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

> import tensorflow as tf
> reload(tf)

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Simple bug:

Trying to reload the module causes a failure.  Not a major problem, in general, but troublesome for the task, which is automated testing of the tensorflow Python API using TSTL (https://github.com/agroce/tstl).

The exact sequence is trivial:

>>> import tensorflow as tf
>>> reload(tf)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/__init__.py"", line 40, in <module>
    del python
NameError: name 'python' is not defined

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,8,2017-12-30T22:51:59Z,2018-01-15T21:07:10Z,NONE,2017-12-31T08:29:45Z
15740,Simplify the dense_to_one_hot method,cla: yes,"Simplified the `dense_to_one_hot` method.
The updated method behaves exactly like the old one, but is more concise.",0,,3,2017-12-30T21:01:09Z,2017-12-31T12:23:35Z,NONE,2017-12-31T05:53:40Z
15739,Fix the headers error due to recent CUDA9.1 change,"awaiting testing (then merge),cla: yes",Some headers in CUDA 9.1 has been move to cuda/include/crt directory. ,0,,10,2017-12-30T19:41:04Z,2017-12-31T22:26:25Z,CONTRIBUTOR,2017-12-31T05:54:15Z
15738,"wrong output(extra symbol ""b"")",,"**System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows  Home
TensorFlow installed from (source or binary): binary (pip)
TensorFlow version (use command below): 1.4.0 (GPU)
Python version: 3.6**
GPU: GTX1050 Ti M (disabled Intel visualization)

Basically any output I try to write in console is with extra symbol : ""b""
for example:
with cmmand:
```
C: python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'unknown' 1.4.0 //OUTPUT
```
I downloaded GPU verison form guide from official webiste(with:Anaconda):
[https://www.tensorflow.org/install/install_windows#installing_with_anaconda](url)

i have tried the validation example and same problem (extra symbol : ""b"")
",0,,3,2017-12-30T18:37:28Z,2017-12-31T05:03:02Z,NONE,2017-12-31T03:48:03Z
15737,"AttentionWrapper zero_state(batch_size, tf.float32).clone(cell_state=encoder_state) fails when batch size is 1",stat:awaiting tensorflower,"Hello!
I believe to have found a small bug when using the `zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)` command. When batch size is 1, the error `ValueError: The shape for decoder/while/Merge_5:0 is not an invariant for the loop. It enters the loop with shape (1, 512), but has shape (?, 512) after one iteration. Provide shape invariants using either the shape_invariants argument of tf.while_loop or set_shape() on the loop variables.` is thrown. This error does not occur when batch size is 2 or larger. The error also doesn't occur if I remove the .clone command. 
I tried investigating where the error is, but couldn't find the cause. I'm using this in context of trying to build a neural transducer, but also get the same error for basic seq2seq:
(Based on the NMT tutorial)

```python
# .... Encoder, constants etc...
# Decoder
helper = tf.contrib.seq2seq.TrainingHelper(
    decoder_inputs_embedded, decoder_full_length, time_major=True) 

attention_states = tf.transpose(encoder_outputs, [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]
attention_mechanism = tf.contrib.seq2seq.LuongAttention(
    encoder_hidden_units, attention_states,
    memory_sequence_length=encoder_inputs_length)
decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
    tf.contrib.rnn.LSTMCell(decoder_hidden_units),
    attention_mechanism,
    attention_layer_size=decoder_hidden_units)

projection_layer = layers_core.Dense(
    vocab_size, use_bias=False)


decoder = tf.contrib.seq2seq.BasicDecoder(
    decoder_cell, helper, decoder_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state),
    output_layer=projection_layer)

# ---- Training ----
outputs, last_state, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True)
logits = outputs.rnn_output
decoder_prediction = outputs.sample_id
```


TF Version: ('v1.4.0-19-ga52c8d9', '1.4.1')
System details:
```
== cat /etc/issue ===============================================
Linux nikita-coolboi 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""17.10 (Artful Aardvark)""
VERSION_ID=""17.10""
VERSION_CODENAME=artful

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 7.2.0-8ubuntu3) 7.2.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux nikita-coolboi 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.5.1)
tensorflow (1.4.1)
tensorflow-tensorboard (0.4.0rc3)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.4.1
tf.GIT_VERSION = v1.4.0-19-ga52c8d9
tf.COMPILER_VERSION = v1.4.0-19-ga52c8d9
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```
I believe this is an important issue, as often times when experimenting with new seq2seq models I start off with trying to get it to work for a batch size of 1.

Thanks!
Nikita",2,,12,2017-12-30T18:04:33Z,2018-01-12T02:46:30Z,NONE,2017-12-31T08:36:14Z
15736,Importing submodules from tensorflow.keras fails with No module named 'tensorflow.keras',,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary (pip)
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 3.6
- **Exact command to reproduce**:
from tensorflow.keras import datasets # fails but should work
import tensorflow as tf #succeeds
tf.keras.datasets #succeeds
from tensorflow.python.keras import datasetst # succeeds

### Describe the problem
Importing submodules from tensorflow.keras fails with error: `ModuleNotFoundError: No module named 'tensorflow.keras'`. but `import tensorflow as tf` and then doing `tf.keras.datasets` works. This is a big inconsistency, also it means that every time an element from within the `tensforlow.keras` module you need to write the complete path (which is very annoying) this removes the simplicity and readability of the `keras` API. A work around is to import submodules from `tensorflow.python.keras`, which again is inconsistent. 

In my opinion, since the documentation states that `keras` is availabe at `tf.keras` that should be the access path to the submodules and not `tensorflow.python.keras`. I'll try to  make a pull request for this.

### Source code / logs

```python
# fails but should work
from tensorflow.keras import datasets
```
```python
# succeeds
import tensorflow as tf
tf.keras.datasets
```
```python
# succeeds
from tensorflow.python.keras import datasetst
```",0,,9,2017-12-30T16:44:49Z,2017-12-31T22:36:00Z,NONE,2017-12-31T07:05:41Z
15734,fix typo,"awaiting testing (then merge),cla: yes",fix typo,0,,1,2017-12-30T14:58:24Z,2017-12-30T17:08:37Z,CONTRIBUTOR,2017-12-30T16:17:44Z
15733,"""Failed to load the native TensorFlow runtime."" error on Raspbian stretch",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Raspbian Strecth
- **TensorFlow installed from (source or binary)**: Source (Compiled from git on raspberry pi 3)
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: gcc version 4.8.5 (Raspbian 4.8.5-4) 
- **CUDA/cuDNN version**: No (nonconfigured)
- **GPU model and memory**:No (nonconfigured)
- **Exact command to reproduce**: python3 "">>> import tensorflow""
Hi,
I have compiled tensorflow's 1.4.1 source code from git source. There was no error while compiling from source but after installition by pip3, I can't import tensorflow library in python3
When I gave ""import tensorflow"" command in the python 3 shell it gives this error:

 "">>> import tensorflow
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws11Environment6GetEnvEPKc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws11Environment6GetEnvEPKc


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems""

",0,,4,2017-12-30T14:40:22Z,2018-01-30T01:03:49Z,NONE,2017-12-31T08:46:34Z
15728,Clarify the description of batch_norm in order to highlight the dimen,"awaiting testing (then merge),cla: yes","sion selection for normalization.

This should work now.",1,,2,2017-12-30T08:25:29Z,2018-01-23T18:26:23Z,CONTRIBUTOR,2018-01-04T18:01:48Z
15727,using tf.layers.batch_normalization() gives erratic validation loss though implementation seems correct.,,"I am trying to use Batch Normalization using [tf.layers.batch_normalization()][1] and I have followed the documentation closely. My code looks like this:

<!-- language: python -->

    def create_conv_exp_model(fingerprint_input, model_settings, is_training):
      

      # Dropout placeholder
      if is_training:
        dropout_prob = tf.placeholder(tf.float32, name='dropout_prob')

      # Mode placeholder
      mode_placeholder = tf.placeholder(tf.bool, name=""mode_placeholder"")

      he_init = tf.contrib.layers.variance_scaling_initializer(mode=""FAN_AVG"")

      # Input Layer
      input_frequency_size = model_settings['bins']
      input_time_size = model_settings['spectrogram_length']
      net = tf.reshape(fingerprint_input,
                       [-1, input_time_size, input_frequency_size, 1],
                       name=""reshape"")
      net = tf.layers.batch_normalization(net, 
                                          training=mode_placeholder,
                                          name='bn_0')

      for i in range(1, 6):
        net = tf.layers.conv2d(inputs=net,
                               filters=8*(2**i),
                               kernel_size=[5, 5],
                               padding='same',
                               kernel_initializer=he_init,
                               name=""conv_%d""%i)
        net = tf.layers.batch_normalization(net,
                                            training=mode_placeholder,
                                            name='bn_%d'%i)
        with tf.name_scope(""relu_%d""%i):
          net = tf.nn.relu(net)
        net = tf.layers.max_pooling2d(net, [2, 2], [2, 2], 'SAME', 
                                      name=""maxpool_%d""%i)

      net_shape = net.get_shape().as_list()
      net_height = net_shape[1]
      net_width = net_shape[2]
      net = tf.layers.conv2d( inputs=net,
                              filters=1024,
                              kernel_size=[net_height, net_width],
                              strides=(net_height, net_width),
                              padding='same',
                              kernel_initializer=he_init,
                              name=""conv_f"")
      net = tf.layers.batch_normalization( net, 
                                            training=mode_placeholder,
                                            name='bn_f')
      with tf.name_scope(""relu_f""):
        net = tf.nn.relu(net)

      net = tf.layers.conv2d( inputs=net,
                              filters=model_settings['label_count'],
                              kernel_size=[1, 1],
                              padding='same',
                              kernel_initializer=he_init,
                              name=""conv_l"")

      ### Squeeze
      squeezed = tf.squeeze(net, axis=[1, 2], name=""squeezed"")

      if is_training:
        return squeezed, dropout_prob, mode_placeholder
      else:
        return squeezed, mode_placeholder

And my train step looks like this:

<!-- language: python -->

    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):
      optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_input)
      gvs = optimizer.compute_gradients(cross_entropy_mean)
      capped_gvs = [(tf.clip_by_value(grad, -2., 2.), var) for grad, var in gvs]
      train_step = optimizer.apply_gradients(gvs))

During training, I am feeding the graph with:

<!-- language: python -->

    train_summary, train_accuracy, cross_entropy_value, _, _ = sess.run(
        [
            merged_summaries, evaluation_step, cross_entropy_mean, train_step,
            increment_global_step
        ],
        feed_dict={
            fingerprint_input: train_fingerprints,
            ground_truth_input: train_ground_truth,
            learning_rate_input: learning_rate_value,
            dropout_prob: 0.5,
            mode_placeholder: True
        })

During validation, 

<!-- language: python -->

    validation_summary, validation_accuracy, conf_matrix = sess.run(
                    [merged_summaries, evaluation_step, confusion_matrix],
                    feed_dict={
                        fingerprint_input: validation_fingerprints,
                        ground_truth_input: validation_ground_truth,
                        dropout_prob: 1.0,
                        mode_placeholder: False
                    })

My loss and accuracy curves (orange is training, blue is validation):
[Plot of loss vs number of iterations][2],
[Plot of accuracy vs number of iterations][3]

The validation loss (and accuracy) seem very erratic. Is my implementation of Batch Normalization wrong? Or is this normal with Batch Normalization and I should wait for more iterations? Or maybe, moving statistics are not being saved and hence poor performance. I tried StackOverflow and found many people have the same problem and there is no definitive guide on how to resolve this.

  [1]: https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization
  [2]: https://i.stack.imgur.com/ZAqDw.png
  [3]: https://i.stack.imgur.com/CYKJX.png",0,,1,2017-12-30T07:00:14Z,2017-12-30T09:12:21Z,NONE,2017-12-30T09:12:21Z
15725,Source Built r1.4 on GPUs with CPU optimized is always slower than 'no cpu optimization',stat:awaiting tensorflower,"### System information
- **Have I written custom code**: Yes, the model named PSIque [arXiv:1711.10644](https://arxiv.org/abs/1711.10644)
- **OS Platform and Distribution**: CentOS 7.1/CentOS 7.3
- **TensorFlow installed from**: source build w/ Bazel
- **TensorFlow version**: 1.4
- **Python version**: Anaconda 3.6.2
- **Bazel version**: 0.8.1
- **GCC/Compiler version (if compiling from source)**: gcc version 4.8.3 20140911 (Red Hat4.8.3-9) (GCC)
- **CUDA/cuDNN version**: CUDA 8.0/r375.26/cuDNN 6.0.0 & CUDA 9.0/r384.81/cuDNN 7.0.5 
- **GPU model and memory**: E5-2660v3*2 Socket, K40m 12GB, P100-PCIE-16GB
- **Exact command to reproduce**: python model.py

### Describe the problem
* I built tensorflow from source for boosting operation performance.

* 6 different distributions were built;
  * CPU only & No CPU optimization (NO EXTRA flags)
  * CPU only & CPU optimization (--config=opt)
  * GPU support & CUDA 8/9 & No CPU optimization (--config=cuda)
  * GPU support & CUDA 8/9 & CPU optimization (--config=opt --config=cuda)

* Experiments were basically done by 5 phases; experiments on CPU only are still going on,
so please focus on GPU version results.

* Results are quite frustrating me, because 'most of CPU optimized versions' gave me slow results.
![results](https://user-images.githubusercontent.com/20734988/34451067-1d8c1cf0-ed5e-11e7-9a4f-3ff21a5c9aea.png)

* Test were made on multiple machine with random order.
  * P100: 2 nodes
  * K40m: 7 nodes
  * CPU only: 8 nodes

* I am curious why CPU optimized version is slow
  * on every experiment combinations
  * even different GPU environments
  * even Dual CPU socket (E5-2660v3)

* (Extra) I believe my current model does not require high throughputs

### tf_env_collect.sh
== cat /etc/issue ===============================================
Linux <hostname> 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7 (Core)""
VERSION_ID=""7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 4.8.3 20140911 (Red Hat 4.8.3-9)
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux vis5 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.4.0)
tensorflow (1.4.0)
tensorflow-tensorboard (0.4.0rc3)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.4.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Sat Dec 30 12:39:08 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |
| N/A   28C    P0    35W / 250W |      0MiB / 16276MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P100-PCIE...  On   | 00000000:82:00.0 Off |                    0 |
| N/A   35C    P0    38W / 250W |  15661MiB / 16276MiB |     19%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    1     68169      C   python                                     15643MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-9.0/lib64/libcudart_static.a
",0,,6,2017-12-30T03:46:20Z,2018-01-30T02:00:04Z,NONE,2017-12-31T08:41:20Z
15724,How to register all kernels to Android lib,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.0.1
- **Python version**: 3.4
- **Bazel version (if compiling from source)**: 0.4

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I am trying to compile a Android lib which can load a MetaGraph into a session as [this link](https://stackoverflow.com/questions/35508866/tensorflow-different-ways-to-export-and-run-graph-in-c/43639305#43639305) specifies. I first extracts a GraphDef from this MetaGraph and uses this GraphDef to generate `ops_to_register.h`. Then I compile the lib as 

`bazel build my_model/test:test_lib --copt=-DSELECTIVE_REGISTRATION  --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a`

However when I trying to run the test_lib, it complains some kernels are not registered. Then I brutally adds all kernels to be registered as

`#define SHOULD_REGISTER_OP_KERNEL(clz) true`

However, it still complains

```
Error creating graph: Invalid argument: No OpKernel was registered to support Op 'Const' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[Node: save/RestoreV2_8/shape_and_slices = Const[_output_shapes=[[1]], dtype=DT_STRING, value=Tensor<type: string shape: [1] values: >]()]]
```

How can I solve this by adding this Const kernel or simply registering all kernels?",0,,1,2017-12-30T03:16:38Z,2018-01-03T01:57:02Z,NONE,2018-01-03T01:57:02Z
15723,Load region from `~/.aws/config` if possible in S3,"awaiting review,cla: yes","This fix tries to address the issue raised in https://github.com/tensorflow/tensorflow/issues/15662#issuecomment-354272697 where TensorFlow does not load region from `~/.aws/config` if exists. The reason was that AWS C++ SDK does not use the config file by default.

This fix adds the loading of config file (`~/.aws/config`) explicitly, if either AWS_REGION or S3_REGION is not available. In case none of the `AWS_REGION`, `S3_REGION`, `~/.aws/config` is available, then the default `use-east-1` is used (by AWS C++ SDK).

This fix is related to #15562.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,8,2017-12-30T01:58:15Z,2018-01-23T18:34:28Z,MEMBER,2017-12-30T04:13:48Z
15719,Fixed a typo for CreateBody(),"awaiting testing (then merge),cla: yes",,0,,1,2017-12-29T19:44:28Z,2017-12-30T21:54:57Z,CONTRIBUTOR,2017-12-29T23:17:06Z
15718,MKL: update mkldnn to the latest release,"awaiting testing (then merge),cla: yes","This commit will pull the latest changes from the mkl-dnn tree.

the mirror.bazel.build URL doesn't exist: ""https://mirror.bazel.build/github.com/01org/mkl-dnn/archive/e0bfcaa7fcb2b1e1558f5f0676933c1db807a729.tar.gz"" can you create it?",0,,3,2017-12-29T18:43:38Z,2017-12-31T19:52:17Z,CONTRIBUTOR,2017-12-29T18:45:06Z
15714,Optimizing code and adding from http to https,"awaiting testing (then merge),cla: yes",,0,,3,2017-12-29T09:35:10Z,2018-01-01T07:26:27Z,CONTRIBUTOR,2018-01-01T05:20:01Z
15711,Update estimator.md,"awaiting testing (then merge),cla: yes","The anchor ""#fit_dnnclassifier"" can not locate to a valid position. You can try the following two links:

https://www.tensorflow.org/get_started/estimator#fit_dnnclassifier

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/get_started/estimator.md#fit-dnnclassifier",0,,3,2017-12-29T07:21:34Z,2017-12-31T22:26:01Z,CONTRIBUTOR,2017-12-29T23:20:43Z
15710,fix doc `tf.data.contrib.map_and_batch` to `tf.contrib.data.map_and_batch`,"awaiting testing (then merge),cla: yes",change from unexist `tf.data.contrib.map_and_batch` to `tf.contrib.data.map_and_batch`,0,,5,2017-12-29T05:18:09Z,2018-01-01T00:40:47Z,CONTRIBUTOR,2017-12-29T06:56:43Z
15709,Update graphs.md,"awaiting testing (then merge),cla: yes",,0,,3,2017-12-29T04:52:40Z,2017-12-31T22:25:34Z,CONTRIBUTOR,2017-12-29T06:55:57Z
15708,Update estimator.md,"awaiting testing (then merge),cla: yes",,0,,3,2017-12-29T04:46:32Z,2017-12-31T22:25:15Z,CONTRIBUTOR,2017-12-29T06:55:26Z
15707,updated the latest mkldnn,cla: no,"This commit will pull the latest changes from the mkl-dnn tree.

the mirror.bazel.build URL doesn't exist: ""https://mirror.bazel.build/github.com/01org/mkl-dnn/archive/e0bfcaa7fcb2b1e1558f5f0676933c1db807a729.tar.gz"" can you create it?",0,,5,2017-12-29T03:44:35Z,2017-12-29T18:34:11Z,CONTRIBUTOR,2017-12-29T04:10:15Z
15705,updated the latest mkldnn,cla: no,"This commit will pull the latest changes from the mkl-dnn tree.

the mirror.bazel.build URL doesn't exist: ""https://mirror.bazel.build/github.com/01org/mkl-dnn/archive/e0bfcaa7fcb2b1e1558f5f0676933c1db807a729.tar.gz"" can you create it?",0,,2,2017-12-29T03:22:37Z,2017-12-29T03:37:43Z,CONTRIBUTOR,2017-12-29T03:37:43Z
15704,Used template version of SafeStringToNumeric to reduce code duplication,"awaiting testing (then merge),cla: yes","This fix uses template version of SafeStringToNumeric to avoid customerized ConvertHelper, for the purpose of reduce code duplication.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,3,2017-12-29T01:33:00Z,2018-01-01T00:05:43Z,MEMBER,2017-12-29T01:33:57Z
15703,Branch 180304271,"awaiting testing (then merge),cla: yes",,0,,10,2017-12-29T01:11:30Z,2017-12-31T06:33:08Z,MEMBER,2017-12-29T01:47:38Z
15702,Fix out of memory issue on Tegra devices,"awaiting testing (then merge),cla: yes","TF used to allocate (available memory - 300MB) or (available memory - 225MB)
for TF to use. This is fine for graphic cards, but will cause out of memory
issue on Tegra.
Modify to allocate (available memory - 1GB) for Tegra.
1GB should be enough for OS and other apps
(available memory - 1GB) should be 0.8-1.5GB which is enough for most graph for TF.",0,,5,2017-12-28T20:18:32Z,2017-12-31T22:24:43Z,CONTRIBUTOR,2017-12-28T23:44:39Z
15699,[Android] Dex cannot parse version 52 byte code,"stat:awaiting response,type:support","When including:

`compile 'org.tensorflow:tensorflow-lite:0.1.1'`

I get the error:

> [Android] Dex cannot parse version 52 byte code

If I use AGP 3.0.0+ the problem goes away since it has support for Java 8, but the problem exists if I try to use 2.3.3. 
",0,,2,2017-12-28T16:25:40Z,2018-01-04T15:32:51Z,NONE,2017-12-29T02:35:20Z
15698,Exception trying to import a retrained model in android classifier demo app,"stat:awaiting response,type:support","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
('v1.3.0-rc1-5733-gb43d0f3', '1.4.0')
- **Python version**: 
2.7.12
- **Bazel version (if compiling from source)**:
0.9.0
- **GCC/Compiler version (if compiling from source)**:
5.4.0
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


I have retrained a mobilenet_v1_1.0_224 frozen_graph.pb model with the following script:

export PYTHONPATH=/usr/local/lib/python2.7/dist-packages:/usr/lib/python2.7/dist-packages
export IMAGE_SIZE=224
export WIDTH_MUL=1.0 # 0.75 0.50 0.25
export ARCHITECTURE=""mobilenet_${WIDTH_MUL}_${IMAGE_SIZE}""
export BASE_DIR=""$( cd ""$( dirname ""${BASH_SOURCE[0]}"" )"" && pwd )""
export TF_FILES_DIR=""tf_files""
python -m scripts.retrain \
  --bottleneck_dir=""${TF_FILES_DIR}""/bottlenecks \
  --how_many_training_steps=4000 \
  --model_dir=""${TF_FILES_DIR}""/models/ \
  --summaries_dir=""${TF_FILES_DIR}""/training_summaries/""${ARCHITECTURE}"" \
  --output_graph=""${TF_FILES_DIR}""/retrained_graph.pb \
  --output_labels=""${TF_FILES_DIR}""/retrained_labels.txt \
  --architecture=""${ARCHITECTURE}"" \
  --image_dir=""${TF_FILES_DIR}""/flower_photos

I copied ""retrained_graph.pb"" and ""retrained_labels.txt"" in the ""assets"" directory of the android demo app in tensorflow/examples/android and modified the source code in ClassifierActivity.java as follows:

/* Original code:
  private static final String INPUT_NAME = ""input"";
  private static final String OUTPUT_NAME = ""output"";
  private static final String MODEL_FILE = ""file:///android_asset/tensorflow_inception_graph.pb"";
  private static final String LABEL_FILE = ""file:///android_asset/imagenet_comp_graph_label_strings.txt"";
*/
/* Replaced by: */
  private static final String INPUT_NAME = ""input"";
  private static final String OUTPUT_NAME = ""final_result"";
  private static final String MODEL_FILE = ""file:///android_asset/retrained_graph.pb"";
  private static final String LABEL_FILE = ""file:///android_asset/retrained_labels.txt"";
/* End */

I cleaned and rebuilt the whole project in Android Studio, successfully.

The app was then loaded on a Huawei P8 Lite (Android 7.0) and starting TF Classify, the following error occurs:

E/tensorflow: CameraActivity: Exception!
              java.lang.RuntimeException: Failed to load model from 'file:///android_asset/retrained_graph.pb'
                  at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:113)
                  at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:103)
                  at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:118)
                  at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:120)
                  at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1204)
                  at android.os.Handler.dispatchMessage(Handler.java:105)
                  at android.os.Looper.loop(Looper.java:156)
                  at android.app.ActivityThread.main(ActivityThread.java:6523)
                  at java.lang.reflect.Method.invoke(Native Method)
                  at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:942)
                  at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:832)
               **Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[""SAME"", ""VALID""]; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"", ""NCHW""]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_0/convolution = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true](input, MobilenetV1/Conv2d_0/weights/read)**. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
                  at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:535)
                  at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:105)
                  at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:103)
                  at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:118)
                  at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:120)
                  at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1204)
                  at android.os.Handler.dispatchMessage(Handler.java:105)
                  at android.os.Looper.loop(Looper.java:156)
                  at android.app.ActivityThread.main(ActivityThread.java:6523)
                  at java.lang.reflect.Method.invoke(Native Method)
                  at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:942)
                  at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:832)
Application terminated.

How to fix this error?",0,,5,2017-12-28T16:13:06Z,2018-01-03T18:28:29Z,NONE,2017-12-29T02:11:39Z
15697,Very slow tf.transpose on CPU (compared to numpy),,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Tested on Linux Ubuntu 16.04 and Mac OS
- **TensorFlow installed from (source or binary)**: Both affected
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**:  3.6.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: 

### Describe the problem
Tensorflow transpose is 10000 slower than numpy transpose on my example.

### Source code / logs
```
import numpy as np
a = np.random.randn(*(10, 10, 10, 100, 10, 10, 10))
%timeit np.transpose(a, [3, 0, 1, 2, 4, 5, 6])
# 744 ns on my machine
b = tf.Variable(tf.random_normal((10, 10, 10, 100, 10, 10, 10))) # variable to avoid generating random numbers while measuring time
sess = tf.Session()
sess.run(tf.global_variables_initializer())
op = tf.transpose(b, [3, 0, 1, 2, 4, 5, 6]).op
%timeit sess.run(op)
# 7.94 s
```",0,,3,2017-12-28T15:33:56Z,2017-12-28T16:39:46Z,CONTRIBUTOR,2017-12-28T15:56:01Z
15696,A fix for error in tf.layers.conv3d_transpose when inferred batch size,stat:contributions welcome,"## Context
When using the `tf.layers.conv3d_tranpose` Op with a dynamic batch size and when `use_bias=True` then there is a [well known error that occurs](https://github.com/tensorflow/tensorflow/issues/10520).
The error is due to a [tf.reshape Op that chunks together some of the axes before adding the bias for a slight performance improvement](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L1628).
E.g. in the case of `data_format == 'channels_last'`, 
```
outputs_4d = array_ops.reshape(outputs, [
            outputs_shape[0], outputs_shape[1],
            outputs_shape[2] * outputs_shape[3], outputs_shape[4]
        ])
```
gives an error when `outputs_shape[0] == None`, i.e. when batch size is inferred. 

## Simple fix
To fix this with minimal modification to other code, it would be great if someone could replace `outputs_shape[0]` with `-1` in these two lines:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L1627
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L1632",0,,8,2017-12-28T15:27:47Z,2018-02-01T12:41:42Z,NONE,2017-12-29T01:08:27Z
15691,The problem of the tensorboard,,"Hi:
  I am the newbie of the tensorflow,  there is a problem during learning the usage of  tensorboard. 
  There is no problem with the first code compilation after start the Compiler(Anaconda spyder), but recompiling the code will go wrong, that is very strange. can anyone help me?

thank you very much!

**First compilation:**
![1](https://user-images.githubusercontent.com/31270354/34410598-60c20af0-ec0c-11e7-9acc-b3a5757b2101.PNG)
![2](https://user-images.githubusercontent.com/31270354/34410606-6776095a-ec0c-11e7-8dcb-6607336bd7d2.PNG)
![3](https://user-images.githubusercontent.com/31270354/34410611-6cab0768-ec0c-11e7-8f39-38a7cee2087f.PNG)
everything is ok during first compilation

**Second compilation:**
![4](https://user-images.githubusercontent.com/31270354/34410636-8f812b64-ec0c-11e7-91c9-22ad651245d9.PNG)
Something was wrong!!!

**the code of the tensorflow:**
```
import tensorflow as tf

x = tf.placeholder(tf.int32)
y = x + 2
                   
sess = tf.Session()

tf.summary.scalar('Accuracy' , y)
merged = tf.summary.merge_all()
writer = tf.summary.FileWriter(""logs/"", sess.graph)

for i in range(200):     
    rs = sess.run(merged , feed_dict = {x: 10})
    writer.add_summary(rs, i)
```
**Error report:**
```
runfile('C:/Users/Administrator/.spyder-py3/temp.py', wdir='C:/Users/Administrator/.spyder-py3')
Traceback (most recent call last):

  File ""<ipython-input-2-66eb2d566452>"", line 1, in <module>
    runfile('C:/Users/Administrator/.spyder-py3/temp.py', wdir='C:/Users/Administrator/.spyder-py3')

  File ""C:\Program Files\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 866, in runfile
    execfile(filename, namespace)

  File ""C:\Program Files\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/Administrator/.spyder-py3/temp.py"", line 13, in <module>
    rs = sess.run(merged , feed_dict = {x: 10})

  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 895, in run
    run_metadata_ptr)

  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)

  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1321, in _do_run
    options, run_metadata)

  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)

InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype int32
	 [[Node: Placeholder = Placeholder[dtype=DT_INT32, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op 'Placeholder', defined at:
  File ""C:\Program Files\Anaconda3\lib\site-packages\spyder\utils\ipython\start_kernel.py"", line 223, in <module>
    main()
  File ""C:\Program Files\Anaconda3\lib\site-packages\spyder\utils\ipython\start_kernel.py"", line 219, in main
    kernel.start()
  File ""C:\Program Files\Anaconda3\lib\site-packages\ipykernel\kernelapp.py"", line 474, in start
    ioloop.IOLoop.instance().start()
  File ""C:\Program Files\Anaconda3\lib\site-packages\zmq\eventloop\ioloop.py"", line 162, in start
    super(ZMQIOLoop, self).start()
  File ""C:\Program Files\Anaconda3\lib\site-packages\tornado\ioloop.py"", line 887, in start
    handler_func(fd_obj, events)
  File ""C:\Program Files\Anaconda3\lib\site-packages\tornado\stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Program Files\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""C:\Program Files\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""C:\Program Files\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""C:\Program Files\Anaconda3\lib\site-packages\tornado\stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Program Files\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 276, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""C:\Program Files\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 228, in dispatch_shell
    handler(stream, idents, msg)
  File ""C:\Program Files\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 390, in execute_request
    user_expressions, allow_stdin)
  File ""C:\Program Files\Anaconda3\lib\site-packages\ipykernel\ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""C:\Program Files\Anaconda3\lib\site-packages\ipykernel\zmqshell.py"", line 501, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""C:\Program Files\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2717, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""C:\Program Files\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2827, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\Program Files\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-66eb2d566452>"", line 1, in <module>
    runfile('C:/Users/Administrator/.spyder-py3/temp.py', wdir='C:/Users/Administrator/.spyder-py3')
  File ""C:\Program Files\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 866, in runfile
    execfile(filename, namespace)
  File ""C:\Program Files\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)
  File ""C:/Users/Administrator/.spyder-py3/temp.py"", line 3, in <module>
    x = tf.placeholder(tf.int32)
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1548, in placeholder
    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 2094, in _placeholder
    name=name)
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype int32
	 [[Node: Placeholder = Placeholder[dtype=DT_INT32, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]    
```




",0,,4,2017-12-28T12:25:23Z,2018-01-30T00:34:55Z,NONE,2017-12-28T19:33:41Z
15689,TensorFlowInferenceInterface: readNodeFloat error,"stat:awaiting response,type:support","This is part of my Tensorflow frozen graph, I have named the input and output nodes.

    >>> g.ParseFromString(open('frozen_graph.pb','rb').read())
    >>> g
    node {
      name: ""input""
      op: ""Placeholder""
      attr {
        key: ""dtype""
        value {
          type: DT_FLOAT
        }
      }
      attr {
        key: ""shape""
        value {
          shape {
            dim {
              size: -1
            }
            dim {
              size: 68
            }
          }
        }
      }
    }
    ...
    node {
      name: ""output""
      op: ""Softmax""
      input: ""add""
      attr {
        key: ""T""
        value {
          type: DT_FLOAT
        }
      }
    }

I ran this model by the following code
(CELL is name of directory where my file is located)

    final String MODEL_FILE = ""file:///android_asset/"" + CELL + ""/optimized_graph.pb"" ;
    final String INPUT_NODE = ""input"" ;
    final String OUTPUT_NODE = ""output"" ;
    final int[] INPUT_SIZE = {1,68} ;
    float[] RESULT = new float[8];
    
    inferenceInterface = new TensorFlowInferenceInterface();
    inferenceInterface.initializeTensorFlow(getAssets(),MODEL_FILE) ;
    inferenceInterface.fillNodeFloat(INPUT_NODE,INPUT_SIZE,input);

and finally

    inferenceInterface.readNodeFloat(OUTPUT_NODE,RESULT);


But I get this error in Log

    12-28 16:42:48.622 9890-12178/com.getfocus.signalsimilarity I/native: tensorflow_inference_jni.cc:151 Initialization done in 52.275ms
    12-28 16:42:51.048 9890-12178/com.getfocus.signalsimilarity E/native: tensorflow_inference_jni.cc:170 Output [output] not found, aborting!


I have searched a lot for the solution but nothing seems to solve this. Thanks in advance",0,,2,2017-12-28T12:08:09Z,2017-12-29T02:56:55Z,NONE,2017-12-29T02:32:28Z
15686,fix description of HASHTABLE_LOOKUP in smartreply doc,"awaiting testing (then merge),cla: yes",HASHTABLE_LOOKUP is a builtin op rather than a custom one.,1,,3,2017-12-28T09:30:22Z,2017-12-31T22:24:32Z,CONTRIBUTOR,2017-12-29T00:47:52Z
15685,"fake_quant_with_min_max_vars doesn't change min, max vars ",stat:awaiting response,"# System Information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): pip3 install --upgrade tensorflow-gpu
- TensorFlow version (use command below): v1.4.0-19-ga52c8d9, 1.4.1
- Python version:3.5.2 
- Bazel version (if compiling from source): 0.7.0
- GCC/Compiler version (if compiling from source): GCC 5.4.0
- CUDA/cuDNN version: Cuda compilation tools, release 8.0, V8.0.61, cuDNN : 6.0.21
- GPU model and memory: Two GeForce GTX 1080 Ti devices.
- Exact command to reproduce: N/A
# Problem description
I've got a problem with tf-lite conversion tool. There is learned graph which should be converted in tflite format and quantinized.
I have read the answer https://stackoverflow.com/questions/47463204/tensorflow-lite-convert-error-for-the-quantized-graphdef where authors recommend to create new network with fake_quant operations and retrain it. However variables passed in tf.fake_quant_with_min_max_vars op did not change their values during training. Here is modeling code which shows the problem.
# Code to reproduce
```
import tensorflow as tf
import numpy as np

khe_init = tf.random_normal_initializer(mean=0.0, stddev=np.sqrt(2.0 / 1000))


def quant_conv_op(inpt, num_filters, filter_size=[3, 3], strides=[1, 1, 1, 1], padding=""VALID"", layer_name=""layer1"", use_acivation=True):
    num_input_map = inpt.get_shape().as_list()[-1]
    kernel_shape = filter_size + [num_input_map, num_filters]    
    with tf.variable_scope(layer_name):
        W = tf.get_variable(""weights"", shape=kernel_shape, initializer=khe_init)
        max_w = tf.get_variable(""max_quant_weights"", shape=[], initializer=tf.constant_initializer(1), trainable=True)
        min_w = tf.get_variable(""min_quant_weights"", shape=[], initializer=tf.constant_initializer(-1), trainable=True)
        b = tf.get_variable(""bias"", shape=[num_filters, ], initializer=tf.zeros_initializer)
 
        q_W = tf.fake_quant_with_min_max_vars(W, min_w, max_w)
        out = tf.nn.conv2d(inpt, q_W, strides=strides, padding=padding, name=""2d_convolution_operation"")
        out = tf.nn.bias_add(out, b)
        
        if use_acivation: 
            out = tf.nn.relu6(out, name=layer_name+""out"")
            out = tf.fake_quant_with_min_max_args(out, 0, 6)
        else:
            max_out = tf.get_variable(""max_quant_output"", shape=[], initializer=tf.constant_initializer(1), trainable=True)
            min_out = tf.get_variable(""min_quant_output"", shape=[], initializer=tf.constant_initializer(-1), trainable=True)  
            out = tf.fake_quant_with_min_max_vars(out, min_out, max_out, name=""fake_quant_with_min_max_out_quantinization"")
    
    return out, max_w, min_w, W


def loss(logits, batch):
    return tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf.ones(shape=[batch, 1]))


def build_net(ph_input):
    inp = tf.fake_quant_with_min_max_args(ph_input, -1, 1)
    out, max_w1, min_w1, W1 = quant_conv_op(inp, num_filters=64, filter_size=[3, 3], layer_name=""layer1"")    
    out, max_w2, min_w2, W2 = quant_conv_op(out, num_filters=128, filter_size=[3, 3], layer_name=""layer2"")
    out, max_w3, min_w3, W3 = quant_conv_op(out, num_filters=256, filter_size=[3, 3], layer_name=""layer3"")
    out = tf.reduce_mean(out, axis=[1, 2], keep_dims=True, name=""avg_pool"")
    out = tf.fake_quant_with_min_max_args(out, 0, 6, name=""fake_quant_with_min_max_avgpool_quantinization"")
    logits, max_w4, min_w4, W4 = quant_conv_op(out, num_filters=1, filter_size=[1, 1], layer_name=""layer4"", use_acivation=False)
    
    max_list = [max_w1, max_w2, max_w3, max_w4]
    min_list = [min_w1, min_w2, min_w3, min_w4]
    W_list = [W1, W2, W3, W4]
    logits = tf.reshape(logits, [-1, 1])
    sig_loss = tf.reduce_mean(loss(logits, batch=64))
    adam_op = tf.train.AdamOptimizer(10**-0)
    train_op = adam_op.minimize(sig_loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)) 
    return train_op, max_list, min_list, sig_loss, W_list


def main():
    sess = tf.InteractiveSession()
    ph_input = tf.placeholder(tf.float32, [None, 28, 28, 3], name=""network_input"") 
    train_op, max_list, min_list, sig_loss, W_list = build_net(ph_input)
    sess.run(tf.global_variables_initializer())
    tf.summary.FileWriter('.', graph=tf.get_default_graph())
    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
    for t in trainable_vars:
        if len(t.shape) == 0:
           print(t.name, sess.run(t))
        if len(t.shape) == 4:
           v = sess.run(t)
           print(t.name, np.max(v), np.min(v))

    for i in range(10):
        res = sess.run([train_op, sig_loss, W_list[0]]+max_list, feed_dict={ph_input:np.random.uniform(low=-1, high=1, size=[64, 28, 28, 3])})
    
    print(""========="")
    for t in trainable_vars:
        if len(t.shape) == 0:
           print(t.name, sess.run(t))
        if len(t.shape) == 4:
           v = sess.run(t)
           print(t.name, np.max(v), np.min(v))

if __name__ == ""__main__"":
    main()
```

  
  ",0,,1,2017-12-28T08:39:27Z,2018-01-24T04:46:01Z,NONE,2018-01-03T07:35:16Z
15683,R1.4,cla: no,,0,,3,2017-12-28T06:45:36Z,2017-12-28T17:51:27Z,NONE,2017-12-28T17:51:27Z
15682,Eager: tfe.implicit_value_and_gradients uses functions operating on raw tf variables,"comp:eager,type:support","## System information
- Tensorflow version: 1.5.0-dev20171126
- Python version: Python 3.5.0 (v3.5.0:374f501f4567, Sep 12 2015, 11:00:19)

## Problem
Forgive me if I'm re-iterating something that's discussed before. Even though I don't think the issue described here is a bug, I nevertheless believe it is worthy to point out. The specific issue is that when we pass a loss function, e.g. ```loss```, to ```tfe.implicit_value_and_gradients```, it seems that backprop only happens if the variables used by ```loss``` are is their ""raw states"". Here's an example:

```python
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()
v = tf.get_variable(name='v', initializer=1., trainable=True)
v_add_1 = v + 1.  # this causes the problem

def loss():
    return 2. * v_add_1
value_and_gradients_fn = tfe.implicit_value_and_gradients(loss)
print (value_and_gradients_fn())
```
In this case I get the error as follows:
```
Traceback (most recent call last):
  File ""test.py"", line 17, in <module>
    val = g()
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py"", line 360, in grad_fn
    raise ValueError(""No trainable variables were accessed while the ""
ValueError: No trainable variables were accessed while the function was being computed.
```
After a little bit of pondering, I found the problem to be the line ```v = v + 1.```. As soon as we **delete** this line, the program runs without bugs. My understanding of this behavior is that, the gradients and the backprop process somehow only ""live"" in the scope of the loss function. We **cannot** backprop to some variable that is modified outside of ```loss```, even if, implicitly, the computed loss depends on that variable. 

Here's a more obscure example:

```python
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

v = tf.get_variable(name='v', initializer=1., trainable=True)
v_add_1 = v + 1.
u = tf.get_variable(name='u', initializer=20., trainable=True)


def loss():
    result = v_add_1 + u
    return 2. * result

value_and_gradients_fn = tfe.implicit_value_and_gradients(loss)
optimizer = tf.train.AdamOptimizer(1e-1)

# before training
print (v)
print (u)

for i in range(100):
    _, gradients_and_variables = value_and_gradients_fn()
    optimizer.apply_gradients(gradients_and_variables)

# after training
print (v)
print (u)
```
After running, we could see that ```u``` is updated and ```v``` is not:
```
<tf.Variable 'v:0' shape=() dtype=float32, numpy=1.0>
<tf.Variable 'u:0' shape=() dtype=float32, numpy=20.0>
<tf.Variable 'v:0' shape=() dtype=float32, numpy=1.0>
<tf.Variable 'u:0' shape=() dtype=float32, numpy=9.9999638>
```

This might be irrelevant, but is there a way we could by pass the restriction and have gradient pass outside the function given to ```tfe.implicit_value_and_gradients```?",0,,2,2017-12-28T05:50:28Z,2017-12-29T22:05:16Z,NONE,2017-12-29T22:05:16Z
15681,tf.image.decode_image does not support png grayscale 16bit.,,"Code to reproduce:
```
import tensorflow as tf
import numpy as np

print (""TF Version: %s"" % tf.__version__)

import urllib

png16 = urllib.request.urlopen('http://www.schaik.com/pngsuite/basn0g16.png').read(1000)
with tf.Session() as session:
    content = tf.placeholder(tf.string)
    tensor = tf.image.decode_image(
        contents=content,
        channels=1
    )
    
    out = (session.run(tensor, {content: png16}))
    np_out = np.array(out)
    print ('Shape', np_out.shape)
    print ('Max', np_out.max())
    

    
```
Outputs:
``` 
TF Version: 1.4.1
Shape (32, 32, 1)
Max 255
```
Same file, file command:
```
file basn0g16.png
basn0g16.png: PNG image data, 32 x 32, 16-bit grayscale, non-interlaced
```
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS
- **TensorFlow installed from (source or binary)**: pip3 install tensorflow
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: nope
- **GCC/Compiler version (if compiling from source)**: nope
- **CUDA/cuDNN version**: no gpu
- **GPU model and memory**: no gpu
- **Exact command to reproduce**: see abve


python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.4.0-19-ga52c8d9 1.4.1

### Describe the problem
It should return uint16, but returns uint8.


### Source code / logs
see above
",0,,2,2017-12-28T05:41:20Z,2017-12-28T08:16:42Z,NONE,2017-12-28T08:16:39Z
15680,"Improve doc of TFRecordDataset, shuffle ahead of map","awaiting review,cla: yes","In the origin document, the code to demonstrate TFRecordDataset do `dataset.map(parser)` then do `dataset.shuffle(10000)`.
This code use a high number of buffer size (10000), and since `map` do ahead of `shuffle`, means when the first time this dataset yield one result it will need to run `map` over 10000 items and this can take a lot of time.
So, instead we can do `shuffle` ahead of `map`, since the item of `TFRecordDataset` is one `Example` raw data, `shuffle` ahead will not compromise the randomness and then the `map(parser)` only need to process one batch of items at a time. Which results much faster startup.
",0,,2,2017-12-28T03:56:00Z,2017-12-28T19:34:39Z,CONTRIBUTOR,2017-12-28T19:34:39Z
15677,De-Bazel check_load_py_test sanity check,"awaiting testing (then merge),cla: yes","See https://github.com/tensorflow/tensorflow/pull/15368#issuecomment-354156560 for reference.
@gunan @martinwicke Friendly ping.",1,,7,2017-12-28T01:36:12Z,2018-01-23T19:15:58Z,CONTRIBUTOR,2018-01-23T17:50:52Z
15676,Enabling tests to pass with python3.6. Updating dependencies for dock,cla: yes,er tests.,0,,1,2017-12-28T00:16:36Z,2017-12-28T17:53:59Z,MEMBER,2017-12-28T00:48:31Z
15675,Branch 180224227,"awaiting testing (then merge),cla: yes",Need a push for the release.,0,,1,2017-12-28T00:11:27Z,2017-12-28T03:04:20Z,MEMBER,2017-12-28T00:48:12Z
15674,Remove third_party/ prefixes,"cla: yes,pending merge internally",,1,,5,2017-12-27T23:54:21Z,2018-01-09T19:10:55Z,CONTRIBUTOR,2017-12-27T23:55:42Z
15671,Turn check_futures_test into a sanity check,"awaiting testing (then merge),cla: yes","See https://github.com/tensorflow/tensorflow/pull/15368#issuecomment-354156560 for reference.
@gunan @martinwicke /cc",1,,11,2017-12-27T22:23:12Z,2018-01-23T19:16:30Z,CONTRIBUTOR,2017-12-27T23:49:31Z
15670,[CMake] Add sanity tests for python file lists,"awaiting testing (then merge),cla: yes","Replaces #15166
See discussion in #15368
@gunan @martinwicke Friendly ping.",0,,8,2017-12-27T22:04:32Z,2018-01-22T21:38:57Z,CONTRIBUTOR,2018-01-03T06:47:16Z
15668,MNIST dataset - gzip: train-images-idx3-ubyte.gz: not in gzip format,,"initiated from tensorflow-discuss From: greina@eng.ucsd.edu 

> 
> 
> I'm trying to use:
> 
> from tensorflow.examples.tutorials.mnist import input_data
> mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)
> 
> but I am getting the error that the downloaded file is not in GZip format.
> 
> There are several bug reports on this, but none of them seem to solve the issue. 
> 
> I found train-images-idx3-ubyte.gz in the local directory MNIST_data, but even trying `gunzip` fails:
> 
> ```(tf) [bduser@param03 MNIST_data]$ gunzip
> gzip: compressed data not read from a terminal. Use -f to force decompression.
> For help, type: gzip -h
> (tf) [bduser@param03 MNIST_data]$ gunzip train-images-idx3-ubyte.gz
> 
> gzip: train-images-idx3-ubyte.gz: not in gzip format
> ```
> 
> I'm thinking that this is a problem with the original datafile. Maybe the GZip format has changed or is in conflict?? Or perhaps my OS' version of GZip can't understand the GZip file??
> 
> I am using TF 1.4 on CentOS Linux release 7.4.1708 (Core)
> 
> Thanks.
> -Tony
> 
> 
> 
> ```
> >>> from tensorflow.examples.tutorials.mnist import input_data
> >>> mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)
> Successfully downloaded train-images-idx3-ubyte.gz 727 bytes.
> Extracting MNIST_data/train-images-idx3-ubyte.gz
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""/home/bduser/miniconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 242, in read_data_sets
>     train_images = extract_images(f)
>   File ""/home/bduser/miniconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 56, in extract_images
>     magic = _read32(bytestream)
>   File ""/home/bduser/miniconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 38, in _read32
>     return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]
>   File ""/home/bduser/miniconda2/envs/tf/lib/python2.7/gzip.py"", line 268, in read
>     self._read(readsize)
>   File ""/home/bduser/miniconda2/envs/tf/lib/python2.7/gzip.py"", line 303, in _read
>     self._read_gzip_header()
>   File ""/home/bduser/miniconda2/envs/tf/lib/python2.7/gzip.py"", line 197, in _read_gzip_header
>     raise IOError, 'Not a gzipped file'
> IOError: Not a gzipped file
> >>> exit()
> ```
> ",0,,3,2017-12-27T20:59:09Z,2017-12-28T20:23:19Z,MEMBER,2017-12-27T21:01:03Z
15667,minor wording fix in (slim) readme,"awaiting testing (then merge),cla: yes","""we want to restore a model from a checkpoint
whose variables have different names **to** those in the current graph.""  (add the **to** in the line)",0,,3,2017-12-27T20:09:58Z,2017-12-28T01:06:03Z,CONTRIBUTOR,2017-12-27T22:13:02Z
15666,updated Readme.md,"awaiting testing (then merge),cla: yes",tf-nightly whl files download links added for Linux of python 3.6 version,0,,2,2017-12-27T19:53:55Z,2017-12-27T22:49:18Z,CONTRIBUTOR,2017-12-27T21:04:20Z
15664,Fix small typo in docstring which causes the API doc to render incorr,"awaiting testing (then merge),cla: yes","ectly
See
![screen shot 2017-12-27 at 19 53 06](https://user-images.githubusercontent.com/11613312/34390366-adc111e8-eb3f-11e7-8d31-bdb0c24c32a9.png)
on https://www.tensorflow.org/api_docs/python/tf/train/piecewise_constant",0,,5,2017-12-27T18:54:06Z,2017-12-28T00:07:37Z,CONTRIBUTOR,2017-12-27T18:56:36Z
15661,Fix typo 'updaye' in audio_recognition.md,"awaiting testing (then merge),cla: yes",Fix typo 'updaye' to 'update' in audio_recognition.md,0,,7,2017-12-27T16:47:29Z,2017-12-28T00:07:17Z,CONTRIBUTOR,2017-12-27T16:49:28Z
15660,Distributed training fault tolerance,"stat:awaiting tensorflower,type:support","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04.5 LTS
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: Python 3.6.3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: release 8.0, V8.0.44/ libcudnn v6.0.21
- **GPU model and memory**: Titan X (Pascal) 12 GB
- **Exact command to reproduce**: In the description

### Describe the problem
I am using the parameter server/master/worker paradigm to run model training in distributed mode. The master node does the training and evaluation, while the worker nodes only do the training (on their shard of the data). I should also note that I am using the `tf.contrib.learn.Experiment` interface.

This model of distributed training works as expected, however, sometimes one or more of the worker nodes fail. While they have failed, the master node and other worker nodes continue the training. The problem is that when this occurs (even when only one of the worker nodes have failed), the loss suddenly becomes zero, and as a result gradients as well become zero, while the metrics suddenly change to the value of a random model, as can be seen in the figures below.

* Loss curve. As can be seen one or more of the workers have failed three times during the training.
![loss_fail](https://user-images.githubusercontent.com/1921165/34386146-3ed3d652-eaf5-11e7-99de-7923862089e6.jpg)

* Gradient norm curve
![gradient_fail](https://user-images.githubusercontent.com/1921165/34386155-48760dba-eaf5-11e7-86f9-d653cda03a3e.PNG)

* Accuracy (on the validation set)
![accuracy_fail](https://user-images.githubusercontent.com/1921165/34386161-51b1f114-eaf5-11e7-8a6c-303d8972b2b8.PNG)


**Is there a way to prevent this behavior, either by stopping the training when one of the workers fails, or pausing the training until the failed worker comes back online again (like in the beginning of the training when training only starts when all of the workers come online)?**

### Source code / logs
As indicated above, I use the experiment interface. This is the configuration for distributed training:
```python
dist_config = {}
dist_config['cluster'] = {
    'master': ['127.0.0.1:{}'.format(dist_start_port + 1)],
    'ps': ['127.0.0.1:{}'.format(dist_start_port)],
    'worker': ['127.0.0.1:{}'.format(dist_start_port + 2 + i)
            for i in range(worker_count)]
}
dist_config['task'] = {
    'type': dist_type,
    'index': (worker_index if args.dist_type == 'worker' else 0)
}
dist_config['environment'] = 'cloud'
os.environ['TF_CONFIG'] = json.dumps(dist_config)
```
And this is how I start each node:
```python
if dist_type == 'master':
    experiment.train_and_evaluate()
elif dist_type == 'ps':
    experiment.run_std_server()
else:
    experiment.train()
```
",0,,5,2017-12-27T16:10:38Z,2018-01-17T19:54:38Z,CONTRIBUTOR,2017-12-30T09:20:34Z
15659,Document Bazel-Tensorflow-Cuda interdependencies,stat:awaiting response,"Since each version of Tensorflow appears to require some specific release of bazel, it would be helpful to have documentation like [this](https://www.tensorflow.org/install/install_sources#tested_source_configurations) also for people who are stuck on an older version of CUDA. For instance, I cannot upgrade to CUDA 8 on the machine I am using and am now left with the exercise of finding a working config in a space of 5 dimensions (python version, bazel version, tf version, cuda version, cudnn version).
I had it once working with CUDA 7.0 and python 3.5 (and I think bazel 0.3), but cannot reproduce now.
In this concrete case, I am trying to build `r0.11` with python 3.6, cuda 7.5, cudnn 6 and bazel 0.3/0.4/0.8 and nothing's working.",0,,2,2017-12-27T16:04:11Z,2018-01-12T20:01:36Z,CONTRIBUTOR,2017-12-28T19:41:34Z
15658,add a function to add extra logging handler,"API review,cla: yes","I want to separate different level of logs. So error message will not be buried by info/debug logs. In r1.4.  I use following code to do the job.

```
  file_hanlder = logging.FileHandler(""running_error.log"", mode='w', encoding=None, delay=False)
  file_hanlder.setLevel(logging.WARNING)
  tf.logging._logger.addHandler(file_hanlder)
```
but, when I change to r1.5, it don't work any more. because tensorflow rewrite tf_logging.py and _logger  is not exposed.  So I think add a function to add extra handler should be useful.",0,,5,2017-12-27T15:40:57Z,2018-01-04T00:55:42Z,CONTRIBUTOR,2018-01-02T13:44:57Z
15657,Removed misplaced quote char,"awaiting testing (then merge),cla: yes",,0,,2,2017-12-27T15:35:48Z,2017-12-27T23:46:24Z,CONTRIBUTOR,2017-12-27T21:25:18Z
15656,CUDA 9.1 and TensorFlow,"stat:awaiting tensorflower,type:support","I am using NVIDIA GeForce GTX 1050 and installed NVIDIA 387.26. I installed cuDNN 7.0.5 and CUDA 9.1. As of my understanding, I know that, tensorflow is not supported in CUDA 9.1. My question is when I can expect the next build/release of TF to support CUDA 9.1. For the time being, shall I make a link from CUDA 9.0 to CUDA 9.1 and expect to work? Or is there any better way to solve the problem?",0,,24,2017-12-27T14:10:43Z,2017-12-31T05:44:23Z,NONE,2017-12-30T09:22:12Z
15655,"tf.layers.conv3d with ""channels_first"" does not accept batch dimension to be None",,"code to reproduce:

```python
import tensorflow as tf
x = tf.placeholder(dtype=tf.float32, shape=[None, 1, 32, 32, 32])
y = tf.layers.conv3d(x, 32, 9, data_format='channels_first')
```

traceback
```
Traceback (most recent call last):
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 468, in make_tensor_proto
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 468, in <listcomp>
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/util/compat.py"", line 65, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got None

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py"", line 809, in conv3d
    return layer.apply(inputs)
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 671, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 575, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py"", line 185, in call
    outputs_shape[4]])
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3938, in reshape
    ""Reshape"", tensor=tensor, shape=shape, name=name)
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 513, in _apply_op_helper
    raise err
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 926, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 229, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 208, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 472, in make_tensor_proto
    ""supported type."" % (type(values), values))
TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [None, 32, 576, 24]. Consider casting elements to a supported type.
```

The error source appears [here][1] and can be simply fixed by adding

```python
if outputs_shape[0] is None:
  outputs_shape[0] = -1
```

however you might suggest a deeper fix?


[1]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L181-L185",0,,7,2017-12-27T13:55:37Z,2018-01-30T01:58:15Z,NONE,2017-12-27T21:32:53Z
15654,Enable `axis` support for `tf.unique`,"awaiting testing (then merge),cla: yes","The `axis` support for `Unique` has been Added in PR #12952 (defined in `UniqueV2` ops). The support for `axis` in python version of the `tf.unique` was not enabled yet, due to the API workflow porcess (3 weeks). This fix adds the support for `axis` with `tf.unique` by adds `Unique` to `hidden.txt`, and adds a python wrapper of `tf.unique` to pointing to `UniqueV2`.

This fix addresses part of the issue #15644.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,28,2017-12-27T12:07:40Z,2018-01-11T22:40:30Z,MEMBER,2017-12-30T02:53:50Z
15653,Clarify batch_norm documentation to highlight that the dimensions for,"awaiting testing (then merge),cla: no,stat:awaiting response",... normalization depend on the shape of the tensor.,0,,11,2017-12-27T10:49:05Z,2017-12-30T07:53:32Z,CONTRIBUTOR,2017-12-27T10:52:30Z
15652,fix variable name,"awaiting testing (then merge),cla: yes",,0,,3,2017-12-27T06:34:10Z,2017-12-28T19:19:02Z,CONTRIBUTOR,2017-12-27T21:21:26Z
15650,Unable to compile tensorflow r1.4 from source with cuda 8.0 and cudnn 7 and after downgrading bazel?,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.4
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.8.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 8.0/7.0.4
- **GPU model and memory**: 1080 Ti
- **Exact command to reproduce**: 
`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""`


### Describe the problem
I'm trying to compile TF r1.4 from source but didn't manage to get this working although I tried several fixes:

1. Downgrading bazel from 0.9.0 to 0.8.1 based on #15492 

2. `sudo sh -c ""echo '/usr/local/cuda-8.0/lib64' >> /etc/ld.so.conf.d/nvidia.conf""`  and `sudo ldconfig` based on #13481

3. adding the `action_env` argument based on https://stackoverflow.com/questions/47080760/tensorflow-fails-to-compile/47295278#47295278

Note: When installing cudnn, I used both the runtime library and the tar file which i extracted and placed it in the /usr/local/cuda library respective folders.

### Source code / logs
```
ERROR: /home/kwotsin/tensorflow/tensorflow/core/BUILD:2131:1: C++ compilation of rule '//tensorflow/core:gpu_runtime_impl' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/kwotsin/.cache/bazel/_bazel_kwotsin/041f6cc3555a2d9f6211c6d126ede477/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64 \
    PATH=/usr/local/cuda/bin:/home/kwotsin/bin:/home/kwotsin/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -g0 -MD -MF bazel-out/host/bin/tensorflow/core/_objs/gpu_runtime_impl/tensorflow/core/common_runtime/gpu/gpu_device.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/_objs/gpu_runtime_impl/tensorflow/core/common_runtime/gpu/gpu_device.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DSNAPPY -iquote . -iquote bazel-out/host/genfiles -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/genfiles/external/local_config_cuda -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/host/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' -msse3 -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c tensorflow/core/common_runtime/gpu/gpu_device.cc -o bazel-out/host/bin/tensorflow/core/_objs/gpu_runtime_impl/tensorflow/core/common_runtime/gpu/gpu_device.pic.o)
In file included from ./tensorflow/stream_executor/stream_executor.h:35:0,
                 from ./tensorflow/core/platform/stream_executor.h:38,
                 from ./tensorflow/core/common_runtime/gpu/gpu_event_mgr.h:28,
                 from ./tensorflow/core/common_runtime/gpu/gpu_device.h:30,
                 from tensorflow/core/common_runtime/gpu/gpu_device.cc:22:
./tensorflow/stream_executor/stream_executor_pimpl.h:87:63: internal compiler error: Segmentation fault
   PlatformKind platform_kind() const { return platform_kind_; }
                                                               ^
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 145.189s, Critical Path: 22.73s
FAILED: Build did NOT complete successfully

```

Thank you.

====

Further updates:

1. I tried switching to the more updated 7.0.5 CuDNN (although some mentioned in #12052 that their build worked with 7.0.4. I'm now using CUDA 9.0 + CuDNN 7.05 for CUDA 9.0, and with bazel 0.8.1. The build unfortunately still doesn't work.

2. CUDA 8.0 + CuDNN 6.0.21 also doesn't work, with the similar reasons as such:

```
C++ compilation of rule '//tensorflow/core/kernels:sparse_conditional_accumulator_op' failed (Exit 1)
In file included from tensorflow/core/kernels/sparse_conditional_accumulator_op.cc:19:0:
./tensorflow/core/kernels/sparse_conditional_accumulator.h: In destructor 'tensorflow::SparseConditionalAccumulator<Device, T>::~SparseConditionalAccumulator() [with Device = Eigen::ThreadPoolDevice; T = float]':
./tensorflow/core/kernels/sparse_conditional_accumulator.h:68:3: internal compiler error: Segmentation fault
   };
   ^
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 505.239s, Critical Path: 114.19s
FAILED: Build did NOT complete successfully

```

```
ERROR: /home/kwotsin/tensorflow/tensorflow/core/kernels/BUILD:2554:1: C++ compilation of rule '//tensorflow/core/kernels:cwise_op' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/kwotsin/.cache/bazel/_bazel_kwotsin/041f6cc3555a2d9f6211c6d126ede477/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-9.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_VERSION=9.0 \
    TF_CUDNN_VERSION=7.0.5 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++11' '-march=native' '-D_GLIBCXX_USE_CXX11_ABI=0' -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/cwise_op/tensorflow/core/kernels/cwise_op_floor_div.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/cwise_op/tensorflow/core/kernels/cwise_op_floor_div.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DSNAPPY -iquote . -iquote bazel-out/k8-opt/genfiles -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/k8-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/k8-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' -msse3 -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c tensorflow/core/kernels/cwise_op_floor_div.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/cwise_op/tensorflow/core/kernels/cwise_op_floor_div.pic.o)
In file included from tensorflow/core/kernels/cwise_op_floor_div.cc:16:0:
./tensorflow/core/kernels/cwise_ops_common.h: In instantiation of 'void tensorflow::functor::BinaryFunctor<Eigen::ThreadPoolDevice, Functor, NDIMS, false>::BCast(const CPUDevice&, typename tensorflow::TTypes<typename Functor::out_type, NDIMS>::Tensor, typename tensorflow::TTypes<typename Functor::in_type, NDIMS>::ConstTensor, Eigen::array<long int, NDIMS>, typename tensorflow::TTypes<typename Functor::in_type, NDIMS>::ConstTensor, Eigen::array<long int, NDIMS>, bool*) [with Functor = tensorflow::functor::floor_div_real<double>; int NDIMS = 4; tensorflow::functor::CPUDevice = Eigen::ThreadPoolDevice; typename tensorflow::TTypes<typename Functor::out_type, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<double, 4, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<typename Functor::in_type, NDIMS>::ConstTensor = Eigen::TensorMap<Eigen::Tensor<const double, 4, 1, long int>, 16, Eigen::MakePointer>]':
./tensorflow/core/kernels/cwise_ops_common.h:136:7:   required from 'void tensorflow::BinaryOp<Device, Functor>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; Functor = tensorflow::functor::floor_div_real<double>]'
tensorflow/core/kernels/cwise_op_floor_div.cc:53:1:   required from here
./tensorflow/core/kernels/cwise_ops_common.h:417:3: internal compiler error: Segmentation fault
   }
   ^
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 430.853s, Critical Path: 77.17s
FAILED: Build did NOT complete successfully

```",0,,6,2017-12-27T03:02:31Z,2018-01-05T13:50:41Z,CONTRIBUTOR,2018-01-03T01:42:29Z
15648,Predictor fixes for core estimators,"awaiting testing (then merge),cla: yes","Creating a predictor from a core estimator was broken due to:

- a wrong instance check
- wrong initialization of the ChiefSessionCreator

This PR fixes both.",0,,4,2017-12-26T21:56:54Z,2017-12-28T01:05:38Z,CONTRIBUTOR,2017-12-27T02:36:34Z
15647,Tensorflow.org - master version is not updated,stat:awaiting tensorflower,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Not Relevant
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Not Relevant
- **TensorFlow installed from (source or binary)**: Not Relevant
- **TensorFlow version (use command below)**: Not Relevant
- **Python version**: Not Relevant
- **Bazel version (if compiling from source)**: Not Relevant
- **GCC/Compiler version (if compiling from source)**: Not Relevant
- **CUDA/cuDNN version**: Not Relevant
- **GPU model and memory**: Not Relevant
- **Exact command to reproduce**: Not Relevant

### Describe the problem
tensorflow.org master version should be updated to master, however it seems that it hasn't been regenerated for a while.

For example the latest addition of the performance guide for `tf.data` (https://github.com/tensorflow/tensorflow/commit/ba32ea1547af74d549f35a42e4de83c88652a636) hasn't yet made it to the website:

Doc source: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/performance_guide.md

Generated web page in tensorflow.org (master version):
https://www.tensorflow.org/versions/master/performance/performance_guide

(Currently shows ""Last updated November 14, 2017."" in the bottom)

",0,,3,2017-12-26T20:42:01Z,2018-01-04T19:56:55Z,NONE,2017-12-28T19:42:43Z
15646,Branch 180147476,cla: yes,,0,,1,2017-12-26T20:02:43Z,2017-12-27T01:26:58Z,MEMBER,2017-12-27T01:27:22Z
15642,contrib/all_reduce not update to latest nccl,,"send_op, dst_tensors = nccl.broadcast(level_2_output[w], dst_devices)

this line is out of date, hope update to latest",0,,4,2017-12-26T13:39:35Z,2018-01-03T01:41:42Z,NONE,2018-01-03T01:41:42Z
15641,Compile with selective register on meta file,,"I want to apply the selective registration feature on my model to decrease the lib size. However, I need to apply it on a meta file saved via `saver` rather than a frozen pb file as shown in many posts I have found. When I try to run 

`bazel-bin/tensorflow/python/tools/print_selective_registration_header --graphs=model_test.ckpt-390760.meta`

it comes to the error

`[libprotobuf ERROR external/protobuf/src/google/protobuf/wire_format_lite.cc:621] String field 'tensorflow.NodeDef.op' contains invalid UTF-8 data when parsing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.`

I tried to add `--proto_fileformat=textproto` but another error comes up:

`raise self.ParseError('Expected identifier or number.')
google.protobuf.text_format.ParseError: 2:1 : Expected identifier or number.`

Is it even possible to do this at all? The ultimate goal of compiling this lib is to restore a pretrained model and incrementally train it on Android.",0,,4,2017-12-26T13:23:09Z,2018-01-30T01:38:03Z,NONE,2017-12-28T19:48:34Z
15639,fix doc for benchmark_model for android,"awaiting review,cla: yes","after configure.py set framework_shared_object=true,
benchmark_model won't build for Android without tweeks. Add
'--config monolithic' to avoid confusion.",0,,8,2017-12-26T09:53:39Z,2018-01-23T20:19:07Z,CONTRIBUTOR,2017-12-26T19:04:54Z
15637,Fix an lib error while building with CUDA9.1,"awaiting review,cla: yes",Some header files in CUDA9.1 was moved into dir cuda/include/crt causing when building with CUDA9.1 headers like math_functions.hpp could no long be loaded.  Adding the directory into BUILD file fixs the issue. ,0,,8,2017-12-26T07:48:16Z,2017-12-26T20:03:26Z,CONTRIBUTOR,2017-12-26T07:50:33Z
15636,Read tflite file failed on iOS,"comp:lite,stat:awaiting response","Hi

I tried the examples on iOS according to TF Lite guide, but failed when assign data to tflite because address ""out"" is NULL. probably my tflite file is incorrect, but I am not sure, can anybody give some help? 

My test step is as follows:
1.  Try the following code(https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite),  get the tflite file converteds_model.tflite

```
import tensorflow as tf
img = tf.placeholder(name=""img"", dtype=tf.float32, shape=(1, 64, 64, 3))
val = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])
out = tf.identity(val, name=""out"")
with tf.Session() as sess:
  tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [img], [out])
  open(""converteds_model.tflite"", ""wb"").write(tflite_model)
```

2. Integrated the tflite into my app, which is from iOS sample code ""simple""(/Users/Sensteer/Software/tensorflowclone/tensorflow/tensorflow/contrib/lite/examples/ios/simple).But exception happed because address ""out"" is NULL

```
int input = interpreter->inputs()[0];                                 //input is 3
float* out = interpreter->typed_tensor<float>(input);          //out is NULL
```

So my questions are:
1. The tflite created above is right or not?
2. The reading tflite code is right or not?
2. If the tflite file is not right, do I must create tflite with  ""pb"", ""ckpt"" and ""FrozenGraphDef"" mentioned in guide?

Thanks
",0,,5,2017-12-26T07:37:09Z,2018-01-30T07:26:52Z,NONE,2017-12-29T02:21:12Z
15634,ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory with python 2.7,,"Ubuntu 16.04
GPU: 1080 Ti
Cuda 9.0
CuDDN 7.0 v7
Using my PC, not a VM
Installed Intel MKL-DNN by this [guilde](https://github.com/mind/wheels#mkl), but looks like something wrong, because when trying to make a test for tensorflow, got error:

```
gagazet@woof:~/Desktop$` python 123.py 
Traceback (most recent call last):
  File ""123.py"", line 1, in <module>
    import tensorflow as tf
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

Recently someone created same problem and it was closed with reason: PR [#12975](https://github.com/tensorflow/tensorflow/pull/12975), but it was for a VM.
Can anyone explain, please, what i can be and how to fix it? Dont have any build_pip_package scrips at the TF folder. 
Its doesnt looks like same problem as PR [#12975](https://github.com/tensorflow/tensorflow/pull/12975)
",0,,1,2017-12-26T07:02:56Z,2017-12-29T02:19:56Z,NONE,2017-12-29T02:19:56Z
15632,Branch 180053468,cla: yes,,0,,1,2017-12-26T01:09:22Z,2017-12-26T04:53:52Z,MEMBER,2017-12-26T01:10:39Z
15631,ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory,,"GPU: 1080 Ti
cuda 9.0
cuddn 7.0 v7
centos 7.
installed Intel MKL-DNN but the error is still present.


here is the output:
````
ipython
Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49) 
Type 'copyright', 'credits' or 'license' for more information
IPython 6.1.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import tensorflow as tf
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~/anaconda3/lib/python3.6/imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

~/anaconda3/lib/python3.6/imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-41389fad42b5> in <module>()
----> 1 import tensorflow as tf

~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in <module>()
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     70 for some common reasons and solutions.  Include the entire stack trace
     71 above this error message when asking for help."""""" % traceback.format_exc()
---> 72   raise ImportError(msg)
     73 
     74 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/home/sb0709/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/sb0709/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/sb0709/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/sb0709/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/sb0709/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```
`



",0,,8,2017-12-25T21:05:09Z,2017-12-26T06:45:20Z,NONE,2017-12-26T06:45:20Z
15630,tf.name_scope does not work with tf.layers,,"When I define a layer with
```
with tf.name_scope(""MY_SCOPE""):
    layer1 = tf.layers.dense(inputs = X,
                             units = 100,
                             kernel_initializer = he_init,
                             activation = tf.nn.elu,
                             name = ""layer1"")


```

and try to get its variables with

`tf.global_variables(scope=""MY_SCOPE"")`

it returns nothing because name scope did not apply to the layers from tf.layers

Shouldn't their name be

[<tf.Variable 'MY_SCOPE/layer1/kernel:0' shape=(784, 100) dtype=float32_ref>,
 <tf.Variable 'MY_SCOPE/layer1/bias:0' shape=(100,) dtype=float32_ref>]

instead of

[<tf.Variable 'layer1/kernel:0' shape=(784, 100) dtype=float32_ref>,
 <tf.Variable 'layer1/bias:0' shape=(100,) dtype=float32_ref>]

so that I can use my scope to reach my layers? This works with everything but tf.layers module.",0,,4,2017-12-25T18:39:45Z,2017-12-26T18:06:27Z,NONE,2017-12-26T05:13:42Z
15623,"I cannot use Binomial.sample(), could you please help me?",,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,1,2017-12-25T07:28:37Z,2017-12-25T18:11:25Z,NONE,2017-12-25T18:11:25Z
15622,Unable to compile from source on High Sierra (10.13.2) with bazel 0.9.0,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: tf 1.4
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **Exact command to reproduce**: bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=- msse4.1 --copt=-msse4.2 --config=opt -k //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
I'm unable to compile tensorflow from source. I get too many errors as shown below in the logs:

### Source code / logs
```
Rakshiths-MacBook-Pro:tensorflow rakshithgb$ ./configure
WARNING: Running Bazel server needs to be killed, because the startup options are different.
You have bazel 0.9.0-homebrew installed.
Please specify the location of python. [Default is /Users/rakshithgb/miniconda3/bin/python]: 


Found possible Python library paths:
  /Users/rakshithgb/miniconda3/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/Users/rakshithgb/miniconda3/lib/python3.6/site-packages]

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n
No Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n
No Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: n
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: n
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL support? [y/N]: n
No OpenCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Add ""--config=mkl"" to your bazel command to build with MKL support.
Please note that MKL on MacOS or windows is still not supported.
If you would like to use a local MKL instead of downloading, please set the environment variable ""TF_MKL_ROOT"" every time before build.
Configuration finished
Rakshiths-MacBook-Pro:tensorflow rakshithgb$ bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=- msse4.1 --copt=-msse4.2 --config=opt -k //tensorflow/tools/pip_package:build_pip_package
..............
ERROR: Skipping 'msse4.1': no such target '//:msse4.1': target 'msse4.1' not declared in package '' defined by /Users/rakshithgb/Documents/Tensorflow/tensorflow/BUILD
WARNING: Target pattern parsing failed.
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):
	File ""/private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD"", line 27
		cc_library(name = ""syclrt"", srcs = [sycl_libr..."")], <3 more arguments>)
	File ""/private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD"", line 30, in cc_library
		sycl_library_path
name 'sycl_library_path' is not defined
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD:39:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '@local_config_sycl//sycl:sycl'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/platform/default/build_config/BUILD:115:1: Target '@com_googlesource_code_re2//:re2' contains an error and its package is in error and referenced by '//tensorflow/core/platform/default/build_config:platformlib'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/third_party/eigen3/BUILD:20:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//third_party/eigen3:eigen3'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3169:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:pooling_ops'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3798:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:variable_ops'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:717:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:compare_and_bitpack_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3776:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:scatter_nd_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3764:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:dense_update_ops'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:643:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:gather_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:607:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:bitcast_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:619:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:constant_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:625:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:diag_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:631:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:edit_distance_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:687:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:mirror_pad_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:711:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:quantize_and_dequantize_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:787:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:split_v_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:774:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:slice_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3770:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:scatter_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3758:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:count_up_to_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/BUILD:2215:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//tensorflow/core:sycl_runtime'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:780:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:split_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:681:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:matrix_set_diag_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:601:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:bcast_ops'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:756:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:reverse_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:699:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:pack_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:813:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:transpose_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:705:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:pad_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:693:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:one_hot_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:649:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:identity_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:661:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:listdiff_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:533:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:immutable_constant_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:655:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:identity_n_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:613:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:concat_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:675:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:matrix_diag_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:839:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:where_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:762:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:reverse_sequence_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:794:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:inplace_ops'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:827:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:unique_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:768:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:shape_ops'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:667:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:matrix_band_part_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:637:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:gather_nd_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:750:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:reshape_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:800:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:tile_ops'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:833:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:unpack_op'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:550:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:debug_ops'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/tools/pip_package/BUILD:101:1: Target '@com_googlesource_code_re2//:LICENSE' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:licenses'
ERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/tools/pip_package/BUILD:101:1: Target '@local_config_sycl//sycl:LICENSE.text' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:licenses'
WARNING: errors encountered while analyzing target '//tensorflow/tools/pip_package:build_pip_package': it will not be built
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (204 packages loaded).
INFO: Found 0 targets...
ERROR: command succeeded, but there were errors parsing the target pattern
INFO: Elapsed time: 51.902s, Critical Path: 0.02s
FAILED: Build did NOT complete successfully
```


",0,,17,2017-12-25T06:42:27Z,2017-12-29T22:14:03Z,NONE,2017-12-25T13:42:27Z
15620,Tf Lite only support 4D l2_normalize?,"comp:lite,type:support","I build some feature extract network model and converted tflite using by toco successfully.
But I got error `""tensorflow/contrib/lite/kernels/l2norm.cc:47 NumDimensions(input) != 4 (2 != 4)`, when run interpreter->AllocateTensors().

I extract feature using by tf.nn.l2_normalize.
`embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name='embeddings')`
where prelogits is 2D tensor.
How can I extract normalized feature with tflite?
",1,,3,2017-12-25T04:05:11Z,2018-01-25T03:19:52Z,NONE,2017-12-30T09:54:34Z
15619,Crelu should have axis,stat:awaiting tensorflower,"Currently, the `tf.nn.crelu` activation concatenates along the last axis. This would work fine for dense layers and conv layers where the `data_fromat=channels_last`, but this would be incorrect if invoked on the widely used `data_format=channels_first` for conv layers on the GPU.
",0,,3,2017-12-25T02:06:17Z,2018-01-11T17:33:24Z,NONE,2017-12-28T19:19:07Z
15617,Fix periodic resample,"awaiting testing (then merge),cla: yes",,1,,20,2017-12-24T17:57:55Z,2018-01-22T22:46:37Z,CONTRIBUTOR,2017-12-24T18:29:07Z
15616,[CMake] Include example compile script,"awaiting testing (then merge),cla: yes",@mrry Friendly ping.,0,,11,2017-12-24T17:30:07Z,2017-12-28T23:45:44Z,CONTRIBUTOR,2017-12-25T02:36:44Z
15614,Support Negativo17 Fedora Packaging,"awaiting review,cla: yes","Support the [Negativo17](https://negativo17.org/nvidia-driver/) Nvidia driver packaging for Fedora. `libdevice` libraries are under `/usr/share/cuda`, includes are under `/usr/include/cuda` and libraries are under `/usr/lib64`. This PR should help #8264 too.

In addition, the gcc 5.3 in the Negativo17 repository (installed as `/usr/bin/gcc53`) only has a static non-PIC version of `libgomp.a`, so I have this local patch to force Tensorflow to link to the global (`/usr/lib64`) shared version:

````diff
diff --git a/tensorflow/contrib/cmake/tf_stream_executor.cmake b/tensorflow/contrib/cmake/tf_stream_executor.cmake
index 91ca33f4c4..7719ee096d 100644
--- a/tensorflow/contrib/cmake/tf_stream_executor.cmake
+++ b/tensorflow/contrib/cmake/tf_stream_executor.cmake
@@ -75,7 +75,7 @@ endif()
 #list(REMOVE_ITEM tf_stream_executor_srcs ${tf_stream_executor_test_srcs})
 
 if (NOT WIN32)
-  set (CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -lgomp"")
+  set (CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -l:libgomp.so.1"")
 endif (NOT WIN32)
 add_library(tf_stream_executor OBJECT ${tf_stream_executor_srcs})
 
diff --git a/third_party/gpus/cuda/BUILD.tpl b/third_party/gpus/cuda/BUILD.tpl
index b752734a08..0ce972291e 100644
--- a/third_party/gpus/cuda/BUILD.tpl
+++ b/third_party/gpus/cuda/BUILD.tpl
@@ -109,7 +109,7 @@ cc_library(
         ""."",
         ""cuda/include"",
     ],
-    linkopts = [""-lgomp""],
+    linkopts = [""-l:libgomp.so.1""],
     linkstatic = 1,
     visibility = [""//visibility:public""],
 )
diff --git a/third_party/toolchains/gpus/cuda/BUILD b/third_party/toolchains/gpus/cuda/BUILD
index 39136de99c..6f697919fd 100644
--- a/third_party/toolchains/gpus/cuda/BUILD
+++ b/third_party/toolchains/gpus/cuda/BUILD
@@ -114,7 +114,7 @@ cc_library(
         ""."",
         ""cuda/include"",
     ],
-    linkopts = [""-lgomp""],
+    linkopts = [""-l:libgomp.so.1""],
     linkstatic = 1,
     visibility = [""//visibility:public""],
 )
````

Building with clang is a lot more difficult, as it'd require making Tensorflow's CUDA symlink repo look enough like the unpacked tarball to pass [this detection logic](https://github.com/jyknight/llvm-monorepo/blob/6a6c3cae76a0839429c0b552572c46af9b194b86/clang/lib/Driver/ToolChains/Cuda.cpp)!

My `.tf_configure.bazelrc` (for FC 26) looks like:

````
build --action_env PYTHON_BIN_PATH=""/home/nicholas/miniconda3/bin/python""
build --action_env PYTHON_LIB_PATH=""/home/nicholas/miniconda3/lib/python3.6/site-packages""
build --force_python=py3
build --host_force_python=py3
build --python_path=""/home/nicholas/miniconda3/bin/python""
build --define with_jemalloc=true
build:gcp --define with_gcp_support=true
build:hdfs --define with_hdfs_support=true
build:s3 --define with_s3_support=true
build:xla --define with_xla_support=true
build:gdr --define with_gdr_support=true
build:verbs --define with_verbs_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env CUDA_TOOLKIT_PATH=""/usr""
build --action_env TF_CUDA_VERSION=""8.0""
build --action_env CUDNN_INSTALL_PATH=""/usr""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env NVVMIR_LIBRARY_DIR=""/usr/share/cuda""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --action_env TF_CUDA_CLANG=""0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc53""
build --config=cuda
test --config=cuda
build --define grpc_no_ares=true
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
build --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
build:mkl --define using_mkl=true
build:mkl -c opt
build:monolithic --define framework_shared_object=false
build --define framework_shared_object=true
build:android --crosstool_top=//external:android/crosstool
build:android --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
build:android_arm --config=android
build:android_arm --cpu=armeabi-v7a
build:android_arm64 --config=android
build:android_arm64 --cpu=arm64-v8a
````",0,,5,2017-12-24T13:05:18Z,2018-01-23T19:21:34Z,NONE,2018-01-23T18:49:39Z
15612,Optimize FusedBatchNormGrad.,"awaiting review,cla: yes,stat:awaiting response","Reuse the output buffer and allocate only one temporary tensor, when data format is NHWC and
GPU is used. This is based on the observation that cudnn can perform the backward computation
in place. The same idea is used in PR #15601 . 

This lowers GPU memory consumption and may improve performance, because fewer distinct memory addresses are accessed. It also permits a higher batch size.

I've also added a new test for the gradient computation.",0,,7,2017-12-24T10:38:20Z,2018-01-13T02:08:45Z,CONTRIBUTOR,2017-12-25T02:34:44Z
15611,'saved_model_cli.py' bug fix!,type:bug/performance,"In file `python/tools/saved_model_cli.py`  at function `def _print_tensor_info(tensor_info):`
The first line should be:

 `  print('    dtype: ' + {value:key for (key,value) in types_pb2.DataType.items()}[tensor_info.dtype])`

Not be : ` print('    dtype: ' + types_pb2.DataType.keyss()[tensor_info.dtype])`

because `tensor_info.dtype`  is an Integer which is the value of types(not the index of type values).",1,,7,2017-12-24T09:33:59Z,2017-12-31T06:33:44Z,NONE,2017-12-24T15:34:24Z
15610,Cannot load pretrained models in Android via jcenter-provided library,,"It's a great step to support importing Android lib via jcenter. However, I'm having trouble in loading pretrained models into `TensorFlowInferenceInterface`, which says `NodeDef mentions attr 'dilations' not in Op...`. I think it's a compatibility issue, meaning that the model graph is not consistent with the graph interpreter. The models directly downloaded from `slim` and frozen by myself. Anyone can help me?",0,,1,2017-12-24T08:35:28Z,2017-12-30T10:06:50Z,NONE,2017-12-30T10:06:50Z
15609,The relationship between neural network depth and accuracy,,"Hi:
I am learning to design a simple neural network, and try to identify 28x28 pixel greyscale images in the MNIST dataset.
I find that the neural network depth is not directly proportional to the recognition rate,

one layer neural network recognition rate: 92%
two layer neural network recognition rate: 94%
four layer neural network recognition rate: 91.8%

Can someone help me analyze it?
![6](https://user-images.githubusercontent.com/31270354/34325172-85768d48-e8c5-11e7-831a-6d11b9b02bf5.PNG)
(Please click on the picture to see the complete picture information)

**thank you very much!**",0,,1,2017-12-24T08:15:10Z,2017-12-28T19:08:04Z,NONE,2017-12-28T19:08:04Z
15607,"Fix `tf.pow(x, y)` edge case with integer x and negative integer y","awaiting testing (then merge),cla: yes,kokoro:run","This fix tries to address the issue raised in #12156 and #9560 (and PR #11852) where pow(x, y) hangs with an integer x and a negative value of y.

This fix tries to throw out an error like numpy in this case:
```
>>> np.power([5, 5], [2, -2])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: Integers to negative integer powers are not allowed.
```

This fix adds error to the C++ functor like safe div/mod so that and InvalidArgument error could be triggered if any one of the values of y is negative.

NOTE: Similar to safe_div/mod this fix also does not cover GPU (not working yet).

This fix fix #12156. This fix is also related to #9560.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,5,2017-12-24T00:17:32Z,2018-01-23T18:00:19Z,MEMBER,2017-12-25T02:31:55Z
15605,Make `frame` positions configurable,"API review,cla: yes","This also fixes a bug in `get_root_dir_with_all_resources`.

The value of `sys._getframe(1)` is caller-sensitive.
This leads to unexpected bugs where it gives the path of the wrong file.

For example, `get_root_dir_with_all_resources` is supposed to find the `runfiles` directoy.
But because of the extra call to `get_data_files_path`, everything shifts by one and now it resolves all paths on the location of the `resource_loader.py` file loaded (repository or PIP, but not a `runfiles` path). In the places it's used currently, it doesn't break anything, tho.

This PR fixes the behavior of `get_root_dir_with_all_resources` and makes the `frame` positions configurable on all methods (with a default for compatibility).

@martinwicke Friendly ping.",0,,4,2017-12-23T15:55:25Z,2018-01-03T21:29:00Z,CONTRIBUTOR,2017-12-27T21:17:37Z
15604,ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory,,"I installed tf-nightly build and I get the following error on import of tensorflow.
`ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory`.

If I check for cuda 9, I get the following:
```
ldconfig -v
/usr/local/cuda-8.0/targets/x86_64-linux/lib:
	libnvgraph.so.8.0 -> libnvgraph.so.8.0.61
	libnppicom.so.8.0 -> libnppicom.so.8.0.61
	libnppial.so.8.0 -> libnppial.so.8.0.61
	libcufftw.so.8.0 -> libcufftw.so.8.0.61
	libcufft.so.8.0 -> libcufft.so.8.0.61
	libnppif.so.8.0 -> libnppif.so.8.0.61
	libcublas.so.8.0 -> libcublas.so.8.0.88
	libnvblas.so.8.0 -> libnvblas.so.8.0.88
	libnppi.so.8.0 -> libnppi.so.8.0.61
	libcusolver.so.8.0 -> libcusolver.so.8.0.61
	libnppidei.so.8.0 -> libnppidei.so.8.0.61
	libnvrtc-builtins.so.8.0 -> libnvrtc-builtins.so.8.0.61
	libnvrtc.so.8.0 -> libnvrtc.so.8.0.61
	libnpps.so.8.0 -> libnpps.so.8.0.61
	libcuinj64.so.8.0 -> libcuinj64.so.8.0.61
	libnppig.so.8.0 -> libnppig.so.8.0.61
	libOpenCL.so.1 -> libOpenCL.so.1.0.0
	libnppicc.so.8.0 -> libnppicc.so.8.0.61
	libnppist.so.8.0 -> libnppist.so.8.0.61
	libnppisu.so.8.0 -> libnppisu.so.8.0.61
	libnppim.so.8.0 -> libnppim.so.8.0.61
	libcurand.so.8.0 -> libcurand.so.8.0.61
	libcudart.so.8.0 -> libcudart.so.8.0.61
	libnvToolsExt.so.1 -> libnvToolsExt.so.1.0.0
	libnppitc.so.8.0 -> libnppitc.so.8.0.61
	libnppc.so.8.0 -> libnppc.so.8.0.61
	libcusparse.so.8.0 -> libcusparse.so.8.0.61
/usr/local/cuda-9.1/targets/x86_64-linux/lib:
	libnppicc.so.9.1 -> libnppicc.so.9.1.85
	libnppisu.so.9.1 -> libnppisu.so.9.1.85
	libcufftw.so.9.1 -> libcufftw.so.9.1.85
	libcufft.so.9.1 -> libcufft.so.9.1.85
	libnppial.so.9.1 -> libnppial.so.9.1.85
	libnppist.so.9.1 -> libnppist.so.9.1.85
	libcublas.so.9.1 -> libcublas.so.9.1.85
	libnvblas.so.9.1 -> libnvblas.so.9.1.85
	libnppitc.so.9.1 -> libnppitc.so.9.1.85
	libcusolver.so.9.1 -> libcusolver.so.9.1.85
	libnvrtc.so.9.1 -> libnvrtc.so.9.1.85
	libnvrtc-builtins.so.9.1 -> libnvrtc-builtins.so.9.1.85
	libnppidei.so.9.1 -> libnppidei.so.9.1.85
	libOpenCL.so.1 -> libOpenCL.so.1.0.0
	libnppig.so.9.1 -> libnppig.so.9.1.85
	libnppc.so.9.1 -> libnppc.so.9.1.85
	libcudart.so.9.1 -> libcudart.so.9.1.85
	libnvToolsExt.so.1 -> libnvToolsExt.so.1.0.0
	libnvgraph.so.9.1 -> libnvgraph.so.9.1.85
	libnppif.so.9.1 -> libnppif.so.9.1.85
	libcusparse.so.9.1 -> libcusparse.so.9.1.85
	libaccinj64.so.9.1 -> libaccinj64.so.9.1.85
	libcuinj64.so.9.1 -> libcuinj64.so.9.1.85
	libnppim.so.9.1 -> libnppim.so.9.1.85
	libnppicom.so.9.1 -> libnppicom.so.9.1.85
	libnpps.so.9.1 -> libnpps.so.9.1.85
	libcurand.so.9.1 -> libcurand.so.9.1.85
```
I that due to a name mismatch. `libcublas.so.9.0 =! libcublas.so.9.1`? And if so how can we overcome this?",0,,18,2017-12-23T13:58:25Z,2017-12-29T22:12:20Z,NONE,2017-12-29T22:12:20Z
15603,[CMake] Remove invalid python modules,"awaiting testing (then merge),cla: yes","Split from #15368
@mrry Friendly ping.",0,,12,2017-12-23T13:43:51Z,2017-12-28T19:15:24Z,CONTRIBUTOR,2017-12-26T18:46:54Z
15602,[CMake] Test existence of python entries,"awaiting testing (then merge),cla: yes","Split from #15166
@mrry Friendly ping,",0,,9,2017-12-23T13:34:36Z,2018-01-01T00:04:12Z,CONTRIBUTOR,2017-12-29T00:55:23Z
15601,Optimize FusedBatchNorm (and fix a bug).,"awaiting review,cla: yes","I discovered experimentally that `cudnn` computations can be performed in place. Therefore there is no need to allocate two temporary tensors in `FusedBatchNorm` for GPU and data format NHWC. One is enough. This lowers memory consumption, and hence increases the maximum possible batch size.

This might seem risky (because NVIDIA doesn't mention the property), but in fact the current implementation already uses it: by doing forward_input_or_allocate_output in `FusedBatchNormOp`.
If data format is NCHW, and the input is forwarded, then `cudnn` would be forced
to do the computation in place (see line 247). This is how I discovered that the whole approach works:
I was trying to see if forwarding the input is a bug or not.

I added several tests to ensure that the change is correct.

While doing this, I discovered that ops_testutil does not properly synchronize at the end.
The reason seems to be the call `context_->eigen_gpu_device().synchronize()`. Somehow
it does nothing. I think the problem is that Eigen is not compiled with the flag `EIGEN_CUDACC`.
So I changed it to `GPUUtil::Sync(device_.get())`.",0,,5,2017-12-23T13:22:22Z,2018-01-13T23:45:21Z,CONTRIBUTOR,2017-12-25T02:24:08Z
15600,Exclude tf_stream_executor.cmake for CPU only,"awaiting testing (then merge),cla: yes","Bug introduced in #15099
Fixes #3996",0,,3,2017-12-23T12:46:56Z,2017-12-28T01:58:24Z,CONTRIBUTOR,2017-12-23T12:50:09Z
15599,Freeze pb model will not remove is_training flag of bn attr and it is moreover still set as true,stat:awaiting response,"Using /tensorflow/python/framework/graph_util_impl.py # **convert_variables_to_constants** to freeze graph to pb model will not remove **is_training** flag of **batch_normalization** as well as **fused batch_normalization** ops (tf.layers.batch_normalziation) in the inference graph.  Moreover, the **is_training** flag is still set as true. This issue will not cause any errors to use the pb model on Android. However, it will cause converting errors to tflite model by toco. Is it possible to optimize this conversion tool to resolve the problem in further versions. 

_Following is the node output of the pb model during loading:
name: ""convolution_layer/block_layer1/batch_normalization/FusedBatchNorm""
op: ""FusedBatchNorm""
input: ""convolution_layer/block_layer1/conv2d/Conv2D""
input: ""batch_normalization/gamma/read""
input: ""batch_normalization/beta/read""
input: ""convolution_layer/block_layer1/batch_normalization/Const""
input: ""convolution_layer/block_layer1/batch_normalization/Const_1""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""data_format""
  value {
    s: ""NHWC""
  }
}
attr {
  key: ""epsilon""
  value {
    f: 0.0010000000475
  }
}
**attr {
  key: ""is_training""
  value {
    b: true
  }**
}_",0,,4,2017-12-23T08:54:29Z,2017-12-28T19:06:42Z,NONE,2017-12-25T02:21:27Z
15598,R1.4,cla: no,,0,,3,2017-12-23T07:29:24Z,2017-12-25T02:18:33Z,NONE,2017-12-25T02:18:33Z
15597,Adding installation dependencies for python3.6 based on failures from,cla: yes, the release job.,0,,2,2017-12-23T00:39:58Z,2018-01-05T20:39:06Z,MEMBER,2017-12-23T00:46:17Z
15596,Update tf-learn reference to TF Estimators,cla: yes,,0,,1,2017-12-22T21:57:47Z,2017-12-23T22:47:10Z,MEMBER,2017-12-23T22:47:17Z
15594,MKL: Adding MKL-DNN support for LRN op,"awaiting testing (then merge),cla: yes",,0,,2,2017-12-22T21:06:09Z,2017-12-24T01:12:06Z,CONTRIBUTOR,2017-12-22T22:12:11Z
15593,Branch 179953488,cla: yes,push,0,,7,2017-12-22T21:05:48Z,2017-12-22T23:55:38Z,CONTRIBUTOR,2017-12-22T21:06:01Z
15592,Refactor methods for path calculation,cla: yes,"@jart I really appreciate your advice. I've rewritten #15315 to be more ""boring"":
* `get_grandparent(path, degree)` -> a files grandparent of the given degree
* `get_abs_data_path(path, depth, frame=0)` -> path relative to a file upwards the callstack
* `get_data_files_path(frame=0)` -> no change in behavior, but now shared with other methods

They should be about boring enough to be useful and to be shared.

* This should resolve the issues with `get_data_files_path` by making it less brittle...
* And I think `get_abs_data_path` is how `get_path_to_datafile` should really look like?

Further thoughts on this are appreciated!",0,,4,2017-12-22T19:48:27Z,2017-12-22T20:00:32Z,CONTRIBUTOR,2017-12-22T20:00:32Z
15589,Segmentation fault on get_session_handle after 1.4.1 upgrade,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 (from 1.4.1 pip install)
- **Python version**: 2.7.6
- **Bazel version (if compiling from source)**: N/a
- **GCC/Compiler version (if compiling from source)**: N/a
- **CUDA/cuDNN version**: N/a
- **GPU model and memory**: N/a
- **Exact command to reproduce**: 

After upgrading from 1.3 to 1.4.1, running the following code produces a segmentation fault:

```
import tensorflow as tf

with tf.Session() as S: S.run( tf.get_session_handle(tf.constant(1, dtype=tf.float32)) )
```


### Source code / logs

GDB stack traces:

```
(gdb) py-bt 
0x00007fffb765b307 in nsync::nsync_mu_lock(nsync::nsync_mu_s_*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
(gdb) py-bt
#22 Frame 0xfea2d0, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 1302, in _run_fn (session=<SwigPyObject at remote 0x7fff6e664330>, feed_dict={}, fetch_list=['GetSessionHandle:0'], target_list=[], options=None, run_metadata=None, status=<SwigPyObject at remote 0x7fff6e670f00>)
    status, run_metadata)
#27 Frame 0xff8a00, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 1323, in _do_call (self=<Session(_config=None, _graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={}, _name_stack='', _gradient_override_map={}, _seed=None, _handle_movers={}, _op_to_kernel_label_map={}, _nodes_by_id={1: <Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fffeb2f2150>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=1) at remote 0x7ffff377d950>, _consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fff6e612810>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=7) at remote 0x7ffff377db10>, _consumers=[], _id=1L) at remote 0x7fff6e612750>], _control_flow_context=None, _id_value=2, _original_op=None, _traceback=[('minimal_core_dump.py', 3, '<module>', {'__builtins__': <module at remote 0x7ffff7f9ab08>, '__fi...(truncated)
    return fn(*args)
#31 Frame 0xfe2fe0, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 1317, in _do_run (self=<Session(_config=None, _graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={}, _name_stack='', _gradient_override_map={}, _seed=None, _handle_movers={}, _op_to_kernel_label_map={}, _nodes_by_id={1: <Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fffeb2f2150>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=1) at remote 0x7ffff377d950>, _consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fff6e612810>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=7) at remote 0x7ffff377db10>, _consumers=[], _id=1L) at remote 0x7fff6e612750>], _control_flow_context=None, _id_value=2, _original_op=None, _traceback=[('minimal_core_dump.py', 3, '<module>', {'__builtins__': <module at remote 0x7ffff7f9ab08>, '__fil...(truncated)
    options, run_metadata)
#35 Frame 0xf12bc0, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 1120, in _run (self=<Session(_config=None, _graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={}, _name_stack='', _gradient_override_map={}, _seed=None, _handle_movers={}, _op_to_kernel_label_map={}, _nodes_by_id={1: <Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fffeb2f2150>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=1) at remote 0x7ffff377d950>, _consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fff6e612810>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=7) at remote 0x7ffff377db10>, _consumers=[], _id=1L) at remote 0x7fff6e612750>], _control_flow_context=None, _id_value=2, _original_op=None, _traceback=[('minimal_core_dump.py', 3, '<module>', {'__builtins__': <module at remote 0x7ffff7f9ab08>, '__file__...(truncated)
    feed_dict_tensor, options, run_metadata)
#39 Frame 0xfa91e0, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 889, in run (self=<Session(_config=None, _graph=<Graph(_default_original_op=None, _handle_feeders={}, _collections={}, _name_stack='', _gradient_override_map={}, _seed=None, _handle_movers={}, _op_to_kernel_label_map={}, _nodes_by_id={1: <Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fffeb2f2150>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=1) at remote 0x7ffff377d950>, _consumers=[<Operation(_graph=<...>, _control_inputs=[], _outputs=[<Tensor(_op=<...>, _shape=<TensorShape(_dims=[]) at remote 0x7fff6e612810>, _handle_data=None, _value_index=0, _dtype=<DType(_type_enum=7) at remote 0x7ffff377db10>, _consumers=[], _id=1L) at remote 0x7fff6e612750>], _control_flow_context=None, _id_value=2, _original_op=None, _traceback=[('minimal_core_dump.py', 3, '<module>', {'__builtins__': <module at remote 0x7ffff7f9ab08>, '__file__':...(truncated)
    run_metadata_ptr)
#43 Frame 0x7ffff7f4da00, for file minimal_core_dump.py, line 3, in <module> ()
    with tf.Session() as S: S.run( tf.get_session_handle(tf.constant(1, dtype=tf.float32)) )
```


```
(gdb) bt  
#0  0x00007fffb765b307 in nsync::nsync_mu_lock(nsync::nsync_mu_s_*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#1  0x00007fffb494de1f in tensorflow::SessionState::GetNewId() () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/../libtensorflow_framework.so
#2  0x00007fffb636bd24 in tensorflow::GetSessionHandleOp::Compute(tensorflow::OpKernelContext*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007fffb7852729 in tensorflow::grappler::ConstantFolding::EvaluateNode(tensorflow::NodeDef const&, tensorflow::gtl::InlinedVector<tensorflow::TensorValue, 4> const&, tensorflow::gtl::InlinedVector<tensorflow::TensorValue, 4>*) const () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007fffb7858a03 in tensorflow::grappler::ConstantFolding::EvaluateOneFoldable(tensorflow::NodeDef const&, std::vector<tensorflow::NodeDef, std::allocator<tensorflow::NodeDef> >*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007fffb7859486 in tensorflow::grappler::ConstantFolding::FoldNode(tensorflow::NodeDef*, tensorflow::GraphDef*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007fffb785ab03 in tensorflow::grappler::ConstantFolding::FoldGraph(tensorflow::GraphDef*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007fffb785b526 in tensorflow::grappler::ConstantFolding::RunOptimizationPass(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007fffb785b9c9 in tensorflow::grappler::ConstantFolding::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007fffb7843657 in tensorflow::grappler::MetaOptimizer::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007fffb7844925 in tensorflow::grappler::RunMetaOptimizer(tensorflow::grappler::GrapplerItem const&, tensorflow::RewriterConfig const&, tensorflow::DeviceBase*, tensorflow::grappler::Cluster*, tensorflow::GraphDef*)
    () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007fffb782f316 in tensorflow::GraphExecutionState::OptimizeGraph(tensorflow::BuildGraphOptions const&, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#12 0x00007fffb782fe80 in tensorflow::GraphExecutionState::BuildGraph(tensorflow::BuildGraphOptions const&, std::unique_ptr<tensorflow::ClientGraph, std::default_delete<tensorflow::ClientGraph> >*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#13 0x00007fffb76678ae in tensorflow::DirectSession::CreateGraphs(tensorflow::BuildGraphOptions const&, std::unordered_map<std::string, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> > > > >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*, tensorflow::DirectSession::RunStateArgs*, tensorflow::gtl::InlinedVector<tensorflow::DataType, 4>*, tensorflow::gtl::InlinedVector<tensorflow::DataType, 4>*)
    () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#14 0x00007fffb7669daa in tensorflow::DirectSession::GetOrCreateExecutors(tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#15 0x00007fffb766b5bb in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#16 0x00007fffb5d5e0fa in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#17 0x00007fffb5d5e434 in TF_Run () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#18 0x00007fffb5a7c9da in tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#19 0x00007fffb5a7cdd1 in tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#20 0x00007fffb5a410b1 in _wrap_TF_Run () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#21 0x00000000004c7f54 in call_function (oparg=<optimized out>, pp_stack=0x7fffffffd1a0) at ../Python/ceval.c:4020
#22 PyEval_EvalFrameEx (
    f=f@entry=Frame 0xfea2d0, for file /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py, line 1302, in _run_fn (session=<SwigPyObject at remote 0x7fff6e664330>, feed_dict={}, fetch_list=['GetSessionHandle:0'], target_list=[], options=None, run_metadata=None, status=<SwigPyObject at remote 0x7fff6e670f00>), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666
#23 0x00000000004704ea in PyEval_EvalCodeEx (closure=<optimized out>, defcount=<optimized out>, defs=0x0, kwcount=<optimized out>, kws=<optimized out>, argcount=<optimized out>, args=<optimized out>, locals=0x0,
    globals=<optimized out>, co=<optimized out>) at ../Python/ceval.c:3252
#24 function_call.15337 (func=<optimized out>, arg=<optimized out>, kw=<optimized out>) at ../Objects/funcobject.c:526
#25 0x00000000004c9aa5 in PyObject_Call (kw=0x0, arg=(<SwigPyObject at remote 0x7fff6e664330>, {}, ['GetSessionHandle:0'], [], None, None), func=<function at remote 0x7fff6e610d70>) at ../Objects/abstract.c:2529
#26 ext_do_call (nk=<optimized out>, na=<optimized out>, flags=<optimized out>, pp_stack=0x7fffffffd3e0, func=<function at remote 0x7fff6e610d70>) at ../Python/ceval.c:4333
... # partial output
```
",0,,6,2017-12-22T17:50:43Z,2018-01-05T14:05:19Z,NONE,2017-12-22T22:23:01Z
15587,lite model file size,type:support,"how to reduce my model size(my own trained tensorflow model )
i want to use tensorflow lite converter get a smaller tflite file, but the tflite file has the same size with my trained model,
-------------------------------------------------------------------------

bazel-bin/tensorflow/contrib/lite/toco/toco \
  --allow_custom_ops \
  --input_file=/data/log/frozen_model.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \
  --output_file=/data/log//mobilenet.tflite --inference_type=FLOAT \
  --input_data_types=FLOAT --input_arrays=input \
  --output_arrays=output --input_shapes=1,224,224,3

--------------------------------------------------------------------------",0,,1,2017-12-22T13:22:38Z,2017-12-22T22:30:39Z,NONE,2017-12-22T22:30:39Z
15583,tensorflow logging glog redefined warning,stat:awaiting response,"I just integrated a tensorflow module to my existing c++ project. I was already using glog for logging in my existing project, logging to files.

Now when I build the new project it throws warnings like:
`/usr/local/include/google/tensorflow/tensorflow/core/platform/default/logging.h:254:0: warning: ""CHECK_GT"" redefined
#define CHECK_GT(val1, val2) CHECK_OP(Check_GT, >, val1, val2)
/usr/local/include/glog/logging.h:793:0: note: this is the location of the previous definition
#define CHECK_GT(val1, val2) CHECK_OP(_GT, > , val1, val2)`

Is this behavior safe? I am not concerned that much with the logs that tensorflow generates, I need my existing modules to continue logging to files.

If this behavior is unsafe, this could be a good feature request to have glog logging not affected by tensorflow logging.",0,,3,2017-12-22T11:19:13Z,2018-01-06T20:11:15Z,NONE,2017-12-22T19:00:14Z
15581,TfLiteCameraDemo only contains 32-bit libtensorflowlite_jni.so,"comp:lite,stat:awaiting tensorflower","I strictly followed the steps on https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/ to
build the TfLiteCameraDemo. But I found only 32-bit libtensorflowlite_jni.so was contained in the final APK.

I modified the source code of libneuralnetworks.so to test my employer's NN accelerator, which needed the 64-bit libtensorflowlite_jni.so.

even I configed bazel with
./configure --config=android_arm64

and built with 
bazel build --cxxopt=--std=c++11 --cpu=arm64-v8a //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo

could not work.

Where was my mistake? Or how can I pack the 64-bit  libtensorflowlite_jni.so into the final APK?
",1,,7,2017-12-22T09:47:47Z,2018-01-05T22:59:43Z,NONE,2017-12-28T19:03:09Z
15579,[Bazel/MSVC] Fix build error since aae439c,"awaiting testing (then merge),cla: yes",#15213,0,,5,2017-12-22T09:29:53Z,2017-12-26T19:24:16Z,CONTRIBUTOR,2017-12-22T10:51:30Z
15578,Portability fix for StrAppend,"cla: yes,stat:awaiting response",#15557,0,,6,2017-12-22T08:22:38Z,2017-12-29T01:52:18Z,CONTRIBUTOR,2017-12-25T02:12:33Z
15577,update README.md,"awaiting review,cla: yes",Added whl file downloading links of Python 3.6 for Linux with CPU only and GPU both :),0,,6,2017-12-22T08:13:44Z,2017-12-27T21:03:56Z,CONTRIBUTOR,2017-12-23T01:32:36Z
15575,TensorFlow automatically modify variable scope name,,"check the following code:

```python
with tf.variable_scope('test'):
      v1 = tf.placeholder(tf.float32, shape=(10,10), name='v1')

with tf.variable_scope('test'):
      v2 = tf.placeholder(tf.float32, shape=(5, 5), name='v2')

print(v1)
print(v2)
```


the code output:

 ```python
Tensor(""test/v1:0"", shape=(10, 10), dtype=float32)
Tensor(""test_1/v2:0"", shape=(5, 5), dtype=float32)
```

It looks ridiculous for TF modfiy variable scope name 'test' into 'test_1' while my new placeholder named 'v2' is different from 'v1'. However, if i add variables, everything is going to be normal. code like:

```python
    with tf.variable_scope('test'):
        v1 = tf.placeholder(tf.float32, shape=(10,10), name='v1')
        w1 = tf.get_variable('w1', shape=(2,2))

    with tf.variable_scope('test'):
        v2 = tf.placeholder(tf.float32, shape=(5, 5), name='v2')
        w2 = tf.get_variable('w2', shape=(3, 3))
    print(v1)
    print(v2)
    print(w1)
    print(w2)
```
outputs:
```python
Tensor(""test/v1:0"", shape=(10, 10), dtype=float32)
Tensor(""test_1/v2:0"", shape=(5, 5), dtype=float32)
<tf.Variable 'test/w1:0' shape=(2, 2) dtype=float32_ref>
<tf.Variable 'test/w2:0' shape=(3, 3) dtype=float32_ref>
```

I dont know why TF **just modify the variable scope name** while i try to add a new placeholder into the existed scope.Is it a BUG?
",0,,2,2017-12-22T07:08:33Z,2017-12-22T07:39:35Z,NONE,2017-12-22T07:31:38Z
15574,Fix adding elements to collections.deque.,"awaiting testing (then merge),cla: yes","In some cases, [word2vec_basic.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L123) will fail at L123 and throw the following error:
```
TypeError: sequence index must be integer, not 'slice'
```
It seams the ```text8``` dataset and parameters will not touch that line code, but I hit this issue when I run this script against my own dataset. This is caused by Python ```collections.deque``` doesn't support index by slice:
```
>>> from collections import deque
>>> d = deque('tensorflow')
>>> d[:]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: sequence index must be integer, not 'slice'
```
This PR fix this issue by using ```deque.extend```, and this is consistent with L114.",0,,5,2017-12-22T07:07:18Z,2017-12-26T19:23:54Z,CONTRIBUTOR,2017-12-22T07:16:30Z
15573,'tensorflow/contrib/reduce_slice_ops/_objs/python/ops/_reduce_slice_ops_gpu/tensorflow/contrib/reduce_slice_ops/kernels/reduce_slice_ops_gpu.cu.pic.o' was not created,,"Hi i'm running the tensorflow from the source and after building the tensorflow using bazel using below command 

**bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package** 

**The error message pasted below**

10 errors detected in the compilation of ""/home/lb/.cache/bazel/_bazel_lb/144f5944ef8ff562c57e67fdaa41565f/execroot/org_tensorflow/tmpae8_5f0170552fd082f4/tmpxft_0000245c_00000000-6_reduce_slice_ops_gpu.cu.cpp1.ii"".
ERROR: /home/lb/WorkSpace/AI-WORK/tensorflow/tensorflow/contrib/reduce_slice_ops/BUILD:14:1: output 'tensorflow/contrib/reduce_slice_ops/_objs/python/ops/_reduce_slice_ops_gpu/tensorflow/contrib/reduce_slice_ops/kernels/reduce_slice_ops_gpu.cu.pic.o' was not created
ERROR: /home/lb/WorkSpace/AI-WORK/tensorflow/tensorflow/contrib/reduce_slice_ops/BUILD:14:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 74.030s, Critical Path: 8.38s
FAILED: Build did NOT complete successfully
",0,,16,2017-12-22T06:40:28Z,2017-12-22T20:05:30Z,NONE,2017-12-22T20:05:30Z
15571,why keras run slower and slower,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,1,2017-12-22T03:35:22Z,2017-12-22T18:58:36Z,NONE,2017-12-22T18:58:36Z
15570,Using grid_rnn_cell.Grid1LSTMCell makes stack_bidirectional_dynamic_rnn Throw different shape errors,,"I encountered a problem using grid_rnn_cell.Grid1LSTMCell. I have no problems using it on contrib static_rnn and contrib static_bidirectional_rnn.

```
class GridRNNTest(tf.test.TestCase):
    def setUp(self):
        self.num_features = 1
        self.time_steps = 1
        self.batch_size = 1
        tf.reset_default_graph()
        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])
        self.cell = grid_rnn_cell.Grid1LSTMCell(num_units=8)

    def test_simple_grid_rnn(self):
        self.input_layer = tf.unstack(self.input_layer, self.time_steps, 1)
        rnn.static_rnn(self.cell, self.input_layer, dtype=tf.float32)

class BidirectionalGridRNNTest(tf.test.TestCase):
    def setUp(self):
        self.num_features = 1
        self.time_steps = 1
        self.batch_size = 1
        tf.reset_default_graph()
        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])
        self.cell_fw = grid_rnn_cell.Grid1LSTMCell(num_units=8)
        self.cell_bw = grid_rnn_cell.Grid1LSTMCell(num_units=8)

    def test_simple_bidirectional_grid_rnn(self):
        self.input_layer = tf.unstack(self.input_layer, self.time_steps, 1)
        rnn.static_bidirectional_rnn(self.cell_fw, self.cell_bw, self.input_layer, dtype=tf.float32)
```

However, when I test it on contrib stack_bidirectional_dynamic_rnn:

```
class StackBidirectionalGridRNNTest(tf.test.TestCase):
    def setUp(self):
        self.num_features = 1
        self.time_steps = 1
        self.batch_size = 1
        tf.reset_default_graph()
        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])
        self.cells_fw = [grid_rnn_cell.Grid1LSTMCell(num_units=8) for _ in range(2)]
        self.cells_bw = [grid_rnn_cell.Grid1LSTMCell(num_units=8) for _ in range(2)]

    def test_stack_bidirectional_grid_rnn(self):
        self.input_layer = tf.unstack(self.input_layer, self.time_steps, 1)
        rnn.stack_bidirectional_dynamic_rnn(self.cells_fw, self.cells_fw, self.input_layer, dtype=tf.float32)
```

I encounter these shape errors:

(1) When the input_layer is unstacked: `Shape (1, 1) must have rank at least 3`
(2) When the input_layer is not unstacked: Shape `(1, 2, 8) must have rank 2`

Which are, apparently,  conflicting with each other.",1,,7,2017-12-22T03:34:15Z,2018-01-04T16:28:23Z,CONTRIBUTOR,2017-12-22T03:40:54Z
15569,Dataset shuffle operation is not deterministic,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:elementary OS 0.4.1 Loki
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: Not related
- **GPU model and memory**: Not related 
- **Exact command to reproduce**: mentioned below


### Describe the problem
Dataset Shuffle operation is not determenastic 

### Source code / logs
```python
import numpy as np
import tensorflow as tf

def test():
  np.random.seed(42)
  tf.set_random_seed(42)
  numbers = np.arange(1,100)

  def get_data():
    dataset = tf.data.Dataset.from_tensor_slices(numbers)  # some initial dataset
    dataset =  dataset.shuffle(100)
    x = dataset.make_one_shot_iterator().get_next()
    return x

  # execution 1
  x = get_data()
  with tf.Session() as sess:
    x_batch1 = sess.run(x)
    print(x_batch1)
  # clear out everything
  tf.reset_default_graph()

  # execution 2
  x = get_data()
  with tf.Session() as sess:
    x_batch2 = sess.run(x)
    print(x_batch2)
  # results should be equivalent
  assert np.allclose(x_batch1, x_batch2)
test()
```
test code sample taken from @dusenberrymw",0,,2,2017-12-22T01:07:43Z,2017-12-22T15:12:54Z,NONE,2017-12-22T14:29:46Z
15568,InvalidArgumentError: slice index 0 of dimension 0 out of bounds.,,"Hi,

### Describe the problem
I am trying to encode and decode an image using tf.image.decode_jpeg and b64 encoding. The code is so simple but it seems I have an error regarding StrideSlice. I am not sure if the problem coming from the undefined shape of input placeholder or related to jpeg_decode.  Thanks. 

### Source code / logs
```
import tensorflow as tf
import base64
import functools


def build_graph(input_len=None):
    graph = tf.Graph()
    with graph.as_default():
        image_bytes = tf.placeholder(tf.string, name='input_node')
        images = tf.map_fn(
            functools.partial(tf.image.decode_jpeg, channels=3),
            image_bytes,
            dtype=tf.uint8
        )
        image_floats = tf.cast(images, tf.float32, name=""output_node"") / 255.0

    return graph


if __name__ == '__main__':
    file_name = ""test_1.jpg""
    with open(file_name, ""rb"") as imageFile:
        image_string = imageFile.read()
        image_encode = base64.b64encode(image_string).decode(""utf-8"")

    graph = build_graph(input_len=len(image_encode))

    with tf.Session(graph=graph) as session:
        init_op = tf.group(tf.global_variables_initializer(),
                           tf.local_variables_initializer())
        session.run(init_op)
        input_node = graph.get_tensor_by_name(""input_node:0"")
        output_node = graph.get_tensor_by_name(""output_node:0"")
        image_out = session.run(output_node, feed_dict={input_node: image_encode})
```

These is the traceback:

```
Caused by op u'map/TensorArrayUnstack/strided_slice', defined at:
  File ""/.../Simple_Graph_Design/ImageTest.py"", line 26, in <module>
    graph = build_graph(input_len=len(image_encode))
  File ""/.../Simple_Graph_Design/ImageTest.py"", line 13, in build_graph
    dtype=tf.uint8
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/functional_ops.py"", line 354, in map_fn
    elem_ta.unstack(elem) for elem_ta, elem in zip(elems_ta, elems_flat)]
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py"", line 107, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_ops.py"", line 412, in unstack
    num_elements = array_ops.shape(value)[0]
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 538, in _SliceHelper
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 706, in strided_slice
    shrink_axis_mask=shrink_axis_mask)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 5430, in strided_slice
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): slice index 0 of dimension 0 out of bounds.
	 [[Node: map/TensorArrayUnstack/strided_slice = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](map/TensorArrayUnstack/Shape, map/TensorArrayUnstack/strided_slice/stack, map/TensorArrayUnstack/strided_slice/stack_1, map/TensorArrayUnstack/strided_slice/stack_1)]]

```
### System information
- TF version: 1.4.0
- Linux Ubuntu 16.04
- Python version: 2.7
- No CUDA or GPU 




",0,,6,2017-12-21T23:52:52Z,2018-01-31T02:18:22Z,NONE,2017-12-22T07:26:22Z
15567,Recognize more environments as interactive,"awaiting review,cla: yes,kokoro:run","... and log to stdout in those cases, and show INFO messages by default.

Fixes #6438.",0,,2,2017-12-21T23:15:02Z,2018-01-10T02:07:26Z,OWNER,2017-12-25T02:04:46Z
15566,"MKL: Reverting PR #14478, which breaks Inception v3 with MKL.","awaiting testing (then merge),cla: yes",PR #14478 https://github.com/tensorflow/tensorflow/pull/14478 (based on commit e7b69e4) removed line 276 from  #ifdef INTEL_MKL section and breaks Inception v3 with MKL. Perhaps someone meant to remove line 202?,0,,2,2017-12-21T23:12:36Z,2017-12-26T00:11:05Z,CONTRIBUTOR,2017-12-22T22:11:39Z
15565,Linux GPU build failing (__CUDACC_VER__ is no longer supported),,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 17.10, kernel 4.14.8
- **TensorFlow installed from (source or binary)**:
Source (failed build)
- **TensorFlow version (use command below)**:
master
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
0.9.0
- **GCC/Compiler version (if compiling from source)**:
4.9
- **CUDA/cuDNN version**:
9.0/7.0.4
- **GPU model and memory**:
Two 1080 Ti (11Gb each)
- **Exact command to reproduce**:
bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=cuda //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
I note the build bot is failing too for Linux GPU, since the latest merge in the past 1 hour.

### Source code / logs
ERROR: /home/daniel/build/tensorflow/tensorflow/contrib/image/BUILD:106:1: error while parsing .d file: /home/daniel/.cache/bazel/_bazel_daniel/a40ff47569db15fec953d4e5b5812083/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/contrib/image/_objs/python/ops/_distort_image_ops_gpu/tensorflow/contrib/image/kernels/adjust_hsv_in_yiq_op_gpu.cu.pic.d (No such file or directory)
In file included from /usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/common_functions.h:50:0,
                 from /usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/cuda_runtime.h:115,
                 from <command-line>:0:
/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token """"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead."""" is not valid in preprocessor expressions
 #define __CUDACC_VER__ ""__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.""
                        ^
./tensorflow/core/util/cuda_device_functions.h:37:7: note: in expansion of macro '__CUDACC_VER__'
 #elif __CUDACC_VER__ >= 7050
       ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 237.382s, Critical Path: 22.79s
",0,,3,2017-12-21T21:53:54Z,2017-12-22T20:58:44Z,NONE,2017-12-21T23:28:16Z
15563,Cannot statically link against Tensorflow library in Golang,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.4.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: `go build -a -v -o hellotf --ldflags '-linkmode external -extldflags ""-static -L /usr/local/lib""' -x <GITHUB_PROJ_NAME>/hellotf`

### Describe the problem
I'm trying to compile a **statically-linked** binary of the hello world app against the TF 1.4.0 binary per the [golang install & hello-world instructions](https://www.tensorflow.org/install/install_go) and am not able to successfully link against the TF library as it reports that `ltensorflow` cannot be found by `/usr/bin/ld` in the `go build`.

The command used is:
`go build -a -v -o hellotf --ldflags '-linkmode external -extldflags ""-static -L /usr/local/lib""' -x <GITHUB_PROJ_NAME>/hellotf`

I've tried linking against `/usr/local/lib` (where libtensorflow lives) using `-extldflags`, CGO_LDFLAGS, LDFLAGS etc, done `sudo ldconfig` and the env setup with LD_LIBRARY_PATH and LIBRARY_PATH, but cannot move past this step and always get the error status:

```
/.gvm/gos/go1.8.3/pkg/tool/linux_amd64/link: running gcc failed: exit status 1
/usr/bin/ld: cannot find -ltensorflow
```

The only related issue I've encountered for this is: https://stackoverflow.com/questions/44428816/tensorflow-for-go-demo-example-run-failed, but that didn't help much either.

That being said, I can dynamically link and build the helloworld go code e.g. `go build hello_tf.go` and `go test github.com/tensorflow/tensorflow/tensorflow/go` all work successfully; the issue arises when I try to statically link and cannot link to libtensorflow after it compiles, no matter what settings I try using.

Any help or advice would be greatly appreciated. Thanks!",0,,2,2017-12-21T20:35:00Z,2017-12-24T18:10:46Z,NONE,2017-12-22T07:26:29Z
15561,_Assert3DImage that adds a control dependency for the shape check.,"awaiting review,cla: yes,stat:awaiting response","I noticed that the `_Check3DImage` was always used in exactly the same way and extracted this into its own convenience function. 

The only remaining use of `_Check3DImage` was an import in `gen_image_ops.py` saying
""# TODO(drpng): remove these once internal use has discontinued."", so maybe it could be removed entirely.",0,,3,2017-12-21T19:14:42Z,2018-01-07T12:33:12Z,CONTRIBUTOR,2017-12-29T00:17:55Z
15560,Branch 179822007,cla: yes,,0,,2,2017-12-21T19:02:46Z,2017-12-21T20:37:30Z,MEMBER,2017-12-21T19:27:55Z
15558,ONNX Model Support,,Please add support for the [ONNX](https://github.com/onnx/onnx) open model standard or a conversion tool would be great.,0,,1,2017-12-21T16:51:24Z,2017-12-22T00:13:48Z,NONE,2017-12-22T00:13:48Z
15556,Little error in example how to read data,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 3.5.4
- **Exact command to reproduce**: 

```
python tensorflow/tensorflow/examples/how_tos/reading_data/convert_to_record.py
python tensorflow/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py
```

### Describe the problem

Error when doing the above second command: `AttributeError: module 'tensorflow' has no attribute 'data'` The data attribute is actually in the contrib module then you should change the line 108 of the file `tensorflow/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py`:

`dataset = tf.data.TFRecordDataset(filename)`
to
`dataset = tf.contrib.data.TFRecordDataset(filename)`

Sorry, I don't know if I should have submitted a pull request for such a little change.

",0,,4,2017-12-21T14:31:56Z,2017-12-21T15:17:56Z,NONE,2017-12-21T15:17:56Z
15555,RGB<->YIQ colorspace conversion,"awaiting testing (then merge),cla: yes","Implementation of functions for colorspace conversion RGB <-> YIQ, according to [https://en.wikipedia.org/wiki/YIQ#Formulas](Wikipedia).
This PR adds CPP functions and python wrappers in tf.image namespace",1,,23,2017-12-21T14:11:06Z,2018-01-25T17:02:18Z,CONTRIBUTOR,2017-12-21T18:40:00Z
15553,[XLA] Replace GCC vector extension with portable Intel SIMD,"awaiting testing (then merge),cla: yes","MSVC does not support GCC vector extension, so replace it with Intel SIMD library. Eigen also uses Intel SIMD library to implement vector functions.

MSVC does not have `__SSE4_1__` macro, although it does define `__AVX__` when building with `/arch:AVX`. Since Eigen enables SSE4.1 when `__AVX__` is defined ([`Eigen/Core`](https://bitbucket.org/eigen/eigen/src/034b6c3e101792a3cc3ccabd9bfaddcabe85bb58/Eigen/Core?at=default&fileviewer=file-view-default#Core-156)), we just follow Eigen.

#15213",0,,6,2017-12-21T12:14:31Z,2017-12-27T22:25:52Z,CONTRIBUTOR,2017-12-21T18:41:37Z
15552,Tensorflow 1.4 C++ API considerably slower than Python,stat:awaiting response,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source with all optimizations
- **TensorFlow version (use command below)**: 1.4
- **Python version**:  3.5.2
- **Bazel version (if compiling from source)**: 0.8.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 8.0 / 6.0
- **GPU model and memory**: GTX960M

### Describe the problem

I was trying to run several models and evaluate the performance with different batch sizes in python and c++ and noticed that the c++ API version is considerably slower than the python one. Both were compiled with the same optimizations and with cuda support. 

When I try to predict the output of a single 256x256 image in python it takes me 0.5 seconds, and when i do it in tensorflow c++ api it takes me 1.7 seconds. Notice that in python I was using a non deployed model (without freezing and transforming graph), whereas in C++ I did those transformations. 

Does anyone knows why this is happening? Is it because of the frozen and transformed graph?

I always thought the C++ API would be at least as fast as the Python version. 
",0,,5,2017-12-21T11:13:17Z,2017-12-22T16:20:14Z,NONE,2017-12-21T23:39:31Z
15550,fix random_distributions_test,"awaiting testing (then merge),cla: yes","[iota](http://en.cppreference.com/w/cpp/algorithm/iota) is declared in \<numeric\>
",0,,2,2017-12-21T08:28:01Z,2017-12-21T19:02:34Z,CONTRIBUTOR,2017-12-21T18:09:27Z
15548,Bug fix for example_parsing_ops_test,"awaiting testing (then merge),cla: yes","Delay tensor allocation.

Static variables' initialization order is not determined in C++.
In one static variable's constructor, you can't access other static variables unless they are constexpr, which is not true for tensor's allocators.

Replacing it with std::call_once.

https://isocpp.org/wiki/faq/ctors#static-init-order",0,,3,2017-12-21T07:45:12Z,2017-12-26T19:23:25Z,CONTRIBUTOR,2017-12-22T06:07:46Z
15547,"how to build tensorflow-lite.so  for linux  ubuntu, thanks?",type:support,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,1,2017-12-21T07:29:59Z,2017-12-22T22:46:30Z,NONE,2017-12-22T22:46:30Z
15544,support unknown shape for `sparse_multiclass_hing_loss`,"awaiting testing (then merge),cla: yes","Fix #15480.

### How to test

+ [x] add test case.
+ [ ] pass all tests.",0,,2,2017-12-21T06:29:42Z,2017-12-21T19:13:03Z,CONTRIBUTOR,2017-12-21T18:12:15Z
15543,Maven Nightlies,stat:awaiting response,"@asimshankar Would it be possible to start releasing Maven nightlies for the Java API packages? I only depend on the org.tensorflow Proto package, but I guess it would be more consistent to do that for all the Java API packages.",0,,2,2017-12-21T05:39:27Z,2018-01-23T20:17:27Z,CONTRIBUTOR,2018-01-04T09:31:19Z
15540,No PyPi package for Tensorflow 1.4 on Windows?,stat:awaiting tensorflower,"Running Windows 10 with Python 3.6.
Just wondering when that's gonna come out.",0,,4,2017-12-21T03:58:55Z,2017-12-21T18:55:25Z,NONE,2017-12-21T15:21:08Z
15539,Remove quantized_matmul_op_for_hexagon_test in Windows build scripts,"awaiting testing (then merge),cla: yes",,0,,2,2017-12-21T03:55:07Z,2017-12-21T18:04:39Z,CONTRIBUTOR,2017-12-21T04:23:40Z
15538,[solved]session->Create(graph_def) No OpKernel was registered to support Op 'RandomUniform',,"issue: crashes when loading pb file in C++ program:

>     Session * session;
>     GraphDef graph_def;
>     SessionOptions opts;
>     TF_CHECK_OK(ReadBinaryProto(Env::Default(), heartPrintPbPathFile, &graph_def));
>     TF_CHECK_OK(NewSession(opts, &session));
>     TF_CHECK_OK(session->Create(graph_def));
> 

os: linux
train: python
inference: C++ interface

runtime error on inference part:

> Non-OK-status: session->Create(graph_def) status: Invalid argument: No OpKernel was registered to support Op 'RandomUniform' with these attrs.  Registered devices: [CPU], Registered kernels:
>   <no registered kernels>
> 
> 	 [[Node: rnn_net/rnn/dropout_63/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0](rnn_net/rnn/dropout_63/Shape)]]

python train code:

>         with tf.variable_scope(""rnn_net"") as scope:
>             cell = []
>             for i in range(num_layers):
>                 cell.append(tf.nn.rnn_cell.LSTMCell(hidden_size, state_is_tuple=True))
> 
>             cell = tf.nn.rnn_cell.MultiRNNCell(cell)
> 
>             cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = self.keep_prob)
> 
>             initial_state = cell.zero_state(batch_size, tf.float32)
> 
>             input_list = tf.unstack(conv_output, axis=1)
> 
>             rnn_output, _ = tf.nn.static_rnn(cell, input_list, dtype=tf.float32)
>             self.rnn_output = rnn_output[-1]
>             print ""rnn output shape: ""
>             print self.rnn_output.get_shape()

hint one : i found that if i delete this line:
cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = self.keep_prob)
then the trained pb file can be loaded normally.

hint two: beside rnn part, i also have cnn part in the network, and the dropout in cnn works just fine:

>         with tf.variable_scope(""conv_net"") as scope:
>             filters = [15, 11, 7, 5]
>             kernel_size = [64, 64, 32, 32]
>             input_layer = tf.reshape(self.x, [-1, seq_len, 1])
>             conv1_1 = tf.layers.conv1d(inputs=input_layer, filters=filters[0], kernel_size=kernel_size[0], padding=""same"", activation=tf.nn.tanh, name='conv1_1')
>             conv1_2 = tf.layers.conv1d(inputs=conv1_1, filters=filters[1], kernel_size=kernel_size[1], padding=""same"", activation=tf.nn.tanh)
>             pool1 = tf.layers.max_pooling1d(inputs=conv1_2, pool_size=4, strides=4)
> 
>             bn1 = batch_norm(pool1, self.is_train, scope='bn1')
>             dropout1 = tf.layers.dropout(inputs=bn1, rate=(1 - self.keep_prob))
> 
>             conv2_1 = tf.layers.conv1d(inputs=dropout1, filters=filters[2], kernel_size=kernel_size[2], padding=""same"", activation=tf.nn.tanh, name='conv2_1')
>             conv2_2 = tf.layers.conv1d(inputs=conv2_1, filters=filters[3], kernel_size=kernel_size[3], padding=""same"", activation=tf.nn.tanh)
>             pool2 = tf.layers.max_pooling1d(inputs=conv2_2, pool_size=4, strides=4)
> 
>             bn2 = batch_norm(pool2, self.is_train, scope='bn2')
>             dropout2 = tf.layers.dropout(inputs=bn2, rate=(1 - self.keep_prob))
> 
>             conv3_1 = tf.layers.conv1d(inputs=dropout2, filters=filters[2], kernel_size=kernel_size[2], padding=""same"", activation=tf.nn.tanh)
>             conv3_2 = tf.layers.conv1d(inputs=conv3_1, filters=filters[3], kernel_size=kernel_size[3], padding=""same"", activation=tf.nn.tanh)
>             pool3 = tf.layers.max_pooling1d(inputs=conv2_2, pool_size=2, strides=2)
> 
>             conv_output = pool3

",0,,6,2017-12-21T03:35:22Z,2017-12-21T18:43:29Z,NONE,2017-12-21T18:43:29Z
15537,tensorflow lite converter: why empty lite file,"comp:lite,stat:awaiting response","I have a tensorflow pb file and try to use tflite converter to convert it to lite file,but the result file is empty and no error. I don't know why
`bazel run --config=opt --copt=-msse4.1 --copt=-msse4.2  //tensorflow/contrib/lite/toco:toco --   --input_file=/Users/Lavector/code/keras_to_tensorflow/mobilenet_regression.pb   --output_file=/tmp/mobilenet_regression.lite   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --inference_type=FLOAT   --input_shape=1,224,224,3   --input_array=input   --output_array=output_node0`

`WARNING: Config values are not defined in any .rc file: opt
INFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).
INFO: Found 1 target...
Target //tensorflow/contrib/lite/toco:toco up-to-date:
  bazel-bin/tensorflow/contrib/lite/toco/toco
INFO: Elapsed time: 0.223s, Critical Path: 0.01s
INFO: Build completed successfully, 1 total action

INFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/Users/Lavector/code/keras_to_tensorflow/mobilenet_regression.pb' '--output_file=/tmp/mobilenet_regression.lite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=FLOAT' '--input_shape=1,224,224,3' '--input_array=input' '--output_array=output_node0'
2017-12-21 11:20:11.351544: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 472 operators, 695 arrays (0 quantized)
2017-12-21 11:20:11.384604: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 85 operators, 200 arrays (0 quantized)
2017-12-21 11:20:11.387067: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 85 operators, 200 arrays (0 quantized)`

I use mobilenet to do regression, and it is successful when test.
I test the example code of [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md](url), and it is correct.",0,,8,2017-12-21T03:19:09Z,2018-01-12T02:08:18Z,NONE,2017-12-22T07:27:03Z
15534,mpi_collectives: Refactor to fix build issues,"awaiting testing (then merge),cla: yes","After TF commit 5c7f9e3, the mpi_collectives package would no longer build
ops and kernels. This build issue caused mpi_collectives import to fail in
Python with the following error: ""NameError: Could not find operator MPISize
in dynamic library mpi_collectives.so"" (ref: issue #13875).

To fix this issue, add build targets to ensure both ops and kernels are built.
Note, also refactored the build targets and directory structure to more
closely match other contrib packages.",0,,3,2017-12-21T00:35:51Z,2017-12-27T02:51:41Z,CONTRIBUTOR,2017-12-25T23:43:49Z
15533,added note about weights gradient in compute_weighted_loss,"awaiting testing (then merge),cla: yes","Added a note concerning the gradient computation w.r.t. weights in `losses.compute_weighted_loss`, 
see #15046.

I have only added it to this function, and not the other losses (like mean_squared_error) because it is a rare cornercase that should be documented somewhere, but is of no relevance to most users. ",0,,5,2017-12-21T00:20:07Z,2017-12-26T19:23:06Z,CONTRIBUTOR,2017-12-21T00:27:40Z
15532,Update API Docs,"cla: yes,stat:awaiting response",Updated api dos for SampleDistortedBoundingBox v1 and v2 to update tf.image_summary to tf.summary.image,0,,6,2017-12-20T23:08:57Z,2017-12-21T18:11:23Z,CONTRIBUTOR,2017-12-20T23:13:46Z
15531,Fix sample_distorted_bounding_box where min_object_covered could be None,"awaiting testing (then merge),cla: yes","This fix tried to address the issue raised in #15529 where not providing min_object_covered a value will result in a ValueError. In the docstring, however, min_object_covered has been described as default to 0.1.

The reason for the issue is that when sample_distorted_bounding_box switched to V2, min_object_covered has been changed form an attr to an input. As input could not have a default value, min_object_covered=None will result in an error.

This fix adds the check so that a default value 0.1 will be provided if min_object_covered=None.

This fix fixes #15529.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
",1,,3,2017-12-20T22:10:24Z,2018-01-25T01:22:28Z,MEMBER,2018-01-23T21:21:49Z
15529,Sample Distorted Bounding Box Bug,"awaiting review,stat:awaiting tensorflower","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Stock Example
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
16.04.3 LTS
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
Python 3.5.2
- **Bazel version (if compiling from source)**:
0.7.0
- **GCC/Compiler version (if compiling from source)**:
5.4.0
- **CUDA/cuDNN version**:
cuda_9.0.176_384.81 / cudnn 7
- **GPU model and memory**:
GTX 755M 2gb Memory x2
- **Exact command to reproduce**:
begin, size, bbox_for_draw = tf.image.sample_distorted_bounding_box(
        tf.shape(image),
        bounding_boxes=bounding_boxes)


### Describe the problem
When using the tf.image.sample_distorted_bounding_box() function the parameter min_object_covered seems to default to value None which causes an error (ValueError: None values not supported.)  If you give an argument for min_object_covered it seems to work fine.

There seems to be two versions of this function in the source [v2 which takes min_object_covered](https://github.com/tensorflow/tensorflow/blob/fc49f43817e363e50df3ff2fd7a4870ace13ea13/tensorflow/core/ops/image_ops.cc#L930) as a argument and a [v1](https://github.com/tensorflow/tensorflow/blob/fc49f43817e363e50df3ff2fd7a4870ace13ea13/tensorflow/core/ops/image_ops.cc#L844) which has the default value of 0.1 as an attribute.  It appears v2 is the one [being used](https://github.com/tensorflow/tensorflow/blob/73658420db2498ad7f07363bfa72cba6e2d9fdd2/tensorflow/python/ops/image_ops_impl.py#L1536).  Not sure what approach is best to take for fixing this bug but believe the root of the issue is coming from tensorflow/core/ops/image_ops.cc

### Source code / logs
Attached
[boundingbox.txt](https://github.com/tensorflow/tensorflow/files/1576836/boundingbox.txt)

Examples of code being implemented [here](https://github.com/wagonhelm/image_augment/blob/master/DataAug.ipynb).

",0,,2,2017-12-20T20:03:22Z,2018-01-25T01:22:28Z,CONTRIBUTOR,2017-12-20T22:11:10Z
15528,Update debugger.md,"awaiting testing (then merge),cla: yes","When working thru the examples, I'm seeing just `Softmax` rather than `softmax:Softmax`.  If this is indeed correct the output image also needs to be updated.",0,,5,2017-12-20T19:54:48Z,2017-12-21T02:19:45Z,CONTRIBUTOR,2017-12-20T20:00:16Z
15527,[Bug] Tensorflow serving loads incorrect model weights when using saved model main_op,,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 3.16.36 x86_64
- **TensorFlow installed from (source or binary)**: Binary (pip)
- **TensorFlow version (use command below)**: Git version 1.4.0-19-ga52c8d9 Release version 1.4.1
- **Python version**: 2.7.9
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: See attached gist

Hey there,

I've documented a lot of this bug over on this [issue](https://github.com/tensorflow/serving/issues/656) on the tensorflow serving repository, which was closed with the direction to open an issue here.

Essentially, the bug is as follows: I have an embedding layer in Keras that uses some pre-initialized weights. When I export the model for use by TF serving, I note the following behavior:

- The keras model itself has no issue outputting the correct results
- The exported model itself (upon inspection) has the correct weights
- The result values from TF serving are incorrect.

I've narrowed it down to an issue with `tensorflow.python.ops.variables.global_variables_initializer`, as follows:

When specifying the `main_op` argument in `tensorflow.saved_model.builder.SavedModelBuilder.add_meta_graph_and_variables`, if I use `tensorflow.saved_model.main_op.main_op()` I encounter this issue. 

If instead, I use a control flow group that excludes the global variables initializer as follows:

```
main_op_new = control_flow_ops.group(                 
            lookup_ops.tables_initializer(),          
            variables.local_variables_initializer(),  
)                                                     
```

I do not encounter this issue. I've attached the full code for reproducing this issue (with the caveat of needing `tensorflow_model_server` running) [here](https://gist.github.com/zmjjmz/ce9c7a896933a02953cae0069a2ca21e)

Here's the example output with the global variables initializer: https://gist.github.com/zmjjmz/d739cdfa52148eb814450e48cbf8ddb6

If you comment out line 83 of the repro code and uncomment line 84, you should get the correct output as shown here: https://gist.github.com/zmjjmz/9edee5b4eeff94f383122545d80ee55f

Thanks for helpin out




",0,,6,2017-12-20T19:06:51Z,2018-01-26T17:32:16Z,NONE,2017-12-20T23:27:33Z
15526,Branch 179628764,"awaiting testing (then merge),cla: yes",Push,0,,9,2017-12-20T19:03:55Z,2017-12-21T04:13:59Z,CONTRIBUTOR,2017-12-20T19:04:07Z
15525,Op type not registered HashTableV2 error while deploying model in cloud ml,type:support,"Problem while deploying model in Tensorflow 1.4.

**Code :**

```python
def model_fn(features, labels, mode):
    if mode == tf.estimator.ModeKeys.TRAIN:
        tf.keras.backend.set_learning_phase(True)
    else:
        tf.keras.backend.set_learning_phase(False)

    input_feature = features['x']
    table = lookup.index_table_from_file(vocabulary_file='vocab.txt', num_oov_buckets=1, default_value=-1)
    text = tf.squeeze(input_feature, [1])
    words = tf.string_split(text)
    dense_words = tf.sparse_tensor_to_dense(words, default_value=PADWORD)
    numbers = table.lookup(dense_words)
    padding = tf.constant([[0, 0], [0, MAX_LEN]])
    padded = tf.pad(numbers, padding)
    sliced = tf.slice(padded, [0, 0], [-1, MAX_LEN])
    print('words_sliced={}'.format(words))

    embeds = tf.keras.layers.Embedding(MAX_FEATURES+1, 128, input_length=MAX_LEN)(sliced)

    print('words_embed={}'.format(embeds))
    f1 = tf.keras.layers.Dropout(0.2)(embeds)
    f1 = tf.keras.layers.Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)(f1)
    f1 = tf.keras.layers.GlobalAveragePooling1D()(f1)
    f1 = tf.keras.layers.Dense(hidden_dims)(f1)
    f1 = tf.keras.layers.Dropout(0.5)(f1)
    f1 = tf.keras.layers.Activation('relu')(f1)
    logits = tf.keras.layers.Dense(11)(f1)

    predictions_dict = {
        'class': tf.argmax(logits, 1),
        'prob': tf.nn.softmax(logits)
    }

    '''prediction_output = tf.estimator.export.PredictOutput({""classes"": tf.argmax(input=logits, axis=1),
                                                           ""probabilities"": tf.nn.softmax(logits,
                                                                                          name=""softmax_tensor"")})'''

    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions_dict, export_outputs={
            'prediction': tf.estimator.export.PredictOutput(predictions_dict)
        })

    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits=logits)

    if mode == tf.contrib.learn.ModeKeys.TRAIN:
        train_op = tf.contrib.layers.optimize_loss(loss, tf.contrib.framework.get_global_step(), optimizer='Adam',
                                                   learning_rate=0.001)
        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

    eval_metrics_ops = {
        'accuracy': tf.metrics.accuracy(labels=labels, predictions=predictions_dict['class']),
        'precision': tf.metrics.precision(labels=labels, predictions=predictions_dict['class']),
        'recall': tf.metrics.recall(labels=labels, predictions=predictions_dict['class'])
    }
    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics_ops)

def get_train_record(record):
    vector = tf.decode_csv(record, DEFAULTS, use_quote_delim=True)
    return vector[1:], vector[0]

def preprocess(text):
    text = text.lower()
    result = ' '.join([word for word in text.split() if word not in (stop_words)])
    return result


def build_vocab(file_name, vocab_file_name):
    df = pd.read_csv(file_name, header=None, sep=',', skiprows=[1], names=['product', 'consumer_complaint_narrative'])
    df['consumer_complaint_narrative'] = df['consumer_complaint_narrative'].apply(preprocess)
    print(df['consumer_complaint_narrative'][0])
    vocab_processor = tflearn.preprocessing.VocabularyProcessor(max_document_length=MAX_FEATURES, min_frequency=10,
                                                                tokenizer_fn=tflearn.preprocessing.tokenizer)
    vocab_processor.fit(df['consumer_complaint_narrative'])
    with gfile.Open(vocab_file_name, 'wb') as f:
        f.write(""{}\n"".format(PADWORD))
        for word, index in vocab_processor.vocabulary_._mapping.items():
            f.write(""{}\n"".format(word))
    nwords = len(vocab_processor.vocabulary_)
    print('{} words into {}'.format(nwords, vocab_file_name))


def input_fn(file_name, batch_size, repeat_count, shuffle=False):
    def _input_fn():
        data_set = tf.data.TextLineDataset(filenames=file_name)
        data_set = data_set.map(get_train_record)
        if shuffle:
            data_set = data_set.shuffle(shuffle)
        data_set = data_set.repeat(repeat_count)
        batch = data_set.batch(batch_size)
        iterator = batch.make_one_shot_iterator()
        features, labels = iterator.get_next()
        return {'x': features}, labels

    return _input_fn()


def get_train_spec(file_name, batch_size, repeat_count):
    return tf.estimator.TrainSpec(input_fn=lambda: input_fn(file_name, batch_size, repeat_count, shuffle=True), max_steps=1000)


def get_test_spec(file_name, batch_size, repeat_count=1):
    return tf.estimator.EvalSpec(input_fn=lambda: input_fn(file_name, batch_size, repeat_count, shuffle=True))


def serving_input_fn():
    feature_tensor = tf.placeholder(tf.string, [None])
    # features = tf.py_func(preprocess, [feature_tensor], tf.string)
    features = tf.expand_dims(feature_tensor, -1)
    return tf.estimator.export.ServingInputReceiver({'x': features}, {'x': features})

finance_classifier = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir)

print('\n Training .....')
finance_classifier.train(input_fn=lambda: input_fn('dataset/train.csv', batch_size, repeat_count=5, shuffle=True))

print('\n Evaluating.....')
eval_results = finance_classifier.evaluate(input_fn=lambda: input_fn('dataset/valid.csv', batch_size, repeat_count=1,
                                                                  shuffle=False))
for key in eval_results:
    print("" {} was {}"".format(key, eval_results[key]))

print('\n Exporting')
exported_model_dir = finance_classifier.export_savedmodel(model_dir, serving_input_receiver_fn=serving_input_fn)
decoded_model_dir = exported_model_dir.decode(""utf-8"")
```

[Screenshot](https://drive.google.com/open?id=1FAmoo9zCBJBAG2IFdySb_r4s1OV6YExn)

But when I tried with tf1.2 with some changes in the code in model_fn. Basically not using tf.keras but using tf.contrib.keras it was working. **is this bug ?**",0,,5,2017-12-20T18:22:04Z,2017-12-20T23:04:58Z,NONE,2017-12-20T23:04:58Z
15521,Bug/Feature: constant_folding FP16 ,stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
RHEL 7
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
('v1.3.0-rc1-6207-ge210cb1', '1.4.0')
- **Python version**: 
python2.7
- **Bazel version (if compiling from source)**:
bazel 0.9.0
- **GCC/Compiler version (if compiling from source)**:
gcc 4.8
- **Exact command to reproduce**:

python tensorflow/models/tutorial/images/convolutional.py --use_fp16

### Describe the problem
When using FP16 in e.g. the case listed above, the following error occurs:
E tensorflow/core/grappler/optimizers/constant_folding.cc:1272] Unexpected type half
E tensorflow/core/grappler/optimizers/constant_folding.cc:1242] Unexpected type half

When looking into constant_folding.cc, i found out FP16 support exists at given lines, but is commented out.
Why is this not yet included in Tensorflow?

As far as I can see, this is more of a feature request than a bug report, since these code lines simply check if computational effort can be reduced (if the matrix is zero or one)

Also when using above command, the model trains until step 1100, then the learning rate drops to 0 (a known FP16 problem). Still it's annoying to encounter this in an official tutorial file.

",0,,4,2017-12-20T16:09:52Z,2018-01-11T23:03:48Z,NONE,2017-12-20T23:13:59Z
15518,Possible memory leak with tf.py_func() with distributed Tensorflow?,"stat:awaiting tensorflower,type:bug/performance","When running Tensorflow as an distributed process to provide data with tf.data, it gradually consumes more and more memory, and finally consumes all memory of the system.

Scripts to reproduce:
We use a dummy dataset which produce [128, 28, 28, 1] tensors. 
Case1: Without distribute, which works fine, it will only consume 429Mb memory, no matter how many batches we run.
Codes in `test1.py`:
```
#test1.py
import tensorflow as tf
import numpy as np
from tqdm import tqdm

def dataset_generator():
    while True:
        yield np.random.uniform(size=[28, 28, 1]).astype(np.float32)
dataset = tf.data.Dataset.from_generator(dataset_generator, tf.float32)
dataset = dataset.batch(128)
value = dataset.make_one_shot_iterator().get_next()

sess = tf.Session()
for _ in tqdm(range(100000), ascii=True):
    sess.run(value)
```

Case: With distribute, it will consumes more and more memory while running more and more batches. It consumes 10+Gb with less than 1M batches. Use the following two commands in two processes to run the `test2.py`:
```
CUDA_VISIBLE_DEVICES="""" python test2.py dataset
CUDA_VISIBLE_DEVICES="""" python test2.py test
```
Codes in `test2.py`
```
# test2.py
import tensorflow as tf
import numpy as np
from tqdm import tqdm
import sys
def main(role):
    def dataset_generator():
        while True:
            yield np.random.uniform(size=[28, 28, 1]).astype(np.float32)
    cluster = tf.train.ClusterSpec({'dataset': ['localhost:2001'], 'test': ['localhost:2002']})
    if role == 'dataset':
        server = tf.train.Server(cluster, 'dataset', 0)
    elif role == 'test':
        server = tf.train.Server(cluster, 'test', 0)
    else:
        raise ValueError(""Uknown role {}."".format(role))
    with tf.device('/job:dataset/task:0')    :
        dataset = tf.data.Dataset.from_generator(dataset_generator, tf.float32)
        dataset = dataset.batch(128)
        value = dataset.make_one_shot_iterator().get_next()
    if role == 'dataset':
        server.join()
    elif role == 'test':
        sess = tf.Session(target=server.target)
        for _ in tqdm(range(100000000), ascii=True):
            sess.run(value)
            
if __name__ == ""__main__"":
    main(sys.argv[1])
```

Tensorflow: v1.4.0-rc1-11-g130a514 1.4.0
OS: ubuntu mate 16.04.1
Python: 3.6.1 (conda 4.3.30)


",1,,3,2017-12-20T09:47:28Z,2017-12-20T19:39:15Z,NONE,2017-12-20T16:05:16Z
15517,fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h',stat:awaiting tensorflower,"I am new to tensorflow, when I compile tensorfolw with VS2015 in Windows 7, I got the following error, could anyone help on this, thanks!
fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h'",0,,7,2017-12-20T09:15:06Z,2018-01-05T06:57:34Z,NONE,2017-12-22T02:20:51Z
15516,[XLA] Define LANG_CXX11 for >= VS 2015,"awaiting testing (then merge),cla: yes","In MSVC, __cplusplus == 199711 even when /std:c++latest is set because MSVC is still not ""fully"" C++11 compliant.

Split from #15310.

#15213",1,,5,2017-12-20T08:39:07Z,2017-12-20T23:48:55Z,CONTRIBUTOR,2017-12-20T09:45:57Z
15515,[XLA] Use simplified version of TF_ASSIGN_OR_RETURN for MSVC,"awaiting testing (then merge),cla: yes","This simplified version also works for GCC as well actually.

Split from #15310.

#15213",1,,7,2017-12-20T08:38:46Z,2017-12-21T18:12:54Z,CONTRIBUTOR,2017-12-20T22:48:33Z
15514,[XLA] Remove unused dlfcn.h and implement sincos[f] for MSVC,"awaiting testing (then merge),cla: yes","Split from #15310.

#15213",1,,2,2017-12-20T08:37:31Z,2017-12-26T19:22:44Z,CONTRIBUTOR,2017-12-26T00:57:58Z
15513,[XLA] Define ssize_t for Windows,"awaiting testing (then merge),cla: yes","Split from #15310.

#15213",1,,6,2017-12-20T08:37:06Z,2017-12-21T19:12:36Z,CONTRIBUTOR,2017-12-21T00:51:52Z
15512,[XLA] Hide GCC 7.1.1 workaround from MSVC,"awaiting testing (then merge),cla: yes","Split from #15310.

#15213",1,,4,2017-12-20T08:36:53Z,2017-12-20T23:58:26Z,CONTRIBUTOR,2017-12-20T23:06:10Z
15511,[XLA] Fix std::array initialization,"awaiting testing (then merge),cla: yes","Split from #15310.

#15213",1,,2,2017-12-20T08:36:27Z,2017-12-21T00:00:15Z,CONTRIBUTOR,2017-12-20T23:07:57Z
15510,"[XLA] Use tensorflow::port::Aligned{Malloc,Free}","awaiting testing (then merge),cla: yes","Split from #15310.

#15213",1,,2,2017-12-20T08:36:06Z,2017-12-21T18:13:24Z,CONTRIBUTOR,2017-12-21T02:03:29Z
15509,[XLA] Explicitly include <numeric>,"awaiting testing (then merge),cla: yes","`std::accumulate` comes from `<numeric>`, but `<numeric>` is not implicitly included by `<algorithm>` in MSVC.

Split from #15310.

#15213",1,,2,2017-12-20T08:35:35Z,2017-12-21T00:07:39Z,CONTRIBUTOR,2017-12-20T23:10:18Z
15508,[XLA] Add 'const' to custom comparator,"awaiting testing (then merge),cla: yes","Split from #15310.

#15213",1,,2,2017-12-20T08:34:14Z,2017-12-21T00:07:49Z,CONTRIBUTOR,2017-12-20T23:11:45Z
15507,[XLA] Use os.path for path manipulation,"awaiting testing (then merge),cla: yes","Split from #15310.

#15213",0,,2,2017-12-20T08:33:39Z,2017-12-20T21:50:35Z,CONTRIBUTOR,2017-12-20T20:13:11Z
15506,bugfix: long is 32 bits on Windows,cla: yes,"Please avoid to use 'long' as data type.

```c++
int main()
{
	long long threshold = 1L << 31;    
	std::cout << threshold << std::endl;
	return 0;
}
```
result:
-2147483648

",0,,2,2017-12-20T08:25:56Z,2017-12-21T18:43:42Z,CONTRIBUTOR,2017-12-20T19:59:26Z
15503,Behavior change of tf.app.flags parsing boolean args,stat:awaiting tensorflower,"tf version 1.5.0-dev20171219
for example below args
flags.DEFINE_boolean('pre_calc_image_feature', False, '')

when using tf 1.4.1 it is ok to do --pre_calc_image_feature 0
which got FLAGS.pre_calc_image_feature == False.
But for tf 1.5 you must use --pre_calc_image_feature=0 if you still use --pre_calc_image_feature 0
then you will get FLAGS.pre_calc_image_feature == True. 

Not sure if this is a bug or just by design but personally I think tf version 1.4.1 is better handling this case.
",0,,8,2017-12-20T07:22:34Z,2018-01-30T02:34:59Z,NONE,2017-12-22T07:27:17Z
15502,Use BSD fnmatch on Windows,"awaiting review,cla: yes","#15501 

It has a BSD-3-Clause copyright. 
",0,,2,2017-12-20T06:54:56Z,2018-01-03T13:05:01Z,CONTRIBUTOR,2018-01-03T13:05:01Z
15500,fix pooling1D dimension bug,"awaiting review,cla: yes","When `data_format` is `channels_last`, input is `NWC`, so we have to `expand_dim(1)` to make it become `NHWC`. Then we apply pooling on `W` which is the 3rd dimention.

When `data_format` is `channels_first`, input is `NCW`, so we have to `expand_dim(2)` to make it become `NCHW`. Then we apply pooling on `W`, which is the 4th dimention.

RELNOTES: Fixed wrong handling of 1D pooling (pooling was not happening on the correct dimension).",1,,13,2017-12-20T06:27:33Z,2018-01-23T00:37:02Z,CONTRIBUTOR,2017-12-21T09:05:38Z
15499,Go bindings: No shape inference function exists for op 'CreateSummaryFileWriter',,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: NA (using Go bindings)
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: cuda-9.1.85-1 cudnn-7.0.5-1
- **GPU model and memory**: GTX 1060 6GB
- **Exact command to reproduce**: See below

### Describe the problem
Calling `op.CreateSummaryFileWriter()` panics with: `panic: failed to add operation ""CreateSummaryFileWriter"": No shape inference function exists for op 'CreateSummaryFileWriter', did you forget to define it?`

### Source code / logs
```
package main

import (
	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func main() {
	s := op.NewScope()
	writer := op.SummaryWriter(s)
	createSummaryWriter := op.CreateSummaryFileWriter(s,
		writer,
		op.Const(s.SubScope(""log_dir""), ""tb_logs""),
		op.Const(s.SubScope(""max_queue""), int32(10)),
		op.Const(s.SubScope(""flush_millis""), int32(1000)),
		op.Const(s.SubScope(""filename_suffix""), ""tb_demo""),
	)
	scalar := op.Const(s.SubScope(""scalar""), float32(3.1415))
	step := op.Const(s.SubScope(""step""), int64(1))
	tag := op.Const(s.SubScope(""tag""), ""foo_scalar"")
	summary := op.ScalarSummary(s, tag, scalar)
	merged := op.MergeSummary(s, []tf.Output{summary})
	write := op.WriteSummary(s, writer, step, scalar, tag, merged)
	closeSummaryWriter := op.CloseSummaryWriter(s, writer)
	graph, err := s.Finalize()
	if err != nil {
		panic(err)
	}
	sess, err := tf.NewSession(graph, nil)
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{createSummaryWriter})
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{write})
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{closeSummaryWriter})
	if err != nil {
		panic(err)
	}
}
```
Returns:
```
2017-12-19 20:39:06.314322: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
panic: failed to add operation ""CreateSummaryFileWriter"": No shape inference function exists for op 'CreateSummaryFileWriter', did you forget to define it? (Stacktrace: goroutine 1 [running]:
runtime/debug.Stack(0xc420082130, 0x13eb530, 0x14298a0)
	/usr/lib/go/src/runtime/debug/stack.go:24 +0xa7
github.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).UpdateErr(0xc42007c180, 0x4d9527, 0x17, 0x750140, 0xc42000e068)
	/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:120 +0x72
github.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).AddOperation(0xc42007c180, 0x4d9527, 0x17, 0x4d9527, 0x17, 0xc4200820f0, 0x5, 0x5, 0x0, 0xc4200181a0)
	/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:85 +0xfd
github.com/tensorflow/tensorflow/tensorflow/go/op.CreateSummaryFileWriter(0xc42007c180, 0xc420010200, 0x0, 0xc420010210, 0x0, 0xc420010220, 0x0, 0xc420010230, 0x0, 0xc420010240, ...)
	/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/wrappers.go:16474 +0x2cc
main.main()
	/home/isaac/go/src/github.com/is8ac/gotf/tb_demo.go:11 +0x282
)

goroutine 1 [running]:
main.main()
	/home/isaac/go/src/github.com/is8ac/gotf/tb_demo.go:27 +0x7cc
exit status 2
```",0,,2,2017-12-20T05:05:17Z,2017-12-20T18:17:34Z,NONE,2017-12-20T18:17:34Z
15498,gradients_1 generated in graph but not connected with AdamOptimizer ,,"System information: 
OS platform: Linux Unbuntu 16.04
Tensorflow install from: pip install...
Tensorflow version: 1.4.0
Python version: py2.7

Problem:
I want to write a CNN classification network for MNIST dataset. And I write a class named MNIST_classification, then define '_build_model()' and '_train_phase()' in this class. In main function, I define a object of MNIST_classification class, and callback the '_train_phase()' to start a training process. But I found in Graph(), there are two gradients generated at each computation node, i.e. ""gradients"" and ""gradients_1"". I use tf.gradients(loss, train_vars) to print all gradients' names and get '/gradients_1/*', but within the Graph(), gradients_1 are not connected with a AdamOptimizer, which leads to no backward propagation update for each trainable variable...

 
![image](https://user-images.githubusercontent.com/33562173/34189925-f3c6c4ac-e578-11e7-8b8f-3de98611d415.png)

Source code:

import tensorflow as tf
from utils import utils
import numpy as np

class mnist_classification(object):

    def __init__(self, sess, graph, train_param={'num_of_epoches': 1000,
                                        'num_of_classes': 10,
                                        'log_dir': './log',
                                        'model_dir': './model',
                                        'batch_size': 128,
                                        'learn_rate': 1e-4,
                                        'max_iter': 5000,
                                        'dim_feat': 28}):

        self.num_of_epoches = train_param['num_of_epoches']
        self.log_dir      = train_param['log_dir']
        self.model_dir    = train_param['model_dir']
        self.batch_size   = train_param['batch_size']

        self.learn_rate = train_param['learn_rate']
        self.max_iter   = train_param['max_iter']

        self.dim_feat = train_param['dim_feat']
        self.num_of_classes = train_param['num_of_classes']

        self.sess = sess
        self.graph = graph

        # !Build-up MNIST classification model...
        assert self.sess.graph is self.graph
        self._build_model()



    def _convolution_block(self, inp_feat, kernel_size, num_of_kernel_channels, conv_strides, conv_padding, var_scope):
        '''
            Function:
                        _convolution_block, i.e. convolution + Maxpooling + ReLU
            Input:
                    [1] <tensor> inp_feat, i.e. input feature, dimension->[batch_size, height, width, channel]
                    [2] <int32>  kernel_size
                    [3] <int32> num_of_kernel_channels
                    [4] <int32> conv_strides
                    [5] <string> conv_padding
                    [5] <string> var_scope
            Output:
                    <tensor> activ
        '''
        try:
            num_of_feat_channels = inp_feat.shape[3].value
        except:
            num_of_feat_channels = 1

        with tf.variable_scope(var_scope):
            weights = tf.get_variable(name='conv_weights', shape=[kernel_size, kernel_size, num_of_feat_channels, num_of_kernel_channels],
                                      initializer=tf.random_normal_initializer(stddev=0.1))

            biases = tf.get_variable(name='conv_biases', shape=[num_of_kernel_channels], initializer=tf.zeros_initializer())

        # !Convolution layer...
        conv = tf.nn.conv2d(inp_feat, weights, strides=conv_strides, padding=conv_padding, name='conv', data_format='NHWC') + biases

        # !Activation layer...
        activ = tf.nn.relu(conv)

        # !Pooling layer...
        pool = tf.nn.max_pool(activ,ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')

        return pool





    def _fc_layer(self, inp_feat, num_of_outputs, var_scope):
        '''
            Function:
                        _fc_layer
            Input:
                        [1] inp_feat, dimension->[batch_size, dim_feat]
                        [2] num_of_outputs
                        [3] var_scope: reuse=True
            Output:
                        fc
        '''

        dim_feat = inp_feat.shape[1].value

        with tf.variable_scope(var_scope):

            fc_weights = tf.get_variable(name='fc_weights', dtype=tf.float32, shape=[dim_feat, num_of_outputs], initializer=tf.random_normal_initializer(stddev=0.1))
            fc_bias    = tf.get_variable(name='fc_bias',    dtype=tf.float32, shape=[num_of_outputs], initializer=tf.zeros_initializer())


            # tf.nn.xw_plus_b(x, weights, bias) = tf.matmul(x, weights) + biases
            fc = tf.nn.xw_plus_b(x=inp_feat, weights=fc_weights, biases=fc_bias)

            return fc



    def _build_model(self):

        self.digit = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, self.dim_feat, self.dim_feat, 1])
        self.label = tf.placeholder(dtype=tf.int64, shape=[self.batch_size, self.num_of_classes])

        # # !One-hot encoding for label...
        # with tf.name_scope('label_trans'):
        #     self.new_label = utils._array_sparse_to_dense(self.label, self.num_of_classes)

        conv1 = self._convolution_block(inp_feat=self.digit, conv_strides=[1,1,1,1], conv_padding='SAME', kernel_size=5, num_of_kernel_channels=32, var_scope='conv1')

        conv2 = self._convolution_block(inp_feat=conv1, conv_strides=[1,1,1,1], conv_padding='SAME', kernel_size=5, num_of_kernel_channels=64, var_scope='conv2')

        conv3 = self._convolution_block(inp_feat=conv2, conv_strides=[1, 1, 1, 1], conv_padding='SAME', kernel_size=7,num_of_kernel_channels=64, var_scope='conv3')

        flatt = tf.layers.flatten(conv3, name='flatten')

        fc1 = self._fc_layer(inp_feat=flatt, num_of_outputs=1024, var_scope='fc1')
        fc1 = tf.nn.relu(fc1)

        fc2 = self._fc_layer(inp_feat=fc1, num_of_outputs=10, var_scope='fc2')

        self.predict = tf.nn.softmax(logits=fc2, dim=-1)

        self.loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=self.label, logits=fc2))

        # !Define MNIST classification accuracy...
        correct = tf.equal(tf.argmax(self.predict, 1), tf.argmax(self.label, 1))
        self.accuracy = tf.reduce_mean(tf.cast(correct, 'float'))

        # !Define training operations...
        self.train_op = tf.train.AdamOptimizer(self.learn_rate).minimize(self.loss)





    def _train(self, img_file, lab_file):

        # !Load data into memory...
        img, n_rows, n_cols = utils._read_MNIST_file(img_file, fmt='>IIII')
        lab, _, _ = utils._read_MNIST_file(lab_file, fmt='>II')


        # !Intialize the global_variables and local_variables...
        self.sess.run(tf.global_variables_initializer())
        self.sess.run(tf.local_variables_initializer())

        # !List all trainable variables...
        train_vars = tf.trainable_variables()
        for var in train_vars:
            print var.name

        # !Add gradients into tensorboard summary...
        gradients = tf.gradients(self.loss, train_vars)
        for ii in range(len(gradients)):
            if gradients[ii]!=None:
                tf.summary.histogram(train_vars[ii].name, gradients[ii])

        # !Add loss into tensorboard summary...
        tf.summary.scalar('loss', self.loss)
        tf.summary.scalar('accuracy', self.accuracy)

        tf.summary.image('input_image', self.digit)

        summary_writer = tf.summary.FileWriter(""./log"", self.sess.graph)

        merged = tf.summary.merge_all()

        # !Start training process...
        for ii_epoch in range(self.num_of_epoches):
            for ii_iter in range(self.max_iter):


                img_batch = utils._randomly_sample(img, self.batch_size)
                img_batch = np.expand_dims(img_batch, axis=3) / 255

                lab_batch = utils._randomly_sample(lab, self.batch_size)
                lab_batch = utils._array_sparse_to_dense(np.int64(lab_batch), num_of_classes=self.num_of_classes)

                _, pred, los, acc, summary= self.sess.run([self.train_op, self.predict, self.loss, self.accuracy, merged], feed_dict={self.digit: img_batch, self.label: lab_batch})

                if ( (ii_epoch * self.max_iter + ii_iter) % 100 == 0):
                    summary_writer.add_summary(summary, ii_epoch * self.max_iter + ii_iter)
                    print(pred[0,:])
                    print(lab_batch[0])

                print('!Loss at No.%d Epoch, No.%d Iteration=%.5f' % (ii_epoch, ii_iter, los))
                print('!Accuracy at No.%d Epoch, No.%d Iteration=%.5f' % (ii_epoch, ii_iter, acc))



Main function:
from utils.utils import _array_sparse_to_dense
from utils.utils import _read_MNIST_file
from matplotlib import pyplot
import numpy as np
import tensorflow as tf
from model.MNIST_classification import mnist_classification
import os
import sys


# !Check if tensorflow version == '1.4.0', API
assert tf.__version__ == '1.4.0'



# !Set system default encoding method == 'utf-8'
reload(sys)
sys.setdefaultencoding('utf-8')


os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""


with tf.Graph().as_default() as graph:
    with tf.Session() as sess:
        MNIST_inst = mnist_classification(sess=sess, graph=graph)

        MNIST_inst._train(img_file='./data/train-images.idx3-ubyte', lab_file='./data/train-labels.idx1-ubyte')

",0,,1,2017-12-20T03:31:05Z,2017-12-20T05:17:33Z,NONE,2017-12-20T05:17:33Z
15497,Image Input for GridLSTM,type:support,"I would like to use GridLSTM for a handwriting recognition task. Unfortunately, the documentation is lacking info on how to input images into GridLSTMs. 
",0,,5,2017-12-20T03:23:17Z,2017-12-23T00:27:25Z,CONTRIBUTOR,2017-12-20T03:59:38Z
15496,Add ws2_32.lib to gcs_dns_cache_test's linkopts,"awaiting testing (then merge),cla: yes",Needed for Windows,0,,2,2017-12-20T03:20:14Z,2017-12-26T19:22:06Z,CONTRIBUTOR,2017-12-26T00:54:03Z
15495,Update mnist.py,"awaiting testing (then merge),cla: no",Comment modification,0,,6,2017-12-20T02:29:12Z,2017-12-20T20:03:01Z,NONE,2017-12-20T02:35:26Z
15494,Enables 0-D indexing for Gather() in TFLite,"awaiting testing (then merge),cla: yes,comp:lite","Hi,

This patch enables use of scalar (0-D) index for Gather() in TFLite.

Since `Dims<>` still works correctly (which becomes {1, 1, 1, 1}) when calculating a scalar tensor,
no actual internal change is needed. Still, it only supports indexing the first dimension.

At [here](https://github.com/tensorflow/tensorflow/compare/master...scottcjt:gather_0d?expand=1#diff-3aa3a18bc0cba3f7fe969252dbd8ed09L51), `int32` is changed to `int32_t` because my clang on OSX 10.12 complains that `int32` is undefined.",1,,6,2017-12-19T23:27:10Z,2018-01-23T18:14:41Z,CONTRIBUTOR,2017-12-27T01:14:27Z
15493,Add S3 logging to TensorFlow's logging system,"awaiting testing (then merge),cla: yes","This fix is an attempt to help the issue raised in #15159 where there is no logging in S3 file system and it is not easy to debug to diagnose.

This fix adds S3 logging to TensFlow's logging with
```
LogLevel::Info -> INFO
LogLevel::Warn -> WARNING
LogLevel::Error -> ERROR
LogLevel::Fatal -> FATAL
```

This fix is related to #15159.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,15,2017-12-19T20:15:18Z,2018-01-01T01:03:21Z,MEMBER,2017-12-26T05:47:49Z
15491,Branch 179578952,cla: yes,Push,0,,4,2017-12-19T19:28:03Z,2017-12-19T21:55:15Z,CONTRIBUTOR,2017-12-19T19:28:18Z
15489,Hotfix/fix android example for focus mode continuous picture - #15487,"awaiting testing (then merge),cla: yes",,1,,5,2017-12-19T18:22:40Z,2018-01-26T05:31:57Z,CONTRIBUTOR,2017-12-19T18:24:49Z
15487,Android Example breaks for old cameras not having support for FOCUS_MODE_CONTINUOUS_PICTURE,stat:contributions welcome,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

No fixed a bug.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Ubuntu 16.04, Tried on Android 21,22,23,24,25

- **TensorFlow installed from (source or binary)**:

Android App

- **TensorFlow version (use command below)**:

Latest in Android App

- **Python version**: 

3.2

- **Bazel version (if compiling from source)**:

Not Applicable

- **GCC/Compiler version (if compiling from source)**:

Not Applicable


- **CUDA/cuDNN version**:

Not Applicable


- **GPU model and memory**:

Not Applicable

- **Exact command to reproduce**:

Compile the Android Example as it is and execute on any Android device with old camera not supporting FOCUS_MODE_CONTINUOUS_PICTURE

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Android Example breaks for old cameras not having support for FOCUS_MODE_CONTINUOUS_PICTURE

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Creating a pull request for the fix.
",0,,1,2017-12-19T18:00:02Z,2018-01-26T05:32:33Z,CONTRIBUTOR,2017-12-23T04:38:31Z
15486,fix _Pooling1D data format bug,cla: yes,"When `data_format` is `channels_last`, input is `NWC`, so we have to `expand_dim(1)` to make it become `NHWC`. Then we apply pooling on `W` which is the 3rd dimention.

When `data_format` is `channels_first`, input is `NCW`, so we have to `expand_dim(2)` to make it become `NCHW`. Then we apply pooling on `W`, which is the 4th dimention.",0,,6,2017-12-19T17:48:08Z,2017-12-19T22:33:58Z,CONTRIBUTOR,2017-12-19T17:49:58Z
15485,Possible Bug with GPU: matmul_op.cc ,,"Hello,

I am trying to run a matrix multiplication on GPU. This is the code i am running on python. 
```
import tensorflow as tf

with tf.device('/gpu:0'):
  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
  c = tf.matmul(a, b)

sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
print(sess.run(c))
```

In the matmul_op.cc i have added an ```std::cout << ""Device: "" << ctx->device()->name << endl;``` in function ```Compute``` that is on line 456. The weird thing is that this ```cout``` prints: ```Device:  /job:localhost/replica:0/task:0/device:CPU:0 ``` while the log_device_placement from the session reports: 
```
2017-12-19 18:50:35.432196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GT 720 major: 3 minor: 5 memoryClockRate(GHz): 0.797
pciBusID: 0000:82:00.0
totalMemory: 1.95GiB freeMemory: 1.95GiB
2017-12-19 18:50:35.432273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GT 720, pci bus id: 0000:82:00.0, compute capability: 3.5)
Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GT 720, pci bus id: 0000:82:00.0, compute capability: 3.5
2017-12-19 18:50:35.455437: I tensorflow/core/common_runtime/direct_session.cc:300] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GT 720, pci bus id: 0000:82:00.0, compute capability: 3.5

Tensor(""MatMul:0"", shape=(2, 2), dtype=float32, device=/device:GPU:0)
MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2017-12-19 18:50:35.456469: I tensorflow/core/common_runtime/placer.cc:874] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
b: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2017-12-19 18:50:35.456525: I tensorflow/core/common_runtime/placer.cc:874] b: (Const)/job:localhost/replica:0/task:0/device:GPU:0
a: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2017-12-19 18:50:35.456555: I tensorflow/core/common_runtime/placer.cc:874] a: (Const)/job:localhost/replica:0/task:0/device:GPU:0
```

I have also checked the ```bool USE_CUBLAS``` and it has value 0, in function ```Compute```. Is it a possible bug or i am doing something wrong?",0,,3,2017-12-19T17:00:18Z,2017-12-23T00:20:22Z,NONE,2017-12-19T20:41:17Z
15484,[Building Error] clang: error: no such file or directory: 'x86_64',"comp:lite,stat:awaiting response","Hi all,

I met this problem when I was trying to build the Tensorflow Lite for iOS following [this instruction](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/ios.md).



Every thing went well until I tried the command:
`tensorflow/contrib/lite/build_ios_universal_lib.sh`

The error occured:
`clang: error: no such file or directory: 'x86_64'`

What could be the problem? Thank you very much!
My environment: Mac Sierra 10.12.6, XCode 9.2",1,,5,2017-12-19T15:38:54Z,2018-01-09T17:17:47Z,NONE,2017-12-19T22:36:17Z
15483,What is the best practice for running training and evaluation on the same machine?,,"I ask the question at [stackoverflow](https://stackoverflow.com/questions/47883570/what-is-the-best-practice-for-running-training-and-evaluation-on-the-same-machin). No answer so far, maybe I can find people have similar needs.

Here is the question:

**What I want to do?**

1. I only have 1 machine. 

2. I want to evaluate the mode periodically. 

**What I have now?** 


1.  use a placeholder. Say I run 1000 step of training by feeding the training data. then I feed in validation dataset for evaluation. put it in a loop.

    But as google suggested, placeholder is not a good way for long run training.


2. So, I use slim dataset to feed in data. Now, the model is bonded with training dataset like this:
    >      net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID',
    >                                 scope='conv1')

 I have to construct another model(in another graph) which is bonded with validation dataset. 

**Is there a better way of doing that?**

I know that google is focusing on distribution training on large scale, but I think as tensorflow  is a low-level and flexible framwork. There must be a way can do what I want. ",0,,1,2017-12-19T15:00:54Z,2017-12-19T17:40:18Z,CONTRIBUTOR,2017-12-19T17:40:18Z
15482,Fix a compile error in file_block_cache_test.cc,"awaiting testing (then merge),cla: yes","tensorflow/core/platform/cloud/file_block_cache_test.cc(461): error C3493: 'block_size' cannot be implicitly captured because no default capture mode has been specified

Compiler: Visual Studio 2017 v15.4.4",0,,2,2017-12-19T12:52:38Z,2017-12-19T23:23:01Z,CONTRIBUTOR,2017-12-19T22:35:13Z
15481,"[BUG] ""no viable conversion "" ERROR raised in ""tensorflow/core/kernels/eigen_pooling.h"" when build v1.4.1 for opencl",stat:awaiting response,"### System information
- **OS Platform and Distribution**:Linux Ubuntu 17.10
- **TensorFlow installed from**: source
- **TensorFlow version**:1.4.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:0.8.1
- **GCC/Compiler version (if compiling from source)**:7.2

### Describe the problem
""no viable conversion "" ERROR raised when build v1.4.1 with opencl (computecpp CE 0.5.0)

### Source code / logs
ERROR: .../tensorflow/tensorflow/core/kernels/BUILD:3169:1: C++ compilation of rule '//tensorflow/core/kernels:pooling_ops' failed (Exit 1)
In file included from tensorflow/core/kernels/pooling_ops_3d.cc:29:
./tensorflow/core/kernels/eigen_pooling.h:338:12: error: no viable conversion from '__m128' (vector of 4 'float' values) to 'cl::sycl::vec<float, 4>'
    Packet skip_mask =
           ^
./tensorflow/core/kernels/eigen_pooling.h:333:5: note: in instantiation of function template specialization 'Eigen::internal::AvgPoolMeanReducer<float>::reducePacketWithType<cl::sycl::vec<float, 4> >' requested here
    reducePacketWithType(static_cast<T>(0), p, accum);
    ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:186:15: note: in instantiation of function template specialization 'Eigen::internal::AvgPoolMeanReducer<float>::reducePacket<cl::sycl::vec<float, 4> >' requested here
      reducer.reducePacket(self.m_impl.template packet<Unaligned>(firstIndex + j), &p);
              ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:279:56: note: in instantiation of member function 'Eigen::internal::InnerMostDimReducer<Eigen::TensorEvaluator<const Eigen::TensorReductionOp<Eigen::internal::AvgPoolMeanReducer<float>, const Eigen::IndexList<Eigen::type2index<1>>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 3>, const Eigen::TensorVolumePatchOp<-1, -1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 5, 1, long>, 16, MakePointer> > >, MakePointer>, Eigen::ThreadPoolDevice>, Eigen::internal::AvgPoolMeanReducer<float>, true>::reduce' requested here
          InnerMostDimReducer<Self, Op, Vectorizable>::reduce(self, 0, num_coeffs, reducer);
                                                       ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:523:48: note: in instantiation of member function 'Eigen::internal::FullReducer<Eigen::TensorEvaluator<const Eigen::TensorReductionOp<Eigen::internal::AvgPoolMeanReducer<float>, const Eigen::IndexList<Eigen::type2index<1>>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 3>, const Eigen::TensorVolumePatchOp<-1, -1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 5, 1, long>, 16, MakePointer> > >, MakePointer>, Eigen::ThreadPoolDevice>, Eigen::internal::AvgPoolMeanReducer<float>, Eigen::ThreadPoolDevice, true>::run' requested here
      internal::FullReducer<Self, Op, Device>::run(*this, reducer, m_device, data);
                                               ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:133:19: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorReductionOp<Eigen::internal::AvgPoolMeanReducer<float>, const Eigen::IndexList<Eigen::type2index<1>>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 3>, const Eigen::TensorVolumePatchOp<-1, -1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 5, 1, long>, 16, MakePointer> > >, MakePointer>, Eigen::ThreadPoolDevice>::evalSubExprsIfNeeded' requested here
    return m_impl.evalSubExprsIfNeeded(data);
                  ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:132:24: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)
    return m_rightImpl.evalSubExprsIfNeeded(m_leftImpl.data());
                       ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:59: note: in instantiation of member function 'Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 5, 1, long>, 16, MakePointer>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 5>, const Eigen::TensorReductionOp<Eigen::internal::AvgPoolMeanReducer<float>, const Eigen::IndexList<Eigen::type2index<1>>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 3>, const Eigen::TensorVolumePatchOp<-1, -1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 5, 1, long>, 16, MakePointer> > >, MakePointer> > >, Eigen::ThreadPoolDevice, true>::run' requested here
      internal::TensorExecutor<const Assign, DeviceType>::run(assign, m_device);
                                                          ^
tensorflow/core/kernels/pooling_ops_3d.cc:108:71: note: in instantiation of function template specialization 'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<float, 5, 1, long>, 16, MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorReshapingOp<const Eigen::DSizes<long, 5>, const Eigen::TensorReductionOp<Eigen::internal::AvgPoolMeanReducer<float>, const Eigen::IndexList<Eigen::type2index<1>>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 3>, const Eigen::TensorVolumePatchOp<-1, -1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 5, 1, long>, 16, MakePointer> > >, MakePointer> > >' requested here
    output->tensor<T, 5>().device(context->eigen_device<CPUDevice>()) =
                                                                      ^
tensorflow/core/kernels/pooling_ops_3d.cc:194:39: note: in instantiation of member function 'tensorflow::LaunchPoolingOp<Eigen::ThreadPoolDevice, float, tensorflow::PoolingType::AVG>::launch' requested here
    LaunchPoolingOp<Device, T, Type>::launch(context, tensor_in, window, stride,
                                      ^
tensorflow/core/kernels/pooling_ops_3d.cc:133:12: note: in instantiation of member function 'tensorflow::Pooling3DOp<Eigen::ThreadPoolDevice, float, tensorflow::PoolingType::AVG>::Compute' requested here
  explicit Pooling3DOp(OpKernelConstruction* context) : UnaryOp<T>(context) {
           ^
tensorflow/core/kernels/pooling_ops_3d.cc:738:15: note: in instantiation of member function 'tensorflow::Pooling3DOp<Eigen::ThreadPoolDevice, float, tensorflow::PoolingType::AVG>::Pooling3DOp' requested here
TF_CALL_float(REGISTER_CPU_KERNELS);
              ^
external/local_config_sycl/crosstool/../sycl/include/SYCL/vec.h:9461:3: note: candidate constructor not viable: no known conversion from '__m128' (vector of 4 'float' values) to 'const vec<float, 4> &' for 1st argument
  vec(const vec<dataT, kElems> &rhs) {
  ^
external/local_config_sycl/crosstool/../sycl/include/SYCL/vec.h:9437:3: note: candidate template ignored: could not match 'swizzled_vec<float, kElemsRhs, kIndexRhsN...>' against '__attribute__((__vector_size__(4 * sizeof(float)))) float' (vector of 4 'float' values)
  vec(const swizzled_vec<dataT, kElemsRhs, kIndexRhsN...> &rhs) {
  ^
1 error generated.
",0,,3,2017-12-19T12:26:34Z,2018-01-05T15:37:13Z,NONE,2017-12-20T01:05:50Z
15480, sparse_multiclass_hinge_loss() Error,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows/MacOS
- **TensorFlow installed from (source or binary)**:N
- **TensorFlow version (use command below)**:1.4
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:N
- **GCC/Compiler version (if compiling from source)**:N
- **CUDA/cuDNN version**:NA
- **GPU model and memory**:NA
- **Exact command to reproduce**:See below

### Describe the problem
There seems to be a bug in sparse_multiclass_hinge_loss(), as per the example below.

### Source code / logs

```python
import numpy as np
import tensorflow as tf

x = np.random.uniform(0, 1, size = (100, 5))
y = np.random.choice(3, 100) 
y = y.reshape(100, 1)

X = tf.placeholder(""float32"", [None, 5])
Y = tf.placeholder(""int32"", [None, 1])

weights = {'w': tf.Variable(tf.random_uniform([5, 3]))}
biases = {'b': tf.Variable(tf.zeros([3]))}

logits = tf.add(tf.matmul(X, weights['w']), biases['b'])

loss = tf.reduce_mean(tf.contrib.kernel_methods.sparse_multiclass_hinge_loss(logits=logits, labels=Y))

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    res = sess.run(loss, feed_dict={X: x, Y: y}) 


res
```

 make_tensor_proto(values, dtype, shape, verify_shape)
    369   else:
    370     if values is None:
--> 371       raise ValueError(""None values not supported."")
    372     # if dtype is provided, forces numpy array to be the type
    373     # provided if possible.

ValueError: None values not supported.

",1,,7,2017-12-19T11:52:38Z,2017-12-21T19:13:03Z,NONE,2017-12-19T22:46:21Z
15479,Fix issue building memory_stats with opencl,"awaiting testing (then merge),cla: yes","This PR fixes #15477 
This bug exists in at least 1.4, 1.5 and master branch, should I send PR for every branch?",0,,2,2017-12-19T11:51:55Z,2017-12-20T00:05:50Z,CONTRIBUTOR,2017-12-19T22:47:04Z
15478,[XLA/tfcompile] Add Env::CreateUniqueFileName and use it in SaveGraph,"awaiting testing (then merge),cla: yes","Split part of `tensorflow::Env::LocalTempFilename` into `tensorflow::Env::CreateTempFilename` so that it can be used in `SaveGraph`.

This PR is to replace #15335.

I am terrible in creating descriptive function name, better name suggestion for `CreateTempFilename` is welcomed.

/cc @jlebar.

 #15213",0,,6,2017-12-19T10:36:24Z,2017-12-20T20:05:09Z,CONTRIBUTOR,2017-12-20T02:15:31Z
15477,"[BUG] Undeclared error in: ""tensorflow/contrib/memory_stats/kernels/memory_stats_ops.cc""",,"### System information
- **Linux Ubuntu 17.10**:
- **TensorFlow installed from source**:
- **TensorFlow version 1.4.1**:
- **Python version 3.6**: 
- **Bazel version (0.8.1)**:
- **GCC/Compiler version (7.2)**:

### Describe the problem
undeclared error raised in ""tensorflow/contrib/memory_stats/kernels/memory_stats_ops.cc"" when build v1.4.1 with jemalloc, OpenCL

### Source code / logs
ERROR: .../tensorflow/tensorflow/contrib/memory_stats/BUILD:17:1: C++ compilation of rule '//tensorflow/contrib/memory_stats:python/ops/_memory_stats_ops.so' failed (Exit 1)
tensorflow/contrib/memory_stats/kernels/memory_stats_ops.cc:64:5: error: unknown type name '**MaxBytesInUseOp**'; did you mean 'BytesInUseOp'?
    MaxBytesInUseOp);
    ^~~~~~~~~~~~~~~
    BytesInUseOp
./tensorflow/core/framework/op_kernel.h:1209:68: note: expanded from macro 'REGISTER_KERNEL_BUILDER'
  REGISTER_KERNEL_BUILDER_UNIQ_HELPER(__COUNTER__, kernel_builder, __VA_ARGS__)
                                                                   ^
./tensorflow/core/framework/op_kernel.h:1212:53: note: expanded from macro 'REGISTER_KERNEL_BUILDER_UNIQ_HELPER'
  REGISTER_KERNEL_BUILDER_UNIQ(ctr, kernel_builder, __VA_ARGS__)
                                                    ^
./tensorflow/core/framework/op_kernel.h:1225:24: note: expanded from macro 'REGISTER_KERNEL_BUILDER_UNIQ'
            return new __VA_ARGS__(context);                          \
                       ^
tensorflow/contrib/memory_stats/kernels/memory_stats_ops.cc:44:7: note: 'BytesInUseOp' declared here
class BytesInUseOp : public MemoryStatsOp {
      ^
1 error generated.
",0,,1,2017-12-19T10:34:05Z,2017-12-20T00:05:50Z,NONE,2017-12-19T11:23:14Z
15475,[WIP]  Add bazel runfiles manifest support for Windows,cla: yes,"To implement the ideas in:
https://groups.google.com/forum/#!msg/bazel-discuss/Po8xN8dhWkI/sWPUYV9YBAAJ

Now //tensorflow/core:example_example_parser_configuration_test is disabled on Windows, because it cannot find the data files.

testing::RunFileRelocator::GetInstance().Relocate() is the replacement of testing::TensorFlowSrcRoot(). We should mark TensorFlowSrcRoot as deprecated or completely remove it.

Places need to change:
c/c_api_test.cc
cc/saved_model/loader_test.cc
compiler/aot/codegen_test.cc
compiler/xla/service/gpu/llvm_gpu_backend/utils_test.cc
compiler/xla/tests/sample_file_test.cc
contrib/ffmpeg/default/ffmpeg_lib_test.cc
contrib/lite/models/test_utils.h
contrib/lite/testing/generated_examples_zip_test.cc
contrib/session_bundle/bundle_shim_test.cc
contrib/session_bundle/test_util.cc
core/distributed_runtime/rpc/grpc_testlib.cc
core/grappler/costs/graph_properties_test.cc
core/grappler/utils/scc_test.cc
core/kernels/hexagon/graph_transferer_test.cc
core/kernels/spectrogram_test.cc
core/platform/cloud/google_auth_provider_test.cc
core/profiler/internal/tfprof_show_test.cc
core/profiler/internal/tfprof_stats_test.cc
core/profiler/internal/tfprof_tensor_test.cc
core/profiler/internal/tfprof_timeline_test.cc
core/example/example_parser_configuration_test.cc
core/platform/cloud/oauth_client_test.cc

Ref: https://github.com/bazelbuild/bazel/issues/4215

TODO:
1. deal with folders
2. do not translate abs path.

",0,,14,2017-12-19T09:55:24Z,2018-01-03T13:02:48Z,CONTRIBUTOR,2017-12-19T09:57:51Z
15474,"Revert changes in TrainingHelper that cause ""no gradient"" error","awaiting testing (then merge),cla: yes","Change occured in https://github.com/tensorflow/tensorflow/commit/69c324591ba4dfeafb403ee59de56ffe063c1e94

Discussed in https://github.com/tensorflow/tensorflow/issues/15278",0,,7,2017-12-19T09:32:21Z,2017-12-20T00:07:46Z,CONTRIBUTOR,2017-12-19T22:49:18Z
15471,BUG: fix name scope collision in `tf.layers`,"awaiting testing (then merge),cla: yes","Fix  #13429.

Since  #14390 has been merged into master branch, we can easily solve the problem with `auxiliary_name_scope=False`.

### How to test

+ [x] add test case.
+ [x] pass all tests.
",0,,6,2017-12-19T05:41:42Z,2017-12-19T23:31:50Z,CONTRIBUTOR,2017-12-19T20:19:50Z
15466,Add a wrapper for cc_library: tf_cc_library,"cla: yes,stat:awaiting response,stat:awaiting tensorflower",I'll replace every cc_library under //tensorflow with tf_cc_library,0,,14,2017-12-19T03:01:16Z,2017-12-29T01:56:57Z,CONTRIBUTOR,2017-12-20T01:06:28Z
15464,[Feature Request] Sparse compute_gradient,stat:awaiting response,"I am working on an extremely large scale linear model and have been trying to optimize the performance of the TF optimizer.

**Have I written custom code**
Version I.
My feature size is huge (500Mil) and sparse, so I was testing if I could make TF only compute the necessary gradients and apply it using some function like tf.scatter_sub()
My graph is like this:
```
cost = fn(w)
vars_to_update = tf.gather(w, non_zero_indices)
grads = tf.gradients(cost, vars_to_update)
update_op = tf.scatter_sub(w, non_zero_indeces, grads)
```
I found that tf.graidents() always returns None for tf.gather(). Similar condition if I pass tf.gather() to any optimizer like tf.train.GradientDescentOpitmizer(cost, tf.gather(w, indices)), it will throw unsupported error for tf.gather(). 

I was wondering if I did anything wrong or TF just doesn't support sparse gradient computation? If latter does TF team plan to have that implemented in short future?

Version II.
In stead of creating a sparse tensor and do sparse_tensor_dense_matmul(), I also tried using tf.gather() follow by tf.segment_sum() to implement W*X. By doing this the optimizer apparently automatically performed sparse grad computation and sparse update. However, the speed of the optimizer was **horribly slower (15seconds)** than the sparse tensor approach. And idea why?

Pseudo code:
```
active_weights = tf.gather(weights, non_zero_indices)
total = tf.segment_sum(
                tf.reshape(activated_weights, [-1]),
                segment_ids //which is the row number e.g. [0,0,0,0,1,1,1,2,2,2,3,3,...]
                )
update_op = tf.train.GradientDescentOptimizer().minimize(total, active_weights)
```

**OS Platform and Distribution**
Centos Linux version 3.10.0-229.4.2.el7.x86_64 (gcc version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC) )

**TensorFlow installed from (source or binary)**
pip install tensorflow

**TF version**
1.3.0

**Python version**
2.7.5

**Bazel version, CUDA/cuDNN, GPU model and memory, Exact command to reproduce**
N/A

**Also there might be a potential bug**
If in the second approach (tf.gather() and tf.segment_sum()) I replace GradientDescentOptimizer with Adam or Adagrad optimizer, the memory would blow up very quickly. I did not look into why that happened so I am not sure if this worth a bug ticket.",0,,9,2017-12-19T00:42:07Z,2018-01-31T08:10:18Z,CONTRIBUTOR,2017-12-19T13:25:58Z
15461,Updating MKL to the latest release.,"awaiting testing (then merge),cla: yes",,1,,6,2017-12-19T00:17:48Z,2017-12-19T22:00:06Z,CONTRIBUTOR,2017-12-19T00:18:36Z
15460,Branch 179464468,,Push,0,,3,2017-12-18T22:44:38Z,2017-12-19T01:33:34Z,CONTRIBUTOR,2017-12-19T00:11:24Z
15458,Upgrade all TF base images to ubuntu 16.,"awaiting testing (then merge),cla: yes",,0,,9,2017-12-18T21:06:34Z,2018-01-01T06:35:57Z,OWNER,2017-12-22T00:44:59Z
15457,Small fix in the example,"awaiting testing (then merge),cla: no,stat:awaiting response","There's a typo in the example

tflite_modeL -> tflite_model",0,,6,2017-12-18T20:11:34Z,2018-01-20T23:47:00Z,NONE,2017-12-18T21:13:50Z
15453,Update math_ops.py,"awaiting testing (then merge),cla: yes",Corrected documentation of tf.reduce_mean(),0,,2,2017-12-18T17:11:05Z,2017-12-19T22:37:34Z,CONTRIBUTOR,2017-12-18T21:33:08Z
15451,Tensorflow Installation Problem in Anaconda3-5.0.1Environment,,"After installing the Anaconda3-5.0.1 on Ubuntu 17.10, I have followed the following steps to install the Tesnorflow -

$ conda create -n tensorflow python=3.6
$ source activate tensorflow
(tensorflow)$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl
After installing the above pakages, I have verified the above installation in Anaconda environment, following issues are faced -
we6aisol@we6aisol-H170-Gaming-3:$ source activate tensorflow
(tensorflow) we6aisol@we6aisol-H170-Gaming-3:$ python
Python 3.6.3 |Anaconda, Inc.| (default, Nov 20 2017, 20:41:42)
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

import tensorflow as tf
/home/we6aisol/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
return f(*args, **kwds)
Please help me to resolve this issue.

Thanks & Regards
Manoj Bansal",0,,2,2017-12-18T15:06:40Z,2017-12-23T04:44:57Z,NONE,2017-12-23T04:44:57Z
15450,TensorFlow binary was not compiled to use: AVX AVX2 from Java 1.8,,"
![image](https://user-images.githubusercontent.com/12063612/34111971-2b81c42a-e446-11e7-8482-6e3f3867e8d6.png)

![image](https://user-images.githubusercontent.com/12063612/34112074-6ad1b824-e446-11e7-8541-02ae054aefa0.png)

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform from Windows 10
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version 1.4.0
- **Java version** : 1.8.0_144
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 
- **GPU model and memory**: GTX 1060 6G and Memory 8G
- **Exact command to reproduce**: javac

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
""C:\Program Files\Java\jdk1.8.0_144\bin\java"" -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:56173,suspend=y,server=n -javaagent:C:\Users\Administrator\.IntelliJIdea2017.3\system\captureAgent\debugger-agent.jar=C:\Users\Administrator\AppData\Local\Temp\capture.props -Dfile.encoding=UTF-8 -classpath ""C:\Program Files\Java\jdk1.8.0_144\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_144\jre\lib\rt.jar;C:\gitspase\zczdemo\target\classes;E:\maven\Jars\org\tensorflow\tensorflow\1.4.0\tensorflow-1.4.0.jar;E:\maven\Jars\org\tensorflow\libtensorflow\1.4.0\libtensorflow-1.4.0.jar;E:\maven\Jars\org\tensorflow\libtensorflow_jni\1.4.0\libtensorflow_jni-1.4.0.jar;E:\maven\Jars\org\projectlombok\lombok\1.16.18\lombok-1.16.18.jar;C:\Program Files\JetBrains\IntelliJ IDEA 2017.3.1\lib\idea_rt.jar"" com.zcz.tensorflow.zczdemo.HelloTF
Connected to the target VM, address: '127.0.0.1:56173', transport: 'socket'
Hello from 1.4.0
Disconnected from the target VM, address: '127.0.0.1:56173', transport: 'socket'
",0,,10,2017-12-18T14:59:54Z,2017-12-19T20:51:40Z,NONE,2017-12-18T15:15:40Z
15449,Tensorflow-gpu 1.4.1 windows binaries couldn't be found,,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 1709
- **TensorFlow installed from (source or binary)**: looking for the binary
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 3.6.3 64-bit
- **Exact command to reproduce**: ""pip3 install --upgrade tensorflow-gpu""

### Describe the problem

Can't find the binaries for the 1.4.1 windows version",0,,2,2017-12-18T14:56:40Z,2017-12-23T04:25:35Z,NONE,2017-12-18T15:17:24Z
15448,[Feature] Dataset API - Reinitializable Iterator resets to first dataset element,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: slightly altered stock example (see below)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: 1.4.0 from source
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: Nvidia 1060

### Describe the problem
Currently the re-initializable iterator API using .from_structure and Iterator.make_initializer always resets the get_next() op of the iterator to fetch the first element of its Dataset instance again after running `sess.run(training_init_op)` or `sess.run(validation_init_op)`, respectively. 

Is this really the intended behavior? This means, that if you want to switch between training and validation Datasets within one epoch (i.e. in a shorter rhythm than the full dataset length) you will always only iterate over the first `batch_size * number-of-training-steps-before-validation-step` elements of the training set during training. 

### Source code / logs
I guess the code example will make it clearer:
```
# Define training and validation datasets with the same structure.
training_dataset = tf.data.Dataset.range(100)
validation_dataset = tf.data.Dataset.range(50)

# A reinitializable iterator is defined by its structure. We could use the
# `output_types` and `output_shapes` properties of either `training_dataset`
# or `validation_dataset` here, because they are compatible.
iterator = tf.data.Iterator.from_structure(training_dataset.output_types,
                                  training_dataset.output_shapes)

next_element = iterator.get_next()


training_init_op = iterator.make_initializer(training_dataset)
validation_init_op = iterator.make_initializer(validation_dataset)

with tf.Session() as sess:
   # Run 20 epochs in which the training dataset is traversed, followed by the
   # validation dataset.
   for i in range(5):
       # Initialize an iterator over the training dataset.
       print(""#########################  "", i)
       sess.run(training_init_op)
       for _ in range(10):
           nel = sess.run(next_element)
           print(""train: "", type(nel), nel)

       # Initialize an iterator over the validation dataset.
       sess.run(validation_init_op)
       for _ in range(5):
           nel = sess.run(next_element)
           print(""valid: "", type(nel), nel)
```

Produces the output:

```
#########################   0
train:  <class 'numpy.int64'> 0
train:  <class 'numpy.int64'> 1
train:  <class 'numpy.int64'> 2
train:  <class 'numpy.int64'> 3
train:  <class 'numpy.int64'> 4
train:  <class 'numpy.int64'> 5
train:  <class 'numpy.int64'> 6
train:  <class 'numpy.int64'> 7
train:  <class 'numpy.int64'> 8
train:  <class 'numpy.int64'> 9
valid:  <class 'numpy.int64'> 0
valid:  <class 'numpy.int64'> 1
valid:  <class 'numpy.int64'> 2
valid:  <class 'numpy.int64'> 3
valid:  <class 'numpy.int64'> 4
#########################   1
train:  <class 'numpy.int64'> 0
train:  <class 'numpy.int64'> 1
train:  <class 'numpy.int64'> 2
train:  <class 'numpy.int64'> 3
train:  <class 'numpy.int64'> 4
train:  <class 'numpy.int64'> 5
train:  <class 'numpy.int64'> 6
train:  <class 'numpy.int64'> 7
train:  <class 'numpy.int64'> 8
train:  <class 'numpy.int64'> 9
valid:  <class 'numpy.int64'> 0
valid:  <class 'numpy.int64'> 1
valid:  <class 'numpy.int64'> 2
valid:  <class 'numpy.int64'> 3
valid:  <class 'numpy.int64'> 4
...
```
Apparently, the latter 90 elements of the training set and the latter 45 elements of the validation set never get evaluated. I don't really see the real-world use-case for this behavior. 


I know that you can implement the other functionality via the feedable iterator scheme using one_shot_iterators (but not using initializable iterators) as highlighted by the code below (pay attention to the different iterators used for training and validation here):

```
# Define training and validation datasets with the same structure.
training_dataset = tf.data.Dataset.range(10000000).repeat(2)
validation_dataset = tf.data.Dataset.range(5000000).repeat(2)

# A feedable iterator is defined by a handle placeholder and its structure. We
# could use the `output_types` and `output_shapes` properties of either
# `training_dataset` or `validation_dataset` here, because they have
# identical structure.
handle = tf.placeholder(tf.string, shape=[])
iterator = tf.data.Iterator.from_string_handle(
    handle, training_dataset.output_types, training_dataset.output_shapes)
next_element = iterator.get_next()

# You can use feedable iterators with a variety of different kinds of iterator
training_iterator = training_dataset.make_one_shot_iterator()
validation_iterator = validation_dataset.make_initializable_iterator()

with tf.Session() as sess:
    # The `Iterator.string_handle()` method returns a tensor that can be evaluated
    # and used to feed the `handle` placeholder.
    training_handle = sess.run(training_iterator.string_handle())
    validation_handle = sess.run(validation_iterator.string_handle())
    # Loop forever, alternating between training and validation.
    for i in range(5):
        print(""######################## "", i)
        i += 1
        # Run 10 steps using the training dataset. Note that the training dataset is
        # 2 * the original set, i.e. we run 2 epochs (see .repeat() argument), and we resume from where
        # we left off in the previous `while` loop iteration.
        for _ in range(10):
            nel = sess.run(next_element, feed_dict={handle: training_handle})
            print(""train: "", type(nel), nel)

        # Run one pass over the validation dataset.
        sess.run(validation_iterator.initializer)
        for _ in range(5):
            nel = sess.run(next_element, feed_dict={handle: validation_handle})
            print(""valid: "", type(nel), nel)
```

creates output:

```
########################  0
train:  <class 'numpy.int64'> 0
train:  <class 'numpy.int64'> 1
train:  <class 'numpy.int64'> 2
train:  <class 'numpy.int64'> 3
train:  <class 'numpy.int64'> 4
train:  <class 'numpy.int64'> 5
train:  <class 'numpy.int64'> 6
train:  <class 'numpy.int64'> 7
train:  <class 'numpy.int64'> 8
train:  <class 'numpy.int64'> 9
valid:  <class 'numpy.int64'> 0
valid:  <class 'numpy.int64'> 1
valid:  <class 'numpy.int64'> 2
valid:  <class 'numpy.int64'> 3
valid:  <class 'numpy.int64'> 4
########################  1
train:  <class 'numpy.int64'> 10
train:  <class 'numpy.int64'> 11
train:  <class 'numpy.int64'> 12
train:  <class 'numpy.int64'> 13
train:  <class 'numpy.int64'> 14
train:  <class 'numpy.int64'> 15
train:  <class 'numpy.int64'> 16
train:  <class 'numpy.int64'> 17
train:  <class 'numpy.int64'> 18
train:  <class 'numpy.int64'> 19
valid:  <class 'numpy.int64'> 0
valid:  <class 'numpy.int64'> 1
valid:  <class 'numpy.int64'> 2
valid:  <class 'numpy.int64'> 3
valid:  <class 'numpy.int64'> 4
########################  2
train:  <class 'numpy.int64'> 20
train:  <class 'numpy.int64'> 21
train:  <class 'numpy.int64'> 22
train:  <class 'numpy.int64'> 23
train:  <class 'numpy.int64'> 24
train:  <class 'numpy.int64'> 25
train:  <class 'numpy.int64'> 26
train:  <class 'numpy.int64'> 27
train:  <class 'numpy.int64'> 28
train:  <class 'numpy.int64'> 29
valid:  <class 'numpy.int64'> 0
valid:  <class 'numpy.int64'> 1
valid:  <class 'numpy.int64'> 2
valid:  <class 'numpy.int64'> 3
valid:  <class 'numpy.int64'> 4
```",1,,8,2017-12-18T14:49:11Z,2018-01-02T18:44:01Z,NONE,2017-12-18T14:57:26Z
15446,call within the loop,"awaiting testing (then merge),cla: yes",optimize call within the loop,0,,3,2017-12-18T12:55:23Z,2017-12-26T19:21:45Z,CONTRIBUTOR,2017-12-19T02:34:02Z
15444,Fix lib_strings_str_util_test on Windows,"awaiting testing (then merge),cla: yes",,0,,3,2017-12-18T11:41:24Z,2017-12-18T22:27:34Z,CONTRIBUTOR,2017-12-18T17:56:15Z
15443,New metric: Cohen's kappa,"awaiting testing (then merge),cla: yes","resolve #15285.

Add new metric: `cohen_kappa`, which is equivalent to `sklearn.metrics.cohen_kappa_score`(>=0.19), but the implementation doesn't support weighted matrix yet.

Ref:
+ [Cohen's kappa - Wiki](https://en.wikipedia.org/wiki/Cohen's_kappa)
+ [Cohen's kappa: Index of Inter-rater Reliability](http://psych.unl.edu/psycrs/handcomp/hckappa.PDF)
+ [sklearn.metrics.cohen_kappa_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html)

### How to test

+ [x] add test cases.
+ [ ] pass all tests.",0,,8,2017-12-18T11:23:25Z,2017-12-27T02:50:50Z,CONTRIBUTOR,2017-12-19T05:59:21Z
15441,[WIP] Add tf_copts to XLA libraries,"cla: yes,stat:awaiting response","Split from #14531 
Required by #15213
Depends on #15466

To fix a build error in //tensorflow/compiler/xla:util
There is a name conflict in  xla_data.pb.h
```
enum PrimitiveType {
  PRIMITIVE_TYPE_INVALID = 0,
  PRED = 1,
  S8 = 2,
  S16 = 3,
  S32 = 4,
  S64 = 5,
  U8 = 6,
  U16 = 7,
  U32 = 8,
  U64 = 9,
  F16 = 10,
  F32 = 11,
  BF16 = 16,
  F64 = 12,
  C64 = 15,
  TUPLE = 13,
  OPAQUE = 14,
  PrimitiveType_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  PrimitiveType_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
```
However, **OPAQUE** is already defined in wingdi.h as:
```
#define OPAQUE              2
```
So, ""/DNOGDI compiler flag  is a must for who includes this file.

@rongjiecomputer 
@jhseu ",0,,13,2017-12-18T09:48:49Z,2017-12-29T01:50:29Z,CONTRIBUTOR,2017-12-18T23:57:34Z
15439,Add an is_external arg to tf_copts,"awaiting testing (then merge),cla: yes","It's for support build custom ops on Windows
@meteorcloudy This is what we talked last week.",0,,6,2017-12-18T09:04:41Z,2017-12-18T22:10:21Z,CONTRIBUTOR,2017-12-18T09:14:05Z
15436,Simple Recurrent Unit,"awaiting testing (then merge),cla: yes",As per https://github.com/tensorflow/tensorflow/pull/15434#issuecomment-352343576.,0,,10,2017-12-18T07:27:09Z,2017-12-21T18:13:47Z,CONTRIBUTOR,2017-12-18T09:00:35Z
15434,"Revert ""Initial SRU Implementation (#13978)""",cla: yes,This reverts commit e3e2ac9181c42eb82548726d8a250944b56180fd.,0,,2,2017-12-18T05:30:45Z,2017-12-18T06:00:35Z,OWNER,2017-12-18T06:52:20Z
15432,[feature request] Switch to nvidia-docker v2?,"stat:awaiting response,type:build/install","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4.1
- **Python version**: 
3.5.4
- **Bazel version (if compiling from source)**:
None
- **GCC/Compiler version (if compiling from source)**:
None
- **CUDA/cuDNN version**:
None
- **GPU model and memory**:
None
- **Exact command to reproduce**:
None

### Describe the problem
Now we are using nvidia-docker v1 for CI build. We should switch to v2 because v1 is now being deprecated. (https://github.com/NVIDIA/nvidia-docker/wiki/About-version-2.0) 

### Source code / logs

",1,,3,2017-12-18T02:57:44Z,2017-12-18T07:14:27Z,CONTRIBUTOR,2017-12-18T06:24:18Z
15431,Add shape function SingleImageRandomDotStereograms,"awaiting testing (then merge),cla: yes,kokoro:run","This fix tries to address the issue raised in #15429 where there is no shape function for SingleImageRandomDotStereograms.

This fix adds the shape function for `SingleImageRandomDotStereograms`.

NOTE: `SingleImageRandomDotStereograms` takes an attribute of `output_image_shape` which is in the format of `[X, Y, C]` (`[ImageX, ImageY, Channel]`. However, the actual
data output is in the format of `[ImageY, ImageX, Channel]` (`[h, w, c]`). So by default the output_image_shape has the value of [1024, 768, 1] but the output data will be [768, 1024, 1].
And if `[1200, 800, 1]` is used explicitly then the output data shape will be `[800, 1200, 1]`.

This fix does not change the behavior for now.

This fix fixes #15429.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,1,2017-12-18T01:26:47Z,2017-12-26T19:21:21Z,MEMBER,2017-12-26T00:19:07Z
15429,RuntimeError: No C++ shape function registered for standard op: SingleImageRandomDotStereograms,,"single_image_random_dot_stereograms (windows bug)
the function works fine on ubuntu 16.04 / tf 1.4.0 (CPU)

I know, that the support for unsupported libraries in tf.contrib on Windows is incomplete, and left up to the individual contributors. I found some sketchy solutions by editing gen_single_image_random_dot_stereograms_ops.py, but i could not figure it out correctly.


**Source code**
```
import tensorflow as tf
from tensorflow.contrib.image import single_image_random_dot_stereograms
img=[[1,2,3,3,2,1],
     [1,2,3,4,5,2],
     [1,2,3,4,5,3],
     [1,2,3,4,5,4],
     [6,5,4,4,5,5]]
session = tf.InteractiveSession()
sirds = single_image_random_dot_stereograms(
    img,
    convergence_dots_size=8,
    number_colors=256,normalize=True)
out = sirds.eval()
png = tf.image.encode_png(out).eval()
with open('picture_out.png', 'wb') as f:
  f.write(png)
```

**Error**
Windows 10 Pro, 1709
conda version : 4.3.30
python version: 3.6.3
tensorflow-gpu 1.4.0

```
File ""C:\Users\###\Anaconda3\lib\site-packages\tensorflow\python\framework\common_shapes.py"", line 696, in _call_cpp_shape_fn_impl
    ""No C++ shape function registered for standard op: %s"" % op.type)
RuntimeError: No C++ shape function registered for standard op: SingleImageRandomDotStereograms
```

Windows 10 Pro, 1709
conda version : 4.3.30
python version: 3.5.4
tensorflow 1.3.0/tensorflow 1.2.0

`AttributeError: 'NoneType' object has no attribute 'single_image_random_dot_stereograms`
",0,,1,2017-12-17T19:44:34Z,2017-12-26T19:21:21Z,NONE,2017-12-18T01:28:59Z
15427,Fix failing test //tensorflow/python:function_test,cla: yes,"@benoitsteiner It seems `//tensorflow/python:function_test` is failing with my last commit from #13998 (sorry about that).

It looks like the `function_test` created a graph with python through a helper class and invoked tests through c api.

I am not familiar with the `function_test`, though as `ClipByValue` is actually hidden, and is exposed through python only, with the gradient defined in python as well, I think replacing it will fix the issue?

Please take a look. Again, really sorry for the caused inconvenience.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,5,2017-12-17T18:21:03Z,2017-12-18T18:20:54Z,MEMBER,2017-12-17T18:21:44Z
15424,"""Not a valid TensorFlow Graph serialization"" at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph.loadGraph",stat:awaiting response,"Hi,
I build libtensorflow_inference.so & libandroid_tensorflow_inference_java.jar from source, then use them in an android studio project. But when I run
TensorFlowInferenceInterface tflite = new TensorFlowInferenceInterface(assetManager, MODEL_PATH);
the program always crashes and prints
""
Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Invalid GraphDef
at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:551)
at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:105)
......
""
The tflite file in MODEL_PATH is generated successfully by bazel toco from a model graph, and the tflite file is successfully read into byte array. Are there any special restrictions on the graph? What are the possible reasons why function TensorFlowInferenceInterface.loadGraph() throws an Exception?

Thanks a lot.",0,,1,2017-12-17T09:45:49Z,2017-12-18T01:35:20Z,NONE,2017-12-17T18:59:43Z
15423,Can't build with basel Python Configuration Error: --define PYTHON_BIN_PATH,stat:awaiting tensorflower,"Can't get what im doing wrong.
Win7 python 3.6, tensorflow from master, cuda 9.0, cudnn 7.0.5 for cuda 9.0, basel and swig loaded today 

https://github.com/tensorflow/tensorflow/issues/12052

> C:\Users\Andrey\Desktop\tensorflow>bazel clean
> ...........
> INFO: Starting clean (this may take a while). Consider using --async if the clea
> n takes more than several minutes.
> 
> C:\Users\Andrey\Desktop\tensorflow>python configure.py
> WARNING: Running Bazel server needs to be killed, because the startup options ar
> e different.
> You have bazel 0.8.1 installed.
> Please specify the location of python. [Default is C:\Users\Andrey\Anaconda3\pyt
> hon.exe]:
> 
> 
> Found possible Python library paths:
>   C:\Users\Andrey\Anaconda3\lib\site-packages
> Please input the desired Python library path to use.  Default is [C:\Users\Andre
> y\Anaconda3\lib\site-packages]
> 
> Do you wish to build TensorFlow with XLA JIT support? [y/N]: y
> XLA JIT support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with GDR support? [y/N]: y
> GDR support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with VERBS support? [y/N]: y
> VERBS support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with CUDA support? [y/N]: y
> CUDA support will be enabled for TensorFlow.
> 
> Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to d
> efault to CUDA 9.0]:
> 
> 
> Please specify the location where CUDA 9.0 toolkit is installed. Refer to README
> .md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/
> CUDA/v9.0]:
> 
> 
> Please specify the cuDNN version you want to use. [Leave empty to default to cuD
> NN 7.0]:
> 
> 
> Please specify the location where cuDNN 7 library is installed. Refer to README.
> md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/C
> UDA/v9.0]:
> 
> 
> Please specify a list of comma-separated Cuda compute capabilities you want to b
> uild with.
> You can find the compute capability of your device at: https://developer.nvidia.
> com/cuda-gpus.
> Please note that each additional compute capability significantly increases your
>  build time and binary size. [Default is: 3.5,5.2]
> 
> 
> Do you wish to build TensorFlow with MPI support? [y/N]: n
> No MPI support will be enabled for TensorFlow.
> 
> Please specify optimization flags to use during compilation when bazel option ""-
> -config=opt"" is specified [Default is -march=native]:
> 
> 
> Add ""--config=mkl"" to your bazel command to build with MKL support.
> Please note that MKL on MacOS or windows is still not supported.
> If you would like to use a local MKL instead of downloading, please set the envi
> ronment variable ""TF_MKL_ROOT"" every time before build.
> 
> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
>  n
> Not configuring the WORKSPACE for Android builds.
> 
> 
> C:\Users\Andrey\Desktop\tensorflow>bazel build --config=opt --config=win-cuda //
> tensorflow/tools/pip_package:build_pip_package
> ...........
> Loading:
> Loading: 0 packages loaded
> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (3 packages l
> oaded)
> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (24 packages
> loaded)
> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (34 packages
> loaded)
> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (72 packages
> loaded)
> ERROR: C:/users/andrey/desktop/tensorflow/third_party/py/numpy/BUILD:11:1: no su
> ch package '@local_config_python//': Traceback (most recent call last):
>         File ""C:/users/andrey/desktop/tensorflow/third_party/py/python_configure
> .bzl"", line 291
>                 _create_local_python_repository(repository_ctx)
>         File ""C:/users/andrey/desktop/tensorflow/third_party/py/python_configure
> .bzl"", line 251, in _create_local_python_repository
>                 _check_python_bin(repository_ctx, python_bin)
>         File ""C:/users/andrey/desktop/tensorflow/third_party/py/python_configure
> .bzl"", line 204, in _check_python_bin
>                 _fail((""--define %s='%s' is not execut...)))
>         File ""C:/users/andrey/desktop/tensorflow/third_party/py/python_configure
> .bzl"", line 27, in _fail
>                 fail((""%sPython Configuration Error:%...)))
> Python Configuration Error: --define PYTHON_BIN_PATH='C:/Users/Andrey/Anaconda3/
> python.exe' is not executable. Is it the python binary?
>  and referenced by '//third_party/py/numpy:headers'
> Analyzing: target //tensorflow/tools/pip_package:build_pip_package (72 packages
> loaded)
> ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' fai
> led; build aborted: Loading failed
> INFO: Elapsed time: 6,710s
> FAILED: Build did NOT complete successfully (72 packages loaded)",0,,5,2017-12-17T02:51:21Z,2018-01-03T12:28:49Z,NONE,2017-12-17T12:53:04Z
15422,Add common error documentation,"awaiting review,awaiting testing (then merge),cla: yes",See https://github.com/tensorflow/tensorflow/issues/15258,0,,5,2017-12-17T01:12:28Z,2017-12-19T22:14:23Z,CONTRIBUTOR,2017-12-17T01:14:24Z
15420,IteratorGetNext should have a None gradient defined,,"Currently `IteratorGetNext` has no gradient defined. This can cause failures like below in `tf.gradients`. The solution is to define `None` gradient, like the `tf.stop_gradient` op. A work-around when this failure occurs is to wrap dataset ops inside `tf.stop_gradient`

```
  File ""/Users/yaroslav/anaconda/envs/sep22/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 590, in gradients
    (op.name, op.type))
LookupError: No gradient defined for operation 'IteratorGetNext' (op type: IteratorGetNext)

```",0,,1,2017-12-16T23:17:24Z,2017-12-17T00:03:35Z,CONTRIBUTOR,2017-12-17T00:03:35Z
15418,Eager:  `gradients_function` can't compute the gradient for simple functions,comp:eager,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64-bit
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0-dev20171215
- **Python version**: 3.6.3 |Anaconda, Inc.| (default, Nov  8 2017, 15:10:56) [MSC v.1900 64 bit (AMD64)]
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: See description

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

First, define a loss function:

```
import tensorflow as tf
import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()

def loss(w):
    prediction = 2 * w + 1
    true_value = 11
    return tf.cast((true_value - prediction)**2, tf.float32)
```

Then, compute the gradient when w=0.1:

`tfe.gradients_function(loss)(0.1)`

The output is as expected. Next, compute the gradient when w=50:

`tfe.gradients_function(loss)(50)`

The output is:

`[None]`

I expected the output to be 360 because the gradient is -40 + 8 w.",1,,3,2017-12-16T20:55:58Z,2017-12-20T17:58:39Z,CONTRIBUTOR,2017-12-17T01:33:27Z
15417,no protobuf package for macos Python 3.6,stat:awaiting tensorflower,"Following instructions on
https://www.tensorflow.org/install/install_mac

`pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp35-none-macosx_10_11_x86_64.whl
`

This doesn't work for Python 3.6 with error
`protobuf-3.1.0-cp35-none-macosx_10_11_x86_64.whl is not a supported wheel on this platform.
`
If I just change URL cp36, there's no such file

```pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl

  Could not install requirement protobuf==3.1.0 from https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl because of error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl
Could not install requirement protobuf==3.1.0 from https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl because of HTTP error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl for URL https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp36-none-macosx_10_11_x86_64.whl

```",0,,6,2017-12-16T20:33:16Z,2017-12-20T00:50:18Z,CONTRIBUTOR,2017-12-17T07:19:07Z
15415,sparse_multiclass_hinge_loss() error,stat:awaiting response,"Hello, 

I'm getting the error below using `sparse_multiclass_hinge_loss()`. Any hints would be highly appreciated.

```python
import numpy as np
import tensorflow as tf

x = np.random.uniform(0, 1, size = (100, 5))
y = np.random.choice(3, 100) 
y = y.reshape(100, 1)

X = tf.placeholder(""float32"", [None, 5])
Y = tf.placeholder(""int32"", [None, 1])

weights = {'w': tf.Variable(tf.random_uniform([5, 3]))}
biases = {'b': tf.Variable(tf.zeros([3]))}

logits = tf.add(tf.matmul(X, weights['w']), biases['b'])

loss = tf.reduce_mean(tf.contrib.kernel_methods.sparse_multiclass_hinge_loss(logits=logits, labels=Y))

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    res = sess.run(loss, feed_dict={X: x, Y: y}) 


res
```
 make_tensor_proto(values, dtype, shape, verify_shape)
    369   else:
    370     if values is None:
--> 371       raise ValueError(""None values not supported."")
    372     # if dtype is provided, forces numpy array to be the type
    373     # provided if possible.

ValueError: None values not supported.


",0,,2,2017-12-16T12:10:12Z,2017-12-19T11:53:22Z,NONE,2017-12-17T01:20:32Z
15414,Failed to convert a .pb file to a .lite file where there is a custom lite op sin,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04.5 LTS 
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.4.0
- **Python version**: 2.7.6
- **Bazel version (if compiling from source)**: 0.8.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

I followed ""[How to use custom operators](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/custom_operators.md)"" to create a .pb file and ""sin.cc"" in //tensorflow/contrib/lite/kernel/. Then I added ""TfLiteRegistration* Register_SIN();"" and ""AddCustom(""Sin"", Register_SIN());"" in ""register.cc""
But when I used the bazel command to covert the pb file, the sin op can not be converted

Here is the command I used:
bazel build //tensorflow/contrib/lite/toco:toco
bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=tftest/sin.pb --input_format=TENSORFLOW_GRAPHDEF --output_file=tftest/sin.tflite --output_format=TFLITE --inference_type=FLOAT --input_array=input --input_shape=1 --output_array=output
Here is the corresponding ERROE information:
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: Sin.

I tried to use ----allow_custom_ops, but it did not work. Here is the ERROR information:
Converting unsupported operation: Sin

I think I have to modify more files, but I do not know which files I should modify and how to modify. Could you please give a detailed demo?
Thx",0,,3,2017-12-16T08:04:56Z,2018-01-05T06:28:58Z,NONE,2017-12-16T18:57:13Z
15409,MKL: Fixes for concat and elementwise ops,"cla: yes,stat:awaiting response",Fixes concat for Svd test cases and disable 4 elementwise ops to get SSG/VGG-16 model to work with MKL DNN. Disabled tanh due to differences between Tensorflow and DNN primitives.,0,,12,2017-12-15T23:37:18Z,2018-01-03T18:17:54Z,CONTRIBUTOR,2017-12-18T21:46:30Z
15406,MKL: Adding missing reorders in ReLU and AddN,"awaiting testing (then merge),cla: yes",Commit to add missing reorder primitive in ReLU and AddN operators.,0,,2,2017-12-15T21:33:25Z,2017-12-26T19:21:07Z,CONTRIBUTOR,2017-12-26T02:14:17Z
15403,Computing gradients of loop variables return None,,"### Problem description
I got `None` when computing gradient of the two loop variables that are supposed to have gradient..

### Minimum code to reproduce the error
```python
def loop_cond(i, *_):
    with tf.control_dependencies([tf.Print(i, [i])]):
        return i < 5

def loop_body(i, a, b):
    c = tf.gradients(b, a)[0]
    b = b + c
    return i + 1, b,  b ** 2
        
a = tf.constant(3.0)
f_i, f_a, f_b = tf.while_loop(loop_cond, loop_body, [0, a, a])
```

It seems to have failed in the first iteration as there was no output from print statement.

### Complete logs
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-23-a766942a3da2> in <module>()
      9 
     10 a = tf.constant(3.0)
---> 11 f_i, f_a, f_b = tf.while_loop(loop_cond, loop_body, [0, a, a])

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)
   2814     loop_context = WhileContext(parallel_iterations, back_prop, swap_memory)  # pylint: disable=redefined-outer-name
   2815     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)
-> 2816     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
   2817     return result
   2818 

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)
   2638       self.Enter()
   2639       original_body_result, exit_vars = self._BuildLoop(
-> 2640           pred, body, original_loop_vars, loop_vars, shape_invariants)
   2641     finally:
   2642       self.Exit()

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)
   2588         structure=original_loop_vars,
   2589         flat_sequence=vars_for_body_with_tensor_arrays)
-> 2590     body_result = body(*packed_vars_for_body)
   2591     if not nest.is_sequence(body_result):
   2592       body_result = [body_result]

<ipython-input-23-a766942a3da2> in loop_body(i, a, b)
      5 def loop_body(i, a, b):
      6     c = tf.gradients(b, a)[0]
----> 7     b = b + c
      8     return i + 1, b,  b ** 2
      9 

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
    883       if not isinstance(y, sparse_tensor.SparseTensor):
    884         try:
--> 885           y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
    886         except TypeError:
    887           # If the RHS is not a tensor, it might be a tensor aware object

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)
    834       name=name,
    835       preferred_dtype=preferred_dtype,
--> 836       as_ref=False)
    837 
    838 

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
    924 
    925     if ret is None:
--> 926       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    927 
    928     if ret is NotImplemented:

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    227                                          as_ref=False):
    228   _ = as_ref
--> 229   return constant(v, dtype=dtype, name=name)
    230 
    231 

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)
    206   tensor_value.tensor.CopyFrom(
    207       tensor_util.make_tensor_proto(
--> 208           value, dtype=dtype, shape=shape, verify_shape=verify_shape))
    209   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
    210   const_tensor = g.create_op(

~/.conda/envs/conda-local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)
    369   else:
    370     if values is None:
--> 371       raise ValueError(""None values not supported."")
    372     # if dtype is provided, forces numpy array to be the type
    373     # provided if possible.

ValueError: None values not supported.
```

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 7.11 (wheezy)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0-4-g9283868 1.4.0
- **Python version**:  3.5.4
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA",0,,3,2017-12-15T18:27:24Z,2017-12-15T21:00:37Z,NONE,2017-12-15T21:00:37Z
15401,module 'tensorflow.contrib' has no attribute 'lite',"comp:lite,stat:awaiting response","Hello folks.

Everytime I try to run the example fo TOCO:

```
import tensorflow as tf

img = tf.placeholder(name=""img"", dtype=tf.float32, shape=(1, 64, 64, 3))
val = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])
out = tf.identity(val, name=""out"")
with tf.Session() as sess:
  tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [img], [out])
  open(""test.tflite"", ""wb"").write(tflite_modeL)
``` 

I get the error: **module 'tensorflow.contrib' has no attribute 'lite'**

OS Platform and Distribution: Mac OS High Sierra
Tensorflow installed from pip version 1.4.1. 
Python version: 2.7.14 and 3.6.3 :: Anaconda custom (64-bit)

The informations about my system are those:

== cat /etc/issue ===============================================
Darwin Leandros-MacBook-Pro.local 17.3.0 Darwin Kernel Version 17.3.0: Thu Nov  9 18:09:22 PST 2017; root:xnu-4570.31.3~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.2

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.0.0 (clang-900.0.37)
Target: x86_64-apple-darwin17.3.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin Leandros-MacBook-Pro.local 17.3.0 Darwin Kernel Version 17.3.0: Thu Nov  9 18:09:22 PST 2017; root:xnu-4570.31.3~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.13.3)
numpydoc (0.7.0)
protobuf (3.5.0.post1)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.8)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

Seams like tensorflow lite is not available for reason =/

Sorry if this is not a bug and I am just being dumb about how to make TOCO works. 
",0,,3,2017-12-15T18:02:55Z,2017-12-15T22:28:51Z,NONE,2017-12-15T22:19:10Z
15398,Feature request: tf.info to show docstrings in jupyter notebook,,"When working with jupyter notebooks, it is really helpful to see the documentation within the notebook. Numpy has a function called `np.info` which takes a numpy function as argument and prints its docstring. For example, np.info(np.mean) prints the docstring for `np.mean`. It would be really helpful to have an equivalent `tf.info` for those of us who work with jupyter notebooks (and who doesn't?).",0,,2,2017-12-15T14:36:36Z,2017-12-15T22:52:40Z,NONE,2017-12-15T21:21:08Z
15397,Tensorflow c++ memory leak - Valgrind,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 17.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.1
- **GCC/Compiler version (if compiling from source)**: 6.0.3
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Describe the problem**:


I am executing simple tensorflow code to create graph def as shown below

```
tensorflow::NewSession (options, &session)
ReadBinaryProto (tensorflow::Env::Default(), ""/home/ashok/eclipseWorkspace/faceRecognition-x86_64/Data/models/optimized_facenet.pb"", &graph_def));
session->Create (graph_def);
```

But when I run Valgrind as shown below
```
valgrind --leak-check=full --show-leak-kinds=all --vex-guest-max-insns=25 ./faceRecognition-x86_64 -r -i
```
 I get below errors

```
==12366== 16,000 bytes in 1 blocks are still reachable in loss record 47,782 of 47,905
==12366==    at 0x4C2E19F: operator new(unsigned long) (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==12366==    by 0xBF875DC: std::vector<tensorflow::CostModel::MemUsage, std::allocator<tensorflow::CostModel::MemUsage> >::reserve(unsigned long) (in /usr/lib/libtensorflow_cc.so)
==12366==    by 0xBF90128: tensorflow::CostModel::InitFromGraph(tensorflow::Graph const&) (in /usr/lib/libtensorflow_cc.so)
==12366==    by 0xBEE48D3: tensorflow::SimpleGraphExecutionState::InitBaseGraph(tensorflow::BuildGraphOptions const&) (in /usr/lib/libtensorflow_cc.so)
==12366==    by 0xBEE52CF: tensorflow::SimpleGraphExecutionState::MakeForBaseGraph(tensorflow::GraphDef*, tensorflow::SimpleGraphExecutionStateOptions const&, std::unique_ptr<tensorflow::SimpleGraphExecutionState, std::default_delete<tensorflow::SimpleGraphExecutionState> >*) (in /usr/lib/libtensorflow_cc.so)
==12366==    by 0xBE68B9D: tensorflow::DirectSession::MaybeInitializeExecutionState(tensorflow::GraphDef const&, bool*) (in /usr/lib/libtensorflow_cc.so)
==12366==    by 0xBE68CF9: tensorflow::DirectSession::ExtendLocked(tensorflow::GraphDef const&) (in /usr/lib/libtensorflow_cc.so)
==12366==    by 0xBE68FC7: tensorflow::DirectSession::Create(tensorflow::GraphDef const&) (in /usr/lib/libtensorflow_cc.so)
==12366==    by 0x26B899: TensorFlow::initializeRecognition() (in /home/ashok/eclipseWorkspace/faceRecognition-x86_64/Debug/faceRecognition-x86_64)
==12366==    by 0x24197D: RecognitionWithImages::RecognitionWithImages() (in /home/ashok/eclipseWorkspace/faceRecognition-x86_64/Debug/faceRecognition-x86_64)
==12366==    by 0x12F27C: main (in /home/ashok/eclipseWorkspace/faceRecognition-x86_64/Debug/faceRecognition-x86_64)

```
These type of errors are also generated when I do **session -> run ()** 

Due to the above issues, the memory needed to run the program keeps increasing as time passes and the application crashes due to insufficient memory after  a certain point of time.

I have also posted the issue in stack overflow - https://stackoverflow.com/questions/47834054/tensorflow-c-memory-leak-valgrind",0,,1,2017-12-15T14:14:46Z,2017-12-15T21:08:24Z,NONE,2017-12-15T21:08:24Z
15391,tf.nn.leaky_relu does not work with float64,"stat:contributions welcome,type:feature","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 26
- **TensorFlow installed from (source or binary)**: binary, pip3 install tensorflow
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: Python 3.6.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: None
- **GPU model and memory**: Doesn't apply
- **Exact command to reproduce**: See below

### Describe the problem

`tf.nn.leaky_relu` does not work with float64. It only seems to work with float32. I can't think of a reason why it should not work with float64, so I consider this a bug, your mileage might vary. I'm also not familiar enough with tf code to fix it myself without any help. :-)

It seems at least [ops.py#L926](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/framework/ops.py#L926) should be in a `try` block just as [ops.py#L912](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/framework/ops.py#L912)

Also the unit tests only test float32.

### Source code / logs

```
In [1]: import tensorflow as tf
/usr/lib64/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)

In [2]: c = tf.constant(5.0, dtype=tf.float32)

In [3]: lr = tf.nn.leaky_relu(c)

In [4]: c = tf.constant(5.0, dtype=tf.float64)

In [5]: lr = tf.nn.leaky_relu(c)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-5-71e83d060e7b> in <module>()
----> 1 lr = tf.nn.leaky_relu(c)

~/.emacs.d/.python-environments/jedi/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py in leaky_relu(features, alpha, name)
   1541     features = ops.convert_to_tensor(features, name=""features"")
   1542     alpha = ops.convert_to_tensor(alpha, name=""alpha"")
-> 1543     return math_ops.maximum(alpha * features, features)
   1544 
   1545 

~/.emacs.d/.python-environments/jedi/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
    883       if not isinstance(y, sparse_tensor.SparseTensor):
    884         try:
--> 885           y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
    886         except TypeError:
    887           # If the RHS is not a tensor, it might be a tensor aware object

~/.emacs.d/.python-environments/jedi/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)
    834       name=name,
    835       preferred_dtype=preferred_dtype,
--> 836       as_ref=False)
    837 
    838 

~/.emacs.d/.python-environments/jedi/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
    924 
    925     if ret is None:
--> 926       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    927 
    928     if ret is NotImplemented:

~/.emacs.d/.python-environments/jedi/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
    772     raise ValueError(
    773         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r"" %
--> 774         (dtype.name, t.dtype.name, str(t)))
    775   return t
    776 

ValueError: Tensor conversion requested dtype float32 for Tensor with dtype float64: 'Tensor(""Const_1:0"", shape=(), dtype=float64)'
```",0,,4,2017-12-15T10:58:31Z,2018-01-20T23:43:09Z,NONE,2017-12-15T13:11:18Z
15390,Update C API test data comparison for s390x,"awaiting testing (then merge),cla: yes",Base64 encode/decode is not endian-dependent. The hash passed to Base64 will generate different string on big-endian platform. Correcting the test data comparison for s390x architecture. ,0,,3,2017-12-15T09:55:10Z,2017-12-26T19:20:55Z,CONTRIBUTOR,2017-12-20T12:48:53Z
15389,Fatal error while compiling Tensorflow with CUDA 9.1,type:build/install,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-104-generic x86_64)
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
unknown 1.4.0 (Source code is cloned from 798fa36d11119e6fdc13b90a14abfe1805e7de90)
- **Python version**: 
3.6.3
- **Bazel version (if compiling from source)**:
0.8.1
- **GCC/Compiler version (if compiling from source)**:
gcc version 5.4.0 20160609
- **CUDA/cuDNN version**:
CUDA 9.1
cuDNN 7.0.5
- **GPU model and memory**:
2 * Tesla V100-PCIE-16GB
- **Exact command to reproduce**:
See description below.

### Describe the problem
While trying to compile the latest TensorFlow(cloned from 798fa36d11119e6fdc13b90a14abfe1805e7de90), such error will be raised:
```
ERROR: /home/ubuntu/tensorflow/tensorflow/contrib/seq2seq/BUILD:64:1: error while parsing .d file: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/contrib/seq2seq/_objs/python/ops/_beam_search_ops_gpu/tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.pic.d (No such file or directory)
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14:0,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/contrib/seq2seq/kernels/beam_search_ops.h:19,
                 from tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.cc:20:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:59:34: fatal error: math_functions.hpp: No such file or directory
```

It turns out that in CUDA 9.1, `math_functions.hpp` is located at `cuda/include/crt/math_functions.hpp`, rather than `cuda/include/math_functions.hpp` (CUDA 9.0 does), which leads to this error.
`ln -s /usr/local/cuda/include/crt/math_functions.hpp /usr/local/cuda/include/math_functions.hpp` will fix this problem and complete the compiling process.

#### Reference
https://stackoverflow.com/a/47807106/2666624

### Source code / logs
Traceback is available above.
",0,,19,2017-12-15T09:19:39Z,2018-01-30T01:39:42Z,NONE,2017-12-15T22:35:11Z
15388,cannot install with virtualenv python3.6,stat:awaiting response,"Have I written custom code      No
OS Platform and Distribution .  MacOS 10.13.2
TensorFlow installed from        conda  URL of the TensorFlow Python package
TensorFlow version .                1.4
Bazel version                             N/A
CUDA/cuDNN version              N/A
GPU model and memory          N/A
Exact command to reproduce  I just follow Install with conda, My python was installed with conda, I think maybe that is the problem.



virtualenv --system-site-packages -p python3 ~/tensorflow
Running virtualenv with interpreter /usr/local/bin/python3
Using base prefix '/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6'
New python executable in /Users/xinhai/tensorflow/bin/python3.6
Also creating executable in /Users/xinhai/tensorflow/bin/python
Installing setuptools, pip, wheel...
  Complete output from command /Users/xinhai/tensorflow/bin/python3.6 - setuptools pip wheel:
  stringstringstringstringstringstringstringstring
Traceback (most recent call last):
  File ""<stdin>"", line 7, in <module>
  File ""/usr/local/lib/python2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/__init__.py"", line 5, in <module>
  File ""/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/logging/__init__.py"", line 28, in <module>
    from string import Template
ImportError: cannot import name 'Template'



...Installing setuptools, pip, wheel...done.
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/virtualenv.py"", line 2328, in <module>
    main()
  File ""/usr/local/lib/python2.7/site-packages/virtualenv.py"", line 713, in main
    symlink=options.symlink)
  File ""/usr/local/lib/python2.7/site-packages/virtualenv.py"", line 945, in create_environment
    download=download,
  File ""/usr/local/lib/python2.7/site-packages/virtualenv.py"", line 901, in install_wheel
    call_subprocess(cmd, show_stdout=False, extra_env=env, stdin=SCRIPT)
  File ""/usr/local/lib/python2.7/site-packages/virtualenv.py"", line 797, in call_subprocess
    % (cmd_desc, proc.returncode))
OSError: Command /Users/xinhai/tensorflow/bin/python3.6 - setuptools pip wheel failed with error code 1",0,,4,2017-12-15T08:54:21Z,2017-12-16T10:10:21Z,NONE,2017-12-15T18:30:12Z
15383,Feature request: Use placeholders to specify the inputs of TFGAN model.,stat:awaiting response,,0,,3,2017-12-15T06:02:16Z,2017-12-19T06:13:54Z,CONTRIBUTOR,2017-12-15T19:00:07Z
15380,CMake: external package: PIC option not working,,"In many of external cmake files (```/tensorflow/contrib/cmake/external/*.cmake```), it tries to turn PIC on and off with:
```
      CMAKE_CACHE_ARGS
                if(tensorflow_ENABLE_POSITION_INDEPENDENT_CODE)
                        -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=ON
                else()
                        -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=OFF
                endif()
```

However, the result (with cmake 3.5) is not what we wanted in CMakeCache:
```
CMAKE_POSITION_INDEPENDENT_CODE:BOOL=OFF; endif();
```
Where it should be
```
CMAKE_POSITION_INDEPENDENT_CODE:BOOL=ON
```
because ```tensorflow_ENABLE_POSITION_INDEPENDENT_CODE``` is ON.

This can be corrected by:
```
  -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=${tensorflow_ENABLE_POSITION_INDEPENDENT_CODE}
```

I'll send a PR after some testing along with other fixes (e.g., linking to ZLIB as shared object, not static linking)

---- update requested from @tensorflowbutler ----
Have I written custom code: No.
OS Platform and Distribution: PR-tested at Ubuntu-16.04 and Tizen
TensorFlow installed from: N/A
TensorFlow version: github master branch of last week: 00f8b97fc601381546aea89315dee549bdbbbdfc
Bazel version: N/A (doing it without bazel)
CUDA/cuDNN version: Tizen: 9/7 / Ubuntu: 8/6
GPU model and memory: Titan Xp
Exact command to reproduce: N/A (it is about build)",0,,5,2017-12-15T05:30:13Z,2018-01-23T21:40:45Z,CONTRIBUTOR,2017-12-15T12:59:21Z
15376,"With tf-nightly-gpu, getting error: ImportError: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.23' not found",stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: `tf-nightly-gpu` -- I can't import TensorFlow to check the version
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: Cuda 9.0, cuDNN 7.0.4
- **GPU model and memory**: GTX 1080 8GB
- **Exact command to reproduce**:

```
virtualenv --system-site-packages ~/tftest
source ~/tftest/bin/activate
pip install tf-nightly-gpu
python -c 'import tensorflow'
```

### Describe the problem
When I run the commands above, I get the following error:
```
(tftest) reedwm@reedwm2:~$ python -c 'import tensorflow'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/buildtools/current/sitecustomize/sitecustomize.py"", line 152, in SetupPathsAndImport
    return real_import(name, globals, locals, fromlist, level)
  File ""/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 73, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.23' not found (required by /usr/local/google/home/reedwm/tftest/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

This occurs if I `pip install tf-nightly-gpu==1.5.0.dev20171212`, which is the earliest version of `tf-nightly-gpu` it occurs on. When I pip install the previous version with `pip install tf-nightly-gpu==1.5.0.dev20171207`, the issue does not occur.

This issue is similar to #53 and #3127.

/CC @gunan @jhseu @martinwicke, any ideas what the issue could be?
",0,,7,2017-12-15T00:43:20Z,2018-01-08T18:06:53Z,MEMBER,2017-12-15T16:03:53Z
15375,Performance  problem TF VS Keras,,"Hello , 
I just got huge difference in results using Keras (Back-end TensorFlow) and TensorFlow. I want to know if the difference in performances is normal .

The keras model produces a loss of 0.2
`model = k.models.Sequential()
model.add(k.layers.convolutional.Conv2D(64, kernel_size=(3,3), input_shape=(75,75,3)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(k.layers.convolutional.MaxPooling2D(pool_size=(3,3), strides=(2,2)))
model.add(k.layers.Dropout(0.2))
model.add(k.layers.convolutional.Conv2D(128, kernel_size=(3, 3)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(k.layers.Dropout(0.2))
model.add(k.layers.convolutional.Conv2D(128, kernel_size=(3, 3)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(k.layers.Dropout(0.3))
model.add(k.layers.convolutional.Conv2D(64, kernel_size=(3, 3)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(k.layers.Dropout(0.3))
model.add(k.layers.Flatten())
model.add(k.layers.Dense(512))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(k.layers.Dropout(0.2))
model.add(k.layers.Dense(256))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(k.layers.Dropout(0.2))
model.add(k.layers.Dense(1))
model.add(Activation('sigmoid'))
mypotim=Adam(lr=0.01, decay=0.0)
model.compile(loss='binary_crossentropy', optimizer = mypotim, metrics=['accuracy'])`

The TensorFlow normal model produces 0.7 :

`x = tf.placeholder(tf.float32, [None, 75,75,3], name=""DNN_Input"") 
learningRateIn= tf.placeholder(tf.float32)
keep_prob = tf.placeholder(tf.float32)
isTrainPlace=tf.placeholder(tf.bool)
with tf.name_scope('conv_1'):  
    conv_1=tf.layers.conv2d(x,64,[3,3],activation=tf.nn.relu)
    batch_n1 = tf.contrib.layers.batch_norm(conv_1,center=True, scale=True, is_training=isTrainPlace, scope='bn1')
    mpool_1=tf.layers.max_pooling2d(batch_n1,pool_size=(2,2),strides=(2,2))
    dropout_1=tf.layers.dropout(mpool_1,rate=0.8,training=isTrainPlace)
with tf.name_scope('conv_2'):      
    conv_2=tf.layers.conv2d(dropout_1,128,[3,3],activation=tf.nn.relu)
    batch_n2 = tf.contrib.layers.batch_norm(conv_2, center=True, scale=True,is_training=isTrainPlace,scope='bn2')
    mpool_2=tf.layers.max_pooling2d(batch_n2,pool_size=(2,2),strides=(2,2))
    dropout_2=tf.layers.dropout(mpool_2,rate=0.8,training=isTrainPlace)
with tf.name_scope('conv_3'):      
    conv_3=tf.layers.conv2d(dropout_2,128,[3,3],activation=tf.nn.relu)
    batch_n3 = tf.contrib.layers.batch_norm(conv_3, center=True, scale=True,is_training=isTrainPlace,scope='bn3')
    mpool_3=tf.layers.max_pooling2d(batch_n3,pool_size=(2,2),strides=(2,2))
    dropout_3=tf.layers.dropout(mpool_3,rate=0.7,training=isTrainPlace)
with tf.name_scope('conv_4'):     
    conv_4=tf.layers.conv2d(dropout_3,64,[3,3],activation=tf.nn.relu)
    batch_n4 = tf.contrib.layers.batch_norm(conv_4,center=True, scale=True,is_training=isTrainPlace,scope='bn4')
    mpool_4=tf.layers.max_pooling2d(batch_n4,pool_size=(2,2),strides=(2,2))
    dropout_4=tf.layers.dropout(mpool_4,rate=0.7,training=isTrainPlace)
    h4=tf.contrib.layers.flatten(dropout_4)
with tf.name_scope('dense_1'):      
    y_dense_1=tf.layers.dense(h4,512,activation=tf.nn.relu)
    batch_dense_1 = tf.contrib.layers.batch_norm(y_dense_1,center=True, scale=True,is_training=isTrainPlace, scope='bn5')
    dropout_dense_1=tf.layers.dropout(batch_dense_1,rate=0.8,training=isTrainPlace)
with tf.name_scope('dense_2'):      
    y_dense_2=tf.layers.dense(dropout_dense_1,256,activation=tf.nn.relu)
    batch_dense_2 = tf.contrib.layers.batch_norm(y_dense_2, center=True, scale=True, is_training=isTrainPlace,scope='bn6')
    dropout_dense_2=tf.layers.dropout(batch_dense_2 ,rate=0.8, training=isTrainPlace)
    y_estimated=tf.layers.dense(dropout_dense_2,2)  `

PS : the code above , is inspired from : [ https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d](url)
Can anybody help me, please, to undersand , if it's normal or not ? does Keras, uses different tensorflow parameters than the default parameters of tensorflow ?
Thanks in advance.
Toetoe.",0,,1,2017-12-14T23:54:03Z,2017-12-15T01:55:29Z,NONE,2017-12-15T01:55:27Z
15374,tf.matching_files order of returned files,stat:contributions welcome,"As far as I can tell from `https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/file_system.cc`, the order of filenames returned by `tf.matching_files` can be non-determinstic. 

If that is correct, it would be nice if that were stated in the documentation (and also for `Dataset.list_files` and `train.match_filenames_once`).

Even better would be to guarantee alphabetical order, but I am not sure about the performance overhead that would incur.

This would enable to process files given as e.g.
A/1.png, A/2.png, ... and B/1.png, ... jointly by doing to match_files followed by a zip.

Have I written custom code No
OS Platform and Distribution N/A
TensorFlow installed from pip 
TensorFlow version 1.4.1
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce N/A",0,,7,2017-12-14T22:57:06Z,2018-02-01T03:28:02Z,CONTRIBUTOR,2017-12-15T07:28:19Z
15373,GPU memory usage changed from TF 1.3.0 to 1.4.0 - runs out of memory,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code included below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: From pip
- **TensorFlow version (use command below)**: 1.3.0 and 1.4.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: cuda-8.0 cudnn-6.0
- **GPU model and memory**: GTX 1080 8GB
- **Exact command to reproduce**: python <example_script.py>

### Describe the problem
Bug. TensorFlow runs out of GPU memory (ResourceExhaustedError) when using version 1.4.0 when running code that runs fine on version 1.3.0. Please see the following script to reproduce.

### Source code / logs
```
import tensorflow as tf
import tensorflow.contrib.slim.nets as nets
import tensorflow.contrib.slim as slim
from tensorflow.contrib.slim.nets import resnet_v2

kBatchSize = 4
kCropSize = 500
kNumClasses = 10

with tf.device('/gpu:0'):
  images = tf.random_normal([kBatchSize, kCropSize, kCropSize, 3])
  labels = tf.constant(0, dtype=tf.int32, shape=[kBatchSize, kCropSize, kCropSize])

  with slim.arg_scope(resnet_v2.resnet_arg_scope()):
    backbone, end_points = resnet_v2.resnet_v2_101(
        images, None, is_training=True, global_pool=False,
        output_stride=8)

    final_conv = tf.layers.conv2d(backbone, kNumClasses, [1, 1], name='final_conv')
    logits = tf.image.resize_bilinear(final_conv, tf.slice(tf.shape(images), [1], [2]))

  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(
        labels=labels, logits=logits)

optimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)
train_op = slim.learning.create_train_op(loss, optimizer)
slim.learning.train(train_op, '/tmp/resnet')
```
",0,,1,2017-12-14T22:55:32Z,2017-12-15T01:54:55Z,NONE,2017-12-15T01:54:55Z
15371,Standardize arguments in SessionRunHook APIs.,stat:awaiting tensorflower,"Some hooks inheriting from `SessionRunHook` (https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/training/basic_session_run_hooks.py) use different input argument keywords, while implementing the exact same functionality. This should be ironed out.

I wanted to make a PR for this, but I realised this will be backwards incompatible. Still, I think we should standardise this.

E.g.:
`every_secs` (by `SecondOrStepTimer`)
`every_n_secs`  (by `LoggingTensorHook`) [this seems like most descriptive one, to me]
`save_secs` (by `CheckpointSaverHook`)
",0,,2,2017-12-14T21:10:30Z,2017-12-15T16:43:57Z,NONE,2017-12-14T22:18:51Z
15368,Clean bazel `all_files` targets,"awaiting testing (then merge),cla: yes",Broken out of #15166,0,,19,2017-12-14T16:58:32Z,2017-12-26T02:06:23Z,CONTRIBUTOR,2017-12-14T17:17:36Z
15365,Automatic node placement (allocating graph nodes to multiple devices) feature  in distributed tensorflow,,"I read tensorflow white papaer and found node placement which allocates graph nodes to devices without manual configuration.

https://www.reddit.com/r/MachineLearning/comments/4n6a0e/distributed_tensorflow_resource_allocation/

This post says this feature was removed because it did not perform well.
However, it was posted a year ago and I think you are still developing this feature.

Is it included in the current version of tensorflow?
If so, what code do i need to see?
If inot, do you plan to add this feature?
",0,,1,2017-12-14T13:58:56Z,2017-12-14T22:17:10Z,NONE,2017-12-14T22:17:08Z
15362,Fix broken image link in TensorFlow Lite's docs,"awaiting review,awaiting testing (then merge),cla: yes",I fixed the link of image in the same way as other documents in [tensorflow/tensorflow/docs_src/](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/docs_src).,0,,4,2017-12-14T10:51:25Z,2017-12-19T22:01:54Z,CONTRIBUTOR,2017-12-19T22:01:36Z
15361,//tensorflow/python:bfloat16_test and //tensorflow/python:framework_dtypes_test failing on Windows,,"http://ci.tensorflow.org/job/tf-master-win-bzl/2063/console
```
13:00:56 INFO: From Testing //py_test_dir/tensorflow/python:framework_dtypes_test:
13:00:56 ==================== Test output for //py_test_dir/tensorflow/python:framework_dtypes_test:
13:00:56 .........F\\?\C:\tmp\Bazel.runfiles_fnb6t73_\runfiles\org_tensorflow\py_test_dir\tensorflow\python\framework\dtypes_test.py:277: DeprecationWarning: Please use assertEqual instead.
13:00:56   self.assertEquals(dtype.min, np.finfo(numpy_dtype).min)
13:00:56 ......
13:00:56 ======================================================================
13:00:56 FAIL: testIsUnsigned (__main__.TypesTest)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_fnb6t73_\runfiles\org_tensorflow\py_test_dir\tensorflow\python\framework\dtypes_test.py"", line 219, in testIsUnsigned
13:00:56     self.assertEqual(dtypes.as_dtype(""bfloat16"").is_unsigned, False)
13:00:56 AssertionError: True != False
13:00:56 
13:00:56 ----------------------------------------------------------------------
13:00:56 Ran 16 tests in 0.009s
13:00:56 
13:00:56 FAILED (failures=1)
13:00:56 <dtype: 'float32'>: -3.40282e+38 - 3.40282e+38
13:00:56 <dtype: 'float64'>: -1.79769313486e+308 - 1.79769313486e+308
13:00:56 <dtype: 'int32'>: -2147483648 - 2147483647
13:00:56 <dtype: 'uint8'>: 0 - 255
13:00:56 <dtype: 'int16'>: -32768 - 32767
13:00:56 <dtype: 'int8'>: -128 - 127
13:00:56 <dtype: 'int64'>: -9223372036854775808 - 9223372036854775807
13:00:56 <dtype: 'bfloat16'>: 0 - 0
13:00:56 <dtype: 'uint16'>: 0 - 65535
13:00:56 <dtype: 'float16'>: -65504.0 - 65504.0
13:00:56 <dtype: 'uint32'>: 0 - 4294967295
13:00:56 <dtype: 'uint64'>: 0 - 18446744073709551615
13:00:56 <dtype: 'float32_ref'>: -3.40282e+38 - 3.40282e+38
13:00:56 <dtype: 'float64_ref'>: -1.79769313486e+308 - 1.79769313486e+308
13:00:56 <dtype: 'int32_ref'>: -2147483648 - 2147483647
13:00:56 <dtype: 'uint8_ref'>: 0 - 255
13:00:56 <dtype: 'int16_ref'>: -32768 - 32767
13:00:56 <dtype: 'int8_ref'>: -128 - 127
13:00:56 <dtype: 'int64_ref'>: -9223372036854775808 - 9223372036854775807
13:00:56 <dtype: 'bfloat16_ref'>: 0 - 0
13:00:56 <dtype: 'uint16_ref'>: 0 - 65535
13:00:56 <dtype: 'float16_ref'>: -65504.0 - 65504.0
13:00:56 <dtype: 'uint32_ref'>: 0 - 4294967295
13:00:56 <dtype: 'uint64_ref'>: 0 - 18446744073709551615
13:00:56 ================================================================================
13:00:56 INFO: From Testing //py_test_dir/tensorflow/python:bfloat16_test:
13:00:56 ==================== Test output for //py_test_dir/tensorflow/python:bfloat16_test:
13:00:56 FFF.F.FFFFFFFFFFFFFFFF.
13:00:56 ======================================================================
13:00:56 FAIL: testAdd (__main__.Bfloat16NumPyTest)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 189, in testAdd
13:00:56     self.assertAllClose(np.array([[5, 7, 9]]), x + y)
13:00:56   File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\framework\test_util.py"", line 1083, in assertAllClose
13:00:56     self._assertArrayLikeAllClose(a, b, rtol=rtol, atol=atol)
13:00:56   File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\framework\test_util.py"", line 1053, in _assertArrayLikeAllClose
13:00:56     np.testing.assert_allclose(a, b, rtol=rtol, atol=atol, err_msg=msg)
13:00:56   File ""C:\Program Files\Anaconda3\lib\site-packages\numpy\testing\utils.py"", line 1411, in assert_allclose
13:00:56     verbose=verbose, header=header, equal_nan=equal_nan)
13:00:56   File ""C:\Program Files\Anaconda3\lib\site-packages\numpy\testing\utils.py"", line 796, in assert_array_compare
13:00:56     raise AssertionError(msg)
13:00:56 AssertionError: 
13:00:56 Not equal to tolerance rtol=1e-06, atol=1e-06
13:00:56 None
13:00:56 (mismatch 100.0%)
13:00:56  x: array([[5, 7, 9]])
13:00:56  y: array([[ 0.,  0.,  0.]], dtype=float16)
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testArray (__main__.Bfloat16NumPyTest)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 172, in testArray
13:00:56     self.assertEqual(""[[bfloat16(1) bfloat16(2) bfloat16(3)]]"", str(x))
13:00:56 AssertionError: '[[bfloat16(1) bfloat16(2) bfloat16(3)]]' != '[[bfloat16(0) bfloat16(0) bfloat16(0)]]'
13:00:56 - [[bfloat16(1) bfloat16(2) bfloat16(3)]]
13:00:56 ?            ^           ^           ^
13:00:56 + [[bfloat16(0) bfloat16(0) bfloat16(0)]]
13:00:56 ?            ^           ^           ^
13:00:56 
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testCasts (__main__.Bfloat16NumPyTest)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 181, in testCasts
13:00:56     self.assertTrue(np.all(x == y))
13:00:56 AssertionError: False is not true
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testLogSumExp (__main__.Bfloat16NumPyTest)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 196, in testLogSumExp
13:00:56     atol=2e-2)
13:00:56   File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\framework\test_util.py"", line 1083, in assertAllClose
13:00:56     self._assertArrayLikeAllClose(a, b, rtol=rtol, atol=atol)
13:00:56   File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\framework\test_util.py"", line 1053, in _assertArrayLikeAllClose
13:00:56     np.testing.assert_allclose(a, b, rtol=rtol, atol=atol, err_msg=msg)
13:00:56   File ""C:\Program Files\Anaconda3\lib\site-packages\numpy\testing\utils.py"", line 1411, in assert_allclose
13:00:56     verbose=verbose, header=header, equal_nan=equal_nan)
13:00:56   File ""C:\Program Files\Anaconda3\lib\site-packages\numpy\testing\utils.py"", line 796, in assert_array_compare
13:00:56     raise AssertionError(msg)
13:00:56 AssertionError: 
13:00:56 Not equal to tolerance rtol=1e-06, atol=0.02
13:00:56 None
13:00:56 (mismatch 100.0%)
13:00:56  x: array([[ 4.048587,  5.048587,  6.048587]], dtype=float32)
13:00:56  y: array([[ 0.693359,  0.693359,  0.693359]], dtype=float16)
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testAdd (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 89, in testAdd
13:00:56     self._assertFloatIdentical(1, float(bfloat16(1) + bfloat16(0)))
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 48, in _assertFloatIdentical
13:00:56     self.assertEqual(v, w)
13:00:56 AssertionError: 1 != 0.0
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testDiv (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 123, in testDiv
13:00:56     self.assertTrue(math.isnan(float(bfloat16(0) / bfloat16(0))))
13:00:56 AssertionError: False is not true
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testEqual (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 156, in testEqual
13:00:56     self.assertEqual(v == w, bfloat16(v) == bfloat16(w))
13:00:56 AssertionError: False != True
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testGreater (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 146, in testGreater
13:00:56     self.assertEqual(v > w, bfloat16(v) > bfloat16(w))
13:00:56 AssertionError: True != False
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testGreaterEqual (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 151, in testGreaterEqual
13:00:56     self.assertEqual(v >= w, bfloat16(v) >= bfloat16(w))
13:00:56 AssertionError: False != True
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testHash (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 79, in testHash
13:00:56     self.assertEqual(0x3f80, hash(bfloat16(1.0)))
13:00:56 AssertionError: 16256 != 0
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testLess (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 136, in testLess
13:00:56     self.assertEqual(v < w, bfloat16(v) < bfloat16(w))
13:00:56 AssertionError: True != False
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testLessEqual (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 141, in testLessEqual
13:00:56     self.assertEqual(v <= w, bfloat16(v) <= bfloat16(w))
13:00:56 AssertionError: False != True
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testMul (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 114, in testMul
13:00:56     self._assertFloatIdentical(-1, float(bfloat16(1) * bfloat16(-1)))
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 48, in _assertFloatIdentical
13:00:56     self.assertEqual(v, w)
13:00:56 AssertionError: -1 != 0.0
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testNegate (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 85, in testNegate
13:00:56     self._assertFloatIdentical(-v, float(-bfloat16(v)))
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 48, in _assertFloatIdentical
13:00:56     self.assertEqual(v, w)
13:00:56 AssertionError: -0.0 != 4.591774807899561e-41
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testNotEqual (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 161, in testNotEqual
13:00:56     self.assertEqual(v != w, bfloat16(v) != bfloat16(w))
13:00:56 AssertionError: True != False
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testRepr (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 69, in testRepr
13:00:56     self.assertEqual(""bfloat16(1)"", repr(bfloat16(1)))
13:00:56 AssertionError: 'bfloat16(1)' != 'bfloat16(0)'
13:00:56 - bfloat16(1)
13:00:56 ?          ^
13:00:56 + bfloat16(0)
13:00:56 ?          ^
13:00:56 
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testRoundTripToFloat (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 52, in testRoundTripToFloat
13:00:56     self._assertFloatIdentical(v, float(bfloat16(v)))
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 48, in _assertFloatIdentical
13:00:56     self.assertEqual(v, w)
13:00:56 AssertionError: 1.0 != 0.0
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testRoundTripToInt (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 56, in testRoundTripToInt
13:00:56     self.assertEqual(v, int(bfloat16(v)))
13:00:56 AssertionError: -256 != 0
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testStr (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 60, in testStr
13:00:56     self.assertEqual(""1"", str(bfloat16(1.0)))
13:00:56 AssertionError: '1' != '0'
13:00:56 - 1
13:00:56 + 0
13:00:56 
13:00:56 
13:00:56 ======================================================================
13:00:56 FAIL: testSub (__main__.Bfloat16Test)
13:00:56 ----------------------------------------------------------------------
13:00:56 Traceback (most recent call last):
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 101, in testSub
13:00:56     self._assertFloatIdentical(1, float(bfloat16(1) - bfloat16(0)))
13:00:56   File ""\\?\C:\tmp\Bazel.runfiles_5lnmqasq\runfiles\org_tensorflow\py_test_dir\tensorflow\python\lib\core\bfloat16_test.py"", line 48, in _assertFloatIdentical
13:00:56     self.assertEqual(v, w)
13:00:56 AssertionError: 1 != 0.0
13:00:56 
13:00:56 ----------------------------------------------------------------------
13:00:56 Ran 23 tests in 0.014s
13:00:56 
13:00:56 FAILED (failures=20)
13:00:56 not close where =  (array([0, 0, 0], dtype=int64), array([0, 1, 2], dtype=int64))
13:00:56 not close lhs =  [5 7 9]
13:00:56 not close rhs =  [ 0.  0.  0.]
13:00:56 not close dif =  [ 5.  7.  9.]
13:00:56 not close tol =  [  1.01327896e-06   1.01327896e-06   1.01327896e-06]
13:00:56 dtype = int32, shape = (1, 3)
13:00:56 not close where =  (array([0, 0, 0], dtype=int64), array([0, 1, 2], dtype=int64))
13:00:56 not close lhs =  [ 4.04858732  5.04858732  6.04858732]
13:00:56 not close rhs =  [ 0.69335938  0.69335938  0.69335938]
13:00:56 not close dif =  [ 3.35522795  4.35522795  5.35522795]
13:00:56 not close tol =  [ 0.02000427  0.02000427  0.02000427]
13:00:56 dtype = float32, shape = (1, 3)
13:00:56 ================================================================================
```


@gunan ",0,,2,2017-12-14T09:50:57Z,2017-12-14T13:01:40Z,MEMBER,2017-12-14T10:55:21Z
15359,code is jammed when evaluate,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:win10
- **TensorFlow installed from (source or binary)**:pip3
- **TensorFlow version (use command below)**:1.4
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:8.0 6.46
- **GPU model and memory**:2GB
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

the code is jammed when run `eval_results = pc_classifier.evaluate(input_fn = pcd.test_input_fn_np)`
I built model under the guidance of `cnn_mnist.py` . the differences are that I change the network architecture  and using my input_functioin. Everything is normal during the training process, but it jammed during the evaluate process. 
the call stack is:
![default](https://user-images.githubusercontent.com/22407275/33982216-b1cba1e0-e0ea-11e7-85e6-68d8f91457ff.JPG)
and it jammed at the code
`    for hook in self._hooks:
      hook.after_run(
          run_context,
          session_run_hook.SessionRunValues(
              results=outputs[hook] if hook in outputs else None,
              options=options,
              run_metadata=run_metadata))`

what's the problem?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,1,2017-12-14T08:22:43Z,2017-12-14T18:59:35Z,NONE,2017-12-14T18:59:35Z
15355,Dockerfile.devel-gpu: optimize the size of the generated image,cla: yes,"- Use `nvidia/cuda:9.0-base-ubuntu16.04` as the base image to select
  just the CUDA libraries we need.
- Remove the installed static libraries.
- Remove the dependency on openjdk-8 since Bazel ships with a local copy.
- Perform a shallow clone of the repository.

The image is 2.94GB, down from 4.87GB.

Signed-off-by: Felix Abecassis <fabecassis@nvidia.com>

See initial discussion here: https://github.com/tensorflow/tensorflow/issues/15284
@gunan @martinwicke @yongtang ",0,,6,2017-12-14T05:39:52Z,2017-12-14T20:38:56Z,CONTRIBUTOR,2017-12-14T05:40:50Z
15353,[Go]: Make op wrapper generation more robust.,"awaiting testing (then merge),cla: yes","- Since Go 1.8, GOPATH has a default value, so handle
  that (https://golang.org/doc/go1.8#gopath)
- generate.sh expected bash (for the string substitution syntax)
  while 'sh' may point to another shell. So explicitly require bash.",0,,1,2017-12-14T02:09:33Z,2017-12-15T21:51:04Z,MEMBER,2017-12-15T07:09:44Z
15351,questions about shared variables between CPU and GPU ,,"Dear developers:

I looked at cifar10_multi_gpu_train.py, the idea about sharing model params among CPU and GPUs is inspiring. However, I have a few questions that I want to understand well before I can apply to my own problem. 

As far as I can tell, the model params are stored in CPU by looking into the tower_loss() function since cifar10.py explicitly pinned down all variables at ""/cpu:0"". Then function train() wraps up tower_loss() with gpu device like this:

`for i in xrange(FLAGS.num_gpus):`
`       with tf.device('/gpu:%d' % i):`
`           loss = tower_loss(scope)`
`           tf.get_variable_scope().reuse_variables()`

Using this way, I bet model params are stored in CPU and there is no extra copy anywhere because it is set to just reuse the same variables in the scope, while GPU stored gradient operations written in tower_loss(). In the way, I believe the model params have to transfer from CPU to GPU whenever GPU calls for these params to operate upon. It would be inefficient if doing multiple transfer to GPU I believe. I notice ""identity"" operation in the end of tower_loss(). Is ""tf.identity(total_loss)"" doing the trick so CPU transfers model params to the GPU only once, then GPU just holds the local copy from then on?

",0,,1,2017-12-13T23:50:48Z,2017-12-13T23:58:58Z,NONE,2017-12-13T23:58:58Z
15350,Branch 178965261,cla: yes,Fixed a minor merge conflict in tensorflow/core/platform/cloud/gcs_dns_cache.cc,0,,2,2017-12-13T23:29:02Z,2017-12-14T19:12:39Z,CONTRIBUTOR,2017-12-14T01:12:11Z
15347,Object Detection frozen graph issue,,"# Training environment 

### System information
- MAC OSX 10.13.2
- Tensorflow 1.4-rc1 (GPU support)
- Installation through source
- Python 3.6 (Anaconda)
- Bazel 0.8
- CUDA 9/ cuDNN7
- GPU 1080Ti

I have trained my own model for object detection. Extracted graph from checkpoint as well. This graph is working with the Mac GPU and CPU. It is also working with Raspberry PI but when I am trying to run it on AWS EC2 (Deep Learning AMI (Amazon Linux) and Deep Learning AMI (Ubuntu) both) instance. Built tensorflow 1.4-rc1 from source as well as installed using pip but it keep giving me following error:

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1322     try:
-> 1323       return fn(*args)
   1324     except errors.OpError as e:

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1301                                    feed_dict, fetch_list, target_list,
-> 1302                                    status, run_metadata)
   1303 

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    472             compat.as_text(c_api.TF_Message(self.status.status)),
--> 473             c_api.TF_GetCode(self.status.status))
    474     # Delete the underlying status object from memory otherwise it stays alive

InvalidArgumentError: NodeDef mentions attr 'T' not in Op<name=Where; signature=input:bool -> index:int64>; NodeDef: Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Greater). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Greater)]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-25-7493eea60222> in <module>()
     20       (boxes, scores, classes, num) = sess.run(
     21           [detection_boxes, detection_scores, detection_classes, num_detections],
---> 22           feed_dict={image_tensor: image_np_expanded})
     23       # Visualization of the results of a detection.
     24       vis_util.visualize_boxes_and_labels_on_image_array(

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    887     try:
    888       result = self._run(None, fetches, feed_dict, options_ptr,
--> 889                          run_metadata_ptr)
    890       if run_metadata:
    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1119       results = self._do_run(handle, final_targets, final_fetches,
-> 1120                              feed_dict_tensor, options, run_metadata)
   1121     else:
   1122       results = []

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1315     if handle is None:
   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1317                            options, run_metadata)
   1318     else:
   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1334         except KeyError:
   1335           pass
-> 1336       raise type(e)(node_def, op, message)
   1337 
   1338   def _extend_graph(self):

InvalidArgumentError: NodeDef mentions attr 'T' not in Op<name=Where; signature=input:bool -> index:int64>; NodeDef: Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Greater). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Greater)]]

Caused by op 'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Where', defined at:
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py"", line 3, in <module>
    app.launch_new_instance()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 478, in start
    self.io_loop.start()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 281, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 232, in dispatch_shell
    handler(stream, idents, msg)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 397, in execute_request
    user_expressions, allow_stdin)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2728, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2850, in run_ast_nodes
    if self.run_code(code, result):
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-21-0d8b8f2357e8>"", line 7, in <module>
    tf.import_graph_def(od_graph_def, name='')
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 313, in import_graph_def
    op_def=op_def)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): NodeDef mentions attr 'T' not in Op<name=Where; signature=input:bool -> index:int64>; NodeDef: Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Greater). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Greater)]]
```

When I am extracting frozen graph on EC2 instance from the same previous checkpoint it gives me much lower accuracy than my MAC and raspberryPI",0,,2,2017-12-13T19:45:53Z,2017-12-13T23:58:35Z,NONE,2017-12-13T23:58:35Z
15345,Using wrong location for x86_64 android build,"stat:awaiting tensorflower,type:bug/performance","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
A: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
A: OSX 10.13.1
- **TensorFlow installed from (source or binary)**:
A: Source
- **TensorFlow version (use command below)**:
A: 1.4.1
- **Python version**: 
A: 2.7
- **Bazel version (if compiling from source)**:
A: 0.8
- **GCC/Compiler version (if compiling from source)**:
A:
```
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1
Apple LLVM version 9.0.0 (clang-900.0.38)
Target: x86_64-apple-darwin17.2.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
```

- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
`make -f tensorflow/contrib/makefile/Makefile TARGET=ANDROID ANDROID_ARCH=x86_64`

### Describe the problem
Android x86_64 build fails with Makefile using make -f tensorflow/contrib/makefile/Makefile TARGET=ANDROID ANDROID_ARCH=x86_64 because it cannot find the binary `x86-64-linux-android-g++`

It can be fixed by changing the `tensorflow/contrib/makefile/Makefile` at line 303 from 
`BIN_PREFIX := x86-64-linux-android` to
`BIN_PREFIX := x86_64-linux-android`
",1,,2,2017-12-13T18:38:09Z,2017-12-13T22:58:31Z,NONE,2017-12-13T19:14:56Z
15344,Fix broken link in tensorflow lite readme,cla: yes,,0,,4,2017-12-13T18:21:36Z,2017-12-19T00:27:41Z,CONTRIBUTOR,2017-12-13T18:24:27Z
15343,Iterator on cached tf.data Dataset cannot be reinitialized ,"stat:awaiting response,type:bug/performance","Found a likely bug when trying to use a reinitializable iterator to read from two cached datasets, one for validation and one for training. The iterator can however only be initialized once per cached dataset. Seems to me like the iterator should remove the lock file when being reinitialized, it is not in my case and that is why I get this issue. Here's a minimal example with only one cached dataset.

(basic system information below)

### Example
```python
import os

import numpy as np
import tensorflow as tf


data = np.random.rand(10, 3).astype(np.float32)
dataset = tf.data.Dataset.from_tensor_slices(data)
batches = dataset.shuffle(10).repeat().batch(5)

config = tf.ConfigProto(device_count = {'GPU': 0})
sess = tf.Session(config=config)

cache_dir = os.path.join(os.getcwd(), 'cache_dir')
try:
    os.makedirs(cache_dir)
except OSError:
    print('Cache directory already exists')

cached = batches.cache(os.path.join(cache_dir, 'cache'))
iterator = tf.data.Iterator.from_structure(output_types=tf.float32, output_shapes=(5, 3))
batch = iterator.get_next()

init1 = iterator.make_initializer(cached)
init2 = iterator.make_initializer(batches)

sess.run(init1)
sess.run(batch)
```
> array([[ 0.11960778,  0.3081578 ,  0.96522039],
       [ 0.90339011,  0.12458269,  0.30650312],
       [ 0.58160347,  0.55877644,  0.50363588],
       [ 0.2350398 ,  0.33509603,  0.4165386 ],
       [ 0.76757395,  0.50134581,  0.93601096]], dtype=float32)

```python
sess.run(init2)
sess.run(batch)
```
> array([[ 0.76757395,  0.50134581,  0.93601096],
       [ 0.2350398 ,  0.33509603,  0.4165386 ],
       [ 0.90339011,  0.12458269,  0.30650312],
       [ 0.13266359,  0.82675195,  0.26691398],
       [ 0.58160347,  0.55877644,  0.50363588]], dtype=float32)

```python
sess.run(init1)
sess.run(batch)
```
> AlreadyExistsError (see above for traceback): There appears to be a concurrent caching iterator running - cache lockfile already exists ('/home/ubuntu/ai_notebooks/notebooks/projects/deep-purple/cache_dir/cache.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1513187725
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[5,3]], output_types=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]


### Sytem information
Tensorflow version: v1.4.0-rc1-11-g130a514 1.4.0 (installed from pip)
Python version: 3.5.2
OS: Linux Ubuntu 16.04.3
CUDA: 8.0.61
cuDNN: 6",1,,9,2017-12-13T18:15:07Z,2017-12-14T19:52:24Z,NONE,2017-12-13T18:41:41Z
15342,Windows Environment and TF 1.4.1 - Unavailable through PyPI,stat:awaiting response,"Hello dear Tensorflowers,

When running the following code `pip install tensorflow==1.4.1`, I obtain the following error:

```
Could not find a version that satisfies the requirement tensorflow==1.4.1 (from versions: 1.2.0rc2, 1.2.0, 1.2.1, 1.3.0rc0, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.4.0rc0, 1.4.0rc1, 1.4.0)
No matching distribution found for tensorflow==1.4.1
```

This error seem logical because the _wheel_ file does not exist for the windows distribution: 

- TF 1.4.1 - No Windows Compiled Library: https://pypi.python.org/pypi/tensorflow/1.4.1
- TF 1.4.0 - Windows Compiled Library is present: https://pypi.python.org/pypi/tensorflow/1.4.0

Is the support for the windows platform dropped ? Or maybe some compilation pipeline broke somewhere.

Thanks for your help,

Jonathan",0,,5,2017-12-13T16:09:02Z,2017-12-15T09:51:53Z,CONTRIBUTOR,2017-12-14T01:32:54Z
15341,Feature Request: enable rechanging tf.device of a tensor,stat:awaiting tensorflower,"It seems that once a tensor's GPU was defined using tf.device, the GPU cannot be changed anymore. 
When loading saved graphs, the graph will use the same GPU that was chosen years ago and it cannot be set again.

thanks.",0,,8,2017-12-13T15:01:38Z,2018-01-30T20:45:09Z,NONE,2017-12-13T19:17:37Z
15339,opencv cannot read frame from video with tensorflow,,"I am using tensorflow r1.4 and opencv3.1 in ubuntu14.04.
When I include #include <tensorflow/core/public/session.h> or 
#include ""tensorflow/cc/ops/standard_ops.h"" I cannot read images from cv::VideoCapture adn I got empty mat. When I didn't include these tensorflow headers, I can read frame successfully. Anyone could help me? Thanks a lot!!!
I noticed other issues like [#1924](https://github.com/tensorflow/tensorflow/issues/1924?from=singlemessage) [#6496](https://github.com/tensorflow/tensorflow/issues/6496) but got no idea.

Here is my cpp file:
#include <tensorflow/core/platform/env.h>
#include <tensorflow/core/public/session.h>
#include ""tensorflow/cc/ops/standard_ops.h""
#include <opencv2/opencv.hpp>
#include <iostream>
using namespace std;
using namespace tensorflow;

int main()
{
cv::VideoCapture cap;
if(!cap.open(""/home/kx/project/RM-dataset/01.avi"")){
std::cout<<""cannot open video ""<<std::endl;
}
cv::Mat frame;
while(1){
cap>>frame;
if(frame.empty()){
std::cout<<""no frame""<<std::endl;
continue;
}
cv::imshow(""frame"",frame);
cv::waitKey(0);
}
return 0;
}

my cmake file:

cmake_minimum_required (VERSION 2.8)
project (tf_example)

set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -g -std=c++11 -W"")
find_package(OpenCV 3.1.0 REQUIRED)
include_directories(
/home/kx/something/tensorflow-r1.4
/home/kx/something/tensorflow-r1.4/tensorflow/bazel-genfiles
/home/kx/something/tensorflow-r1.4/tensorflow/contrib/makefile/gen/protobuf/include
/home/kx/something/tensorflow-r1.4/tensorflow/contrib/makefile/gen/host_obj
/home/kx/something/tensorflow-r1.4/tensorflow/contrib/makefile/gen/proto
/home/kx/something/tensorflow-r1.4/tensorflow/contrib/makefile/downloads/nsync/public
/home/kx/something/tensorflow-r1.4/tensorflow/contrib/makefile/downloads/eigen
/home/kx/something/tensorflow-r1.4/bazel-out/local-py3-opt/genfiles
${OPENCV_INCLUDE_DIRS}
)

add_executable(tf_test tf_test.cpp)
target_link_libraries(tf_test
/home/kx/something/tensorflow-r1.4/bazel-bin/tensorflow/libtensorflow_cc.so
/home/kx/something/tensorflow-r1.4/bazel-bin/tensorflow/libtensorflow_framework.so
${OpenCV_LIBS}
)

The results:
no frame

",0,,2,2017-12-13T13:15:26Z,2017-12-13T19:18:09Z,NONE,2017-12-13T19:18:09Z
15335,[XLA/tfcompile] Implement mkstemps for MSVC,"awaiting review,cla: yes","`mkstemps` used in `SaveGraph` is not available on Windows.

Implementation adapted from https://github.com/git-for-windows/git/blob/master/wrapper.c#L470.",0,,8,2017-12-13T08:28:16Z,2017-12-20T23:27:48Z,CONTRIBUTOR,2017-12-18T03:22:44Z
15334,Quantized graph on ssd mobilenet fails with InvalidArgumentError,stat:awaiting response,"I am using ssd_mobilenet_v1_coco_2017_11_17 and quantized it using following command:

bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
  --in_graph=ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb \
  --out_graph=ssd_mobilenet_v1_coco_2017_11_17/frozen_quant.pb \
  --inputs='image_tensor' \
  --outputs='detection_boxes,detection_scores,detection_classes' \
  --transforms='add_default_attributes strip_unused_nodes(type=float, shape=""1,299,299,3"")
    remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true)
    fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes
    strip_unused_nodes sort_by_execution_order'

I am using tensorflow v 1.4.1 for detecting bounding box and it throws following error:

InvalidArgumentError: The node 'Preprocessor/map/while/ResizeImage/ResizeBilinear/eightbit' has inputs from different frames. The input 'Preprocessor/map/while/ResizeImage/size' is in frame 'Preprocessor/map/while/while_context'. The input 'Preprocessor/map/while/ResizeImage/ResizeBilinear_eightbit/Preprocessor/map/while/ResizeImage/ExpandDims/quantize' is in frame ''.",0,,3,2017-12-13T08:24:37Z,2018-01-24T08:52:28Z,NONE,2017-12-13T19:02:03Z
15331,Graph building and optimization is really slow,,"I'm using tensorflow 1.4 and tflearn 0.3.2. The os of my machine is Win 8.1. Apparently, the graph building and cost optimization in my code is real slow which I think is a bug in Tensorflow (either that or my machine is just real slow). Here's the code:

```
class Model(object):
    def __init__(self):
        self.num_classes = 80
        self.num_time_steps = 1596
        self.input_dimension = 48
        self.inputs = network_utils.input_data([None, self.num_time_steps, self.input_dimension], name=""input"")
        self.labels = network_utils.sparse_input_data()
        self.seq_lens = network_utils.input_data([None], name=""seq_len"", input_type=network_utils.get_type('int32'))
        self.learning_rate = 0.01

    def _inference(self):
        model = network_utils.bidirectional_lstm(self.inputs, 50, return_seq=True)
        model = network_utils.bidirectional_lstm(model, 100, return_seq=True)
        model = network_utils.bidirectional_lstm(model, 200)
        logits = network_utils.get_time_major(model, self.num_classes, network_utils.get_shape(self.inputs)[0], 200)
        return logits

    def loss(self):
        y_predict = self._inference()
        loss = network_utils.ctc_loss(predictions=y_predict, labels=self.labels, sequence_length=self.seq_lens)
        cost = network_utils.cost(loss)
        decoded = network_utils.decode(inputs=y_predict, sequence_length=self.seq_lens)
        label_error_rate = network_utils.label_error_rate(y_pred=decoded[0], y_true=self.labels)
        return loss, label_error_rate, cost

    def optimize(self, cost, optimizer):
        return network_utils.optimize(loss=cost, optimizer=optimizer, learning_rate=self.learning_rate)
```

```
import tensorflow as tf
import tflearn
from tflearn import bidirectional_rnn, BasicLSTMCell

from optimizer_enum import Optimizers

def ctc_loss(predictions, labels, sequence_length,
             preprocess_collapse_repeated_labels=True,
             ctc_merge_repeated=True,
             inputs_are_time_major=True):
    return tf.nn.ctc_loss(inputs=predictions, labels=labels, sequence_length=sequence_length,
                          preprocess_collapse_repeated=preprocess_collapse_repeated_labels,
                          ctc_merge_repeated=ctc_merge_repeated,
                          time_major=inputs_are_time_major)

def input_data(shape, name: str = 'InputData', input_type=tf.float32):
    return tflearn.input_data(shape=shape, dtype=input_type, name=name)

def reshape(tensor: tf.Tensor, new_shape: list):
    return tf.reshape(tensor, new_shape, name=""reshape"")

def bidirectional_lstm(inputs, num_hidden: int, return_seq=False):
    return bidirectional_rnn(inputs, BasicLSTMCell(num_hidden), BasicLSTMCell(num_hidden), return_seq=return_seq)


def decode(inputs, sequence_length, merge_repeated=True):
    decoded, _ = tf.nn.ctc_beam_search_decoder(inputs, sequence_length, merge_repeated)
    return decoded

def label_error_rate(y_pred, y_true):
    return tf.reduce_mean(tf.edit_distance(tf.cast(y_pred, tf.int32), y_true))

def optimize(loss, optimizer, learning_rate):
    if optimizer == Optimizers.MOMENTUM:
        return tf.train.MomentumOptimizer(learning_rate, momentum=0.9).minimize(loss)
    if optimizer == Optimizers.ADAM:
        return tf.train.AdamOptimizer(learning_rate).minimize(loss)
    if optimizer == Optimizers.ADADELTA:
        return tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)
    if optimizer == Optimizers.RMSPROP:
        return tf.train.RMSPropOptimizer(learning_rate).minimize(loss)
    raise NotImplementedError(""{} is not implemented."".format(optimizer))

def sparse_input_data(input_type=tf.int32):
    return tf.sparse_placeholder(input_type)

def get_time_major(model, num_classes, batch_size, num_hidden_units):
    outputs = reshape(model, [-1, num_hidden_units])

    W = tf.Variable(tf.truncated_normal([num_hidden_units,
                                         num_classes],
                                        stddev=0.1, dtype=tf.float32), name='W')
    b = tf.Variable(tf.constant(0., dtype=tf.float32, shape=[num_classes], name='b'))

    logits = tf.matmul(outputs, W) + b
    logits = tf.reshape(logits, [batch_size, -1, num_classes])
    logits = tf.transpose(logits, (1, 0, 2))
    return logits

def cost(loss):
    return tf.reduce_mean(loss)


def get_type(type_str):
    if type_str == 'int32':
        return tf.int32
    return tf.float32


def get_shape(tensor):
    return tf.shape(tensor)
```

Can anyone help me address this issue? To reproduce it, you can run main/train.py in this [repository](https://github.com/selcouthlyBlue/simplified_bi_lstm_ocr)",0,,1,2017-12-13T05:26:31Z,2017-12-13T06:50:06Z,CONTRIBUTOR,2017-12-13T06:50:06Z
15330,Fix issues in doc `tf.Placeholder` should be `tf.placeholder`,cla: yes,"This fix fixes issues in the doc (data_feeder.py) where `tf.Placeholder` should be `tf.placeholder`
",0,,1,2017-12-13T03:47:59Z,2017-12-17T01:05:00Z,MEMBER,2017-12-13T16:49:30Z
15327,Feature Request: Easy way to predict after training model with Estimator and Dataset API,,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Windows 10
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: Python 3.5.2 :: Anaconda custom (64-bit)
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None

### Describe the problem

I have beed trained a image classification cnn model with the Estimator and Dataset(`tf.data.TFRecordsDataset`) API. The relative model files have bee saved in `model_dir`. The last few days I try very hard to figure out how to predict one or more images label using the saved model files. 

However I failed and don't know what to do. I can't find relative contents in the official doc. So adding an easy method to do this may be a good idea. Or is there another solution I missed?

FYI, my training code is [here](https://github.com/secsilm/understaing-datasets-estimators-tfrecords/blob/master/cifar10-estimator-dataset.py).",0,,5,2017-12-13T01:45:45Z,2017-12-13T19:29:21Z,CONTRIBUTOR,2017-12-13T07:49:45Z
15325,Minor documentation mistake,,"https://www.tensorflow.org/programmers_guide/tensors
Under the ""Getting a tf.Tensor object's rank"" subheading
`r = tf.rank(my3d)` should be `r = tf.rank(my_image)`",1,,1,2017-12-12T23:22:56Z,2017-12-18T19:56:11Z,NONE,2017-12-13T00:02:05Z
15324,Make GANEstimator global_step_inc dependent on gen and dis losses,"awaiting review,cla: yes","This solves a non-deterministic problem that prevents to use `global_step` in `tf.cond` during training.
See also https://github.com/tensorflow/tensorflow/issues/15271#issuecomment-351212183",0,,8,2017-12-12T22:40:46Z,2017-12-22T09:51:06Z,CONTRIBUTOR,2017-12-13T12:38:18Z
15323,Beam Search Decoder API,type:feature,"@ebrevdo Why is it that the user needs to call `tile_batch` explicitly for beam search decoders when using attention models? Couldn't the beam search decoder internally tile the provided `initial_state` in its constructor? It seems that this API is prone to wrong usage so I'm trying to understand why it's necessary.

Thank you!",0,,5,2017-12-12T21:45:03Z,2018-01-30T01:55:50Z,CONTRIBUTOR,2018-01-03T07:41:34Z
15320,Multi-core CPU performance dropped for MKL TF build,type:bug/performance,"### System information

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS (64-bit)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: Tensorflow r1.4
- **Python version**: Python version: 2.7.12
- **Bazel version (if compiling from source)**: Bazel release 0.7.0
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
- **CUDA/cuDNN version**: no CUDA
- **GPU model and memory**: no GPU, but i7-6850K with 32Gb ddr4
- **Exact command to reproduce**: run the script below

Tested on two machines:
1) i7-6850K with 32Gb ddr4
2) two Xeon x5650 with 24Gb ddr3

### Describe the problem
When I build Tensorflow with MKL it dropped CPU performance in a strange way. Performance of individual core is much higher, but for multicore is much worse.
It's a big epic bottleneck for my project and I can't solve it by myself. I will appreciate any help!

1) TF installation from sources with MKL support
> Tensorflow r1.4 installed from source. Configured with jemalloc as malloc support and other configure settings ignored.
> $ bazel build --config=mkl -c opt //tensorflow/tools/pip_package:build_pip_package
> $ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
> $ pip install /tmp/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27mu-linux_x86_64.whl

**Run tests:** 
one core: 0.03s
all cores: 0.12s

2) TF installation with pip
> $ pip install tensorflow
> (tensorflow-1.4.1-cp27-cp27mu-manylinux1_x86_64.whl installed)

**Run tests:**
one core: 0.16s
all cores: 0.03s


### Source code / logs

```
    import time
    import numpy as np
    import tensorflow as tf

    from tensorflow.contrib import slim
    from tensorflow.contrib.slim.python.slim.nets.inception_v1 import inception_v1, inception_v1_arg_scope


    input_shape = (1, 224, 224, 3)
    features = tf.placeholder(tf.float32, input_shape)

    with slim.arg_scope(inception_v1_arg_scope()):
        predictions, end_points = inception_v1(features, is_training=False)

    # remove to utilize all cores
    session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,
                                  inter_op_parallelism_threads=1)

    with tf.Session(config=session_conf) as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())

        images = np.random.random(input_shape)
        consumption = []
        for i in range(10):
            tick = time.time()
            sess.run(predictions, feed_dict={features: images})
            consumption.append(time.time() - tick)

        print np.mean(consumption)
```",1,,13,2017-12-12T19:43:31Z,2018-01-31T21:28:18Z,NONE,2017-12-13T07:30:56Z
15319,Cannot import tensorflow after installing tensorflow-gpu - Windows,,"I am running windows 10 64bit. 
Tensorflow-gpu version - 1.4.0
CUDA version - 8.0
cuDNN - v6.0

I installed it using the cmd command: `pip install tensorflow-gpu ` and do not have any other version of tensorflow installed. now when I try to execute 

```
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
```
I get the error `ModuleNotFoundError: No module named 'tensorflow'`

I am hoping getting this working will solve my bigger issue of tensorflow not recognizing my gpu for gpu processing. ",0,,2,2017-12-12T18:58:42Z,2017-12-12T22:12:56Z,NONE,2017-12-12T22:07:55Z
15318,tensorflow multigpu with dataset api is not converging,,"
I am trying to train a resNet50  on multi_gpus,
I have used 
[https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py)](url)



and I have added tensorflow Dataset input pipeline to the link code
But after some steps loss is some value and does not changes any more!(value =0.693147 always)

I have tested this network with dataset API on single gpu,and worked fine.
Im sure something is wrong with my data feeding,because when I feed a simple random nd_array,it converges!!!!!

I have tested my tfrecord file,it was OK .
but I dont know the problem.

I use
ubuntu 16.04,

tensorflow 1.4,

python 2.7.12,

gtx 1080 Gpus

this is all my train code:





[github.txt](https://github.com/tensorflow/tensorflow/files/1552082/github.txt)
",0,,4,2017-12-12T15:23:58Z,2017-12-13T01:25:07Z,NONE,2017-12-13T01:25:07Z
15316,Exclude tests from contrib_py,cla: yes,@gunan @mrry Don't you think?,0,,2,2017-12-12T14:05:53Z,2017-12-12T18:04:46Z,CONTRIBUTOR,2017-12-12T15:54:44Z
15315,Refactor methods for path calculation,"awaiting testing (then merge),cla: yes","This allows other scripts to access this logic.
@gunan This is what I meant.",0,,38,2017-12-12T13:56:10Z,2017-12-22T18:25:07Z,CONTRIBUTOR,2017-12-13T16:32:56Z
15314,tensorflow.python.framework.errors_impl.NotFoundError: Can not get size for:,,"D:\Python\Python35\models-master\research\object_detection>D:\Python\Python35\python train.py --logtostderr --train_dir=training\ --pipline_config_path=training\ssd_mobilenet_v1_pets.config
Traceback (most recent call last):
  File ""train.py"", line 163, in <module>
    tf.app.run()
  File ""C:\Users\USER\AppData\Roaming\Python\Python35\site-packages\tensorflow\python\platform\app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""train.py"", line 106, in main
    overwrite=True)
  File ""C:\Users\USER\AppData\Roaming\Python\Python35\site-packages\tensorflow\python\lib\io\file_io.py"", line 384, in copy
    compat.as_bytes(oldpath), compat.as_bytes(newpath), overwrite, status)
  File ""D:\Python\Python35\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""C:\Users\USER\AppData\Roaming\Python\Python35\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Can not get size for:  : path d\udc92acc\udce8s sp\udce9cifi\udce9 not found",0,,8,2017-12-12T13:43:35Z,2018-01-31T19:35:54Z,NONE,2017-12-13T01:26:34Z
15312,iOS build_all_ios_ssd.sh 'double-conversion/double-conversion.h' file not found,stat:awaiting response,"iOS `build_all_ios_ssd.sh` `'double-conversion/double-conversion.h' file not found`

```

export TF_ROOT=$(~/tensorflow-master)


cd ~/tensorflow_ios_detector/config
bash config.sh

export TF_ROOT=~/tensorflow-master
cd $TF_ROOT
tensorflow/contrib/makefile/build_all_ios_ssd.sh

gcc --std=c++11 -I. -I/Users/admin/tensorflow-master/tensorflow/contrib/makefile/downloads/ -I/Users/admin/tensorflow-master/tensorflow/contrib/makefile/downloads/eigen -I/Users/admin/tensorflow-master/tensorflow/contrib/makefile/downloads/gemmlowp -I/Users/admin/tensorflow-master/tensorflow/contrib/makefile/downloads/nsync/public -I/Users/admin/tensorflow-master/tensorflow/contrib/makefile/downloads/fft2d -I/Users/admin/tensorflow-master/tensorflow/contrib/makefile/gen/host_obj/ -I/Users/admin/tensorflow-master/tensorflow/contrib/makefile/gen/protobuf-host/include -I/usr/local/include -c tensorflow/core/lib/random/simple_philox.cc -o /Users/admin/tensorflow-master/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/lib/random/simple_philox.o
tensorflow/core/lib/strings/numbers.cc:26:10: fatal error: 
      'double-conversion/double-conversion.h' file not found
#include ""double-conversion/double-conversion.h""
         ^
1 error generated.
make: *** [/Users/admin/tensorflow-master/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/lib/strings/numbers.o] Error 1
make: *** Waiting for unfinished jobs....
+ '[' 2 -ne 0 ']'
+ echo 'arm64 compilation failed.'
arm64 compilation failed.
+ exit 1
Mac-Admin:tensorflow-master admin$ 

```
but script downloaded `double-conversion` in `downloads` folder located at `tensorflow/contrib/makefile`
",0,,3,2017-12-12T13:33:18Z,2017-12-18T12:53:45Z,NONE,2017-12-13T01:35:59Z
15311,Support python3.6 Linux,,Python3.6 support Linux,0,,2,2017-12-12T13:02:00Z,2017-12-12T23:09:45Z,CONTRIBUTOR,2017-12-12T13:26:57Z
15310,[XLA/tfcompile] Various fixes for MSVC Part 1,cla: yes,"This is the initial work to solve #15213. More changes will come in other PRs.

- `tensorflow/compiler/aot/tests/make_test_graphs.py`: Use `os.path` for path manipulation.

- `tensorflow/compiler/xla/array.h`: Explicitly include `<numeric>` for `std::accumulate`.

- `tensorflow/compiler/xla/service/cpu/external_constant_pool.{cc,h}`: Use `tensorflow::port::Aligned{Malloc,Free}`.

- `tensorflow/compiler/xla/service/cpu/llvm_ir_runtime.cc`: Fix `std::array` initialization.

- `tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc`: Remove unused `<dlfcn.h>` and polyfill `sincos[f]` for MSVC.

- `tensorflow/compiler/xla/service/heap_simulator.h`: Add `const` to custom comparator.

- `tensorflow/compiler/xla/status_macros.h`: Use simplified version of `TF_ASSIGN_OR_RETURN` for MSVC. The simplified version also works for GCC as well actually.

- `tensorflow/compiler/xla/util.h`: Hide the workaround for GCC 7 from MSVC's eyes.

- `tensorflow/core/lib/gtl/compactptrset.h`: Define `ssize_t` for MSVC.

- `tensorflow/core/platform/macros.h`: Define `LANG_CXX11` for >= VS 2015. In MSVC, `__cplusplus == 199711` even when `/std:c++latest` is set because MSVC is still not ""fully"" C++11 compliant.",0,,3,2017-12-12T12:39:35Z,2017-12-20T08:42:46Z,CONTRIBUTOR,2017-12-13T10:05:20Z
15309,Correct channels_first format in 1d_pooling for tf.layers,"awaiting testing (then merge),cla: yes","The code was equivalent in the case of self.data_format == 'channels_last' or self.data_format == 'channels_first'.
My modifications fix it when data_format == 'channels_first'.",1,,11,2017-12-12T12:10:03Z,2018-01-24T19:01:44Z,NONE,2018-01-24T13:17:45Z
15308,improve compute high rank hessians,"awaiting testing (then merge),cla: yes",fix possible compute high rank hessians,0,,13,2017-12-12T12:03:42Z,2018-01-01T00:02:51Z,CONTRIBUTOR,2017-12-12T12:19:52Z
15307,Setting proper sonames on Linux,"awaiting testing (then merge),cla: yes","Setting proper soname prevents from linking with absolute path when using `cmake`.

The reference below contains long conversation about the issue with linking, but the general idea is when a library has no `DT_SONAME` field, executable is linked with absolute path and can't be used in a different environment, setting `LD_LIBRARY_PATH` doesn't help.

http://cmake.3232098.n2.nabble.com/How-to-avoid-the-explicit-library-location-when-linking-with-imported-library-targets-td5542269.html",1,,6,2017-12-12T11:33:48Z,2018-01-24T19:24:01Z,CONTRIBUTOR,2017-12-26T01:58:32Z
15306,GO Tests Fail for Tensorflow 1.4.0 but example code works on amd64 and arm64,stat:awaiting response,"### System information
- ~**Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**~:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04 on amd64 and arm64
- **TensorFlow installed from (source or binary)**: amd64: binary (https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.4.0.tar.gz); arch64: source (d752244f) `//tensorflow:libtensorflow.so`
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: python3.5
- **Bazel version (if compiling from source)**:  arm64: 0.7.0
- **GCC/Compiler version (if compiling from source)**: arm64: 5.4.0-6ubuntu1~16.04.5
- **CUDA/cuDNN version**: amd64: cuda-8.0 | libcudnn.so.6
- **GPU model and memory**: amd64: GeForce GTX 1060 6071MiB
- **Exact command to reproduce**: `go test github.com/tensorflow/tensorflow/tensorflow/go`

### Describe the problem
Test fail but [example code](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go#example-package) work on both platforms. 

### Source code / logs
amd64:
```
2017-12-12 11:37:29.916413: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2
2017-12-12 11:37:29.918709: F tensorflow/core/framework/tensor.cc:822] Unexpected type: 23
SIGABRT: abort
PC=0x7f9b403b4428 m=4 sigcode=18446744073709551610
signal arrived during cgo execution

goroutine 29 [syscall, locked to thread]:
runtime.cgocall(0x657730, 0xc4200479d0, 0xc4200479f8)
	/usr/local/go/src/runtime/cgocall.go:132 +0xe4 fp=0xc4200479a0 sp=0xc420047960 pc=0x405574
github.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_SetAttrTensor(0x7f9b0800c380, 0x7f9b0800e190, 0x7f9b0800c840, 0x7f9b0800dc80)
	github.com/tensorflow/tensorflow/tensorflow/go/_test/_obj_test/_cgo_gotypes.go:919 +0x45 fp=0xc4200479d0 sp=0xc4200479a0 pc=0x52d725
github.com/tensorflow/tensorflow/tensorflow/go.setAttr.func18(0x7f9b0800c380, 0x7f9b0800e190, 0x7f9b0800c840, 0x7f9b0800dc80)
	/home/meldron/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:306 +0xec fp=0xc420047a08 sp=0xc4200479d0 pc=0x539cdc
github.com/tensorflow/tensorflow/tensorflow/go.setAttr(0x7f9b0800c380, 0xc42000e0c0, 0x6f01be, 0x5, 0x6b70c0, 0xc4200ec4c0, 0x0, 0x0)
	/home/meldron/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:306 +0x10f1 fp=0xc420047c00 sp=0xc420047a08 pc=0x530481
github.com/tensorflow/tensorflow/tensorflow/go.(*Graph).AddOperation(0xc42000e080, 0x6f0024, 0x5, 0xc42050a3b8, 0x6, 0x0, 0x0, 0x0, 0xc420080f00, 0x4b393b, ...)
	/home/meldron/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:209 +0x4a0 fp=0xc420047d60 sp=0xc420047c00 pc=0x52f190
github.com/tensorflow/tensorflow/tensorflow/go.Const(0xc42000e080, 0xc42050a3b8, 0x6, 0x683760, 0xc4200ec300, 0xc42050a3b8, 0x6, 0x4d499d, 0x7abe18)
	/home/meldron/go/src/github.com/tensorflow/tensorflow/tensorflow/go/util_test.go:38 +0x221 fp=0xc420047e38 sp=0xc420047d60 pc=0x52a781
github.com/tensorflow/tensorflow/tensorflow/go.TestOutputDataTypeAndShape.func1(0xc420102780)
	/home/meldron/go/src/github.com/tensorflow/tensorflow/tensorflow/go/operation_test.go:137 +0x11e fp=0xc420047fa8 sp=0xc420047e38 pc=0x53708e
testing.tRunner(0xc420102780, 0xc4200ec480)
	/usr/local/go/src/testing/testing.go:746 +0xd0 fp=0xc420047fd0 sp=0xc420047fa8 pc=0x4d4a40
runtime.goexit()
	/usr/local/go/src/runtime/asm_amd64.s:2337 +0x1 fp=0xc420047fd8 sp=0xc420047fd0 pc=0x45fa31
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:789 +0x2de

goroutine 1 [chan receive]:
testing.(*T).Run(0xc420102000, 0x6f62b2, 0x1a, 0x703770, 0x47b401)
	/usr/local/go/src/testing/testing.go:790 +0x2fc
testing.runTests.func1(0xc420102000)
	/usr/local/go/src/testing/testing.go:1004 +0x64
testing.tRunner(0xc420102000, 0xc420057de0)
	/usr/local/go/src/testing/testing.go:746 +0xd0
testing.runTests(0xc4200ec220, 0xa423e0, 0x11, 0x11, 0xc420057e78)
	/usr/local/go/src/testing/testing.go:1002 +0x2d8
testing.(*M).Run(0xc420057f18, 0xc420057f70)
	/usr/local/go/src/testing/testing.go:921 +0x111
main.main()
	github.com/tensorflow/tensorflow/tensorflow/go/_test/_testmain.go:84 +0xdb

goroutine 25 [chan receive]:
testing.(*T).Run(0xc4201023c0, 0xc4200145a0, 0x13, 0xc4200ec480, 0x2)
	/usr/local/go/src/testing/testing.go:790 +0x2fc
github.com/tensorflow/tensorflow/tensorflow/go.TestOutputDataTypeAndShape(0xc4201023c0)
	/home/meldron/go/src/github.com/tensorflow/tensorflow/tensorflow/go/operation_test.go:136 +0x56e
testing.tRunner(0xc4201023c0, 0x703770)
	/usr/local/go/src/testing/testing.go:746 +0xd0
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:789 +0x2de

rax    0x0
rbx    0x7f9b1fbfca30
rcx    0x7f9b403b4428
rdx    0x6
rdi    0x20a0
rsi    0x20a3
rbp    0x7f9b1fbfca20
rsp    0x7f9b1fbfc8e8
r8     0x7f9b0800eb80
r9     0x0
r10    0x8
r11    0x206
r12    0x7f9b1fbfcc50
r13    0x17
r14    0x5
r15    0x7f9b1fbfcc50
rip    0x7f9b403b4428
rflags 0x206
cs     0x33
fs     0x0
gs     0x0
FAIL	github.com/tensorflow/tensorflow/tensorflow/go	0.059s
```

arm64:
```
2017-12-12 11:40:20.357262: F tensorflow/core/framework/tensor.cc:822] Unexpected type: 23
SIGABRT: abort
PC=0x7f7a686528 m=0 sigcode=18446744073709551610
signal arrived during cgo execution

goroutine 56 [syscall, locked to thread]:
runtime.cgocall(0x5e9a48, 0x442003d9d8, 0x29)
	/usr/local/go/src/runtime/cgocall.go:132 +0xa0 fp=0x442003d9a0 sp=0x442003d960 pc=0x405280
github.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_SetAttrTensor(0xa2bac10, 0xa24bfa0, 0x9fcbb50, 0xa289620)
	github.com/tensorflow/tensorflow/tensorflow/go/_test/_obj_test/_cgo_gotypes.go:919 +0x38 fp=0x442003d9d0 sp=0x442003d9a0 pc=0x508468
github.com/tensorflow/tensorflow/tensorflow/go.setAttr.func18(0xa2bac10, 0xa24bfa0, 0x9fcbb50, 0xa289620)
	/home/rock64/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:306 +0xa4 fp=0x442003da00 sp=0x442003d9d0 pc=0x511e04
github.com/tensorflow/tensorflow/tensorflow/go.setAttr(0xa2bac10, 0x442008a0c0, 0x6809f9, 0x5, 0x648200, 0x4420106480, 0x0, 0x0)
	/home/rock64/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:306 +0xd7c fp=0x442003dc00 sp=0x442003da00 pc=0x50aa3c
github.com/tensorflow/tensorflow/tensorflow/go.(*Graph).AddOperation(0x442008a088, 0x680864, 0x5, 0x4420086728, 0x6, 0x0, 0x0, 0x0, 0x442007ef00, 0x4a1f7c, ...)
	/home/rock64/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:209 +0x3b8 fp=0x442003dd60 sp=0x442003dc00 pc=0x509b38
github.com/tensorflow/tensorflow/tensorflow/go.Const(0x442008a088, 0x4420086728, 0x6, 0x615100, 0x44201062c0, 0x4420086728, 0x6, 0x445a01, 0x80)
	/home/rock64/go/src/github.com/tensorflow/tensorflow/tensorflow/go/util_test.go:38 +0x190 fp=0x442003de30 sp=0x442003dd60 pc=0x505a90
github.com/tensorflow/tensorflow/tensorflow/go.TestOutputDataTypeAndShape.func1(0x442011e780)
	/home/rock64/go/src/github.com/tensorflow/tensorflow/tensorflow/go/operation_test.go:137 +0xc4 fp=0x442003dfa0 sp=0x442003de30 pc=0x50fd44
testing.tRunner(0x442011e780, 0x4420106440)
	/usr/local/go/src/testing/testing.go:746 +0xb0 fp=0x442003dfc0 sp=0x442003dfa0 pc=0x4bd240
runtime.goexit()
	/usr/local/go/src/runtime/asm_arm64.s:931 +0x4 fp=0x442003dfc0 sp=0x442003dfc0 pc=0x456a44
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:789 +0x244

goroutine 1 [chan receive]:
testing.(*T).Run(0x442011e000, 0x686c26, 0x1a, 0x694188, 0x5a2fb201)
	/usr/local/go/src/testing/testing.go:790 +0x258
testing.runTests.func1(0x442011e000)
	/usr/local/go/src/testing/testing.go:1004 +0x54
testing.tRunner(0x442011e000, 0x4420051dd0)
	/usr/local/go/src/testing/testing.go:746 +0xb0
testing.runTests(0x44201061c0, 0x7d8300, 0x11, 0x11, 0x3e7)
	/usr/local/go/src/testing/testing.go:1002 +0x280
testing.(*M).Run(0x4420051f18, 0x42d01c)
	/usr/local/go/src/testing/testing.go:921 +0xf0
main.main()
	github.com/tensorflow/tensorflow/tensorflow/go/_test/_testmain.go:84 +0xd0

goroutine 52 [chan receive]:
testing.(*T).Run(0x442011e3c0, 0x44200c02c0, 0x13, 0x4420106440, 0x2)
	/usr/local/go/src/testing/testing.go:790 +0x258
github.com/tensorflow/tensorflow/tensorflow/go.TestOutputDataTypeAndShape(0x442011e3c0)
	/home/rock64/go/src/github.com/tensorflow/tensorflow/tensorflow/go/operation_test.go:136 +0x540
testing.tRunner(0x442011e3c0, 0x694188)
	/usr/local/go/src/testing/testing.go:746 +0xb0
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:789 +0x244

r0      0x0
r1      0x363f
r2      0x6
r3      0x7f7c729000
r4      0x363f
r5      0x7f7c7296f0
r6      0x0
r7      0x0
r8      0x83
r9      0x9fa59e0
r10     0x7fdb6f6760
r11     0x7fdb6f6760
r12     0xa3d70a3d70a3d70b
r13     0x7fdb6f6706
r14     0x0
r15     0x1db
r16     0x7f7a639120
r17     0x7f7a687830
r18     0x14
r19     0x7f7a797000
r20     0x7f7c729000
r21     0x7f7a7979d8
r22     0xa322700
r23     0x5
r24     0xa2bac10
r25     0x0
r26     0x6941e0
r27     0x10
r28     0x80b700
r29     0x7fdb6f66e0
lr      0x7f7a6879e0
sp      0x7fdb6f66e0
pc      0x7f7a686528
fault   0x0
FAIL	github.com/tensorflow/tensorflow/tensorflow/go	0.261s
```",0,,3,2017-12-12T10:41:30Z,2018-01-03T08:12:37Z,NONE,2017-12-13T01:58:34Z
15303,Fix initialization of tf.contrib.layers.spatial_softmax temperature,"awaiting testing (then merge),cla: yes",It's now possible to initialize a trainable `temperature` with values other than 1,0,,3,2017-12-12T09:07:58Z,2017-12-31T22:21:58Z,CONTRIBUTOR,2017-12-29T01:02:33Z
15302,Fix #15297: bfloat16 is unsigned on Windows,cla: yes,"#15297 

__BYTE_ORDER__ is not a builtin macro in VC++.  Need to include ""tensorflow/core/platform/cpu_info.h"" before using it. ",0,,8,2017-12-12T08:05:53Z,2017-12-18T06:05:29Z,CONTRIBUTOR,2017-12-12T08:35:54Z
15301,"Removing extra ""d"" after close() method in SessionTest.java","awaiting testing (then merge),cla: yes",,0,,7,2017-12-12T07:57:21Z,2017-12-19T22:03:18Z,CONTRIBUTOR,2017-12-12T07:57:42Z
15300,Documentation - Fixing 'if' spelling,cla: yes,Fixing 'if' spelling,0,,5,2017-12-12T07:30:03Z,2017-12-12T10:50:09Z,CONTRIBUTOR,2017-12-12T07:34:08Z
15299,Model Average Optimizer,"awaiting testing (then merge),cla: yes",I have implemented model average optimizer and open a new pr since the [original one](https://github.com/tensorflow/tensorflow/pull/11581) is closed. ,0,,11,2017-12-12T07:26:09Z,2018-01-20T23:39:20Z,CONTRIBUTOR,2017-12-12T07:30:50Z
15295,Add name scope to tf.image,"awaiting testing (then merge),cla: yes","This PR fixes #1560 
I searched through the tf.image API list and found the following APIs need to add name scope

> crop_to_bounding_box
> pad_to_bounding_box
> flip_left_right
> random_flip_left_right
> flip_up_down
> random_flip_up_down
> per_image_standardization
> central_crop
> resize_images
> resize_image_with_crop_or_pad
> transpose_image

@martinwicke Thanks for quick reply before and could you please take a look?

- [x] Add name scope to above functions
- [x] Add test case(s)

RELNOTES: Add name scopes to tf.image functions.",0,,6,2017-12-12T05:32:02Z,2017-12-28T18:55:40Z,CONTRIBUTOR,2017-12-26T17:00:05Z
15291,Dockerfile.devel-gpu: infinite prompt loop,type:build/install,"```sh
$ docker build -f Dockerfile.devel-gpu github.com/tensorflow/tensorflow.git#master:tensorflow/tools/docker
[...]
Step 19/22 : RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 &&     LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs:${LD_LIBRARY_PATH}     tensorflow/tools/ci_build/builds/configured GPU     bazel build -c opt --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""         tensorflow/tools/pip_package:build_pip_package &&     rm /usr/local/cuda/lib64/stubs/libcuda.so.1 &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip --no-cache-dir install --upgrade /tmp/pip/tensorflow-*.whl &&     rm -rf /tmp/pip &&     rm -rf /root/.cache
 ---> Running in 3e8560e2d441
/tensorflow /tensorflow
INFO: Reading 'startup' options from /etc/bazel.bazelrc: --batch
Extracting Bazel installation...
You have bazel 0.5.4 installed.
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]
Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL support? [y/N]: No OpenCL support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 

Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

Invalid path to CUDA 8.0 toolkit. /usr/local/cuda/lib64/libcudart.so.8.0 cannot be found
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 

Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

Invalid path to CUDA 8.0 toolkit. /usr/local/cuda/lib64/libcudart.so.8.0 cannot be found
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 

Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

Invalid path to CUDA 8.0 toolkit. /usr/local/cuda/lib64/libcudart.so.8.0 cannot be found
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 

Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

Invalid path to CUDA 8.0 toolkit. /usr/local/cuda/lib64/libcudart.so.8.0 cannot be found
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 

Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

Invalid path to CUDA 8.0 toolkit. /usr/local/cuda/lib64/libcudart.so.8.0 cannot be found
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 

Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

Invalid path to CUDA 8.0 toolkit. /usr/local/cuda/lib64/libcudart.so.8.0 cannot be found
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 

Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

[... ad infinitum ... ]
```

Is it because the `1.4` branch doesn't support CUDA 9.0?
https://github.com/tensorflow/tensorflow/blob/abd5375ba8d373045321d1eebdb4501c36ab0ccd/tensorflow/tools/docker/Dockerfile.devel-gpu#L74-L76

@gunan",0,,7,2017-12-12T02:16:44Z,2017-12-13T23:55:58Z,CONTRIBUTOR,2017-12-12T06:51:24Z
15288,Eager: eager mode considerably slower than standard TensorFlow for large matrix multiplications ,"comp:eager,stat:awaiting response","We have been benchmarking eager mode versus standard TensorFlow for large square matrix multiplications, specifically the time to run

m = tf.matmul(A, B) (in eager mode) 

versus 

m = sess.run(self.c, feed_dict={self.A:A, self.B:B})

in non-eager mode. 

We find that while runtimes are comparable for small matrices, eager mode is considerably slower for repeated multiplications of large matrices (eg, of dimension 15,000). The first multiplication is fast, but subsequent multiplications take much longer, even after resetting the computation graph. Is this expected behavior? We are running everything on a GPU. 
",0,,8,2017-12-12T01:46:28Z,2018-01-17T19:22:14Z,NONE,2017-12-13T01:33:59Z
15287,Use base_dtype for self._dtype in tf.layers,cla: yes,"This avoids mismatch dtype (ref vs no_ref) when using variables as inputs to a layer.
See #15262",0,,3,2017-12-12T01:23:28Z,2017-12-15T09:52:50Z,CONTRIBUTOR,2017-12-14T17:30:05Z
15286,Branch 178689056,cla: yes,,0,,1,2017-12-12T01:02:50Z,2017-12-12T03:11:58Z,CONTRIBUTOR,2017-12-12T01:46:08Z
15285,Add kappa coefficient as a new metric?,"stat:contributions welcome,type:feature","Currently tensorflow supports using accuracy as a metric for model's performance.

However, for unbalanced datasets, kappa coefficient (https://www.wikiwand.com/en/Cohen%27s_kappa#) is a commonly used metric. Would it be possible to add this one in the model.metrics?",0,,3,2017-12-12T00:42:48Z,2017-12-27T02:50:50Z,NONE,2017-12-12T01:24:02Z
15283,Fix minor typo in CUDNN_VERSION check,"awaiting testing (then merge),cla: yes","Effectively enables CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED in
CudnnSupport::GetConvolveBackwardFilterAlgorithms() for cuDNN v5.1.",0,,3,2017-12-11T23:10:06Z,2017-12-19T22:00:32Z,CONTRIBUTOR,2017-12-19T21:10:29Z
15282,Fixed memory_stats_ops_test,"awaiting testing (then merge),cla: yes",Added explicit dependency to avoid matrix free prior to stats op execution.,0,,2,2017-12-11T21:04:22Z,2017-12-19T22:36:24Z,CONTRIBUTOR,2017-12-19T21:23:35Z
15281,Remove redundant dependencies.,"cla: yes,pending merge internally",Looks like they got added back during merge. #15136,0,,1,2017-12-11T19:42:16Z,2017-12-14T19:11:51Z,MEMBER,2017-12-11T21:21:03Z
15280,Missing dlcose()/FreeLibrary() after dlopen()/LoadLibrary(),stat:awaiting response,"Have I written custom code: N/A
 OS Platform and Distribution: N/A
 TensorFlow installed from: N/A
 TensorFlow version: N/A
 Bazel version: N/A
 CUDA/cuDNN version: N/A
 GPU model and memory: N/A
 Exact command to reproduce: N/A

Problem description:
A. Looking at code below, there are couple of issues:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/nnapi/NeuralNetworksShim.h#L34-L59
   1.There is no dlcose() call after dlopen() and dysym() in the code above.
   2.There can be two successful getLibraryHandle() calls without dlclose() in loadFunction().

B. More generally, the code below shows there is no interface to unload DLL either.
https://github.com/tensorflow/tensorflow/blob/359d6f9716c0bb9bd8201ce600da98b0481a8049/tensorflow/core/platform/env.h#L254-L280",0,,4,2017-12-11T19:28:27Z,2018-01-10T03:08:17Z,NONE,2017-12-12T07:27:47Z
15277,[XLA] Make the client_test able to be disabled using a manifest file,"awaiting testing (then merge),cla: yes","This change allows these 3 tests in the XLA ClientTest to be selectively disabled using the manifest mechanism.

",0,,2,2017-12-11T15:32:39Z,2017-12-27T02:47:54Z,CONTRIBUTOR,2017-12-26T01:23:28Z
15276,[XLA] Fix OS/X compile error with std::transform and  std::addressof,"awaiting review,cla: yes","I don't know if this has been flagged up elsewhere...

on my platform (OS/X, XLA), I receive the following error when trying to compile tensorflow/compiler/xla/service/llvm_ir/kernel_support_library.cc:

```
tensorflow/compiler/xla/service/llvm_ir/kernel_support_library.cc:101:5: error: no matching function for call to 'transform'
    std::transform(function->arg_begin(), function->arg_end(),
    ^~~~~~~~~~~~~~
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/algorithm:1922:1: note: candidate template ignored: couldn't infer template argument '_UnaryOperation'
transform(_InputIterator __first, _InputIterator __last, _OutputIterator __result, _UnaryOperation __op)
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/algorithm:1932:1: note: candidate function template not viable: requires 5 arguments, but 4 were provided
transform(_InputIterator1 __first1, _InputIterator1 __last1, _InputIterator2 __first2,
^
```

It is possible that std::addressof cannot be matched with the unary function template.

This code change replaces addressof with an function, which does match std::function<> ok.

",0,,7,2017-12-11T15:30:36Z,2017-12-28T08:43:10Z,CONTRIBUTOR,2017-12-18T19:58:49Z
15275,[XLA] Allow components in the plugins directory to create devices,"awaiting review,cla: yes","Due to a change in the visibility of sub-components of the XLA JIT, the Graphcore device was unable to use the target needed for creating devices.

This change adds the plugin directory to the set of directories allowed to use the JIT.



",0,,3,2017-12-11T15:23:55Z,2018-01-25T17:25:30Z,CONTRIBUTOR,2018-01-09T10:13:44Z
15273,Dataset Iterator is not an iterator,"stat:awaiting tensorflower,type:feature","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: confidential
- **TensorFlow installed from (source or binary)**: [Pypi](https://pypi.python.org/pypi/tensorflow-gpu/1.4.1)
- **TensorFlow version (use command below)**: `v1.4.0-19-ga52c8d9 1.4.1`
- **Python version**: 3.5.3
- **CUDA/cuDNN version**: (sensitive information replaced by `xxx`)
```
$ apt search cud | grep installed
libcublas8.0/xxx,now 8.0.44-4 amd64 [installed]
libcuda1/xxx,now 375.66-1 amd64 [installed,automatic]
libcuda1-i386/xxx,now 375.66-1 i386 [installed,automatic]
libcudart8.0/xxx,now 8.0.44-4 amd64 [installed]
libcudnn6/now 6.0.21-1+cuda8.0 amd64 [installed,local]
libcufft8.0/xxx,now 8.0.44-4 amd64 [installed]
libcurand8.0/xxx,now 8.0.44-4 amd64 [installed]
libnvidia-fatbinaryloader/xxx,now 375.66-1 amd64 [installed,automatic]
libnvidia-ptxjitcompiler/xxx,now 375.66-1 amd64 [installed,automatic]
```
- **GPU model and memory**: Quadro K1200, 4019 MiB
- **Exact command to reproduce**:
Run [convert_to_records.py](https://github.com/tensorflow/models/tree/5a5d330539dff11eef79ca2e716fb477baf13cf9/official/mnist) from the official MNIST example, then:
```python
>>> import tensorflow as tf
>>> ds = tf.data.TFRecordDataset(['/tmp/mnist_data'])
>>> i  = ds.make_one_shot_iterator()
>>> next(i)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'Iterator' object is not an iterator
```

### Describe the problem

The returned ""iterator"" is not an iterator, because it does not provide a `__next__` or `next` method. It does provide a `get_next` method, but that is not what Python expects.
",0,,4,2017-12-11T12:49:31Z,2017-12-11T19:41:51Z,NONE,2017-12-11T13:14:49Z
15272,Wrongly used dropout bug,,"Hi, 

    I guess it should use **tf.layers.dropout** instead of **tf.nn.dropout** here? Because in the inference stage, all the nodes should be used instead of dropout. I guess this is a bug. 

https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/examples/tutorials/mnist/mnist_deep.py#L92

    Many thanks! 

Best wishes, 

Qiuqiang
    ",0,,2,2017-12-11T12:00:13Z,2017-12-11T16:41:18Z,NONE,2017-12-11T14:54:54Z
15271,TFGAN ideas for pretraining/training,tag:awaiting response,"With the PR #14723 enabling `get_hooks_fn` to be set manually instead of the default `1` step generator and `1` step discriminator there comes a set of new ""problems"" / things to consider.

1. The `Estimator` saves configured tensor summaries in the background. This does not work when doing something like a `2/2` split for generator/discriminator or a `2/3`. Then `Estimator` will only save one step instead of the actual amounts of steps taken (right?). This means that we have to consider an option to configure the `FileWriter`. I believe that is possible for the vanilla `Estimator` with `scaffolds` but not for the `GANEstimator` currently. This `FileWriter` has to be accessible by the `RunTrainOpsHook`. We would also need a generator + discriminator specific ""global_step"" that will be used in the `RunTrainOpsHook`. A quick idea would be to use the `overall_global_step * train_steps` of a `RunTrainOpsHook` or to use `dummy_global_step_generator` and `dummy_global_step_discriminator`.

2. When ""pre-training"" (with a normal call to `.train()` and a modified `sequential_train_hook(10,10)`) the generator for e.g. `10` steps and then the discriminator for `10` steps. Does the discriminator `loss_fn` receive 10 times new data from the generator through `gan_model.discriminator_real_outputs` or will it always be the same data? For pre-training I would assume I can feed in the same output data from the generator in batches multiple epochs. I believe that is not possible in the current setup, but correct me if I am wrong.

3. I have different loss functions for both pre-training and training. There is not `ModeKeys.PRETRAIN` to switch between them.

If I find more things I'll add them here.",1,,19,2017-12-11T11:44:24Z,2018-01-11T02:52:58Z,CONTRIBUTOR,2017-12-11T11:51:15Z
15268,correct the misspell of Quantize,cla: no,,0,,3,2017-12-11T11:04:02Z,2017-12-29T00:18:38Z,NONE,2017-12-26T02:33:28Z
15266, //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel  test fails on ppc64le,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
     Ubuntu 16.04 (ppc64le)
- **TensorFlow installed from (source or binary)**:
      Installed from source (v1.3.1)
- **TensorFlow version (use command below)**:
      TF1.3.1
- **Python version**: 
     Python 2.7.5
- **Bazel version (if compiling from source)**:
       0.5.4
- **CUDA/cuDNN version**:
     NA
- **GPU model and memory**:
      NA
- **Exact command to reproduce**:
      bazel test -c opt //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel       


**Describe the problem**

Here 2 sub tests are failing on ppc64le i.e.` IsFiniteScalarF32`  and `IsFiniteR1F32s` in file array_elementwise_ops_test.cc

For IsFiniteScalarF32 sub-test , error at line 100 : 
https://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/tests/array_elementwise_ops_test.cc#L100
`ComputeAndCompareR0<bool>(&builder, false, {});`
 The failure due to expected: false  vs actual: true

For IsFiniteR1F32s sub-test , error at line 126 :
https://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/tests/array_elementwise_ops_test.cc#L126
`ComputeAndCompareR1<bool>(&builder, {false, true, false, true, false, false}, {});`
The failure due to expected: {010100}  vs actual: {111100}

Currently trying to find the root cause , started debugging further on this. Any inputs/help appreciated.Thanks!

**Source code / logs**

1 .**IsFiniteScalarF32 sub-test log :**
```
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
-----------------------------------------------------------------------------
Note: This is test shard 5 of 25.
[==========] Running 6 tests from 2 test cases.
[----------] Global test environment set-up.
[----------] 5 tests from ArrayElementwiseOpTest
[ RUN      ] ArrayElementwiseOpTest.IsFiniteScalarF32
2017-12-11 09:36:20.653075: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-12-11 09:36:20.654000: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-12-11 09:36:20.654482: I tensorflow/compiler/xla/service/service.cc:187] XLA service 0x100062a6ea0 executing computations on platform Host. Devices:
2017-12-11 09:36:20.654493: I tensorflow/compiler/xla/service/service.cc:195]   StreamExecutor device (0): <undefined>, <undefined>
tensorflow/compiler/xla/tests/literal_test_util.cc:157: Failure
Value of: Equal(expected, actual)
  Actual: false (expected: false
actual:   true)
Expected: true
expected:
false
        vs actual:
true
tensorflow/compiler/xla/tests/literal_test_util.cc:157: Failure
Value of: Equal(expected, actual)
  Actual: false (expected: false
actual:   true)
Expected: true
expected:
false
        vs actual:
true
[  FAILED  ] ArrayElementwiseOpTest.IsFiniteScalarF32 (60 ms)
[ RUN      ] ArrayElementwiseOpTest.LogicalNotZeroElement
2017-12-11 09:36:20.712501: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.LogicalNotZeroElement (6 ms)
[ RUN      ] ArrayElementwiseOpTest.LogOfPowerF32
2017-12-11 09:36:20.719016: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.LogOfPowerF32 (12 ms)
[ RUN      ] ArrayElementwiseOpTest.Max3DAndScalarZeroElementS32s
2017-12-11 09:36:20.731087: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.Max3DAndScalarZeroElementS32s (7 ms)
[ RUN      ] ArrayElementwiseOpTest.Compare1DTo2DS32Ne
2017-12-11 09:36:20.738675: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.Compare1DTo2DS32Ne (13 ms)
[----------] 5 tests from ArrayElementwiseOpTest (99 ms total)

[----------] 1 test from ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount
[ RUN      ] ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount.SquareManyValues/0
2017-12-11 09:36:20.751273: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount.SquareManyValues/0 (34 ms)
[----------] 1 test from ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount (34 ms total)

[----------] Global test environment tear-down
[==========] 6 tests from 2 test cases ran. (133 ms total)
[  PASSED  ] 5 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] ArrayElementwiseOpTest.IsFiniteScalarF32

 1 FAILED TEST

```
2. **IsFiniteR1F32s sub-test log:**

```
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
-----------------------------------------------------------------------------
Note: This is test shard 6 of 25.
[==========] Running 6 tests from 2 test cases.
[----------] Global test environment set-up.
[----------] 5 tests from ArrayElementwiseOpTest
[ RUN      ] ArrayElementwiseOpTest.IsFiniteR1F32s
2017-12-11 09:36:21.201704: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-12-11 09:36:21.202563: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-12-11 09:36:21.203145: I tensorflow/compiler/xla/service/service.cc:187] XLA service 0x10015cc6ea0 executing computations on platform Host. Devices:
2017-12-11 09:36:21.203154: I tensorflow/compiler/xla/service/service.cc:195]   StreamExecutor device (0): <undefined>, <undefined>
tensorflow/compiler/xla/tests/literal_test_util.cc:157: Failure
Value of: Equal(expected, actual)
  Actual: false (expected: {010100}
actual:   {111100})
Expected: true
expected:
{010100}
        vs actual:
{111100}
[  FAILED  ] ArrayElementwiseOpTest.IsFiniteR1F32s (17 ms)
[ RUN      ] ArrayElementwiseOpTest.CompareEqF32s
2017-12-11 09:36:21.218227: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.CompareEqF32s (10 ms)
[ RUN      ] ArrayElementwiseOpTest.MulOfExpF32
2017-12-11 09:36:21.228744: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.MulOfExpF32 (12 ms)
[ RUN      ] ArrayElementwiseOpTest.Min2DTo1DF32s
2017-12-11 09:36:21.240780: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.Min2DTo1DF32s (15 ms)
[ RUN      ] ArrayElementwiseOpTest.Compare1DTo2DS32Ge
2017-12-11 09:36:21.255836: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.Compare1DTo2DS32Ge (19 ms)
[----------] 5 tests from ArrayElementwiseOpTest (73 ms total)

[----------] 1 test from ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount
[ RUN      ] ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount.SquareManyValues/1
2017-12-11 09:36:21.274589: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount.SquareManyValues/1 (73 ms)
[----------] 1 test from ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount (73 ms total)

[----------] Global test environment tear-down
[==========] 6 tests from 2 test cases ran. (146 ms total)
[  PASSED  ] 5 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] ArrayElementwiseOpTest.IsFiniteR1F32s

 1 FAILED TEST

```
",0,,30,2017-12-11T10:06:30Z,2018-01-31T19:55:41Z,CONTRIBUTOR,2017-12-11T19:02:49Z
15265,Test remove __force_inline [DO NOT MERGE],cla: yes,"This is only a test for fixing https://github.com/tensorflow/tensorflow/issues/10521, please don't merge this PR.",0,,4,2017-12-11T09:28:47Z,2018-01-15T14:31:23Z,MEMBER,2017-12-11T09:30:33Z
15264,Support empty input tensor for some ops (fix #14657),"awaiting testing (then merge),cla: yes","Cudnn kernels doesn't work for empty input tensors.
This PR adds support for empty input tensor for FusedBatchNorm,FusedBatchNormGrad,Conv2DBackpropFilter, and cudnn pooling. (fix #14657)",1,,27,2017-12-11T08:20:04Z,2017-12-29T00:38:36Z,CONTRIBUTOR,2017-12-20T00:16:09Z
15263,Cannot parse tensor from proto: dtype: DT_INT32 when using tf.extract_image_patches and tf.reshape,,"Hi,
I'm experiencing a problem when using TensorFlow to extract image patches and then reshape the output. I'm using TensorFlow 1.3.0, what am i doing wrong?

That's my code:
```
import tensorflow as tf
import numpy as np
c = 3
h = 1024
p = 32

image = tf.random_normal([h,h,c])
patch_size = [1,p,p,1]
patches = tf.extract_image_patches([image],
   patch_size, patch_size, [1, 1, 1, 1], 'VALID')
patches = tf.reshape(patches, [h, p, p, c])

sess = tf.Session()
I,P,R_n = sess.run([image,patches])
print(I.shape)
print(P.shape)
```

The error i'm getting is this:
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1326     try:
-> 1327       return fn(*args)
   1328     except errors.OpError as e:

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1305                                    feed_dict, fetch_list, target_list,
-> 1306                                    status, run_metadata)
   1307 

~\AppData\Local\conda\conda\envs\tf\lib\contextlib.py in __exit__(self, type, value, traceback)
     65             try:
---> 66                 next(self.gen)
     67             except StopIteration:

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\errors_impl.py in raise_exception_on_not_ok_status()
    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 466           pywrap_tensorflow.TF_GetCode(status))
    467   finally:

InvalidArgumentError: Cannot parse tensor from proto: dtype: DT_INT32
tensor_shape {
  dim {
    size: 3
  }
}
tensor_content: ""\000\004\000\000\000\004\000\000\003\000\000\000""

	 [[Node: random_normal_21/shape = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [3] values: 1024 1024 3>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-159-454594e78931> in <module>()
     12 
     13 sess = tf.Session()
---> 14 I,P,R_n = sess.run([image,patches])
     15 print(I.shape)
     16 print(P.shape)

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    893     try:
    894       result = self._run(None, fetches, feed_dict, options_ptr,
--> 895                          run_metadata_ptr)
    896       if run_metadata:
    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1122     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1123       results = self._do_run(handle, final_targets, final_fetches,
-> 1124                              feed_dict_tensor, options, run_metadata)
   1125     else:
   1126       results = []

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1319     if handle is None:
   1320       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1321                            options, run_metadata)
   1322     else:
   1323       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1338         except KeyError:
   1339           pass
-> 1340       raise type(e)(node_def, op, message)
   1341 
   1342   def _extend_graph(self):

InvalidArgumentError: Cannot parse tensor from proto: dtype: DT_INT32
tensor_shape {
  dim {
    size: 3
  }
}
tensor_content: ""\000\004\000\000\000\004\000\000\003\000\000\000""

	 [[Node: random_normal_21/shape = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [3] values: 1024 1024 3>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

Caused by op 'random_normal_21/shape', defined at:
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\__main__.py"", line 3, in <module>
    app.launch_new_instance()
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\traitlets\config\application.py"", line 658, in launch_instance
    app.start()
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\zmq\eventloop\ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tornado\ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\zmq\eventloop\zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\zmq\eventloop\zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\zmq\eventloop\zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\IPython\core\interactiveshell.py"", line 2698, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\IPython\core\interactiveshell.py"", line 2802, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\IPython\core\interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-159-454594e78931>"", line 7, in <module>
    image = tf.random_normal([h,h,c])
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\ops\random_ops.py"", line 71, in random_normal
    shape_tensor = _ShapeTensor(shape)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\ops\random_ops.py"", line 42, in _ShapeTensor
    return ops.convert_to_tensor(shape, dtype=dtype, name=""shape"")
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 611, in convert_to_tensor
    as_ref=False)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 676, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 121, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 106, in constant
    attrs={""value"": tensor_value, ""dtype"": dtype_value}, name=name).outputs[0]
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\koko\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Cannot parse tensor from proto: dtype: DT_INT32
tensor_shape {
  dim {
    size: 3
  }
}
tensor_content: ""\000\004\000\000\000\004\000\000\003\000\000\000""

	 [[Node: random_normal_21/shape = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [3] values: 1024 1024 3>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
```",0,,2,2017-12-11T08:16:17Z,2017-12-11T19:09:39Z,NONE,2017-12-11T08:39:00Z
15262,Bug with tf.layers.Dense,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
8.0
- **GPU model and memory**:
GTX 1080 ti
- **Exact command to reproduce**:

with tf.variable_scope('hello') as var_scope:
    var = tf.get_variable('var', [3,4,5])
    dense = tf.layers.Dense(5, name = 'dense_layer')
    b = dense(a)

And the error was : 
RuntimeError: Conversion function <function _TensorConversionFunction at 0x7f5bed522b90> for type <class 'tensorflow.python.ops.variables.Variable'> returned incompatible dtype: requested = float32_ref, actual = float32",1,,11,2017-12-11T07:51:25Z,2017-12-19T01:34:15Z,NONE,2017-12-11T08:30:18Z
15261,"ValueError: Variable Model/LSTMenc/rnn/basic_lstm_cell/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem
If I put reuse=None while creating BasicLSTMCell in the following code, I get this error:
```
Traceback (most recent call last):
  File ""pretrain.py"", line 358, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""pretrain.py"", line 251, in main
    valid_model = build_model(word_vocab, train=False)
  File ""pretrain.py"", line 200, in build_model
    dropout=FLAGS.dropout))
  File ""/home/raghuram.vadapalli/styletransfer/NeuralSum/model.py"", line 218, in lstm_doc_enc
    initial_state=initial_rnn_state, dtype=tf.float32)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py"", line 197, in static_rnn
    (output, state) = call_cell()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py"", line 184, in <lambda>
    call_cell = lambda: cell(input_, state)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py"", line 235, in __call__
    with _checked_scope(self, scope or ""basic_lstm_cell"", reuse=self._reuse):
  File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py"", line 93, in _checked_scope
    ""the argument reuse=True."" % (scope_name, type(cell).__name__))
ValueError: Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'Model/LSTMenc/rnn/basic_lstm_cell'; and the cell was not constructed as BasicLSTMCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True.
```
If I put reuse=True, I get this error
```
Traceback (most recent call last):
  File ""pretrain.py"", line 358, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""pretrain.py"", line 244, in main
    train_model = build_model(word_vocab, train=True)
  File ""pretrain.py"", line 146, in build_model
    dropout=FLAGS.dropout))
  File ""/home/raghuram.vadapalli/styletransfer/NeuralSum/model.py"", line 218, in lstm_doc_enc
    initial_state=initial_rnn_state, dtype=tf.float32)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py"", line 197, in static_rnn
    (output, state) = call_cell()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py"", line 184, in <lambda>
    call_cell = lambda: cell(input_, state)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py"", line 713, in __call__
    output, new_state = self._cell(inputs, state, scope)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py"", line 241, in __call__
    concat = _linear([inputs, h], 4 * self._num_units, True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py"", line 1044, in _linear
    _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size], dtype=dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 1049, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 948, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 356, in get_variable
    validate_shape=validate_shape, use_resource=use_resource)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 341, in _true_getter
    use_resource=use_resource)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 671, in _get_single_variable
    ""VarScope?"" % name)
ValueError: Variable Model/LSTMenc/rnn/basic_lstm_cell/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?
```
### Source code / logs
```
def lstm_doc_enc(input_cnn,
                   batch_size=20,
                   num_rnn_layers=2,
                   rnn_size=650,
                   max_doc_length=35,
                   dropout=0.0):

    # lstm document encoder
    with tf.variable_scope('LSTMenc') as scope:
        def create_rnn_cell():
            cell = tf.contrib.rnn.BasicLSTMCell(rnn_size, state_is_tuple=True, forget_bias=0.0, reuse=True)
            if dropout > 0.0:
                cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.-dropout)
            return cell

        if num_rnn_layers > 1:
            cell = tf.contrib.rnn.MultiRNNCell([create_rnn_cell() for _ in range(num_rnn_layers)], state_is_tuple=True)
        else:
            cell = create_rnn_cell()

        initial_rnn_state = cell.zero_state(batch_size, dtype=tf.float32)

        input_cnn = tf.reshape(input_cnn, [batch_size, max_doc_length, -1])
        input_cnn2 = [tf.squeeze(x, [1]) for x in tf.split(input_cnn, max_doc_length, 1)]

        outputs, final_rnn_state = tf.contrib.rnn.static_rnn(cell, input_cnn2,
                                         initial_state=initial_rnn_state, dtype=tf.float32)

    return adict(
        initial_enc_state=initial_rnn_state,
        final_enc_state=final_rnn_state,
        enc_outputs=outputs
    )

```
",0,,1,2017-12-11T06:59:55Z,2017-12-11T19:11:20Z,NONE,2017-12-11T19:11:20Z
15259,Using tensorflow to compute gradients w.r.t. spectral decomposition error,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.1
- **TensorFlow installed from (source or binary)**: pip install tensorflow
- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: see code

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Hi,

I'm having issues with evaluating the gradients of eigenvalues/eigenvectors with respect to the underlying matrix. Tensorflow evaluates the gradients, however when compared against analytical derivations the gradients are generally inconsistent (I understand numerical can be unstable). I'd like to understand is there's a bug in the gradients or if a different methodology was employed to the one cited below.

We are using tf.self_adjoint_eig to evaluate the spectral decomposition for the tensorflow variable input. We have initialised this with a symmetric matrix to satisfy the self adjoint operator property. We wish to take the derivative of individual eigenvalues or eigenvector with respect to the original matrix (note for eigenvectors we take one element from one eigenvector, e.g. element 2 in eigenvector 1 to avoid the issue of gradient aggregation for now).

The methodology for evaluating analytical gradients of eigenvalues and vectors of a symmetric real matrix can be found in the paper ""On differentiating Eigenvalues and Eigenvectors"" by Magnus (1985), Theorem 1 eqn (6) + (7). We evaluated using our input matrix the gradients under this paper and compared it to tensorflow evaluated gradients and the gradients from finite difference approximation. For the eigenvalues, the gradients are similar (identical on diagonal entries, off by a factor of 2 on off-diagonal elements), however the eigenvectors are off by quite a bit outside the diagonal entries. To start, define a matrix A as (excuse the matrix output formatting from Python)

A=[[-3 -2  4]
      [-2  1  1]
      [ 4  1  5]]

using np.linalg.eig and tf.self_adjoint_eig on A (I simply initialised a variable with A and computed the gradient for the tf implementation)

Tensorflow eigenvalues: [-5.43071561  1.76904987  6.66166575]
Python eigenvalues: [-5.43071561  6.66166575  1.76904987]

I now wish to evaluate the gradient of eigenvalue 1 (-5.43071561) w.r.t. A. 

Analytical gradient:
[[ 0.75896178  0.28555906 -0.31842553]
 [ 0.28555906  0.10744148 -0.11980748]
 [-0.31842553 -0.11980748  0.13359674]]

Tensorflow gradient:
[[[ 0.75896178,  0.        ,  0.        ],
   [ 0.57111812,  0.10744148,  0.        ],
  [-0.63685107, -0.23961495,  0.13359674]]]

The diagonal entries are the same but the off-diagonal entries are clearly off by a factor of 2.  Now we try and evaluate gradients for the eigenvectors.

Tensorflow eigenvectors:
[[-0.87118413 -0.31452619 -0.37697678]
 [-0.32778267  0.94426587 -0.0303395 ]
 [ 0.36550888  0.09713517 -0.92572567]]
Python eigenvectors:
[[ 0.87118413  0.37697678 -0.31452619]
 [ 0.32778267  0.0303395   0.94426587]
 [-0.36550888  0.92572567  0.09713517]]

We try and find the gradient of the eigenvector 1 element 2 (+/-0.32778267). We expect the tensorflow gradient to the equivalent to the analytical gradient (after taking into account the sign difference).

Analytical gradient:
[[ 0.03511309 -0.10795607 -0.01312188]
 [ 0.01321128 -0.04061843 -0.0049371 ]
 [-0.01473184  0.04529341  0.00550534]]

Tensorflow gradient:
[[[-0.03511309,  0.        ,  0.        ],
   [ 0.09474478,  0.04061843,  0.        ],
   [ 0.02785372, -0.04035631, -0.00550534]]]

Besides the entries being different on the off-diagonal, one issue is that tensorflow only returns the lower triangle of the gradient. Despite A being symmetric, the contribution to the gradient is not symmetric as shown in the analytical evaluation above. Thank you for reading!

TL:DR, I'd like to understand where the gradient computations are coming from and why they differ substantially to the results we have been using in our research. 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Here is a script that reproduces the above.

[eigen_decomp_examplev2.py.zip](https://github.com/tensorflow/tensorflow/files/1546631/eigen_decomp_examplev2.py.zip)
",0,,1,2017-12-11T05:32:08Z,2017-12-11T19:15:56Z,NONE,2017-12-11T19:15:56Z
15258,[no such file or directory: 'x86_64'] when building the library in TensorFlow Lite for iOS,stat:awaiting tensorflower,"Hi all, I encountered with an issur when trying to [setting up the environment to build TensorFlow Lite for iOS. ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/ios.md#building).
But when I was trying to build the library for all five supported architectures on iOS using `tensorflow/contrib/lite/build_ios_universal_lib.sh`, I have the follwing issue. 


    xcrun: error: SDK ""iphonesimulator"" cannot be located
    xcrun: error: SDK ""iphonesimulator"" cannot be located
    xcrun: error: unable to lookup item 'PlatformPath' in SDK 'iphonesimulator'
    xcrun: error: SDK ""iphonesimulator"" cannot be located
    xcrun: error: SDK ""iphonesimulator"" cannot be located
    xcrun: error: unable to lookup item 'Path' in SDK 'iphonesimulator'
    xcrun: error: SDK ""iphoneos"" cannot be located
    xcrun: error: SDK ""iphoneos"" cannot be located
    xcrun: error: unable to lookup item 'SDKVersion' in SDK 'iphoneos'
    gcc --std=c++11 -O3 -DNDEBUG -miphoneos-version-min=9.0 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -fembed-bitcode -Wno-c++11-narrowing -mno-thumb -fno-exceptions -isysroot  -arch x86_64 -O3 -I. -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/../../../ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/allocation.cc -o /Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/allocation.o
    gcc -miphoneos-version-min=9.0 -fembed-bitcode -mno-thumb -isysroot  -arch x86_64 -O3 -I. -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/../../../ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/context.c -o /Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/context.o
    gcc --std=c++11 -O3 -DNDEBUG -miphoneos-version-min=9.0 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -fembed-bitcode -Wno-c++11-narrowing -mno-thumb -fno-exceptions -isysroot  -arch x86_64 -O3 -I. -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/../../../ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/downloads/farmhash/src/farmhash.cc -o /Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/downloads/farmhash/src/farmhash.o
    gcc --std=c++11 -O3 -DNDEBUG -miphoneos-version-min=9.0 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -fembed-bitcode -Wno-c++11-narrowing -mno-thumb -fno-exceptions -isysroot  -arch x86_64 -O3 -I. -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/../../../ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/error_reporter.cc -o /Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/error_reporter.o
    gcc --std=c++11 -O3 -DNDEBUG -miphoneos-version-min=9.0 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -fembed-bitcode -Wno-c++11-narrowing -mno-thumb -fno-exceptions -isysroot  -arch x86_64 -O3 -I. -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/../../../ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/interpreter.cc -o /Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/interpreter.o
    clang: error: no such file or directory: 'x86_64'
    clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
    make: *** [/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/allocation.o] Error 1
    make: *** Waiting for unfinished jobs....
    clang: error: no such file or directory: 'x86_64'
    clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
    clang: error: no such file or directory: 'x86_64'
    clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
    make: *** [/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/context.o] Error 1
    make: *** [/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/downloads/farmhash/src/farmhash.o] Error 1
    clang: error: no such file or directory: 'x86_64'
    clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
    make: *** [/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/error_reporter.o] Error 1
    clang: error: no such file or directory: 'x86_64'
    clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]
    make: *** [/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/interpreter.o] Error 1


### System information
- **OS Platform and Distribution**: macOS High Sierra 10.13.2(17C88)
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 3.6.1
- **GCC/Compiler version (if compiling from source)**: GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)
- **Exact command to reproduce**: `bash tensorflow/contrib/lite/build_ios_universal_lib.sh`
- **Have I written custom code?**: no
- **TensorFlow installed from**: source
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A

Has anyone else had the same problem?",0,,6,2017-12-11T04:34:36Z,2018-01-03T15:26:54Z,NONE,2017-12-11T12:58:59Z
15256,Problem about tf.data.Dataset.from_sparse_tensor_slices,type:docs,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.4
- **Python version**: 3.52
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:8.0,6.46
- **GPU model and memory**:2GB
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I used the tf.data.Dataset.from_sparse_tensor_slices to built a dataset. But the website has no enought information.
Here's my code
`    point_cloud_feature_dataset = tf.data.Dataset.from_sparse_tensor_slices(sparse_feature)
    point_cloud_feature_dataset = point_cloud_feature_dataset.shuffle(buffer_size = 100000)
    point_cloud_feature_dataset = point_cloud_feature_dataset.batch(batch_size = BATCH_SIZE)
    point_cloud_feature_dataset = point_cloud_feature_dataset.repeat()
    iterator_feature = point_cloud_feature_dataset.make_one_shot_iterator()`

when I called the iterator_feature.get_nest(). It return 3 Tensors of shape [none,none,1]. Instead of a SparseTensor.  The input Sparse Tensor of dataset has a shape of [1000000,300000]. Each row is a example. I hope talents can replenish the Doc. Thanks!!  

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,13,2017-12-11T02:12:45Z,2017-12-20T01:21:28Z,NONE,2017-12-11T02:22:20Z
15255,Add int64 support for BroadcastArgs and BroadcastGradientArgs,"awaiting testing (then merge),cla: yes","In `array_ops.cc`, both int32 and int64 are expected to be supported for `BroadcastArgs` and `BroadcastGradientArgs`. However, this was not the case as only int32 kernel are registered even though `T` is part of the `TypeConstraint`.

This fix adds the int64 kernel support for `BroadcastArgs` and `BroadcastGradientArgs`, and adds related test cases.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,4,2017-12-11T01:40:34Z,2017-12-27T02:47:15Z,MEMBER,2017-12-26T02:32:39Z
15253,include _solib_* in pip package,"awaiting testing (then merge),cla: yes","See issue #15252 for detail, also should fixes issue #13711.
The problem is that when compile tensorflow with --config=mkl on virtual machines, mkl libraries won't be included because they locate under _solib_local, however, setup.py only includes _solib_k8.",0,,8,2017-12-10T16:46:09Z,2017-12-31T23:19:55Z,CONTRIBUTOR,2017-12-26T02:30:56Z
15252,vm compiled tensorflow - libmklml_intel.so ImportError,stat:contributions welcome,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04 VM
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
Branch `r1.4` and branch `master`
- **Python version**: 
3.6.3
- **Bazel version (if compiling from source)**:
Build label: 0.8.1 (Install using apt repository)
- **GCC/Compiler version (if compiling from source)**:
gcc-6 (Ubuntu 6.4.0-10ubuntu1~16.04.york0) 6.4.0 20171112

- **Exact command to reproduce**:
python -c ""import tensorflow as tf;""

### Describe the problem
After compiling Tensorflow from source with tutorial: `https://www.tensorflow.org/install/install_sources` and install the compiled pip package, I import tensorflow on python console and get those errors:

~~~
python -c ""import tensorflow as tf;""
Traceback (most recent call last):
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/potatoman/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/potatoman/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 73, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/potatoman/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/potatoman/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
~~~

My machine is a headless KVM virtual machine running on a ubuntu 17.10, btw.

I figure out where the problem is somehow, which is the pip packaging script don't include those mkl so files, and in my case, replace `_solib_k8` in script `tensorflow/tools/pip_package/setup.py` at line 185 with `_solib_local` solves the issue.

",0,,4,2017-12-10T16:43:46Z,2017-12-26T12:10:34Z,CONTRIBUTOR,2017-12-10T19:06:50Z
15251,different output size for avg_pool and max_pool,,"Hello,

I have the same bug as this user.

https://stackoverflow.com/questions/47423172/tensorflow-why-does-avg-pool-ignore-one-stride-dimension

My version is uo-to-date

Have I written custom code: Yes
OS Platform and Distribution: Linux Mint 18.3, codename ""Sylvia""
TensorFlow installed from Tensorflow Website
TensorFlow version 14.1.0
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory GeForce 940MX
Exact command to reproduce: see link",0,,4,2017-12-10T16:34:43Z,2017-12-11T21:02:26Z,NONE,2017-12-11T01:01:17Z
15250,Batch Norm variance output mismatches with tf 1.4.0,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from  binary**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**:  2.7.0
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/5.0
- **GPU model and memory**: GTX1080 and 8 GB
- **Exact command to reproduce**: python test_google_bn.py

### Describe the problem
Batch normalization test failed with tensorflow version 1.4.0 but the same test passed with tensorflow version 1.3.0. 


### Source code / logs
```Python
# test_google_bn.py
import numpy as np
import pytest
import tensorflow as tf
from numpy.testing import assert_array_almost_equal
from tensorflow.python.ops import control_flow_ops
def test_delayed_update_moving_vars():
    with tf.Session() as sess:
        height, width = 3, 3
        image_shape = (10, height, width, 3)
        image_values = np.random.rand(*image_shape)
        expected_mean = np.mean(image_values, axis=(0, 1, 2))
        expected_var = np.var(image_values, axis=(0, 1, 2))
        images = tf.constant(image_values, shape=image_shape, dtype=tf.float32)
        decay = 0.1
        epsilon = 1e-5
        output = tf.contrib.layers.batch_norm(images, is_training=True, reuse=None, decay=decay, epsilon=epsilon,
                            updates_collections=tf.GraphKeys.UPDATE_OPS, name='BatchNorm')
        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
        # updates_ops are added to UPDATE_OPS collection.
        assert len(update_ops) == 2
        with tf.control_dependencies(update_ops):
            barrier = tf.no_op(name='barrier')
        output = control_flow_ops.with_dependencies([barrier], output)
        # Initialize all variables
        sess.run(tf.global_variables_initializer())
        moving_mean = tf.contrib.framework.get_variables('BatchNorm/moving_mean')[0]
        moving_variance = tf.contrib.framework.get_variables('BatchNorm/moving_variance')[0]
        mean, variance = sess.run([moving_mean, moving_variance])
        # After initialization moving_mean == 0 and moving_variance == 1.
        assert_array_almost_equal(mean, [0] * 3)
        assert_array_almost_equal(variance, [1] * 3)
        for _ in range(10):
            sess.run([output])
        mean = moving_mean.eval()
        variance = moving_variance.eval()
        # After 10 updates with decay 0.1 moving_mean == expected_mean and
        # moving_variance == expected_var.
        assert_array_almost_equal(mean, expected_mean, decimal=4)
        assert_array_almost_equal(variance, expected_var, decimal=4)
```

>           assert_array_almost_equal(variance, expected_var, decimal=4)
           AssertionError: 
           Arrays are not almost equal to 4 decimals
           
           (mismatch 100.0%)
            x: array([ 0.08  ,  0.0908,  0.0773], dtype=float32)
            y: array([ 0.0792,  0.0898,  0.0764])


",1,,7,2017-12-10T09:58:04Z,2018-01-03T10:22:17Z,CONTRIBUTOR,2017-12-10T19:22:20Z
15248,"Instead of python, use PYTHON_BIN_PATH in pip.sh.",cla: yes,,1,,8,2017-12-10T08:22:47Z,2017-12-11T06:01:38Z,OWNER,2017-12-10T17:59:13Z
15247,Replace `variables.get_global_step()` use `training_util.get_global_step()`,"cla: yes,stat:awaiting response",,1,,6,2017-12-10T08:16:57Z,2017-12-12T02:54:11Z,CONTRIBUTOR,2017-12-10T18:33:36Z
15245,add c++ gradient for op: Pow,"awaiting testing (then merge),cla: yes","Fix  #15239.

### How to test

+ [x] add test case.
+ [ ] pass all tests.",0,,9,2017-12-10T06:48:33Z,2017-12-26T04:48:52Z,CONTRIBUTOR,2017-12-10T18:34:01Z
15244,Switched optimization mode for Pi builds to avoid internal compiler error,cla: yes,"The nightly Pi3 builds have been failing with:

```
In file included from external/eigen_archive/unsupported/Eigen/MatrixFunctions:57:0,
                 from ./third_party/eigen3/unsupported/Eigen/MatrixFunctions:1,
                 from tensorflow/core/kernels/matrix_exponential_op.cc:19:
external/eigen_archive/unsupported/Eigen/src/MatrixFunctions/MatrixFunction.h: In member function 'MatrixType Eigen::internal::MatrixFunctionAtomic<MatrixType>::compute(const MatrixType&) [with MatrixType = Eigen::Matrix<std::complex<float>, -1, -1>]':
external/eigen_archive/unsupported/Eigen/src/MatrixFunctions/MatrixFunction.h:101:1: internal compiler error: in decompose_normal_address, at rtlanal.c:5799
 }
 ^
Please submit a full bug report,
with preprocessed source if appropriate.
See <http://gcc.gnu.org/bugs.html> for instructions.
```

It appears to be the same problem that Chrome hit with gcc here:

https://bugs.chromium.org/p/chromium/issues/detail?id=675648

Their solution was to change the optimization flags to avoid this, so I've followed their lead and switched to -O3. I hope this won'tl make a big latency or size difference, but it does solve the compiler crash at least.",0,,1,2017-12-10T06:40:23Z,2017-12-11T20:55:16Z,MEMBER,2017-12-11T20:49:35Z
15240,"in minimize     ([str(v) for _, v in grads_and_vars], loss)) ValueError: No gradients provided for any variable - This error occurs while using minimize for an adam optimizer()",,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,2,2017-12-09T20:19:06Z,2017-12-10T00:07:42Z,NONE,2017-12-10T00:07:42Z
15239,No gradient defined for op: Pow,"stat:contributions welcome,type:bug/performance","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Bazel version**: N/A
- **Python version**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: -

### Describe the problem
It seems there is no gradient defined for the Pow operation in the C++ API.

I am actually transferring this issue from https://github.com/migueldeicaza/TensorFlowSharp/issues/187. Similar to the case of Select (#14845), it seems there is also no gradient for the Pow operation in the C++ API.",0,,4,2017-12-09T20:09:30Z,2017-12-26T04:48:52Z,NONE,2017-12-10T06:54:32Z
15238,CPU usage drops after several steps,stat:awaiting response,"I am running a tensorflow computation graph. The same graph was running fine before but all of a sudden I noticed that after several steps and checkpoint the CPU usage drops from more than 600% to 100% for the rest of the session. It seems like all executors are dying (when saving maybe) and the computation keeps running on only 1 Core shooting up and down from 100 to 200%.

I also posted the issue on stack overflow: https://stackoverflow.com/questions/47720893/tensor-flow-cpu-usage-drops-after-saving

  ",0,,6,2017-12-09T16:46:23Z,2018-01-03T14:56:14Z,NONE,2017-12-10T00:09:20Z
15236,"Error while using cuda-9.0, libcublas.so.8.0: cannot open shared object file: No such file or directory",stat:awaiting response,"Hi,

I just installed cuda-9.0
and done all the requirements for cuda to run tensorflow like libcudnn7_7.0.5.15-1+cuda9.0_amd64.deb, libcudnn7-dev_7.0.5.15-1+cuda9.0_amd64.deb.
The test example of libcudnn are working fine.

I installed tensorflow-gpu, it finished normally while using ""$ sudo pip3 install tensorflow-gpu"" but at the time of import, it gives me  error "" 
ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory""
Look like tensorflow doesn't support Cuda-9.0?

Error is as follows:

$ ipython
Python 3.5.2 (default, Nov 23 2017, 16:37:01) 
Type 'copyright', 'credits' or 'license' for more information
IPython 6.2.1 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import tensorflow as tf
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

/usr/lib/python3.5/imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

/usr/lib/python3.5/imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py in <module>()
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     70 for some common reasons and solutions.  Include the entire stack trace
     71 above this error message when asking for help."""""" % traceback.format_exc()
---> 72   raise ImportError(msg)
     73 
     74 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.



Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: No use pip install tensorflow
- **TensorFlow version (use command below)**: tensorflow_gpu-1.4.1-cp35-cp35m-manylinux1_x86_64.whl 
- **Python version**:  Python 3.5.2 (default, Nov 23 2017, 16:37:01)
- **Bazel version (if compiling from source)**:No
- **GCC/Compiler version (if compiling from source)**: No
- **CUDA/cuDNN version**:  cuda-9.0
- **GPU model and memory**: GeForce GT 750M
- **Exact command to reproduce**: No just import tensorflow as tf
                                                         
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,15,2017-12-09T14:35:25Z,2017-12-10T12:36:45Z,NONE,2017-12-10T00:10:43Z
15235,Loss should change depending on the number of epochs chosen even if I set the seed?,,"Hi colleagues,

I am using Keras to train different NNs. I would like to know why if I increment the epochs, the result until the same number of epochs is not the same for the evolution of loss. I am using shuffle=False, and np.random.seed(2017), tf.set_random_seed(2017), and I have check that if I repeat with the same number of epochs, the result is the same, so the random initialization is working. After the epochs training, I am deleting the ANN so the training begins at 0 again.

Here I attach the picture of the resulting training with 10 epochs:

<img width=""676"" alt=""captura de pantalla 2017-12-09 a las 15 02 25"" src=""https://user-images.githubusercontent.com/23745991/33796312-05efe5da-dcf2-11e7-9780-09e0be902557.png"">

And here I attach the picture of the resulting training with 8 epochs:

<img width=""679"" alt=""captura de pantalla 2017-12-09 a las 15 02 34"" src=""https://user-images.githubusercontent.com/23745991/33796313-106a6b34-dcf2-11e7-820c-e5d279665785.png"">

Also, I would like to know why the training time is not exactly (8/10) the 10 epochs attempt and how is it possible that some of them have less accuracy with 2 more epochs!

Here is the link to open code Jupyter Notebook. [GitHub Jupyter Notebook - ANN Comparison](https://github.com/PabloRR100/Pruning-Algorithm-Method/blob/master/NN-Comparison.ipynb)

Thanks a lot!",0,,3,2017-12-09T14:03:15Z,2017-12-11T21:07:40Z,NONE,2017-12-10T00:55:15Z
15232,"Install new TensorFlow via anaconda, but failed",stat:awaiting response,"Hello,

When I tried to install TensorFlow via anaconda.
I`ve searched the suitable cite to submit the issue, but failed to find out it.
If this is not sorry for it and advice me.

The target os is 'windows 10', and via anaconda 5.0.1 For Windows (Python 3.6 version 64bit).
After intall anaconda, from my terminal 'cmd.exe',
$ conda create -n tensorflow python=3.6
but received an error message.

But from 'Anaconda Prompt' below 'Anaconda3' of 'start button',it works correctly.

Thus I recommend to include the following description,
start 'Anaconda Prompt' below 'Anaconda3' of 'start button' between the steps.",0,,3,2017-12-09T08:04:49Z,2017-12-10T13:39:33Z,NONE,2017-12-09T18:55:26Z
15230,tensorflow-gpu on mac,,"I can use the following to install tensorflow on mac.

Python 2

~~~
pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.4.0-py2-none-any.whl
~~~

Python 3

~~~
pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.4.0-py3-none-any.whl
~~~

But I don't find the correponding files for tensorflow-gpu. Can it be generated and posted over there? Thanks.
",0,,1,2017-12-09T04:55:15Z,2017-12-09T08:37:32Z,NONE,2017-12-09T08:37:32Z
15229,[XLA] Fix another XLA/tfcompile compile error on OSX ,"awaiting testing (then merge),cla: yes","This fixes issue https://github.com/tensorflow/tensorflow/issues/15196 (though that was accidentally closed)

kernel_support_library.cc:99:5: error: no matching function
for call to 'transform' std::transform(function->arg_begin(),..

TEST=build tfcompile on OSX (also requires PR#14893)",0,,6,2017-12-09T04:54:25Z,2017-12-20T21:12:00Z,CONTRIBUTOR,2017-12-09T05:18:51Z
15227,MKL: Fixing MKL-DNN convolution filter propagation to backprop,"awaiting testing (then merge),cla: yes",Added code to propagate the 2D convolution filter to the backward pass.,0,,2,2017-12-09T00:28:55Z,2017-12-26T19:20:28Z,CONTRIBUTOR,2017-12-22T22:12:54Z
15225,Update .gitignore,"awaiting testing (then merge),cla: yes",,1,,3,2017-12-08T21:58:57Z,2017-12-09T05:19:57Z,CONTRIBUTOR,2017-12-09T04:41:31Z
15224,Add TensorFlow support for tf.repeat (equivalent to np.repeat),cla: yes,"This fix tries to address the feature request proposed in #8246 where there was no equivalent of numpy.repeat in TensorFlow.

This fix adds the support for tf.repeat that is equivalent to np.repeat.

NOTE: in order to allow optional `axis` parameter, this fix adds `Repeat` and `RepeatFlat`  ops where one takes `axis` and another does not take `axis`.

This fix fixes #8246.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,1,2017-12-08T21:28:59Z,2017-12-12T16:08:18Z,MEMBER,2017-12-12T16:08:18Z
15223,stdin,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,1,2017-12-08T19:24:08Z,2017-12-08T19:44:30Z,NONE,2017-12-08T19:44:29Z
15222,tfcompile error - no matching function for call to 'transform',,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.  This is on a clean checkout of tensorflow.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  OSX Sierra
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: building from master (command outputs 1.3.0)
- **Python version**:  2.7
- **Bazel version (if compiling from source)**:  0.6.0
- **GCC/Compiler version (if compiling from source)**: 
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1
Apple LLVM version 9.0.0 (clang-900.0.38)
Target: x86_64-apple-darwin16.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
- **CUDA/cuDNN version**: N/A (ran ./configure without CUDA)
- **GPU model and memory**: N/A (no GPU)
- **Exact command to reproduce**:
1. Check out tensorflow
2. Run ./configure, enable XLA support
3. cd tensorflow/compiler/aot
4.  bazel build :tfcompile

### Describe the problem
```
ERROR: /Users/mattrunchey/gitrepos/tensorflow/tensorflow/compiler/xla/service/llvm_ir/BUILD:171:1: C++ compilation of rule '//tensorflow/compiler/xla/service/llvm_ir:kernel_support_library' failed (Exit 1).
tensorflow/compiler/xla/service/llvm_ir/kernel_support_library.cc:101:5: error: no matching function for call to 'transform'
    std::transform(function->arg_begin(), function->arg_end(),
    ^~~~~~~~~~~~~~
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/algorithm:1922:1: note: candidate template ignored: couldn't infer template argument '_UnaryOperation'
transform(_InputIterator __first, _InputIterator __last, _OutputIterator __result, _UnaryOperation __op)
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/algorithm:1932:1: note: candidate function template not viable: requires 5 arguments, but 4 were provided
transform(_InputIterator1 __first1, _InputIterator1 __last1, _InputIterator2 __first2,
^
1 error generated.
```
This happens across multiple OSes, as well (we tried to compile on a unix distro with the same error).  

### Source code / logs
This seems to stem from a recent change in kernel_support_library.cc (specifically https://github.com/tensorflow/tensorflow/commit/c572bc4fd7c73f4b8014ae43cdf9da5b99592f59#diff-877daea43ebeb1cd4756f960400ee922 ).  ",0,,3,2017-12-08T19:23:08Z,2017-12-08T20:23:06Z,NONE,2017-12-08T20:06:40Z
15221,Dataset.from_generator doesn't support strings,,"I have hit the same problem described in https://stackoverflow.com/questions/47705684/tesnorflow-dataset-generator-does-not-work-with-strings

I think it is a missing feature instead of a bug?",0,,1,2017-12-08T19:10:31Z,2017-12-08T19:26:17Z,NONE,2017-12-08T19:26:17Z
15220,ResidualWrapper and HighwayWrapper require rnn inputs' last dimension to be equal to num_units,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: -
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  v1.4.0-rc1-11-g130a514 1.4.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: - 
- **GPU model and memory**: - 
- **Exact command to reproduce**:
~~~python
from tensorflow.contrib import rnn as rnn_cell
from tensorflow.python.ops import rnn
import tensorflow as tf

rnn_inputs = tf.random_uniform([10, 10, 10])
cell = rnn_cell.LSTMCell(128)
cell = rnn_cell.HighwayWrapper(cell) # rnn_cell.ResidualWrapper(cell)
_, _ = rnn.dynamic_rnn(cell, rnn_inputs, dtype=tf.float32)
~~~

### Problem
`rnn_cell.ResidualWrapper` and `rnn_cell.HighwayWrapper` throw an exception when rnn inputs' last dimension is not equal to `num_units`. Without wrappers, the input tensor can be different from `num_units`.
What's the reason for the different behaviour? Is it intended?

### Full Traceback
https://pastebin.com/YvTb3WM3",1,,2,2017-12-08T18:49:46Z,2017-12-09T16:49:54Z,CONTRIBUTOR,2017-12-09T05:36:44Z
15218,stupid question but please help me to answer this,stat:awaiting response,"I have a problem with parallel computing. I know that CUDA or pyCUDA can do such thing like 3D convolution in parallel. 
I think about using tensorflow GPU by calling tf.nn.conv3d(input, filter), does it uses build in functions in CUDA to operate 3D convolution with parallel computing or there are somethings else? 
I am not familiar with C++ or even with pyCuda I am not sure how to implement 3D convolution with it.

Same question with all type of other implement with tensorflow GPU.

Bests,   ",0,,2,2017-12-08T17:27:13Z,2017-12-14T08:08:04Z,NONE,2017-12-09T01:03:56Z
15217,pip.sh: unify the way virtualenv is invoked,cla: yes,"between python3.6 and the other versions

""python -m"" also seems to be a more robust way of calling virtualenv
than relying on the virtualenv command on path.",1,,1,2017-12-08T16:49:10Z,2017-12-10T03:55:25Z,CONTRIBUTOR,2017-12-09T21:30:19Z
15216,"When data become large,parition variables can not initialized successfully",,"i use tensorflow to distributed trainning models,  i use the partition valriables to store an array data, when the data is not so bigger, everything looks ok,but when the array data become larger, when the session initialize, the partition variables can not  initialized and the session will wait util time out.
### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request. the feat_info can initialize successfully, but the adj_info cannot initialized. the adj_info is larger than feat_info
### Source code / logs
i use ps_num = 4, worker_num =4 and i also try some other distributed config, like ps_num=1, worker_num=4, the result is the same
source code:
    with tf.device(tf.train.replica_device_setter(
        worker_device=""/job:worker/task:%d"" % task_id,
        cluster=cluster_spec)):
      
      feat_info = tf.get_variable(""feature_info"", (len(id_map),FLAGS.features_column), tf.float32, trainable=False, partitioner=tf.fixed_size_partitioner(num_workers))
      adj_info = tf.get_variable(""adj_info"", (len(id_map),FLAGS.max_degree), tf.int64, trainable=False, partitioner=tf.fixed_size_partitioner(num_workers))
     
      with tf.device('/job:worker/task:%d' %task_id):
          adj_local = tf.Variable(tf.constant(minibatch.adj, dtype=tf.int64), trainable=False, name=""adj_local"", collections=[tf.GraphKeys.LOCAL_VARIABLES])
          feat_local = tf.Variable(tf.constant(features, dtype=tf.float32), trainable=False, name=""feat_local"", collections=[tf.GraphKeys.LOCAL_VARIABLES])
     
      length, begin, end = split_node_by_task(len(id_map), task_id, num_workers)
      adj = tf.nn.embedding_lookup(adj_info, [x for x in range(begin, end)])
      adj = adj_local
      
      feat = tf.nn.embedding_lookup(feat_info, [x for x in range(begin, end)])
      feat = feat_local

log:
2017-12-08 23:54:17.377290: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session c2b3ba9b700261ba with config: 
INFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  None, ready: Variables not initialized: adj_info/part_0, adj_info/part_1, adj_info/part_2, adj_info/part_3, adj_info/part_4, adj_info/part_5, adj_info/part_6, adj_info/part_7, adj_info/part_8, adj_info/part_9, adj_info/part_10, adj_info/part_11, adj_info/part_12, adj_info/part_13, adj_info/part_14, adj_info/part_15
2017-12-09 00:00:35.637019: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session f35fcf332e3908ec with config: 
INFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  None, ready: Variables not initialized: adj_info/part_0, adj_info/part_1, adj_info/part_2, adj_info/part_3, adj_info/part_4, adj_info/part_5, adj_info/part_6, adj_info/part_7, adj_info/part_8, adj_info/part_9, adj_info/part_10, adj_info/part_11, adj_info/part_12, adj_info/part_13, adj_info/part_14, adj_info/part_15
and it will alway waiting adj_info to initialize",0,,11,2017-12-08T16:03:41Z,2018-01-30T00:16:16Z,NONE,2017-12-09T01:03:52Z
15215,Tensorflow - Unable to import frozen graph with batchnorm : uninitialized value batch_normalization/moving_mean,,"I am trying to freeze in a pbtxt file a checkpoint containing batchnorm layers (ubuntu, python 2.7, tf 1.1.0).

context : 
**Have I written custom code**
Yes, see below

**OS Platform and Distribution**
Docker with Ubuntu 14.04

**TensorFlow installed from**
pip installer

**TensorFlow version**
tensorflow and tensorflow-gpu 1.1.0

**Bazel version**
N/A

**CUDA/cuDNN version**
Cuda 8, CUDNN 5.1

**GPU model and memory**
Nvidia titan-x * 2, 12Go Ram each

**Exact command to reproduce**

For this, following these posts and issues :

https://github.com/davidsandberg/facenet/issues/161

https://github.com/davidsandberg/facenet/pull/172/commits/0f3ece502550714c91056f3a8630ce8c037f613f

I use this function:

    freeze_and_prune_graph(model_path_and_name, output_file=None):
		""""""
		freezes a model trained and saved by the trainer by :
		    - extracting the trainable variables between input_node and output_node
		    - turning them to constants
		    - changing the 1rst dim of input_node to None
		    -saving the resulting graph as a single .pb file

		:param model_path_and_name: must finish by .ckpt, and the checkpoint must be composed of
		3+ files : .ckpt.index, .ckpt.meta, and .ckpt.data-0000X-of-0000Y

		:param model_path_and_name: path to the trained model
		:param output_file: file to save to. If None, model_path_and_name.[-ckpt][+pb]
		:return: None
		""""""
		config_proto = tf.ConfigProto(allow_soft_placement=True)

		with tf.Session(config=config_proto) as sess:
		    new_saver = tf.train.import_meta_graph(model_path_and_name + '.meta', clear_devices=True)
		    tf.get_default_session().run(tf.global_variables_initializer())
		    tf.get_default_session().run(tf.local_variables_initializer())
		    new_saver.restore(sess, model_path_and_name)

		    # get graph definition
		    gd = sess.graph.as_graph_def()
		    # fix batch norm nodes
		    for node in gd.node:
		        if node.op == 'RefSwitch':
		            node.op = 'Switch'
		            for index in xrange(len(node.input)):
		                if 'moving_' in node.input[index]:
		                    node.input[index] = node.input[index] + '/read'
		        elif node.op == 'AssignSub':
		            node.op = 'Sub'
		            if 'use_locking' in node.attr: del node.attr['use_locking']
		        elif node.op == 'AssignAdd':
		            node.op = 'Add'
		            if 'use_locking' in node.attr: del node.attr['use_locking']

		    # tf.get_collection() returns a list. In this example we only want the
		    input_node = sess.graph.get_tensor_by_name('input_node:0')
		    new_shape = [None] + input_node.get_shape().as_list()[1:]

		    trainables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
		    new_graph_def = tf.graph_util.convert_variables_to_constants(sess, gd, [""output_node""],
		                                                                 variable_names_whitelist=[t.name[:-2] for t in trainables] + ['output_node'])

		    for node in new_graph_def.node:
		        if node.name == 'input_node':
		            node.attr['shape'].CopyFrom(attr_value_pb2.AttrValue(shape=tf.TensorShape(new_shape).as_proto()))
		            break

		    with tf.gfile.GFile(output_file, ""wb"") as f:
		        f.write(new_graph_def.SerializeToString())
		    print(""{0} / {1} ops in the final graph."".format(len(new_graph_def.node), len(sess.graph.as_graph_def().node)))

This goes well and creates the pbtxt file with the following output :

> Converted 201 variables to const ops.
5287 / 41028 ops in the final graph.

I then try to load the pbtxt model using this function :

    def load_frozen_graph(frozen_graph_file):
	    """"""
	    loads a graph frozen via freeze_and_prune_graph and returns the graph, its input placeholder and output tensor

	    :param frozen_graph_file: .pb file to load
	    :return: tf.graph, tf.placeholder, tf.tensor
	    """"""
	    # We load the protobuf file from the disk and parse it to retrieve the
	    # unserialized graph_def
	    with tf.gfile.GFile(frozen_graph_file, ""rb"") as f:
	        graph_def = tf.GraphDef()
	        graph_def.ParseFromString(f.read())

	    # Then, we can use again a convenient built-in function to import a graph_def into the
	    # current default Graph
	    with tf.Graph().as_default() as graph:
	        tf.import_graph_def(
	            graph_def,
	            input_map=None,
	            return_elements=None,
	            name=""prefix"",
	            op_dict=None,
	            producer_op_list=None
	        )

	    input_images_placeholder = graph.get_tensor_by_name('prefix/input_node:0')
	    input_phase_placeholder = None
	    try:
	        input_phase_placeholder = graph.get_tensor_by_name('prefix/phase:0')
	    except KeyError:
	        pass
	    output = graph.get_tensor_by_name('prefix/output_node:0')

	    return graph, input_images_placeholder, input_phase_placeholder, output

using the following snippet:

    graph, input_images_placeholder, is_training_placeholder, output = load_frozen_graph(model_pbtxt)
	sess = tf.Session(config=tf_config, graph=graph)
	feed_dict = {input_images_placeholder: prepared_input}
	if is_training_placeholder is not None:
	    feed_dict[is_training_placeholder] = False
	ret = sess.run([output], feed_dict=feed_dict)

This, however, leads to the following error:

> FailedPreconditionError (see above for traceback): Attempting to use uninitialized value prefix/conv0/BatchNorm/batch_normalization/moving_mean
> [[Node: prefix/conv0/BatchNorm/batch_normalization/moving_mean/read = Identity[T=DT_FLOAT, _class=[""loc:@prefix/conv0/BatchNorm/batch_normalization/moving_mean""], _device=""/job:localhost/replica:0/task:0/gpu:0""](prefix/conv0/BatchNorm/batch_normalization/moving_mean)]]
	 [[Node: prefix/output_node/_381 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_2447_prefix/output_node"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Following the question :
https://stackoverflow.com/questions/36007883/tensorflow-attempting-to-use-uninitialized-value-in-variable-initialization
I tried initializing variables:

    graph, input_images_placeholder, is_training_placeholder, output = load_frozen_graph(model_pbtxt)
	sess = tf.Session(config=tf_config, graph=graph)
	init = [tf.global_variables_initializer(), tf.local_variables_initializer()]
	sess.run(init)

	feed_dict = {input_images_placeholder: prepared_input}
	if is_training_placeholder is not None:
	    feed_dict[is_training_placeholder] = False
	ret = sess.run([self.output], feed_dict=feed_dict)

This, however, changes the error to:

> ValueError: Fetch argument <tf.Operation 'init' type=NoOp> cannot be interpreted as a Tensor. 
(Operation name: ""init"" op: ""NoOp"" is not an element of this graph.)

which seems to show that there is no variable that needs to be initialized.

What am I missing ? How to I freeze and reload the relevant values of a batch_normalization layer ?

PS: I do realize that this might better be on stackoverflow, but I posted there first and got no answer in 2 weeks:
https://stackoverflow.com/questions/47434139/tensorflow-unable-to-import-frozen-graph-with-batchnorm-uninitialized-value",0,,3,2017-12-08T15:19:04Z,2017-12-11T21:15:08Z,NONE,2017-12-09T01:03:47Z
15212,lookup.index_table_from_tensor() emits an error in eager mode when invoked more than once.,comp:eager,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: +
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **TensorFlow installed from (source or binary)**: tf-nightly
- **TensorFlow version (use command below)**: 1.5.0-dev20171206
- **Python version**: 3
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: 
~~~python
from tensorflow.python.ops import lookup_ops
import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()

inpt = ['1611', '1612', '1613', '1615', '1616', '1617', '1618', '1619', '1621']

a = lookup_ops.index_table_from_tensor(inpt, name='a')
b = lookup_ops.index_table_from_tensor(inpt, name='b')
~~~

### Problem
When the eager execution is enabled, the following error occurs:
~~~console
FailedPreconditionError: Table already initialized. [Op:InitializeTableV2] name: string_to_index/hash_table/string_to_index/hash_table//string_to_index/hash_table/table_init//
~~~

### Source code / logs
Full trace:
https://pastebin.com/GsKBjyV6

### Question
Is there a way to specify more than one lookup table (a `shared_name` or a special scope)?",1,,7,2017-12-08T13:14:40Z,2018-01-03T20:59:46Z,CONTRIBUTOR,2017-12-08T13:35:50Z
15210,Incorrect Result from Add Function,,"### System information

- Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 16.04.3 LTS
- Bazel version: Not applicable 
- TensorFlow installed from binary
- TensorFlow version: 1.4.0
- Python version: 3.5.2
- CUDA/cuDNN version: 8.0 / 6.0 for CUDA 8.0
- GPU model and memory: GM107M [GeForce GTX 960M] 4GB
- Exact command to reproduce:

Here is a simple program to add: 

session = tf.Session()

a = tf.placeholder(tf.float32)
#print(""first"")
b = tf.placeholder(tf.float32)
#print(""second"")
result_node = tf.add(a,b)
#print(""starting"")
x = session.run(result_node, {a:2.0, b: 3.5})
print(x)

Output should be 5.5, while I am getting 2.0 on 2 machines

![screenshot from 2017-12-08 17-59-49](https://user-images.githubusercontent.com/19254286/33766026-af9682da-dc41-11e7-8f81-1ff054903deb.png)
![screenshot from 2017-12-08 18-00-21](https://user-images.githubusercontent.com/19254286/33766027-afd8aba6-dc41-11e7-9a85-02f2764a9d67.png)

",0,,7,2017-12-08T12:31:12Z,2018-01-31T19:52:59Z,NONE,2017-12-09T01:03:38Z
15207,Feature request: automatically pick all defaults in ./configure,stat:awaiting tensorflower,"Everytime a new configuration option is added my CI halts on the ./configure step as it's prompted for input. Instead of setting each flag could we have some `USE_DEFAULTS=1 ./configure` flag instead?

I assume the intention is to slowly move away from the shell script to something better (hence configure.py, I wager) but for now, when testing projects with the master release it would be very useful to be able to just pick all default options for the WORKSPACE without knowing about them.",0,,4,2017-12-08T11:02:05Z,2017-12-11T18:38:05Z,CONTRIBUTOR,2017-12-08T12:09:24Z
15205,Delete Dockerfile.devel-gpu-cuda9-cudnn7,cla: yes,"our default dockerfiles now use cuda9-cudnn7.
No need for this file anymore.

CC @flx42 ",0,,4,2017-12-08T07:28:47Z,2017-12-08T14:11:29Z,OWNER,2017-12-08T07:47:34Z
15204,compile error with config sycl,stat:community support,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.4,the latest version I git from tensorflow
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:0.8.1
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:no
- **GPU model and memory**:no
- **Exact command to reproduce**:bazel build -c opt --config=sycl //tensorflow/tools/pip_package:build_pip_package


##something else
OpenCl version:1.2
computecpp path:    /usr/local/computecpp  version 0.5.0


computecpp_info:
Device Info:
Discovered 1 devices matching:
  platform    : <any>
  device type : <any>
--------------------------------------------------------------------------------
Device 0:
  Device is supported                     : UNTESTED - Vendor not tested on this OS
  CL_DEVICE_NAME                          : Hainan
  CL_DEVICE_VENDOR                        : Advanced Micro Devices, Inc.
  CL_DRIVER_VERSION                       : 2482.3
  CL_DEVICE_TYPE                          : CL_DEVICE_TYPE_GPU 



clinfo:
Number of platforms                               1
  Platform Name                                   AMD Accelerated Parallel Processing
  Platform Vendor                                 Advanced Micro Devices, Inc.
  Platform Version                                OpenCL 2.0 AMD-APP (2482.3)
  Platform Profile                                FULL_PROFILE
  Platform Extensions                             cl_khr_icd cl_amd_event_callback cl_amd_offline_devices 
  Platform Extensions function suffix             AMD

  Platform Name                                   AMD Accelerated Parallel Processing
Number of devices                                 1
  Device Name                                     Hainan
  Device Vendor                                   Advanced Micro Devices, Inc.
  Device Vendor ID                                0x1002
  Device Version                                  OpenCL 1.2 AMD-APP (2482.3)
  Driver Version                                  2482.3
  Device OpenCL C Version                         OpenCL C 1.2 
  Device Type                                     GPU
  Device Profile                                  FULL_PROFILE
  Device Board Name (AMD)                         AMD Radeon HD 8500M
  Device Topology (AMD)                           PCI-E, 04:00.0
  Max compute units                               4
  SIMD per compute unit (AMD)                     4
  SIMD width (AMD)                                16
  SIMD instruction width (AMD)                    1
  Max clock frequency                             850MHz
  Graphics IP (AMD)                               6.0
  Device Partition                                (core)
    Max number of sub-devices                     4
    Supported partition types                     none specified
  Max work item dimensions                        3
  Max work item sizes                             256x256x256
  Max work group size                             256
  Preferred work group size multiple              64
  Wavefront width (AMD)                           64
  Preferred / native vector sizes                 
    char                                                 4 / 4       
    short                                                2 / 2       
    int                                                  1 / 1       
    long                                                 1 / 1       
    half                                                 1 / 1        (n/a)
    float                                                1 / 1       
    double                                               1 / 1        (cl_khr_fp64)
  Half-precision Floating-point support           (n/a)
  Single-precision Floating-point support         (core)
    Denormals                                     No
    Infinity and NANs                             Yes
    Round to nearest                              Yes
    Round to zero                                 Yes
    Round to infinity                             Yes
    IEEE754-2008 fused multiply-add               Yes
    Support is emulated in software               No
    Correctly-rounded divide and sqrt operations  Yes
  Double-precision Floating-point support         (cl_khr_fp64)
    Denormals                                     Yes
    Infinity and NANs                             Yes
    Round to nearest                              Yes
    Round to zero                                 Yes
    Round to infinity                             Yes
    IEEE754-2008 fused multiply-add               Yes
    Support is emulated in software               No
    Correctly-rounded divide and sqrt operations  No
  Address bits                                    32, Little-Endian
  Global memory size                              2140311552 (1.993GiB)
  Global free memory (AMD)                        <printDeviceInfo:68: get number of CL_DEVICE_GLOBAL_FREE_MEMORY_AMD : error -33>
  Global memory channels (AMD)                    2
  Global memory banks per channel (AMD)           8
  Global memory bank width (AMD)                  256 bytes
  Error Correction support                        No
  Max memory allocation                           1591773593 (1.482GiB)
  Unified memory for Host and Device              No
  Minimum alignment for any data type             128 bytes
  Alignment of base address                       2048 bits (256 bytes)
  Global Memory cache type                        Read/Write
  Global Memory cache size                        16384
  Global Memory cache line                        64 bytes
  Image support                                   Yes
    Max number of samplers per kernel             16
    Max size for 1D images from buffer            134217728 pixels
    Max 1D or 2D image array size                 2048 images
    Base address alignment for 2D image buffers   256 bytes
    Pitch alignment for 2D image buffers          256 bytes
    Max 2D image size                             16384x16384 pixels
    Max 3D image size                             2048x2048x2048 pixels
    Max number of read image args                 128
    Max number of write image args                8
  Local memory type                               Local
  Local memory size                               32768 (32KiB)
  Local memory syze per CU (AMD)                  65536 (64KiB)
  Local memory banks (AMD)                        32
  Max constant buffer size                        65536 (64KiB)
  Max number of constant args                     8
  Max size of kernel argument                     1024
  Queue properties                                
    Out-of-order execution                        No
    Profiling                                     Yes
  Prefer user sync for interop                    Yes
  Profiling timer resolution                      1ns
  Profiling timer offset since Epoch (AMD)        1512650783761772748ns (Thu Dec  7 20:46:23 2017)
  Execution capabilities                          
    Run OpenCL kernels                            Yes
    Run native kernels                            No
    Thread trace supported (AMD)                  No
    SPIR versions                                 1.2
  printf() buffer size                            1048576 (1024KiB)
  Built-in kernels                                
  Device Available                                Yes
  Compiler Available                              Yes
  Linker Available                                Yes
  Device Extensions                               cl_khr_fp64 cl_amd_fp64 cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_gl_sharing cl_amd_device_attribute_query cl_amd_vec3 cl_amd_printf cl_amd_media_ops cl_amd_media_ops2 cl_amd_popcnt cl_khr_image2d_from_buffer cl_khr_spir cl_khr_gl_event 

NULL platform behavior
  clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...)  AMD Accelerated Parallel Processing
  clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...)   Success [AMD]
  clCreateContext(NULL, ...) [default]            Success [AMD]
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU)  Success (1)
    Platform Name                                 AMD Accelerated Parallel Processing
    Device Name                                   Hainan
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL)  Success (1)
    Platform Name                                 AMD Accelerated Parallel Processing
    Device Name                                   Hainan

ICD loader properties
  ICD loader Name                                 OpenCL ICD Loader
  ICD loader Vendor                               OCL Icd free software
  ICD loader Version                              2.2.8
  ICD loader Profile                              OpenCL 1.2
	NOTE:	your OpenCL library declares to support OpenCL 1.2,
		but it seems to support up to OpenCL 2.1 too.

### Describe the problem
I want Compile With sycl config  with the command 
bazel build -c opt --config=sycl //tensorflow/tools/pip_package:build_pip_package
 but  unfortunately get the error 

Illegal ambiguous match on configurable attribute ""deps"" in @local_config_sycl//sycl:sycl:
@local_config_sycl//sycl:using_sycl_ccpp
@local_config_sycl//sycl:using_sycl_trisycl
Multiple matches are not allowed unless one is unambiguously more specialized.

I did not know what casue this compile error. Please Help

",0,,8,2017-12-08T06:34:19Z,2018-01-03T08:12:39Z,NONE,2017-12-09T01:57:50Z
15203,DataLossError : Checksum does not match,"stat:awaiting response,type:support","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:windows 7 professional,64bit
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.3.0
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:N/A
- **GPU model and memory**:N/A
- **Exact command to reproduce**:N/A

### Describe the problem
I trained on Ubuntu16.04 to get the model, and then restore the model on  Windows7 Professional, but it occured such a mistake:

`DataLossError (see above for traceback): Checksum does not match: stored 1713499277 vs. calculated on the restored bytes 1894941567`

`[Node: save/RestoreV2_282 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""]]`

**Both machines have the same version of python and TensorFlow, and the model tested in another machine succeed, but this machine failed**, how to solve it? Thank you!

### Source code / logs

```
Traceback (most recent call last):
  File ""D:/tensorboxPy3/evaluate.py"", line 138, in <module>
    main()
  File ""D:/tensorboxPy3/evaluate.py"", line 117, in main
    get_results(args, H, os.path.dirname(args.datadir))
  File ""D:/tensorboxPy3/evaluate.py"", line 59, in get_results
    saver.restore(sess, args.weights)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1560, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 895, in run
    run_metadata_ptr)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.DataLossError: Checksum does not match: stored 1713499277 vs. calculated on the restored bytes 1894941567
	 [[Node: save/RestoreV2_282 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_282/tensor_names, save/RestoreV2_282/shape_and_slices)]]

Caused by op 'save/RestoreV2_282', defined at:
  File ""D:/tensorboxPy3/evaluate.py"", line 138, in <module>
    main()
  File ""D:/tensorboxPy3/evaluate.py"", line 117, in main
    get_results(args, H, os.path.dirname(args.datadir))
  File ""D:/tensorboxPy3/evaluate.py"", line 55, in get_results
    saver = tf.train.Saver()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1140, in __init__
    self.build()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1172, in build
    filename=self._filename)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 688, in build
    restore_sequentially, reshape)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 407, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 247, in restore_op
    [spec.tensor.dtype])[0])
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 663, in restore_v2
    dtypes=dtypes, name=name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

DataLossError (see above for traceback): Checksum does not match: stored 1713499277 vs. calculated on the restored bytes 1894941567
	 [[Node: save/RestoreV2_282 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_282/tensor_names, save/RestoreV2_282/shape_and_slices)]]
```


`Process finished with exit code 1`

",0,,7,2017-12-08T06:21:22Z,2018-01-21T00:35:23Z,NONE,2017-12-08T20:52:27Z
15199,missing instruction for BahdanauAttention,"stat:contributions welcome,type:feature","This is just my opinion: 
when call BahdanauAttention instance , it create a variable scope with None name_or_scope variable.
While name_or_scope is None, and get_variable with the same name inside the variable scope repeatedly, it will automatically add '_N' to the name of the scope. And I think it is not compatible with some functions like stati_rnn, because there are explicit 'for loop' inside the function and every loop of 'for loop' will create different variables inside the variable scope but not creating once and sharing",0,,8,2017-12-08T02:55:28Z,2018-01-06T17:02:02Z,NONE,2017-12-08T12:57:38Z
15198,Missing documentation for using the Dataset API in combination with image summaries,type:docs,"The `Dataset` API is now the recommended input pipeline, however I am missing some guidance on how to include summaries of my images.

```python
def get_data():
  dataset = FixedLengthRecordDataset(...)
  dataset = dataset.map(parse_dataset, ...)
  if is_training:
    dataset = dataset.map(preprocess_for_train, ...)
  # Do shuffling, batching...
  return dataset

def preprocess_for_train(image, label):
  # Do preprocessing...
  image = tf.image.random_flip_left_right(image)
  # Add summary
  tf.summary.image('preprocessed_image', tf.expand_dims(image, 0))
  return image, label
```

This is what I would do intuitively, but since `map()` uses a different thread and therefore a different `tf.Graph` instance (?), the summaries are lost.

What is the recommended way of adding image summaries when using the `Dataset` API? I would like to request a comment / example on that in the official docs. ",1,,7,2017-12-08T02:32:28Z,2018-01-15T21:06:34Z,NONE,2017-12-09T01:56:05Z
15197,Fix tag in source_remote_test: no_mac --> nomac,cla: yes,,0,,1,2017-12-08T02:19:29Z,2017-12-08T03:14:19Z,CONTRIBUTOR,2017-12-08T02:51:33Z
15196,[XLA] OSX tfcompile compile failure in ../llvm_ir/kernel_support_library.cc ,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.13.2 
- **TensorFlow installed from (source or binary)**: Source 
- **TensorFlow version (use command below)**: Top of Master (34bcd09c5fd4f6435517a499987b7e5044c8f2c0) 
- **Python version**: 
- **Bazel version (if compiling from source)**:   0.7.0
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 

bazel build tensorflow/compiler/aot/tfcompile

### Describe the problem
Compiler error when compiling tfcompile on OSX. This was introduced by @sanjoy  Commit:
https://github.com/tensorflow/tensorflow/commit/c572bc4fd7c73f4b8014ae43cdf9da5b99592f59

You will need this PR to be able to fix other compile issues on OSX. https://github.com/tensorflow/tensorflow/pull/14893


ERROR: /Users/tfninja/github/tensorflow/tensorflow/compiler/xla/service/llvm_ir/BUILD:171:1: C++ compilation of rule '//tensorflow/compiler/xla/service/llvm_ir:kernel_support_library' failed (Exit 1).
tensorflow/compiler/xla/service/llvm_ir/kernel_support_library.cc:99:5: error: no matching function for call to 'transform'
    std::transform(function->arg_begin(), function->arg_end(),
    ^~~~~~~~~~~~~~
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/algorithm:1922:1: note: candidate template ignored: couldn't infer template argument '_UnaryOperation'
transform(_InputIterator __first, _InputIterator __last, _OutputIterator __result, _UnaryOperation __op)
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/algorithm:1932:1: note: candidate function template not viable: requires 5 arguments, but 4 were provided
transform(_InputIterator1 __first1, _InputIterator1 __last1, _InputIterator2 __first2,
^
1 error generated.
Target //tensorflow/compiler/aot:tfcompile failed to build

### Source code / logs


The issue seems to be with this line of code in which works on Linux but fails on OSX/Clang tensorflow/compiler/xla/service/llvm_ir/kernel_support_library.cc

```
+    std::transform(function->arg_begin(), function->arg_end(),
+                   std::back_inserter(arg_values), std::addressof<llvm::Value>);
```
",0,,5,2017-12-08T01:14:47Z,2017-12-09T01:56:57Z,CONTRIBUTOR,2017-12-08T19:43:40Z
15195,"[MSVC] Add tensorflow::ops prefix for {Read,Write}File","awaiting testing (then merge),cla: yes",`ReadFile` and `WriteFile` collide with the functions in `windows.h`. Tell MSVC we want Tensorflow's ones.,1,,2,2017-12-08T00:46:53Z,2017-12-08T15:56:57Z,CONTRIBUTOR,2017-12-08T15:00:14Z
15192,Branch 178260923,cla: yes,,1,,5,2017-12-07T19:16:28Z,2017-12-08T02:05:43Z,CONTRIBUTOR,2017-12-07T19:24:21Z
15190,S3 reads eventually fail with tensorflow's dataset API,,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CPU
- **GPU model and memory**: N/A
- **Exact command to reproduce**: https://gist.github.com/thunterdb/d5f86c79457eea0f1021117ea4bce0ba

### Describe the problem
The given script reads the MNIST data from S3 repeatedly from a public bucket, on an EC2 machine in the same region as the S3 bucket.

Running this script eventually leads to python crashing after a few hours, and the following error:

```
Finished step 31890, time: 05:45:28 12/07/17, result: 32
Finished step 31920, time: 05:46:01 12/07/17, result: 32
terminate called after throwing an instance of 'std::system_error'
  what():  Resource temporarily unavailable
Aborted
```

### Source code / logs
https://gist.github.com/thunterdb/d5f86c79457eea0f1021117ea4bce0ba

It is hard to say what is happening without debugging symbols. From the very generic error, I suspect this is an issue with the S3 SDK. As a workaround, it would be nice for tensorflow's S3 plugin to retry or be more resilient to networking issues.

It is currently an annoying issue when running large distributed training jobs, because they crash after a few hours of reading data. Is anyone having a similar experience with S3?",0,,14,2017-12-07T19:07:40Z,2017-12-13T01:32:46Z,NONE,2017-12-07T19:11:37Z
15189, fix #15188 replaced isnan with std::isnan to avoid build error ,cla: yes,,1,,11,2017-12-07T18:13:22Z,2017-12-08T01:07:34Z,CONTRIBUTOR,2017-12-07T18:15:43Z
15188,error: 'isnan' was not declared in this scope,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: Master branch commit: `4ad12049`
- **Python version**: N/A
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**: g++ 5.4.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: `bazel build //tensorflow:libtensorflow.so`

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I'm trying to compile using `bazel build //tensorflow:libtensorflow.so`.
I'm getting this error:
```
ERROR: /home/rdi/Workspace/Common/tensorflow/tensorflow/core/BUILD:577:1: C++ compilation of rule '//tensorflow/core:random_ops_op_lib' failed (Exit 1)
In file included from ./tensorflow/core/framework/allocator.h:23:0,
                 from ./tensorflow/core/framework/tensor.h:20,
                 from ./tensorflow/core/framework/attr_value_util.h:24,
                 from ./tensorflow/core/framework/node_def_util.h:23,
                 from ./tensorflow/core/framework/shape_inference.h:20,
                 from ./tensorflow/core/framework/common_shape_fns.h:20,
                 from tensorflow/core/ops/random_ops.cc:16:
./tensorflow/core/framework/numeric_types.h: In constructor 'tensorflow::bfloat16::bfloat16(float)':
./tensorflow/core/framework/numeric_types.h:49:16: error: 'isnan' was not declared in this scope
     if (isnan(v)) {
                ^
```
",0,,1,2017-12-07T18:08:14Z,2017-12-08T01:08:11Z,CONTRIBUTOR,2017-12-07T18:09:35Z
15187,Cherrypicks for 1.4.1,cla: yes,,0,,1,2017-12-07T18:03:50Z,2017-12-07T20:04:34Z,MEMBER,2017-12-07T20:04:26Z
15186,Fix assert_called error on Python3,cla: yes,by replacing it with assertTrue(....called),0,,1,2017-12-07T15:17:07Z,2017-12-07T15:34:59Z,CONTRIBUTOR,2017-12-07T15:17:25Z
15185,[XLA] Add fast path cases for common scatter and gather operations,"awaiting testing (then merge),cla: yes","This change checks if the indices vector passed to a scatter or gather operation is a constant, and does a fast-path operation when it is filled with a zero-based incrementing set.

This is quite a common case because of tensor-array stack and unstack.

",1,,6,2017-12-07T15:12:51Z,2017-12-14T18:08:06Z,CONTRIBUTOR,2017-12-07T15:14:42Z
15182,GPU: Add Complex kernel for tf.exp(),"awaiting testing (then merge),cla: yes","Fix #15103.

Because it's my first contribution for GPU kernel, the PR might be not good. Welcome to feedback, and any help will be appreciated. Thanks.

### How to test

+ [x] add test case.
+ [ ] pass all test.",2,,4,2017-12-07T11:58:42Z,2017-12-15T19:54:38Z,CONTRIBUTOR,2017-12-14T20:13:01Z
15181,Add un-fed placeholder warning to negative-dimension error,"awaiting testing (then merge),cla: yes","Sometimes a user runs a tensor and forgets to initialize one of its placeholders in `feed_dict`. If that placeholder has a shape that includes `None`, the `None` gets converted to -1 and rejected at the C++ level, resulting in [confusing](https://stackoverflow.com/q/45059428/1979005) [error](https://stackoverflow.com/a/45480003/1979005) [messages](https://stackoverflow.com/q/44706840/1979005) (each word a separate link). This problem also appears in #11371.

This PR is a two-line change that briefly mentions this issue in the exception string. The hope is that it will help developers find the problem more quickly.",1,,2,2017-12-07T11:33:57Z,2017-12-07T19:09:47Z,CONTRIBUTOR,2017-12-07T16:41:03Z
15180,More simpler examples of using dataflow_ops.StagingArea,stat:awaiting response,"### Describe the problem
Getting the below error when using StagingArea.
`ValueError: Fetch argument <tf.Operation 'group_deps' type=NoOp> cannot be interpreted as a Tensor. (Operation name: ""group_deps""
op: ""NoOp""`

The error happens only after completing a few 100 steps. It would be a great help if someone can place simpler examples of proper usage of StagingArea

### Source code / logs
`compute_stage_put_op = compute_stage.put(iterator.get_next())
  if compute_stage_put_op.type == 'Stage':
         compute_stage_ops.append(compute_stage_put_op)`
",0,,6,2017-12-07T09:29:46Z,2018-01-03T20:59:14Z,NONE,2017-12-07T19:37:26Z
15179,Failure in LMDBReaderTest while reading testdata,"stat:awaiting tensorflower,stat:community support","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: s390x Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.4.0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: gcc 5.4.0
- **CUDA/cuDNN version**: No GPU
- **GPU model and memory**: No GPU
- **Exact command to reproduce**: bazel test -c opt  //tensorflow/python/kernel_tests:reader_ops_test

### Describe the problem
While executing `reader_ops_test`, came across failure in `LMDBReaderTest`. 
Since I am running it on a big endian system, the failure could be because the testdata(data.mdb) is platform specific and hence gets interpreted wrongly. 
@bowang, @jhseu , Is my understanding correct? How can I generate the above testdata for s390x(big endian)?

### Source code / logs
```
F tensorflow/core/kernels/lmdb_reader_op.cc:49] Check failed: mdb_env_open(mdb_env_, current_work().c_str(), flags, 0664) == 0 (-30793 vs. 0)Invalid argument
012
234
012
234
012
234
012
234
012
234
012
234
Aborted (core dumped)
```
",0,,4,2017-12-07T08:44:16Z,2017-12-08T12:06:02Z,CONTRIBUTOR,2017-12-07T18:28:45Z
15178,Dataset.from_generator doesn't play nice with feature_column.categorical_column_with_vocabulary_list,type:bug/performance,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux 4.4.0-98-generic #121~14.04.1-Ubuntu SMP Wed Oct 11 11:54:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.4.0-rc1-11-g130a514', '1.4.0')
- **Python version**: Python 2.7.6
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**:  -
- **GPU model and memory**: -
- **Exact command to reproduce**: curl -L -o predict_not_working.py 'https://drive.google.com/uc?authuser=0&id=1KHCNOfJOpEFSLzzJqjtP6oy0EvuLvETE&export=download' && python predict_not_working.py

### Describe the problem

I have the same code in two versions, [one using Dataset.from_generator](https://drive.google.com/open?id=1KHCNOfJOpEFSLzzJqjtP6oy0EvuLvETE) and [one using Dataset.from_tensor_slices](https://drive.google.com/open?id=1BFmIEnpuRWPeghUfeBD4225BXZqvLcmS). To me it looks like the created Datasets have the exact same content, but feature_column.categorical_column_with_vocabulary_list doesn't work with the `from_generator` one, which ought to be a bug, right?

### Source code / logs

Sources are in links above.",1,,5,2017-12-07T08:23:46Z,2018-01-03T16:59:38Z,NONE,2017-12-07T18:45:59Z
15175,"Got ""Attempting to use uninitialized value"" error after variable initalization",,"I am working on windows 10, Python 3.6 and tensorflow 1.4.0. I tested the code on two laptops, one with gpu and another without, both of them had this problem.

This is my code:

`def network(self, net_input):
        dense1 = tf.layers.dense(net_input, 64)
        norm1 = tf.contrib.layers.batch_norm(dense1)
        relu1 = tf.nn.relu(norm1)
        dense2 = tf.layers.dense(relu1, 32)
        norm2 = tf.contrib.layers.batch_norm(dense2)
        relu2 = tf.nn.relu(norm2)
        out = tf.layers.dense(relu2, 1)
        return out`

`def train(self):
        init_global = tf.global_variables_initializer()
        init_local = tf.local_variables_initializer()
        sess = tf.InteractiveSession()
        #sess = tf.Session()
        sess.run(init_global)
        sess.run(init_local)
        data = tf.placeholder(tf.float32, [self.batch_size, 53])
        label = tf.placeholder(tf.float32, [self.batch_size, 1])
        prediction = self.network(data)
        loss = tf.reduce_mean(tf.reduce_sum(tf.square(
                label - prediction),reduction_indices=[1]))
        train_step = tf.train.GradientDescentOptimizer(1e-3).minimize(loss)
    
        for i in range(self.epoch):
            for j in range(20000 // self.batch_size):
                batch_data, batch_label = self.next_batch(self.train_data, self.train_label)
                #batch_label = self.next_batch(self.train_label)
                sess.run(train_step, feed_dict = {data : batch_data, label : batch_label})
                if j % 50 == 0:
                    start = time.time()
                    loss = sess.run(loss, feed_dict = {data : batch_data, label : batch_label})
                    print('Epoch: [%2d/%2d], Iter: [%4d/%4d], Loss: %8f, Time cost: %8f'%\
                          (i, self.epoch, j, 20000//self.batch_size, loss, time.time()-start))`

and this is the error message:

`Traceback (most recent call last):

  File ""<ipython-input-17-6e018316c758>"", line 1, in <module>
    runfile('C:/Users/System_Error/Downloads/report1/regression_tf.py', wdir='C:/Users/System_Error/Downloads/report1')

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 880, in runfile
    execfile(filename, namespace)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/System_Error/Downloads/report1/regression_tf.py"", line 99, in <module>
    main()

  File ""C:/Users/System_Error/Downloads/report1/regression_tf.py"", line 92, in main
    regression.train()

  File ""C:/Users/System_Error/Downloads/report1/regression_tf.py"", line 76, in train
    sess.run(train_step, feed_dict = {data : batch_data, label : batch_label})

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 889, in run
    run_metadata_ptr)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1317, in _do_run
    options, run_metadata)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)

`FailedPreconditionError:` Attempting to use uninitialized value dense_18/bias
	 [[Node: dense_18/bias/read = Identity[T=DT_FLOAT, _class=[""loc:@dense_18/bias""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](dense_18/bias)]]

Caused by op 'dense_18/bias/read', defined at:
  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\ipython\start_kernel.py"", line 231, in <module>
    main()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\ipython\start_kernel.py"", line 227, in main
    kernel.start()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\zmq\eventloop\ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tornado\ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2717, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2827, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-17-6e018316c758>"", line 1, in <module>
    runfile('C:/Users/System_Error/Downloads/report1/regression_tf.py', wdir='C:/Users/System_Error/Downloads/report1')
  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 880, in runfile
    execfile(filename, namespace)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)
  File ""C:/Users/System_Error/Downloads/report1/regression_tf.py"", line 99, in <module>
    main()
  File ""C:/Users/System_Error/Downloads/report1/regression_tf.py"", line 92, in main
    regression.train()
  File ""C:/Users/System_Error/Downloads/report1/regression_tf.py"", line 67, in train
    prediction = self.network(data)
  File ""C:/Users/System_Error/Downloads/report1/regression_tf.py"", line 26, in network
    dense1 = tf.layers.dense(net_input, 64)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\layers\core.py"", line 250, in dense
    return layer.apply(inputs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\layers\base.py"", line 671, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\layers\base.py"", line 559, in __call__
    self.build(input_shapes[0])
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\layers\core.py"", line 145, in build
    trainable=True)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\layers\base.py"", line 458, in add_variable
    trainable=trainable and self.trainable)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 1203, in get_variable
    constraint=constraint)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 1092, in get_variable
    constraint=constraint)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 425, in get_variable
    constraint=constraint)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 394, in _true_getter
    use_resource=use_resource, constraint=constraint)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 805, in _get_single_variable
    constraint=constraint)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\variables.py"", line 213, in __init__
    constraint=constraint)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\variables.py"", line 356, in _init_from_args
    self._snapshot = array_ops.identity(self._variable, name=""read"")
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 125, in identity
    return gen_array_ops.identity(input, name=name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 2070, in identity
    ""Identity"", input=input, name=name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

FailedPreconditionError (see above for traceback): Attempting to use uninitialized value dense_18/bias
	 [[Node: dense_18/bias/read = Identity[T=DT_FLOAT, _class=[""loc:@dense_18/bias""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](dense_18/bias)]]`


I searched stack overflow, most of the answers told me to add an global_variables_initializer, and I tried tf.Session and tf.InteractiveSession, global and local variables initializer, but still got this error. 






",0,,1,2017-12-07T04:04:51Z,2017-12-07T04:10:34Z,NONE,2017-12-07T04:10:34Z
15174,Branch 178185697,cla: yes,,1,,5,2017-12-07T03:47:41Z,2017-12-07T05:12:02Z,CONTRIBUTOR,2017-12-07T03:54:42Z
15173,Fix for #12537,"awaiting testing (then merge),cla: yes","I have built the fix on x86 linux no gpu and it's working.

There are some android specific code in logging that I ignored.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/logging.cc#L35-L75",1,,11,2017-12-07T02:17:27Z,2017-12-14T19:16:32Z,CONTRIBUTOR,2017-12-07T02:19:06Z
15171,[FeatureRequest] Decorator for estimator input_fn,"stat:awaiting tensorflower,type:feature","In the documentation for [Passing input_fn Data to Your Model](https://www.tensorflow.org/get_started/input_fn#passing_input_fn_data_to_your_model) there are a few methods provided for using a function to construct input data and then wrapping that to get a function object. The documentation suggests `functools.partial` or `lambda`.

Would it be possible to provide a python decorator (maybe under the `tf.estimator` module)? 

Looking at suggested decorator and usage below, I think it makes the code cleaner when making these reusable input functions. 

## Proposed decorator
_n.b. name to be improved/aligned with TF_
```python
import functools

def tf_data_input_fn(old_func):
    def inside(*args, **kwargs):
        return functools.partial(old_func, *args, **kwargs)
    return inside
```

## Usage
```python
@tf_data_input_fn
def my_input_fn(data_set):
    # Construct dataset, repeat, maps etc
    features, labels = dataset.make_one_shot_iterator().get_next()
    return features, labels

train_input_fn = my_input_fn(training_set)
eval_input_fn = my_input_fn(test_set)

classifier.train(input_fn=train_input_fn, steps=2000)
classifier.evaluate(input_fn= eval_input_fn, steps=2000)
```

If this is favourable, I'd be happy to provide a contribution towards such an addition.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: v1.4.0-8-gbca50da6eb 1.4.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A",0,,10,2017-12-07T01:45:44Z,2018-01-01T10:25:54Z,NONE,2017-12-07T12:30:03Z
15170,Go: Don't require -std=c99 for the cgo code.,cla: yes,"This should fix the error:
github.com/tensorflow/tensorflow/tensorflow/go/graph.go:31:3: error:
'for' loop initial declarations are only allowed in C99 mode
 //  for (int i = 0; i < num_shapes; i++) {
    ^

in some continuous builds like:
https://ci.tensorflow.org/job/tensorflow-master-cpu/3297/consoleFull

Alternative to #15169",1,,3,2017-12-06T22:30:54Z,2017-12-07T01:52:21Z,MEMBER,2017-12-06T22:31:02Z
15169,DO NOT SUBMIT: Temporarily disable go:test,cla: yes,context: b/70154286,1,,3,2017-12-06T22:03:36Z,2017-12-06T23:47:52Z,CONTRIBUTOR,2017-12-06T22:04:11Z
15167,Fix some Bash issues,"awaiting testing (then merge),cla: yes","Argument mixes string and array. Use `*` or separate argument.
Don't use `$` on the left side of assignments.",1,,2,2017-12-06T20:35:47Z,2017-12-07T05:17:50Z,CONTRIBUTOR,2017-12-07T04:07:29Z
15166,[CMake] Add bazel tests for python file lists,"awaiting review,cla: yes","Progressing #10296 
",1,,25,2017-12-06T20:15:04Z,2017-12-28T02:10:34Z,CONTRIBUTOR,2017-12-08T15:16:23Z
15163,Fix problem with camera on Android TV,"awaiting review,cla: yes","For a scenario of using a usb external camera, we should use Camera API 2 according to the definition:
Camera1 API is framework API that had been created to support HALv1.x
Camera2 API is a new API that is meant for HALv3.x.

However, **CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_FULL)** should return false as USB camera doesn' support all the necessary capabilities. 
https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#INFO_SUPPORTED_HARDWARE_LEVEL_FULL

This is a work around to use **facing == CameraCharacteristics.LENS_FACING_EXTERNAL** to detect usb external camera and use Camera2 API instead.",1,,4,2017-12-06T19:03:15Z,2017-12-13T21:34:33Z,CONTRIBUTOR,2017-12-13T19:04:45Z
15162,"tf.data.Dataset.from_generator creates too many threads throwing ""thread constructor failed""",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Adapted an example from documentation
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Mac OS Siera (10.12.6)
- **TensorFlow installed from (source or binary)**:pip install tensorflow 
- **TensorFlow version (use command below)**: tensorflow-1.4.0-cp27-cp27m-macosx_10_11_x86_64.whl
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: CPU
- **GPU model and memory**: NA
- **Exact command to reproduce**:

```
import itertools
import tensorflow as tf

def gen():
  for i in itertools.count(1):
    yield (i, [1] * 5)

ds = tf.data.Dataset.from_generator(
    gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))
ds = ds.make_one_shot_iterator()

with tf.Session() as sess:
    while True:
        sess.run(ds.get_next())  # (1, array([1, 1, 1, 1, 1]))...
```

### Describe the problem
If you run the code above, which is adapted from tf.data.Dataset.from_generator docstring, the program will crash with an error: `libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: thread constructor failed: Resource temporarily unavailable`.

I see the number of threads increasing in the activity monitor of the mac OS and when it reaches ~3K threads the program crashes. It takes several seconds. 

Please let me know if this is not intended use of this API, it is OS releated issue or there is a bug involved.
",1,,2,2017-12-06T18:50:12Z,2017-12-06T19:09:56Z,NONE,2017-12-06T19:09:56Z
15160,Installing Tensorflow from source,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Redhat
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.4.1
- **Python version**: 
2.7.14
- **Bazel version (if compiling from source)**:
0.7.0
- **GCC/Compiler version (if compiling from source)**:
gcc 4.8.5

- **CUDA/cuDNN version**:
8/6
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I am trying to install TF. Its the last part of building TF from source.

pip install /tmp/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27mu-linux_x86_64.whl 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

[amalik@node05 tensorflow]$ pip install /tmp/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27mu-linux_x86_64.whl 
Processing /tmp/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27mu-linux_x86_64.whl
Requirement already satisfied: enum34>=1.1.6 in /lfs1/software7/anaconda2/lib/python2.7/site-packages (from tensorflow==1.4.1)
Requirement already satisfied: backports.weakref>=1.0rc1 in /lfs1/software7/anaconda2/lib/python2.7/site-packages (from tensorflow==1.4.1)
Requirement already satisfied: wheel in /lfs1/software7/anaconda2/lib/python2.7/site-packages (from tensorflow==1.4.1)
Requirement already satisfied: mock>=2.0.0 in /lfs1/software7/anaconda2/lib/python2.7/site-packages (from tensorflow==1.4.1)
Collecting tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow==1.4.1)
  Retrying (Retry(total=4, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f384a2261d0>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-tensorboard/
  Retrying (Retry(total=3, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f384a226350>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-tensorboard/
  Retrying (Retry(total=2, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f384a2264d0>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-tensorboard/
  Retrying (Retry(total=1, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f384a226650>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-tensorboard/
  Retrying (Retry(total=0, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f384a2267d0>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-tensorboard/
  Could not find a version that satisfies the requirement tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow==1.4.1) (from versions: )
No matching distribution found for tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow==1.4.1)



",0,,1,2017-12-06T15:54:22Z,2017-12-06T17:08:40Z,NONE,2017-12-06T17:08:40Z
15159,Unable access S3 using the S3 filesystem,stat:awaiting response,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  docker container based on centos7
- **TensorFlow installed from (source or binary)**: binary, from pip
- **TensorFlow version (use command below)**: ('v1.4.0-rc1-11-g130a514', '1.4.0')
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**:
```
from tensorflow.python.lib.io import file_io
file_io.stat('s3://datasets.elasticmapreduce/ngrams/books/20090715/eng-1M/1gram/data')
```


### Describe the problem
I'm unable to read files using the s3 filesystem. I believe my example above should work, please correct my usage of the api if not. I tried using both an object in a public bucket and one in a private bucket that I have credentials in my ~/.aws/config for. 

I'm not quite sure what the error is, or how to diagnose it further. Setting the log level to Debug had no effect. `tf.logging.set_verbosity(tf.logging.DEBUG)` 

### Source code / logs

Traceback from file_io.stat
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 540, in stat
    return file_statistics
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Object s3://datasets.elasticmapreduce/ngrams/books/20090715/eng-1M/1gram/data does not exist
```

Using the aws cli, you can verify the object exists 
```
aws s3 ls s3://datasets.elasticmapreduce/ngrams/books/20090715/eng-1M/1gram/data
```",0,,17,2017-12-06T15:37:22Z,2018-01-03T19:29:17Z,NONE,2017-12-06T18:01:33Z
15158,"Feature: MonitoredSession should have run() method with 'hooks_to_trigger' argument: run(..., hooks_to_trigger=[hooks[1], hooks[3], ...])",,"At the moment the session_run_hooks are passed to the constructor of MonitoredSession(..., hooks=[...]) 
and then they get executed for EVERY session.run() call within the MonitoredSession block.

This is inefficient and problematic.

E.g., if I define a LoggingTensorHook, I want the logging output to be evaluated and printed at most ONCE per global step and not after some auxiliary session.run() calls that only evaluate the size of some queue or whatever else.

If you use feedable iterators, the current MonitoredSession implementation actually crashes the program, see https://github.com/tensorflow/tensorflow/issues/12859#issuecomment-348290076

The best solution in my opinion would be, to be able to specify which run() calls should actually trigger the before_run and after_run methods of the hooks, e.g. via some flag-argument in MonitoredSession.run(..., execute_hooks=True)
or alternatively pass a list of specific hooks whose before_run and after_run methods should be triggered by a run call, via MonitoredSession.run(..., hooks_to_trigger=[...])
",0,,8,2017-12-06T14:50:46Z,2017-12-06T17:05:57Z,NONE,2017-12-06T17:05:57Z
15157,Feature: GANEstimator allow passing of namedtuples,,"In the current `GANEstimator` implementation in `train.py` `gan_model(..)` both `real_data` as well as `generator_inputs` is converted to tensors with either `_convert_tensor_or_l_or_d` or `ops.convert_to_tensor`. This prevents the user from using own data structures like `namedtuples` to pass information between the `generator` and `discriminator`.

In the current implementation when passing a `namedtuple` the result will be a `list` with all name information being lost.

I propose to either extend the tensor conversion to exclude namedtuples from them or to remove them entirely.

@joel-shor Do you think that is a good idea? I am currently passing logits as well as sample_id from a dynamic_decoding around and I would like to keep the meaning of these across loss_fn and discriminator_fn.

1. I could remove the conversions entirely. This would introduce breaking changes.
2. I could exclude namedtuples from the conversion.
3. Idk yet??

I would create a PR for any of them if we find a suitable solution. ",1,,4,2017-12-06T14:44:17Z,2017-12-08T18:15:24Z,CONTRIBUTOR,2017-12-07T01:44:54Z
15155,Input too short to compute filterbank,,"Hi,
I am new to tensorflow and i am trying to train model with my own data but i am getting below error

2017-12-06 18:56:38.030081: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030095: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030105: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030162: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030203: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030243: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030256: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030267: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030305: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030347: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030359: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030370: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030408: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030446: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030458: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030469: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030481: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030492: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030534: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030547: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030558: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030569: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030632: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030645: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030656: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030668: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030679: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030720: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030732: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030743: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030754: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030790: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030851: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030865: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030877: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030937: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030954: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030967: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031028: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031043: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031054: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031066: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031078: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031118: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031132: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031144: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031155: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031211: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031227: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031238: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031249: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031309: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031325: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031338: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031349: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031408: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031425: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031437: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031474: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031512: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031525: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031537: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031573: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031608: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031620: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031631: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031643: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031687: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031699: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031710: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031769: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031799: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031812: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031823: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031834: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031965: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032014: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032024: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032036: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032046: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032057: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032069: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032081: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032128: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032141: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032153: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032165: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032176: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032236: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032248: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032260: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032272: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032313: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032326: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032336: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032347: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032360: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032399: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank

and i am using below command to train
bazel run tensorflow/examples/speech_commands:train -- \ --data_dir=sound --wanted_words=yes,no --data_url=
",0,,1,2017-12-06T13:31:40Z,2017-12-06T18:05:33Z,NONE,2017-12-06T18:05:32Z
15152,Strange Dataset API behaviour,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
v1.4.0-rc1-11-g130a514 1.4.0
- **Python version**: 
sys.version_info(major=3, minor=4, micro=3, releaselevel='final', serial=0)
- **Bazel version (if compiling from source)**:
n/a
- **GCC/Compiler version (if compiling from source)**:
n/a
- **CUDA/cuDNN version**:
Cuda 8.0 
- **GPU model and memory**:
Titan XP 12Gb
- **Exact command to reproduce**:

### Describe the problem
I get very strange behaviour of image read function. See attached screenshot from tensorboard. This is NOT a tensorboard problem as I get images as they are from Dataset object.
![kodak_messed](https://user-images.githubusercontent.com/6204851/33659749-37586c4e-da79-11e7-8977-7d8f6da31c2d.png)

Source images are fine, I have attached a zip archive with source images:
[Kodak.zip](https://github.com/tensorflow/tensorflow/files/1534996/Kodak.zip)

```
import glob
import tensorflow as tf

def simply_read_image(image_path):
    image_string = tf.read_file(image_path)
    image_source = tf.image.decode_png(image_string, channels=3)
    image_source = tf.image.convert_image_dtype(image_source, dtype=tf.float32)
    return image_source

validation_files = glob.glob(os.path.join(validation_folder, '*.png'))

dataset = tf.data.Dataset.from_tensor_slices(validation_files)
dataset = dataset.map(simply_read_image).batch(len(validation_files)).repeat(30)

next_element = iterator.get_next()

    with tf.Session() as s:
        for i in range(30):
            im = s.run(next_element)
            for j in range(25):
                current_image = im[j]
                pass

```
Any ` current_image` with portrait orientation seems to be read incorrectly. 
### Source code / logs
There are no logs that could help",1,,2,2017-12-06T11:38:01Z,2017-12-07T05:13:24Z,NONE,2017-12-06T17:51:57Z
15148,Using function defun and while loops,type:bug/performance,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: 
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8/6
- **GPU model and memory**: Tesla x Pascal 12gb
- **Exact command to reproduce**: run defun_while.py

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Function.defun when used alongside a while loop is throwing shape errors.

More explicitly: when using a concatenation operation of sliced tensors with the loop variable, the newly defined op throws no errors.  However, when the slices are added to the loop variable, it throws shape errors related to the while loop. 

It might be an issue with the fetch argument that fails to work in this case.

The code (along with a more detailed explanation) can be found on:
https://stackoverflow.com/questions/47646962/tensorflow-function-defun-with-a-a-while-loop-in-the-body-is-throwing-shape-err

 




### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,4,2017-12-06T08:38:54Z,2017-12-23T03:31:03Z,NONE,2017-12-06T17:21:27Z
15147,summary_image_op_test_fixed_on_ppc64le,"awaiting review,cla: yes","Hi @gunan , 
As we discussed here https://github.com/tensorflow/tensorflow/issues/12325 , I have created this PR to fix summary_image_op_test on ppc64le.

Thanks!",1,,6,2017-12-06T08:34:16Z,2017-12-06T18:45:48Z,CONTRIBUTOR,2017-12-07T06:07:34Z
15146,Allow keras applications to load weights from arbitrary path,"awaiting testing (then merge),cla: yes","This PR allows tensorflow.python.keras.applications to load pretrained weights from an arbitrary filepath (rather than only ~/.keras/models).  It is the parallel PR to https://github.com/fchollet/keras/pull/8637 which was merged by @fchollet on November 30.

This change allows useres to load models in environments with limited access to ~/.keras/models 

Kaggle notebooks are an example of this environment, and this PR will help us support Keras in TensorFlow. 

I have locally tested that I get the same predictions when loading a model with `weights='imagenet'` and with `weights` pointing to another location with the same pretrained model file.
",1,,2,2017-12-06T07:41:19Z,2017-12-11T21:11:42Z,CONTRIBUTOR,2017-12-11T20:28:09Z
15141,MKL: Revving mkl-dnn to include all changes before 2017-11-20.,"awaiting testing (then merge),cla: yes","This commit will pull the latest changes from the mkl-dnn tree.

@jart the mirror.bazel.build URL doesn't exist: ""https://mirror.bazel.build/github.com/01org/mkl-dnn/archive/aab753280e83137ba955f8f19d72cb6aaba545ef.tar.gz"" can you create it?",1,,3,2017-12-05T22:30:35Z,2017-12-07T19:36:08Z,CONTRIBUTOR,2017-12-06T14:38:40Z
15139,Support --config=monolithic in tf.sysconfig.get_link_flags(),"awaiting testing (then merge),cla: yes","Currently, `tf.sysconfig.get_link_flags()` always adds `-ltensorflow_framework`.  With this change, it would check whether TensorFlow was built with `--config=monolithic`.",1,,13,2017-12-05T20:22:48Z,2017-12-15T16:59:54Z,CONTRIBUTOR,2017-12-05T20:24:57Z
15138,Document tf-coreml converter in lite/README.md,"cla: yes,comp:lite",,0,,2,2017-12-05T20:14:29Z,2017-12-06T14:27:55Z,CONTRIBUTOR,2017-12-05T20:15:02Z
15137,Tensorflow broken by new Bazel versions,stat:awaiting tensorflower,"Simplest way to reproduce the issue, run:
`$ bazel build --config=opt --incompatible_load_argument_is_label --nobuild //tensorflow/tools/pip_package:build_pip_package`

Suggested fix to `tensorflow/third_party/sycl/sycl/BUILD.tpl`:
```
-load(""platform"", ""sycl_library_path"")
+load("":platform.bzl"", ""sycl_library_path"")

-load(""platform"", ""readlink_command"")
+load("":platform.bzl"", ""readlink_command"")
```

This should address the immediate need.
There are other issues to fix (although not as pressing). You can see them by building using `--all_incompatible_changes`.

Let me know if you need any help.
Thanks!",0,,5,2017-12-05T18:46:47Z,2017-12-08T22:42:29Z,NONE,2017-12-05T18:56:14Z
15135,Solved for MNIST file downloading problem,"awaiting review,cla: yes","A lot of folks (#6742, #8126, #8134,#8116) were having trouble regarding this issue. Even I faced it today while writing the MNIST code as the mnist.py was unable to connect with the source url. I downloaded the files and pasted it in the folder containing the code and made some changes in the code which solves the need for connecting it to the website. Hope so this works out.",1,,9,2017-12-05T16:53:17Z,2017-12-06T21:52:35Z,NONE,2017-12-07T15:15:00Z
15133,"Revert ""Speed up safe_strtod and safe_strtof functions by using doubl",cla: yes,"e-conversion library (#12102)""

This reverts commit 495bb7b9f6b55b0e431fc604ad9dbf5415016d90.",0,,3,2017-12-05T15:06:55Z,2017-12-05T19:38:26Z,CONTRIBUTOR,2017-12-05T16:25:58Z
15132,Add decode_compressed support,"awaiting testing (then merge),cla: yes","This fix tries to address the issue raised in #14887 to add decode_compressed support.

The API will take a string Tensor (compressed with either ZLIB or GZIP) and output a string Tensor of the same shape with content uncompressed.

This fix fixes #14887.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,10,2017-12-05T15:04:15Z,2017-12-20T00:00:50Z,MEMBER,2017-12-19T02:02:23Z
15129,build error: undefined reference to `clock_gettime',stat:community support,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux CentOS 6.9
- **TensorFlow installed from (source or binary)**:
source 
- **TensorFlow version (use command below)**:
master branch: the latest version
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.8.0
- **GCC/Compiler version (if compiling from source)**:
4.8.2
- **CUDA/cuDNN version**:
No
- **GPU model and memory**:
No
- **Exact command to reproduce**:
bazel build --linkopt=-lrt -c opt --verbose_failures //tensorflow:libtensorflow_cc.so

### Describe the problem
I tried to build the tensor flow c++ lib from the source code, but it failed. 

### Source code / logs
ERROR: /home/baigang/Projects/xylib/thirdparty/tenserflow/package/tensorflow/tensorflow/cc/BUILD:422:1: Linking of rule '//tensorflow/cc:ops/random_ops_gen_cc' failed (Exit 1): gcc failed: error executing command 
  (cd /home/baigang/.cache/bazel/_bazel_baigang/d3e5550086b82aa173767408d0f485e7/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/baigang/bin \
    PWD=/proc/self/cwd \
  /usr/bin/gcc -o bazel-out/host/bin/tensorflow/cc/ops/random_ops_gen_cc '-Wl,-rpath,$ORIGIN/../../../_solib_k8/_U_S_Stensorflow_Scc_Cops_Srandom_Uops_Ugen_Ucc___Utensorflow' -Lbazel-out/host/bin/_solib_k8/_U_S_Stensorflow_Scc_Cops_Srandom_Uops_Ugen_Ucc___Utensorflow '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../..' -pthread -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,-S -Wl,@bazel-out/host/bin/tensorflow/cc/ops/random_ops_gen_cc-2.params)
bazel-out/host/bin/_solib_k8/_U_S_Stensorflow_Scc_Cops_Srandom_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `clock_gettime'
collect2: error: ld returned 1 exit status
Target //tensorflow:libtensorflow_cc.so failed to build
INFO: Elapsed time: 418.738s, Critical Path: 35.11s
FAILED: Build did NOT complete successfully",0,,9,2017-12-05T14:19:20Z,2017-12-05T19:15:18Z,NONE,2017-12-05T18:58:43Z
15128,GPU memory increases in multiples of batch_size 64 with allow_growth=True,type:support,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.4.0-14-gb5df90f', '1.4.1')
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0-6
- **CUDA/cuDNN version**: V9.0.176
- **GPU model and memory**: V100 AWS P3 8X large instance
- ** Exact command to reproduce**: Inception V3 training as mentioned in `models/research/slim`. `python train_image_classifier.py`


### Describe the problem
GPU memory is increasing in multiples of batch_size of 64. Using 8.9 GB of GPU memory for `batch_size=16` or `batch_size=32` or `batch_size=64`. And using 15.6 GB of GPU memory for `batch_size=96` and `batch_size=128`. I'd like to use `batch_size=96` so allocator won't throw warnings during global_step as it will have 2-3 GB unused.

Is this a feature? Can we change its functioning?",0,,3,2017-12-05T14:08:47Z,2017-12-07T05:18:41Z,NONE,2017-12-06T17:16:19Z
15126,Is their any java spark api available for using Tensor,stat:awaiting response,,0,,3,2017-12-05T13:36:02Z,2017-12-06T21:56:32Z,NONE,2017-12-06T01:01:20Z
15124,XLA Reshape_test failing for 3rd party platforms,,"I have opened a discussion on the XLA dev mailing list, but I think that this counts as an issue since if it isn't fixed then potentially all XLA tests will migrate to being unusable for 3rd party devices.

https://groups.google.com/forum/#!topic/xla-dev/9cY21Hi0s_s

The issue is that the code in reshape_test.cc assumes that any devices which are not CPU/CPU_PARALLEL/GPU will use the bfloat16 format.  This isn't true for the Graphcore device.


",0,,4,2017-12-05T12:03:28Z,2017-12-09T01:45:52Z,CONTRIBUTOR,2017-12-05T19:11:43Z
15123,Fix typo,cla: yes,*fix typo,0,,4,2017-12-05T11:57:25Z,2017-12-05T14:33:58Z,CONTRIBUTOR,2017-12-05T12:06:36Z
15122,tensorflow lite: error when convert frozen model to lite format,"comp:lite,stat:awaiting response","Hi,
I build the freeze .pb with the guide at https://github.com/tensorflow/models/tree/master/research/slim 
with the below step:
 
python train_image_classifier.py \
--train_dir /home/ubuntu/train/ \
--dataset_dir /home/ubuntu/vegetables/ \
--dataset_name=vegetables \
--dataset_split_name=train  \
--num_clones=2 \
--clone_on_cpu=True \
--checkpoint_path=/home/ubuntu/check_point/mobilenet_v1_1.0_224.ckpt \
--max_number_of_steps=10 \
--checkpoint_exclude_scopes=MobilenetV1/Logits,MobilenetV1/AuxLogits \
--model_name=mobilenet_v1


python tensorflow/python/tools/freeze_graph.py \
    --input_graph=/home/ubuntu/train/mobilenet_v1_224.pb \
    --input_checkpoint=/home/ubuntu/check_point/mobilenet_v1_1.0_224.ckpt \
    --input_binary=true \
    --output_graph=/home/ubuntu/train/frozen_mobilenet_v1_224.pb \
    --output_node_names=MobilenetV1/Predictions/Reshape_1

NOTE: I download mobilenet_v1_1.0_224.ckpt from http://download.tensorflow.org/models/mobilenet_v1_1.0_224_2017_06_14.tar.gz

But when I convert to lite mode with 

ubuntu@ip-172-31-27-248:~/tensorflow$ bazel-bin/tensorflow/contrib/lite/toco/toco \
  --input_format=TENSORFLOW_GRAPHDEF \
   --input_format=TENSORFLOW_GRAPHDEF \
   --input_file=/home/ubuntu/mobilenet_v1_1.0_224/frozen_mobilenet_v1_224.pb \
   --output_format=TFLITE \
   --output_file=/tmp/mobilenet_v1_1.0_224.lite --inference_type=FLOAT \
   --inference_input_type=FLOAT \
   --input_arrays=input \
   --output_arrays=MobilenetV1/Predictions/Reshape_1 --input_shapes=1,224,224,3
 
2017-12-05 09:53:56.604720: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 336 operators, 502 arrays (0 quantized)
2017-12-05 09:53:56.627922: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 31 operators, 88 arrays (0 quantized)
2017-12-05 09:53:56.628156: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 31 operators, 88 arrays (0 quantized)
2017-12-05 09:53:56.628327: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 6422528 bytes, theoretical optimal value: 4816896 bytes.
2017-12-05 09:53:56.628487: I tensorflow/contrib/lite/toco/toco_tooling.cc:268] Estimated count of arithmetic ops: 1.14264 billion (note that a multiply-add is counted as 2 ops).
2017-12-05 09:53:56.628653: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze
Aborted (core dumped)

Pls help me! 

my other question is when I download the pretrain freeze pb from https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_float_2017_11_08.zip 
It's work when I use toco tools. where I can find guide which generate these freeze pb.


Pls Help! 


 





",1,,12,2017-12-05T10:24:56Z,2018-01-19T00:57:43Z,NONE,2017-12-06T17:17:21Z
15121,improve py_func,"awaiting testing (then merge),cla: yes","See #14448
Improved `py_func` to accept nested structures as input and as output like in `tf.data.Dataset.form_generator`

 - relable inp to args and Tout to output_types
 - add arguments kwargs and output_shapes
 - allow args/kwargs/output_types/output_shapes to be a nested structure

Open questions:
 - allow output_types to be callable? (Dynamic output_types inference from args/kwargs)
 - (new) argument names
 - backward compatibility for old names",1,,23,2017-12-05T10:08:33Z,2018-01-13T04:58:13Z,CONTRIBUTOR,2017-12-07T19:59:04Z
15120,[Feature Request] Please make Estimator.export_savedmodel support input types other than tf.Example,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4.1
- **Python version**: 
3.5.3
- **CUDA/cuDNN version**:
None
- **GPU model and memory**:
None
- **Exact command to reproduce**:

### Describe the problem
I have a model,  I want to export it into SavedModel format by using tf.estimator API, because I'm using this API for training. Unfortunately,tf.estimator.export.build_raw_serving_input_receiver_fn and tf.saved_model.signature_def_utils.classification_signature_def require the input features must be encoded in tf.Example format.

In order to use the ""export_savedmodel"" function for exporting, when writing a custom model_fn for Estimator, I must populate the export_outputs element of the tf.estimator.EstimatorSpec return value.  Each output value must be an ExportOutput object such as tf.estimator.export.ClassificationOutput, tf.estimator.export.RegressionOutput, or tf.estimator.export.PredictOutput. Those three output types only support tf.Example as the input. 

```python
    if (classes is not None
        and not (isinstance(classes, ops.Tensor)
                 and dtypes.as_dtype(classes.dtype) == dtypes.string)):
      raise ValueError('Classification classes must be a string Tensor; '
                       'got {}'.format(classes))
```

Because they will use signature_def_utils.classification_signature_def(or predict_signature_def,...) to build the signature.

Though I may build a new output type by myself, it doesn't. 
As the comment in tensorflow\python\estimator\export\export.py:build_all_signature_defs stated:
""the call to is_valid_signature here should not remove anything else.""
If you really want to remove it, please show me a warning.

### Source code / logs
```python
""""""Convolutional Neural Network Estimator for MNIST, built with tf.layers.""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import os
import sys

import tensorflow as tf

parser = argparse.ArgumentParser()

# Basic model parameters.
parser.add_argument('--batch_size', type=int, default=100,
                    help='Number of images to process in a batch')

parser.add_argument('--model_dir', type=str, default='/tmp/mnist_model',
                    help='The directory where the model will be stored.')


parser.add_argument('--data_format', type=str, default=None,
    choices=['channels_first', 'channels_last'],
    help='A flag to override the data format used in the model. channels_first '
         'provides a performance boost on GPU but is not always compatible '
         'with CPU. If left unspecified, the data format will be chosen '
         'automatically based on whether TensorFlow was built for CPU or GPU.')

_NUM_IMAGES = {
    'train': 50000,
    'validation': 10000,
}

def mnist_model(inputs, mode, data_format):
  """"""Takes the MNIST inputs and mode and outputs a tensor of logits.""""""
  # Input Layer
  # Reshape X to 4-D tensor: [batch_size, width, height, channels]
  # MNIST images are 28x28 pixels, and have one color channel
  inputs = tf.reshape(inputs, [-1, 28, 28, 1])

  if data_format is None:
    data_format = ('channels_first' if tf.test.is_built_with_cuda() else
                   'channels_last')

  if data_format == 'channels_first':
    inputs = tf.transpose(inputs, [0, 3, 1, 2])

  conv1 = tf.layers.conv2d(inputs=inputs,
      filters=32,
      kernel_size=[5, 5],
      padding='same',
      activation=tf.nn.relu,
      data_format=data_format)


  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2,
                                  data_format=data_format)


  conv2 = tf.layers.conv2d(inputs=pool1,
      filters=64,
      kernel_size=[5, 5],
      padding='same',
      activation=tf.nn.relu,
      data_format=data_format)

  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2,
                                  data_format=data_format)


  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])

  # Dense Layer
  # Densely connected layer with 1024 neurons
  # Input Tensor Shape: [batch_size, 7 * 7 * 64]
  # Output Tensor Shape: [batch_size, 1024]
  dense = tf.layers.dense(inputs=pool2_flat, units=1024,
                          activation=tf.nn.relu)

  # Add dropout operation; 0.6 probability that element will be kept
  dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=(mode == tf.estimator.ModeKeys.TRAIN))

  # Logits layer
  # Input Tensor Shape: [batch_size, 1024]
  # Output Tensor Shape: [batch_size, 10]
  logits = tf.layers.dense(inputs=dropout, units=10)
  return logits


def mnist_model_fn(features, labels, mode, params):
  if isinstance(features,dict):
    features = features['image_raw']
  """"""Model function for MNIST.""""""
  logits = mnist_model(features, mode, params['data_format'])

  predictions = {
      'classes': tf.argmax(input=logits, axis=1),
      'probabilities': tf.nn.softmax(logits, name='softmax_tensor')
  }

  if mode == tf.estimator.ModeKeys.PREDICT:
    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions,
                                      export_outputs={'class': tf.estimator.export.ClassificationOutput(classes=tf.as_string(predictions['classes']))})  

  loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)

  # Configure the training op
  if mode == tf.estimator.ModeKeys.TRAIN:
    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)
    train_op = optimizer.minimize(loss, tf.train.get_or_create_global_step())
  else:
    train_op = None

  accuracy = tf.metrics.accuracy(tf.argmax(labels, axis=1), predictions['classes'])
  metrics = {'accuracy': accuracy}

  # Create a tensor named train_accuracy for logging purposes
  tf.identity(accuracy[1], name='train_accuracy')
  tf.summary.scalar('train_accuracy', accuracy[1])

  return tf.estimator.EstimatorSpec(mode=mode,
      predictions=predictions,
      loss=loss,
      train_op=train_op,
      eval_metric_ops=metrics)

def main(unused_argv):
  # Create the Estimator
  mnist_classifier = tf.estimator.Estimator(model_fn=mnist_model_fn, model_dir=FLAGS.model_dir,
      params={'data_format': FLAGS.data_format})
  image = tf.placeholder(tf.float32,[None])
  mnist_classifier.export_savedmodel(""bb"", tf.estimator.export.build_raw_serving_input_receiver_fn({""image_raw"":image}))  

if __name__ == '__main__':
  tf.logging.set_verbosity(tf.logging.INFO)
  FLAGS, unparsed = parser.parse_known_args()
  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
```",0,,1,2017-12-05T09:03:56Z,2017-12-05T10:37:47Z,CONTRIBUTOR,2017-12-05T11:05:09Z
15118,"Building Tensorflow from source failed, compilation error",type:build/install,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat Enterprise Linux Server release 6.9 (Santiago)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:
- **Python version**:  2.7.13
- **Bazel version (if compiling from source)**:  0.6.1 
- **GCC/Compiler version (if compiling from source)**: 4.4.7
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:  
bazel build --config=opt --verbose_failures //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
The following errors appears at the building stage when I tried to install Tensorflow for CPU from source.   

### Source code / logs
```
WARNING: /home/localuser/tensorflow/tensorflow/core/BUILD:1813:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/localuser/tensorflow/tensorflow/tensorflow.bzl:1100:30.
WARNING: /home/localuser/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/localuser/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Found 1 target...
ERROR: /home/localuser/tensorflow/tensorflow/core/grappler/costs/BUILD:112:1: C++ compilation of rule '//tensorflow/core/grappler/costs:robust_stats' failed (Exit 1): gcc failed: error executing command 
  (cd /root/.cache/bazel/_bazel_localuser/88f96db14f5044a661b2d9ab97596b51/execroot/org_tensorflow && \
  exec env - \
    PATH=/opt/jdk1.8.0_144/bin:/home/localuser/bazel-0.6.1/output:/usr/local/MATLAB/R2016a/bin:/Ansys/AnsysEM170/AnsysEM17.0/Linux64:/Ansys/AnsysEM170/LayoutIntegrations17.0/Linux64:/usr/local/bin:/usr/local/netscape:/usr/sbin:/usr/bin:/usr/lib:/bin:/sbin:/etc:/opt/lumerical/fdtd/bin:/lib:.:/root/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/local/bin/python2.7 \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-march=native' '-std=c++0x' -MD -MF bazel-out/local-opt/bin/tensorflow/core/grappler/costs/_objs/robust_stats/tensorflow/core/grappler/costs/robust_stats.pic.d '-frandom-seed=bazel-out/local-opt/bin/tensorflow/core/grappler/costs/_objs/robust_stats/tensorflow/core/grappler/costs/robust_stats.pic.o' -fPIC -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/core/grappler/costs/robust_stats.cc -o bazel-out/local-opt/bin/tensorflow/core/grappler/costs/_objs/robust_stats/tensorflow/core/grappler/costs/robust_stats.pic.o).
tensorflow/core/grappler/costs/robust_stats.cc: In function 'std::pair<double, double> tensorflow::grappler::ScaledMedianAbsoluteDeviation(const std::vector<double, std::allocator<double> >&)':
tensorflow/core/grappler/costs/robust_stats.cc:71: error: expected initializer before ':' token
tensorflow/core/grappler/costs/robust_stats.cc:75: error: expected primary-expression before 'return'
tensorflow/core/grappler/costs/robust_stats.cc:75: error: expected ')' before 'return'
tensorflow/core/grappler/costs/robust_stats.cc: In constructor 'tensorflow::grappler::RobustStats::RobustStats(const std::vector<double, std::allocator<double> >&)':
tensorflow/core/grappler/costs/robust_stats.cc:79: error: type 'tensorflow::grappler::RobustStats' is not a direct base of 'tensorflow::grappler::RobustStats'
tensorflow/core/grappler/costs/robust_stats.cc: In function 'double tensorflow::grappler::UpdateHuberMean(const std::vector<double, std::allocator<double> >&, double, double)':
tensorflow/core/grappler/costs/robust_stats.cc:95: error: expected initializer before ':' token
tensorflow/core/grappler/costs/robust_stats.cc:152: error: expected primary-expression at end of input
tensorflow/core/grappler/costs/robust_stats.cc:152: error: expected ';' at end of input
tensorflow/core/grappler/costs/robust_stats.cc:152: error: expected primary-expression at end of input
tensorflow/core/grappler/costs/robust_stats.cc:152: error: expected ')' at end of input
tensorflow/core/grappler/costs/robust_stats.cc:152: error: expected statement at end of input
tensorflow/core/grappler/costs/robust_stats.cc:92: warning: unused variable 'num_within'
tensorflow/core/grappler/costs/robust_stats.cc:93: warning: unused variable 'sum'
tensorflow/core/grappler/costs/robust_stats.cc:152: error: expected '}' at end of input
tensorflow/core/grappler/costs/robust_stats.cc:152: warning: no return statement in function returning non-void
tensorflow/core/grappler/costs/robust_stats.cc: At global scope:
tensorflow/core/grappler/costs/robust_stats.cc:152: error: expected '}' at end of input
tensorflow/core/grappler/costs/robust_stats.cc:152: error: expected '}' at end of input
tensorflow/core/grappler/costs/robust_stats.cc:63: warning: 'std::pair<double, double> tensorflow::grappler::ScaledMedianAbsoluteDeviation(const std::vector<double, std::allocator<double> >&)' defined but not used
cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object""
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 10.342s, Critical Path: 0.72s
```

### Comments
I checked that it's not a memory issue. 

### Related issues
[#8642](https://github.com/tensorflow/tensorflow/issues/8462)
",1,,7,2017-12-05T08:23:00Z,2018-01-24T22:59:15Z,NONE,2017-12-06T17:14:35Z
15117,Fix the iOS makefile on tensorflow lite.,"awaiting testing (then merge),cla: yes,comp:lite","Just make it right.
Ref: https://github.com/tensorflow/tensorflow/issues/15074",1,,5,2017-12-05T08:08:41Z,2017-12-08T03:06:07Z,CONTRIBUTOR,2017-12-05T16:34:15Z
15116,Any other official way(not public in github issues) to report security bug of tensorflow?,stat:awaiting response,"Hi,

I have found a security issue in Tensorflow. An attacker can easily execute arbitary code on victim machine through this issue. I think it has severe secure impact on tensorflow users. So it is not proper to disclosure this issue on github issues publicly before it is fixed.

Is there any other official way to report security issue? I will explain the details.
Thanks.",0,,5,2017-12-05T06:48:56Z,2018-01-03T12:04:34Z,NONE,2017-12-05T19:11:38Z
15115,Wrong result when computing accuracy using tf.metrics.accuracy,,"### System information

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 1709
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: Python 3.5.2 :: Anaconda custom (64-bit)

### Describe the problem

I found the result `tf.metrics.accuracy` returns is incorrect when I trained my model. To verify this I wrote a  simple program.

```python
import tensorflow as tf

sess = tf.Session()
labels = tf.placeholder(tf.int32)
predictions = tf.placeholder(tf.int32)
acc, _ = tf.metrics.accuracy(labels, predictions)
my_acc = tf.reduce_mean(tf.cast(tf.equal(labels, predictions), tf.float32))

feed_dict = {
    labels: [1, 2, 3, 4, 5], 
    predictions: [1, 2, 3, 4, 5]
}
sess.run(tf.global_variables_initializer())
sess.run(tf.local_variables_initializer())

sess.run(acc, feed_dict)  # 0.0
sess.run(my_acc, feed_dict)  # 1.0
```

You can see that `acc` and `my_acc` is different and acc is wrong. I double checked [the doc](https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy) and still confused. Is there anything I missed? Thank you.",0,,6,2017-12-05T04:18:37Z,2017-12-05T18:54:56Z,CONTRIBUTOR,2017-12-05T08:48:48Z
15114,Compiling CPU version on windows X86,stat:awaiting response,"System Information:
WIN 10
Visual studio 2015
Swigwin 3.0.12
Python 3.5.3
Cmake 3.10.0

Question description:

I am a new one about tensorflow, I used cmake command in cmd just like this:

cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=D:/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=D:/Programs/Python/Python35/python.exe -DPYTHON_LIBRARIES=D:/Programs/Python/Python35/libs/python35.lib -Dtensorflow_ENABLE_GPU=OFF

and got the x64 vs project, but when I wanted to generate x86 version with similar commnd like:  

cmake .. -A x86 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=D:/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=D:/Programs/Python/Python35/python.exe -DPYTHON_LIBRARIES=D:/Programs/Python/Python35/libs/python35.lib -Dtensorflow_ENABLE_GPU=OFF

then I got a error:

cmake .. -A x86 -DCMAKE_BUILD_TYPE=Debug -DSWIG_EXECUTABLE=D:/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=D:/Programs/Python/Python35/python.exe -DPYTHON_LIBRARIES=D:/Programs/Python/Python35/libs/python35.lib -Dtensorflow_ENABLE_GPU=OFF
CMake Error at CMakeLists.txt:5 (project):
  Failed to run MSBuild command:

    C:/Program Files (x86)/MSBuild/14.0/bin/MSBuild.exe

  to get the value of VCTargetsPath:


Now I want to ask: does tensorflow support x86 windows? Did anyone build the x86 version once before?  ",0,,2,2017-12-05T03:45:10Z,2017-12-06T08:33:50Z,NONE,2017-12-05T13:01:58Z
15112,variables.get_global_step() is deprecated,"awaiting testing (then merge),cla: yes","variables.get_global_step() is deprecated, use `training_util.get_global_step()` instead

WARNING message:

```
tensorflow/contrib/timeseries/python/timeseries/head.py:63: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
```",1,,5,2017-12-05T03:15:41Z,2017-12-07T05:17:22Z,CONTRIBUTOR,2017-12-05T03:17:06Z
15108,"error: namespace ""Eigen::half_impl"" has no member ""__half_raw"" when building latest TensorFlow/CUDA 9.0",,"When trying to build latest TensorFlow for CUDA 9.0/CuDNN 7.0, from today's head

Complete instructions to reproduce are [here](https://github.com/yaroslavvb/tf_build), but basically I followed steps in in [Dockerfile.devel-gpu](https://github.com/tensorflow/tensorflow/blob/c5c642e051f1a7876d099bfcd9f8a2ecaf7227b8/tensorflow/tools/docker/Dockerfile.devel-gpu) to install dependencies, and then did `bazel build -c opt --config=cuda`

Here are the errors

```
INFO: From Compiling tensorflow/stream_executor/cuda/cuda_blas.cc:
tensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cudaDataType_t perftools::gputools::cuda::{anonymous}::CUDAComputationType(perftools::gputools::blas::ComputationType)':
tensorflow/stream_executor/cuda/cuda_blas.cc:527:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
INFO: From Compiling tensorflow/core/kernels/dense_to_sparse_batch_dataset_op.cc:
tensorflow/core/kernels/dense_to_sparse_batch_dataset_op.cc: In member function 'virtual void tensorflow::{anonymous}::DenseToSparseBatchDatasetOp::MakeDataset(tensorflow::OpKernelContext*, tensorflow::DatasetBase*, tensorflow::DatasetBase**)':
tensorflow/core/kernels/dense_to_sparse_batch_dataset_op.cc:83:71: warning: 'batch_size' may be used uninitialized in this function [-Wmaybe-uninitialized]
         : batch_size_(batch_size), row_shape_(row_shape), input_(input) {
                                                                       ^
tensorflow/core/kernels/dense_to_sparse_batch_dataset_op.cc:41:11: note: 'batch_size' was declared here
     int64 batch_size;
           ^
INFO: From Compiling tensorflow/core/kernels/histogram_op_gpu.cu.cc:
./tensorflow/core/util/cuda_kernel_helper.h(109): error: namespace ""Eigen::half_impl"" has no member ""__half_raw""

./tensorflow/core/util/cuda_kernel_helper.h(109): error: expected a "")""

./tensorflow/core/util/cuda_kernel_helper.h(110): error: namespace ""Eigen::half_impl"" has no member ""__half_raw""

./tensorflow/core/util/cuda_kernel_helper.h(113): error: namespace ""Eigen::half_impl"" has no member ""__half_raw""

./tensorflow/core/util/cuda_kernel_helper.h(113): error: expected a "";""

./tensorflow/core/util/cuda_kernel_helper.h(119): error: namespace ""Eigen::half_impl"" has no member ""__half_raw""

./tensorflow/core/util/cuda_kernel_helper.h(119): error: expected a "")""

./tensorflow/core/util/cuda_kernel_helper.h(120): error: namespace ""Eigen::half_impl"" has no member ""__half_raw""

./tensorflow/core/util/cuda_kernel_helper.h(123): error: namespace ""Eigen::half_impl"" has no member ""__half_raw""

./tensorflow/core/util/cuda_kernel_helper.h(123): error: expected a "";""

10 errors detected in the compilation of ""/tmp/tmpxft_000137d9_00000000-6_histog

```",1,,19,2017-12-04T23:24:06Z,2017-12-23T21:20:44Z,CONTRIBUTOR,2017-12-05T20:51:26Z
15107,Fix link to BUILD file in android readme.,"awaiting testing (then merge),cla: yes",Simple syntax error.,1,,2,2017-12-04T22:02:09Z,2017-12-05T18:25:35Z,CONTRIBUTOR,2017-12-05T14:54:05Z
15106,MKL: Dockerfile Locked to Broadwell/AVX2 arch to work around Eigen Issues with AVX512,"cla: yes,stat:awaiting response","Eigen currently has issues when Tensorflow is compiled with -march=skylake, or the container is build on a skylake system with -march=native. The container is now compiled with -march=broadwell, which adds AVX2 instructions, which are supported on Skylake, Haswell, Broadwell, KNL and KNM architectures.",1,,4,2017-12-04T21:52:45Z,2018-01-08T19:20:49Z,CONTRIBUTOR,2017-12-14T21:38:35Z
15105,Using tfdbg with Monitored Session,stat:awaiting tensorflower,"### System information
- I am using a modified version of  [CIFAR10 tutorial](https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10) TensorFlow.
- CentOS Linux release 7.3.1611
- TensrFlow from Source
- TensorFlow v1.3
-  Python 2.7

### Describe the problem
Hello,

I see that I can not use tfdbg with Monitored session. That is if I try to wrap `mon_sess = tf_debug.LocalCLIDebugWrapperSession(mon_sess)` where `mon_sess` is `tf.train.MonitoredTrainingSession(` I get the following error:

`TypeError: Expected type <class 'tensorflow.python.client.session.BaseSession'>; got type <class 'tensorflow.python.training.monitored_session.MonitoredSession'>`

Can there be a support from Debugger with Monitored Sessions?",1,,4,2017-12-04T21:08:37Z,2017-12-12T03:12:38Z,NONE,2017-12-04T21:51:36Z
15104,Building tensorflow from the sourcefile,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Redhat
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.0
- **Python version**: 
2.7.14
- **Bazel version (if compiling from source)**:

- **GCC/Compiler version (if compiling from source)**:
4.8.5
- **CUDA/cuDNN version**:
8/5
- **GPU model and memory**:

- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Getting the following error:
```
ERROR: /home/amalik/tensorflow/tensorflow/tools/test/BUILD:81:1: Traceback (most recent call last):
	File ""/home/amalik/tensorflow/tensorflow/tools/test/BUILD"", line 81
		tf_cc_logged_benchmark(name = ""cast_op_benchmark"", target..."")
	File ""/home/amalik/tensorflow/tensorflow/tools/test/performance.bzl"", line 23, in tf_cc_logged_benchmark
		list((set(tags) + set([""benchmark-tes...""])))
	File ""/home/amalik/tensorflow/tensorflow/tools/test/performance.bzl"", line 23, in list
		set(tags)
The `set` constructor for depsets is deprecated and will be removed. Please use the `depset` constructor instead. You can temporarily enable the deprecated `set` constructor by passing the flag --incompatible_disallow_set_constructor=false.
ERROR: package contains errors: tensorflow/tools/test.
ERROR: /home/amalik/tensorflow/tensorflow/tools/test/BUILD:86:1: Traceback (most recent call last):
	File ""/home/amalik/tensorflow/tensorflow/tools/test/BUILD"", line 86
		tf_py_logged_benchmark(name = ""rnn_op_benchmark"", target ..."")
	File ""/home/amalik/tensorflow/tensorflow/tools/test/performance.bzl"", line 52, in tf_py_logged_benchmark
		tf_cc_logged_benchmark(name = name, target = target, benchm..., <2 more arguments>)
	File ""/home/amalik/tensorflow/tensorflow/tools/test/performance.bzl"", line 23, in tf_cc_logged_benchmark
		list((set(tags) + set([""benchmark-tes...""])))
	File ""/home/amalik/tensorflow/tensorflow/tools/test/performance.bzl"", line 23, in list
		set(tags)
The `set` constructor for depsets is deprecated and will be removed. Please use the `depset` constructor instead. You can temporarily enable the deprecated `set` constructor by passing the flag --incompatible_disallow_set_constructor=false.
ERROR: /home/amalik/tensorflow/tensorflow/core/kernels/BUILD:58:14: Traceback (most recent call last):
	File ""/home/amalik/tensorflow/tensorflow/core/kernels/BUILD"", line 53
		config_setting(name = ""xsmm_backward"", values = {...""})
	File ""/home/amalik/tensorflow/tensorflow/core/kernels/BUILD"", line 58, in config_setting
		{""define"": ""tensorflow_xsmm=1"", ""define"": ""tensorflow_xsmm_backward=1""}
Duplicated key ""define"" when creating dictionary.
ERROR: package contains errors: tensorflow/core/kernels.
ERROR: error loading package 'tensorflow/core/kernels': Package 'tensorflow/core/kernels' contains errors.
```

Note: Tried other plateforms but could get any reply. I am not sure its a bug or else. I apologize if I am using the wrong forum. I am trying to install tensorflow from source so I can use MPI


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,6,2017-12-04T19:57:22Z,2017-12-05T13:00:06Z,NONE,2017-12-05T06:34:40Z
15103,No GPU OpKernel for tf.exp() operation for Complex64,"stat:contributions welcome,type:feature","I am running tensorflow 1.4.0 from nightly build ('v1.3.0-rc1-5297-g4b7d79b6ea'  on ubuntu 16.04). I've had success working in eager mode (great job with this guys!) however I think I found a small bug:

It seems that there is no OpKernel on device='GPU'  for the tf.exp() operation applied to complex numbers in eager mode.  This can be reproduced with the below code:

```
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

with tf.device('/gpu:0'):
  g = tf.spectral.rfft(tf.ones(64))
  
  tf.exp(g)
```
which results in
```NotFoundError: No registered 'Exp' OpKernel for GPU devices compatible with node Exp = Exp[T=DT_COMPLEX64](dummy_input)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
 [Op:Exp]
```

a more practical example that would lead to this same error:

```
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)

frame_length=256
frame_step=64
n_mels = 64
sr=16000
filename = 'path/to/a.wav'

some_signal = tf.contrib.ffmpeg.decode_audio(tf.read_file(filename), 
                                     file_format='wav', 
                                     samples_per_second=16000, 
                                     channel_count=1)

with tf.device('/gpu:0'):
  stft = tf.contrib.signal.stft(tf.transpose(some_signal), frame_length=frame_length, 
                                  frame_step=frame_step, fft_length=frame_length)

  linear_to_mel_weight_matrix = tf.contrib.signal.linear_to_mel_weight_matrix(
      n_mels, 1+frame_length//2, sr)

  magnitude_spectrograms = tf.abs(stft)
  log_mel_spec = tf.log(1e-6+ tf.tensordot(magnitude_spectrograms,
                                           linear_to_mel_weight_matrix, 
                                           axes = [[2], [0]]))

  mfccs = tf.contrib.signal.mfccs_from_log_mel_spectrograms(log_mel_spec)
```

Keeping operations on CPU works just fine but I figured this would be easy to implement for GPU as well. Thanks
",0,,9,2017-12-04T19:43:46Z,2017-12-15T19:54:38Z,NONE,2017-12-04T21:52:41Z
15102,Documentation required for SetIsStateful() method in OP registration,type:docs,"There are some examples with custom ops that use SetIsStateful() method during registration, e.g. https://www.tensorflow.org/extend/new_data_formats

But there is no documentation about that method.",0,,3,2017-12-04T17:05:47Z,2017-12-09T01:48:32Z,NONE,2017-12-04T17:29:30Z
15100,Branch 177809511,cla: yes,,1,,2,2017-12-04T15:51:49Z,2017-12-04T19:34:23Z,CONTRIBUTOR,2017-12-04T15:52:12Z
15099,[CMake] Re-Enable include,"awaiting testing (then merge),cla: yes",Issue #3996 (and referenced tickets) are already fixed.,1,,4,2017-12-04T15:14:13Z,2017-12-07T19:57:52Z,CONTRIBUTOR,2017-12-04T21:57:29Z
15097,Fixing activate the Virtualenv,cla: yes,in Installing TensorFlow on macOS. Probably needs to be fixed for Linux as well.,0,,2,2017-12-04T13:23:40Z,2017-12-04T16:02:20Z,CONTRIBUTOR,2017-12-04T16:02:06Z
15095,add label_image for tflite,"awaiting testing (then merge),cla: yes,comp:lite","label_image for TensorFlow Lite is inspired by TensorFlow's
[label_image](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image), a command line app to load and run classifier
models.",1,,4,2017-12-04T12:30:47Z,2018-01-13T04:57:20Z,CONTRIBUTOR,2017-12-29T01:24:25Z
15094,Fix typo,"awaiting testing (then merge),cla: yes",,1,,2,2017-12-04T12:20:38Z,2017-12-04T19:15:00Z,CONTRIBUTOR,2017-12-04T16:00:25Z
15093,tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Sub,"comp:lite,stat:awaiting response","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 17.04
- **TensorFlow installed from (source or binary)**:pip
- **TensorFlow version (use command below)**:1.4.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:8.0/6.0
- **GPU model and memory**:GTX 1060 with 6GB memory
- **Exact command to reproduce**:bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/wh/gitmodel/tensorflow/wh/frozen_1.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=/home/wh/gitmodel/tensorflow/wh/frozen_lite.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=image_1 --output_arrays=InferenceTower/output6 --input_shapes=1,320,480,3

### Describe the problem
I am trying to convert a graph from .pb to .lite format using toco, but I get this error:
2017-12-04 20:14:38.202653: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Sub
I think Sub is the basic op, Lite shoud support it. Is it right?",1,,6,2017-12-04T12:15:55Z,2018-01-24T23:54:12Z,NONE,2017-12-04T18:37:49Z
15091,"GatherNd InvalidArgumentError: flat indices[8, :] = [8, -1] does not index into param",,"### System information

- **OS Platform and Distribution**: Ubuntu 17.10 (Linux-4.13.0-17-generic-x86_64-with-debian-stretch-sid)
- **TensorFlow installed from**: binary
- **TensorFlow version**: v1.3.0-rc2-20-g0787eee 1.3.0 (same on 1.4.0)
- **Python version**: 3.6.3
- **GPU model and memory**: N/A (CPU only)
- **Numpy version:** 1.13.3
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **C++ compiler version**: (Ubuntu 7.2.0-8ubuntu3) 7.2.0
- **Have I written custom code**: Yes
- **Exact command to reproduce**: There is no single command. The error arises when trying to fetch a TensorFlow tensor. Please see the description below.

### Describe the problem

I am training a recurrent neural network (RNN) whereby I give it variable-length sequences of random steps in two dimensions and train it to recognize the quadrant in which the random walker ended up. This RNN works fine on a Windows machine with GPU (all other specs are otherwise the same as above). However, on a Linux machine with CPU only, I get an error which mysteriously traces back to a dimensionality hiccup with `GatherNd`. 

The code for the full RNN is too convoluted to post, but as you can see from below, I am printing out the fetches to two tensors `tf_weights` and `tf_last` at each iteration of the batching. (In this case the training data is consumed one batch at a time for a total of 28 batches). The batches are very basic loops so you'd think that if a fetch works in one iteration of the loop it should also work for the next. This is indeed the case for `tf_weights` but not for `tf_last` which, for no apparent reason, fails to evaluate at batch 7/28. `tf_last` can be traced to a `GatherNd` operation.

I thoroughly inspected the data and it looks fine. _Please keep in mind that my code does work under Windows with GPU with all other specs remaining the same, so it's hard to conceive of a bug from within the code itself._

### Source code

```
import tensorflow as tf

graph = tf.Graph()

with graph.as_default():

    tf_features = tf.placeholder(
            tf.float32, [batch_size, max_seq_len, input_dim], 
            name = 'tf_features')

    tf_targets = tf.placeholder(
            tf.float32, [batch_size, target_len],
            name = 'tf_targets')

    tf_seq_len = tf.placeholder(
            tf.int32, [batch_size],
            name = 'tf_seq_len')

    tf_cell = 'tf.contrib.rnn.'+cell_type+'('+str(num_hidden)+')'
    tf_cell = eval(tf_cell)

    tf_output, tf_state = tf.nn.dynamic_rnn(
            tf_cell, tf_features, sequence_length = tf_seq_len, 
            dtype = tf.float32)
  
    tf_output = tf_output[:, :, :num_hidden]
           
    tf_last = tf.gather_nd(
            tf_output, 
            tf.stack([tf.range(batch_size), tf_seq_len-1], axis = 1), # batch_size
            name = 'tf_last')
```

### Logs

```
---------------- RUN 0/0
 ---------------- FOLD 0/2
  ---------------- TRAIN
  Epoch 0
  - Optimizing optimize
   ---------------- BATCH 0/28 [0, 50], len_data = 1499
   *** tf_weights: -0.563565
   *** tf_last: -0.192192
   ---------------- BATCH 1/28 [50, 100], len_data = 1499
   *** tf_weights: -0.553565
   *** tf_last: -0.0799878
   ---------------- BATCH 2/28 [100, 150], len_data = 1499
   *** tf_weights: -0.546777
   *** tf_last: -0.118384
   ---------------- BATCH 3/28 [150, 200], len_data = 1499
   *** tf_weights: -0.538511
   *** tf_last: -0.142531
   ---------------- BATCH 4/28 [200, 250], len_data = 1499
   *** tf_weights: -0.529593
   *** tf_last: -0.147268
   ---------------- BATCH 5/28 [250, 300], len_data = 1499
   *** tf_weights: -0.520294
   *** tf_last: -0.014847
   ---------------- BATCH 6/28 [300, 350], len_data = 1499
   *** tf_weights: -0.510754
   *** tf_last: -0.094138
   ---------------- BATCH 7/28 [350, 400], len_data = 1499
   *** tf_weights: -0.504503
   *** Failed to evaluate tf_last
Traceback (most recent call last):

  File ""<ipython-input-25-49e8720a879a>"", line 1, in <module>
    runfile('/home/ala/Python/domains/main.py', wdir='/home/ala/Python/domains')

  File ""/home/ala/anaconda3/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py"", line 710, in runfile
    execfile(filename, namespace)

  File ""/home/ala/anaconda3/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py"", line 101, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""/home/ala/Python/domains/main.py"", line 166, in <module>
    metrics_valid = RNN_object.validate(data_train, KFolds)

  File ""/home/ala/Python/domains/classifiers.py"", line 727, in validate
    reinitialize = True))

  File ""/home/ala/Python/domains/classifiers.py"", line 366, in train
    self.optimize(data)

  File ""/home/ala/Python/domains/classifiers.py"", line 1102, in optimize
    optimized_driving_metric = self.evaluate(data, driving_metric)

  File ""/home/ala/Python/domains/classifiers.py"", line 1796, in evaluate
    evaluated_tf_var = self.sess.run(eval(tf_var), feed_dict)

  File ""/home/ala/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)

  File ""/home/ala/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)

  File ""/home/ala/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)

  File ""/home/ala/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)

InvalidArgumentError: flat indices[8, :] = [8, -1] does not index into param (shape: [50,300,2]).
	 [[Node: tf_last = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](strided_slice_2, stack)]]

Caused by op 'tf_last', defined at:
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/spyder/utils/ipython/start_kernel.py"", line 245, in <module>
    main()
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/spyder/utils/ipython/start_kernel.py"", line 241, in main
    kernel.start()
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2728, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2856, in run_ast_nodes
    if self.run_code(code, result):
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-25-49e8720a879a>"", line 1, in <module>
    runfile('/home/ala/Python/domains/main.py', wdir='/home/ala/Python/domains')
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py"", line 710, in runfile
    execfile(filename, namespace)
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py"", line 101, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)
  File ""/home/ala/Python/domains/main.py"", line 109, in <module>
    RNN_object.define(data_test)
  File ""/home/ala/Python/domains/classifiers.py"", line 1514, in define
    name = 'tf_last')
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1338, in gather_nd
    name=name)
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/ala/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): flat indices[8, :] = [8, -1] does not index into param (shape: [50,300,2]).
	 [[Node: tf_last = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](strided_slice_2, stack)]]
```",0,,8,2017-12-04T09:45:44Z,2017-12-08T20:47:35Z,NONE,2017-12-04T18:40:31Z
15089,Error when run label_image --graph=/tmp/quantized_graph.pb after quantized model ,stat:awaiting tensorflower,"tensorflow:1.4.0
I followed the command line from tensorflow/tensorflow/docs_src/performance/quantization.md 
ran the following command is ok and  produce a new model quantized_graph.pb:
  bazel build tensorflow/tools/graph_transforms:transform_graph
  bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
  --in_graph=tensorflow/examples/label_image/data/inception_v3_2016_08_28_frozen.pb \
  --out_graph=/tmp/quantized_graph.pb \
  --inputs=input \
  --outputs=InceptionV3/Predictions/Reshape_1 \
  --transforms='add_default_attributes strip_unused_nodes(type=float, shape=""1,299,299,3"")
    remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true)
    fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes
    strip_unused_nodes sort_by_execution_order'

But there is a problem when  I was running with command:
  bazel build tensorflow/examples/label_image:label_image
  bazel-bin/tensorflow/examples/label_image/label_image \
  --graph=/tmp/quantized_graph.pb    (Else: it worked if I replace quantized_graph.pb with inception_v3_2016_08_28_frozen.pb here.)
The error info is:
  2017-12-04 08:27:02.891427: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op:    ""DenseToSparseBatchDataset"" device_type: ""CPU""') for unknown op: DenseToSparseBatchDataset
2017-12-04 08:27:02.891520: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""GroupByWindowDataset"" device_type: ""CPU""') for unknown op: GroupByWindowDataset
2017-12-04 08:27:02.891562: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""IgnoreErrorsDataset"" device_type: ""CPU""') for unknown op: IgnoreErrorsDataset
2017-12-04 08:27:02.891636: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""DatasetToSingleElement"" device_type: ""CPU""') for unknown op: DatasetToSingleElement
2017-12-04 08:27:02.891686: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""SerializeIterator"" device_type: ""CPU""') for unknown op: SerializeIterator
2017-12-04 08:27:02.891712: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""DeserializeIterator"" device_type: ""CPU""') for unknown op: DeserializeIterator
2017-12-04 08:27:02.891733: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""MapAndBatchDataset"" device_type: ""CPU""') for unknown op: MapAndBatchDataset
2017-12-04 08:27:02.891777: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""ParallelInterleaveDataset"" device_type: ""CPU""') for unknown op: ParallelInterleaveDataset
2017-12-04 08:27:02.891805: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""ScanDataset"" device_type: ""CPU""') for unknown op: ScanDataset
2017-12-04 08:27:02.891825: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""SqlDataset"" device_type: ""CPU""') for unknown op: SqlDataset
2017-12-04 08:27:02.901109: E tensorflow/examples/label_image/main.cc:327] Invalid argument: Node 'InceptionV3/InceptionV3/Conv2d_1a_3x3/BatchNorm/batchnorm/mul_eightbit/input__port__0/reduction_dims': Unknown input node '^input:0'
how should I fix it?

",0,,11,2017-12-04T08:35:51Z,2018-01-04T06:41:09Z,NONE,2017-12-04T08:58:41Z
15088,Make a State-ful LSTM with data input from TF Dataset,stat:awaiting response,"#14906 
This is to reference a closed issue.
Thanks for @mrry suggestion on using Dataset.flat_map to slice the input signal sequence.
However, how could we know the beginning and ending of the original sequence after sliced?
And how could we reset the LSTM state so that we can make a state-ful LSTM?
Thanks!",1,,5,2017-12-04T06:00:43Z,2018-01-03T18:07:41Z,NONE,2017-12-04T12:57:49Z
15087,Understanding LSTM cell Kernel values,,"Hi all,

I want to understand better those values in LSTM cell Kernel values extracted from:
<tf.Variable 'rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0' shape=(97, 280) dtype=float32_ref>
from tf.trainable_variables()

My model is very simple, input is a 27 element vector at each time step where the sequence length can be variable. The model is one layer LSTM model with 70 hidden states. 
```
        def lstm_cell():
            cell = tf.nn.rnn_cell.LSTMCell(num_units=state_size, state_is_tuple=True)
            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropKeepRate)
            return cell
        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)], state_is_tuple=True)
        
```

So that is probably why the LSTM kernel is a matrix of 97 rows (27 input features and 70 hidden states). As to the columns, I can clearly see four groups by the pattern of values. I guess each group is consist of 70 columns, so totally 4 groups.

So my guess is that this LSTM kernel, extracted from tf.trainable_variables(), maps input and previous hidden states to current hidden states in order of forget, input, update cell states and output. However, it is hard time for me to figure out the correspondence of these gates and their matrices with the Kernel matrix extracted from tf.trainable_variables().

Could somebody help? Thanks!


---------------------
Update:
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
TensorFlow installed from (source or binary): from pip
TensorFlow version (use command below): 1.3
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 8.0, 6.1
GPU model and memory: k2200",0,,3,2017-12-04T05:57:32Z,2017-12-07T18:48:10Z,NONE,2017-12-04T12:57:45Z
15084,calculation gradients of tf.nn.embedding_lookup,stat:awaiting response,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:1.3.0
- **Python version**: 2.7

### Source code / logs
import tensorflow as tf
types_lookup_table = tf.get_variable(""types_lookup_table"", shape=[234, 10],
                                     initializer=tf.random_normal_initializer(0, 1), dtype=tf.float32,
                                     trainable=True)
embedding_types = tf.nn.embedding_lookup(types_lookup_table,[[2,3,4],[1,2,3]])
opt = tf.train.GradientDescentOptimizer(0.1)
gradients = tf.gradients(embedding_types, xs=types_lookup_table)
train = opt.apply_gradients([(gradients[0], types_lookup_table)])

with tf.Session() as sess:
    tf.global_variables_initializer().run()
    h = sess.run(gradients)
    print(sess.run(train))                                                                              #right
    print(sess.run(opt.apply_gradients([(h[0],types_lookup_table)]))).    # wrong

### Describe the problem
I tried to calculate the gradients of tf.nn.embedding_lookup, but the result shown is an IndexedSliceValue with 3 elements
<img width=""1250"" alt=""2017-12-04 9 23 45"" src=""https://user-images.githubusercontent.com/10001692/33532464-ec59088e-d8d4-11e7-9c08-d53c6d87abf5.png"">
however the corresponding gradient(without sess.run) is an indexSliceValue with 1 elements.I don't know why.
<img width=""1226"" alt=""2017-12-04 9 27 31"" src=""https://user-images.githubusercontent.com/10001692/33532533-6aa0a5bc-d8d5-11e7-9b0d-69a950cb5fb0.png"">

And therefore I can't sess.run(opt.apply_gradients([(h[0],types_lookup_table)]) because the shape of calculation value doesn't match the shape of types_lookup_table, however, when I didn't calculate the intermediate value, and directly 
sess.run(train) (ps:train = opt.apply_gradients([(gradients,types_lookup_table)]))
there is no problem.

But I need to calculate the intermediate value and do an add. I don't know how.
Thanks

",0,,4,2017-12-04T01:47:48Z,2018-01-19T01:40:15Z,NONE,2017-12-04T05:59:44Z
15083,DOC: Fix documentation for dataset.md,"awaiting testing (then merge),cla: yes","The code `image = tf.decode_jpeg(parsed[""image_data""])` in 738 lines is incorrect. It should be `tf.image.decode_jpeg` instead of `tf.decode_jpeg`.",0,,6,2017-12-04T01:33:32Z,2017-12-04T14:35:49Z,CONTRIBUTOR,2017-12-04T01:42:07Z
15082,getting attribute error,stat:awaiting response,"I executed the code and got this error-- 
File ""new1.py"", line 131, in main
    classifier = tf.estimator.Estimator(model_fn=model_fn)
AttributeError: 'module' object has no attribute 'estimator'

pls help
",0,,6,2017-12-03T18:39:58Z,2018-01-11T09:36:46Z,NONE,2017-12-04T07:09:29Z
15081,Update deprecated get_global_step in example,"awaiting testing (then merge),cla: yes","`WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensor_forest/client/random_forest.py:193: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_global_step`
thrown when running random_forest_mnist.py

updated to `training_util.get_global_step()` used elsewhere in the same file",1,,4,2017-12-03T18:28:48Z,2017-12-31T06:40:51Z,CONTRIBUTOR,2017-12-18T01:40:41Z
15080,Add S3 to the list of implemented file systems in doc,"cla: yes,kokoro:run","This fix adds S3 to the list of implemented file systems in doc.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,1,2017-12-03T15:34:59Z,2017-12-04T02:36:39Z,MEMBER,2017-12-04T02:36:48Z
15077,The sequence of session.run and control_dependencies?,,"See three examples:

```python
import tensorflow as tf

x = tf.constant(1.0)
x = tf.Print(x, ['x'])
y = tf.constant(2.0)
y = tf.Print(y, ['y'])
z = tf.constant(3.0)
z = tf.Print(z, ['z'])

with tf.Session() as sess:
    print(sess.run([x,y,z]))
```

The sequence of output of `tf.Print` is indeterminate, which means `sess.run` don't executes tensors from left to right.

```python
import tensorflow as tf

x = tf.constant(1.0)
x = tf.Print(x, ['x'])
y = tf.constant(2.0)
y = tf.Print(y, ['y'])
with tf.control_dependencies([x, y]):
    z = tf.constant(3.0)
    z = tf.Print(z, ['z'])

with tf.Session() as sess:
    print(sess.run(z))
```

The sequence  of `x` and `y` is indeterminate, which means `control_dependencies` don't executes tensors from left to right.

```python
import tensorflow as tf

x = tf.constant(1.0)
x = tf.Print(x, ['x'])
y = tf.constant(2.0)
y = tf.Print(y, ['y'])
d = tf.constant(3.0)
d = tf.Print(d, ['d'])
with tf.control_dependencies([x, y]):
    z = tf.add(d, 3.)
    z = tf.Print(z, ['z'])

with tf.Session() as sess:
    print(sess.run(z))
```

The sequence of `x`, `y` and `d` is indeterminate.

Is it intentional behavior or bug?  I find that if the sequence is indeterminate, the program may get different result.


Another example I can't explain:

```python
import tensorflow as tf

x = tf.placeholder(tf.float32, [])
y = tf.Variable(2.)
op = tf.assign(y, x)
op = tf.Print(op, ['op'])

with tf.control_dependencies([op]):
    q = tf.Print(y, [y])


with tf.Session() as sess:
    tf.global_variables_initializer().run()
    print(sess.run([q], feed_dict={x: -1.0}))
```

The `[op]` is printed before `[y]`, but `q=2.0`, why? If `op` is executed before `y`, y will be assigned by `x`, which means `y=-1.0`, and `q` should be `-1.0`.

",0,,8,2017-12-03T13:57:34Z,2017-12-05T00:27:29Z,NONE,2017-12-03T14:40:11Z
15076,the source code to compile  about tensorflow1.1.0,stat:awaiting response,"I am very distressed~~~
I need some help ~~~~

I refer to ""TensorFlow Bindings for H2O.ai""
I execute "" ./gradlew clean tensorflowCompile"",but always has error,likes
ERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': Error downloading [http://bazel-mirror.storage.googleapis.com/github.com/bazelbuild/rules_closure/archive/5ca1dab6df9ad02050f7ba4e816407f88690cf7d.tar.gz, https://github.com/bazelbuild/rules_closure/archive/5ca1dab6df9ad02050f7ba4e816407f88690cf7d.tar.gz] to /root/.cache/bazel/_bazel_root/c0282f68c3c9fe7828209fa3ece89ea6/external/io_bazel_rules_closure/5ca1dab6df9ad02050f7ba4e816407f88690cf7d.tar.gz: Checksum was 5afc2087ab53b160fb58fde30339a2c2826c1a171404b7b8ff7227d5ebc8225c but wanted 60fc6977908f999b23ca65698c2bb70213403824a84f7904310b6000d78be9ce. :deepwater-tensorflow:tensorflowCompile FAILED
errors with checksum
how can I do ,fix this problem
how can I checksum disabled

please ~~~~
thanks",0,,4,2017-12-03T11:52:33Z,2017-12-14T08:28:08Z,NONE,2017-12-03T18:57:19Z
15074,"When execute ""bash tensorflow/contrib/lite/build_ios_universal_lib.sh"", it gives the error: tensorflow/tensorflow/contrib/lite/ios_makefile.inc:2: *** missing separator.  Stop.",comp:lite,"### System information
- **Install problem**:
- **Darwin MacBook-Pro-2.local 16.7.0 Darwin Kernel Version 16.7.0, xnu-3789.71.6~1/RELEASE_X86_64 x86_64**:
- **TensorFlow installed from source**:
- **v1.0.0-rc2-15-g47bba63-dirty 1.0.0**:
- **Python 3.6.0 :: Anaconda custom (x86_64)**: 
- **Bazel version (None)**:
- **Apple LLVM version 9.0.0 (clang-900.0.37)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

",1,,5,2017-12-03T08:23:51Z,2017-12-09T17:57:17Z,NONE,2017-12-09T17:57:17Z
15071,Tensorflow build fails with --config=sycl,"stat:community support,type:build/install","System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 17.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.3
Python version: 2.7
Bazel version (if compiling from source): 0.5.1
GCC/Compiler version (if compiling from source): 6.0.3
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce:

> bazel build -c opt --config=sycl //tensorflow:libtensorflow_cc.so

**Logs**

> 
> ERROR: /home/ashok/Ashok/tensorflow-c++/tensorflow/core/kernels/BUILD:3355:1: C++ compilation of rule '//tensorflow/core/kernels:sendrecv_ops' failed: computecpp failed: error executing command external/local_config_sycl/crosstool/computecpp -fPIE -fno-omit-frame-pointer -Wall -msse3 -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF ... (remaining 119 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1
> In file included from tensorflow/core/kernels/sendrecv_ops.cc:16:
> In file included from ./tensorflow/core/kernels/sendrecv_ops.h:19:
> In file included from ./tensorflow/core/framework/op_kernel.h:19:
> In file included from /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/functional:55:
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:1404:14: error: no matching constructor for initialization of 'tuple<const tensorflow::Status &&, const tensorflow::Rendezvous::Args &&, const tensorflow::Rendezvous::Args &&, const tensorflow::Tensor &&, bool &&>'
>     { return tuple<_Elements&&...>(std::forward<_Elements>(__args)...); }
>              ^                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/functional:992:13: note: in instantiation of function template specialization 'std::forward_as_tuple<const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool>' requested here
>               std::forward_as_tuple(std::forward<_Args>(__args)...),
>                    ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/functional:1731:2: note: in instantiation of function template specialization 'std::_Bind<(lambda at tensorflow/core/kernels/sendrecv_ops.cc:155:7) (std::function<void ()>, std::_Placeholder<1>, std::_Placeholder<2>, std::_Placeholder<3>, std::_Placeholder<4>, std::_Placeholder<5>)>::operator()<const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool, void>' requested here
>         (*_Base::_M_get_pointer(__functor))(
>         ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/functional:2115:33: note: in instantiation of member function 'std::_Function_handler<void (const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool), std::_Bind<(lambda at tensorflow/core/kernels/sendrecv_ops.cc:155:7) (std::function<void ()>, std::_Placeholder<1>, std::_Placeholder<2>, std::_Placeholder<3>, std::_Placeholder<4>, std::_Placeholder<5>)> >::_M_invoke' requested here
>             _M_invoker = &_My_handler::_M_invoke;
>                                        ^
> tensorflow/core/kernels/sendrecv_ops.cc:154:38: note: in instantiation of function template specialization 'std::function<void (const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool)>::function<std::_Bind<(lambda at tensorflow/core/kernels/sendrecv_ops.cc:155:7) (std::function<void ()>, std::_Placeholder<1>, std::_Placeholder<2>, std::_Placeholder<3>, std::_Placeholder<4>, std::_Placeholder<5>)>, void, void>' requested here
>   Rendezvous::DoneCallback done_cb = std::bind(
>                                      ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:600:18: note: candidate template ignored: disabled by 'enable_if' [with _Dummy = void]
>                  _TCC<_Dummy>::template
>                  ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:611:18: note: candidate template ignored: disabled by 'enable_if' [with _Dummy = void]
>                  _TCC<_Dummy>::template
>                  ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:628:5: note: candidate template ignored: disabled by 'enable_if' [with _UElements = <const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool>]
>                   _TC<sizeof...(_UElements) == 1, _Elements...>::template
>                   ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:641:5: note: candidate template ignored: disabled by 'enable_if' [with _UElements = <const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool>]
>                   _TC<sizeof...(_UElements) == 1, _Elements...>::template
>                   ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:737:19: note: candidate template ignored: disabled by 'enable_if' [with _Alloc = tensorflow::Rendezvous::Args, _UElements = <const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool>]
>         enable_if<_TMC<_UElements...>::template
>                   ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:748:19: note: candidate template ignored: disabled by 'enable_if' [with _Alloc = tensorflow::Rendezvous::Args, _UElements = <const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool>]
>         enable_if<_TMC<_UElements...>::template
>                   ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:579:17: note: candidate constructor template not viable: requires 0 arguments, but 5 were provided
>       constexpr tuple()
>                 ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:589:26: note: candidate constructor template not viable: requires 0 arguments, but 5 were provided
>       explicit constexpr tuple()
>                          ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:670:19: note: candidate constructor template not viable: requires single argument '__in', but 5 arguments were provided
>         constexpr tuple(const tuple<_UElements...>& __in)
>                   ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:682:28: note: candidate constructor template not viable: requires single argument '__in', but 5 arguments were provided
>         explicit constexpr tuple(const tuple<_UElements...>& __in)
>                            ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:694:19: note: candidate constructor template not viable: requires single argument '__in', but 5 arguments were provided
>         constexpr tuple(tuple<_UElements...>&& __in)
>                   ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:705:28: note: candidate constructor template not viable: requires single argument '__in', but 5 arguments were provided
>         explicit constexpr tuple(tuple<_UElements...>&& __in)
>                            ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:721:2: note: candidate constructor template not viable: requires 7 arguments, but 5 were provided
>         tuple(allocator_arg_t __tag, const _Alloc& __a,
>         ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:732:11: note: candidate constructor template not viable: requires 7 arguments, but 5 were provided
>         explicit tuple(allocator_arg_t __tag, const _Alloc& __a,
>                  ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:711:2: note: candidate constructor template not viable: requires 2 arguments, but 5 were provided
>         tuple(allocator_arg_t __tag, const _Alloc& __a)
>         ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:759:2: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided
>         tuple(allocator_arg_t __tag, const _Alloc& __a, const tuple& __in)
>         ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:763:2: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided
>         tuple(allocator_arg_t __tag, const _Alloc& __a, tuple&& __in)
>         ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:772:2: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided
>         tuple(allocator_arg_t __tag, const _Alloc& __a,
>         ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:784:11: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided
>         explicit tuple(allocator_arg_t __tag, const _Alloc& __a,
>                  ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:796:2: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided
>         tuple(allocator_arg_t __tag, const _Alloc& __a,
>         ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:808:11: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided
>         explicit tuple(allocator_arg_t __tag, const _Alloc& __a,
>                  ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:654:17: note: candidate constructor not viable: requires 1 argument, but 5 were provided
>       constexpr tuple(tuple&&) = default; 
>                 ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:652:17: note: candidate constructor not viable: requires 1 argument, but 5 were provided
>       constexpr tuple(const tuple&) = default;
>                 ^
> 1 error generated.
> Target //tensorflow:libtensorflow_cc.so failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 1003.058s, Critical Path: 53.74s
> FAILED: Build did NOT complete successfully
",0,,3,2017-12-03T06:13:06Z,2017-12-14T08:16:45Z,NONE,2017-12-04T19:01:33Z
15070,Multil-model restore in tensorflow,,"I have two model without scope name but they have same variable name, I want load them simultaneously, but it will crash and throw error, which is ""Variable XXX already exists"".
I notice the issue[https://github.com/tensorflow/tensorflow/issues/3270](url), but the problem is solved by re-training the model again and save them under different scope.
However, I do not want to re-train them again. So how can I load existing models and save them into two different scope?
Any advise or help will be appreciated!",0,,5,2017-12-03T04:42:47Z,2017-12-06T17:12:29Z,NONE,2017-12-03T12:57:51Z
15069,DOC: underline that tf.Print behaves like tf.identity,"awaiting testing (then merge),cla: yes","As @alextp suggested in #14788, fix the docstring: print should have the same behavior as identity (and it does)

The pr is opposed to #15068 .",0,,2,2017-12-03T02:54:23Z,2017-12-07T15:19:17Z,CONTRIBUTOR,2017-12-04T16:14:22Z
15066,Failing to retrieve AWS credentials running in ECS with the s3 provider,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**: 3.6.0
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: None

### Describe the problem
When using the S3 file system provider within an ECS container I am finding that even though my task has been assigned a task role that provides access to my S3 bucket it is unable to access it successfully.

Everything works fine if I supply the credentials manually using the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables.

After some extensive head-desking I have found that the version of aws-sdk-cpp that is used (1.0.90) appears to have been released before the ECS support was added to the sdk (https://github.com/aws/aws-sdk-cpp/commit/786666db02f5ac7918642c1ee056b822e37a6f30), at a minimum I think we would want 1.0.97 so that everything works as expected.

I had originally tried to bump the version but it appears that the bazel mirror has nothing but 1.0.90 available.",0,,2,2017-12-03T00:58:17Z,2017-12-04T02:38:19Z,NONE,2017-12-03T02:13:57Z
15065,Update 4_convolutions.ipynb,cla: no,,0,,4,2017-12-03T00:28:17Z,2017-12-29T00:19:40Z,NONE,2017-12-03T14:26:02Z
15062,Bus Error when running a session,stat:community support,"### System information
- Running on an ODROID-XU4
- Have I written custom code: No
- OS Platform and Distribution: Linux Ubuntu Mate 16.04
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A
- TensorFlow installed from: source
- TensorFlow version: 1.4.0
- Python version: 3.5.2
- Bazel version: 0.8.0
- GCC Version: 5.4.0
- Exact command to reproduce:
python3.5
import tensorflow as tf
hello = tf.constant('Hello, TensorFlow')
sess - tf.Session()
print(sess.run(hello))

### Describe the problem
I have built and installed TensorFlow as a pip package onboard an ODROID-XU4 (32-bit ARM), following the steps in this guide: https://hackernoon.com/running-yolo-on-odroid-yolodroid-5a89481ec141
Everything went smoothly, and I was able to install TensorFlow after a lengthy build time. However, when I try to run any session (such as in the basic example above), the program fails with a Bus Error. Running `pip3 list` shows that Tensorflow is indeed installed, and no errors are thrown when I merely import TensorFlow.

### Source code / logs
I traced the bus error by looking into the /var/log/syslog file and found the following lines associated with the error: `Dec  2 21:24:21 odroid kernel: [ 3658.433306] Alignment trap: python3.5 (10189) PC=0xac0bac42 Instr=0xf9068a1f Address=0xbe8a7da4 FSR 0x811
Dec  2 21:24:21 odroid kernel: [ 3658.433313] Alignment trap: not handling instruction f9068a1f at [<ac0bac42>]
Dec  2 21:24:21 odroid kernel: [ 3658.439021] Unhandled fault: alignment exception (0x811) at 0xbe8a7da4`
So, it appears to be an alignment issue. I tried to force the kernel to attempt to fix the error instead of simply failing by using the following command: `echo 3 > /proc/cpu/alignment`
The three is meant to tell the kernel to fix these alignment issues. However, this strategy has not changed anything about the Bus Error when attempting to run a session. 

Perhaps this is related to the 32-bit architecture I am attempting to run on?
",1,,10,2017-12-02T21:57:41Z,2018-01-31T19:34:35Z,NONE,2017-12-03T06:58:34Z
15061,saved_model.pb needs a different file extension,,'saved_model.pb' file extension type clashes with every other '.pb' file created. Apps that register the '.pb' file extension won't be able to tell apart Saved Models from other protobuf files...,0,,12,2017-12-02T20:52:21Z,2018-01-30T21:16:42Z,NONE,2017-12-03T06:58:30Z
15060,"AttentionWrapperZeroState: Input to reshape is a tensor with 32768 values, but the requested shape has 65536",stat:awaiting response,"I am building an encoder-decoder model with attention and BeamSearchDecoder using tensor flow documentation. I am getting following error:

    ---------------------------------------------------------------------------
    InvalidArgumentError                      Traceback (most recent call last)
    <ipython-input-160-ac947b28f4dd> in <module>()
         29                                           summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)],
         30                                           text_length: [len(text)]*batch_size,
    ---> 31                                           keep_prob: 1.0})[0] 
         32         # Remove the padding from the summaries
         33         pad = vocab_to_int[""<PAD>""]

    /Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
        893     try:
        894       result = self._run(None, fetches, feed_dict, options_ptr,
    --> 895                          run_metadata_ptr)
        896       if run_metadata:
        897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

    /Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
       1122     if final_fetches or final_targets or (handle and feed_dict_tensor):
       1123       results = self._do_run(handle, final_targets, final_fetches,
    -> 1124                              feed_dict_tensor, options, run_metadata)
       1125     else:
       1126       results = []

    /Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
       1319     if handle is None:
       1320       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
    -> 1321                            options, run_metadata)
       1322     else:
       1323       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

    /Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
       1338         except KeyError:
       1339           pass
    -> 1340       raise type(e)(node_def, op, message)
       1341 
       1342   def _extend_graph(self):

    InvalidArgumentError: Input to reshape is a tensor with 32768 values, but the requested shape has 65536
         [[Node: decode_1/Reshape_2 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](tile_batch_2/Reshape_2, decode_1/concat_2)]]

    Caused by op u'decode_1/Reshape_2', defined at:
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
        ""__main__"", fname, loader, pkg_name)
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/runpy.py"", line 72, in _run_code
        exec code in run_globals
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py"", line 3, in <module>
        app.launch_new_instance()
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py"", line 658, in launch_instance
        app.start()
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py"", line 474, in start
        ioloop.IOLoop.instance().start()
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
        super(ZMQIOLoop, self).start()
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py"", line 887, in start
        handler_func(fd_obj, events)
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
        return fn(*args, **kwargs)
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
        self._handle_recv()
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
        self._run_callback(callback, msg)
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
        callback(*args, **kwargs)
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
        return fn(*args, **kwargs)
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 276, in dispatcher
        return self.dispatch_shell(stream, msg)
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 228, in dispatch_shell
        handler(stream, idents, msg)
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 390, in execute_request
        user_expressions, allow_stdin)
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py"", line 196, in do_execute
        res = shell.run_cell(code, store_history=store_history, silent=silent)
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py"", line 501, in run_cell
        return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2717, in run_cell
        interactivity=interactivity, compiler=compiler, result=result)
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2821, in run_ast_nodes
        if self.run_code(code, result):
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2881, in run_code
        exec(code_obj, self.user_global_ns, self.user_ns)
      File ""<ipython-input-160-ac947b28f4dd>"", line 18, in <module>
        loader = tf.train.import_meta_graph(checkpoint + '.meta')
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1698, in import_meta_graph
        **kwargs)
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py"", line 656, in import_scoped_meta_graph
        producer_op_list=producer_op_list)
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 313, in import_graph_def
        op_def=op_def)
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
        original_op=self._default_original_op, op_def=op_def)
      File ""/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
        self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

    InvalidArgumentError (see above for traceback): Input to reshape is a tensor with 32768 values, but the requested shape has 65536
         [[Node: decode_1/Reshape_2 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](tile_batch_2/Reshape_2, decode_1/concat_2)]]

The error occurs when trying to Make predictions(Test the Model Section) .Training runs fine. The error occurs when I set beam_size >=2. (The error shown is for beam_size=2). However it runs fine for beam_size = 1. I am not able to figure out what is going wrong. I know the code is long. Any help will be highly appreciated as I am unable to debug it and stuck on it for days. My code is:

My code is here:
# Model Inputs

    def model_inputs():
        input_data = tf.placeholder(tf.int32, [None, None], name='input')
        targets = tf.placeholder(tf.int32, [None, None], name='targets')
        lr = tf.placeholder(tf.float32, name='learning_rate')
        keep_prob = tf.placeholder(tf.float32, name='keep_prob')
        summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')
        max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')
        text_length = tf.placeholder(tf.int32, (None,), name='text_length')

        return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length

    def process_encoding_input(target_data, vocab_to_int, batch_size):  
        ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1]) # slice it to target_data[0:batch_size, 0: -1]
        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)

        return dec_input
# Encoding layer

    def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):
        for layer in range(num_layers):
            with tf.variable_scope('encoder_{}'.format(layer)):
                cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,
                                                  initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))
                cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, 
                                                        input_keep_prob = keep_prob)

                cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,
                                                  initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))
                cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, 
                                                        input_keep_prob = keep_prob)

                enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, 
                                                                        cell_bw, 
                                                                        rnn_inputs,
                                                                        sequence_length,
                                                                        dtype=tf.float32)
                enc_output = tf.concat(enc_output,2)
                # original code is missing this line below, that is how we connect layers 
                # by feeding the current layer's output to next layer's input
                #rnn_inputs = enc_output
        return enc_output, enc_state

# Training Decoding layer  

    def training_decoding_layer(dec_embed_input, summary_length, dec_cell, output_layer,
                                vocab_size, max_summary_length,batch_size,enc_state):
        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,
                                                            sequence_length=summary_length,
                                                            time_major=False)

        training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,
                                                           helper=training_helper,
                                            initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size).clone(cell_state=enc_state),
                                                           output_layer = output_layer)

        training_logits,_,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,
                                                                impute_finished = True,
                                                               maximum_iterations=max_summary_length)
        return training_logits

# Inference Decoding layer  

    def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, output_layer,
                                 max_summary_length, batch_size,lengths,enc_state,beam_width):
        '''Create the inference logits'''

        start_tokens = tf.ones_like(lengths) * start_token



        inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(
                        cell          = dec_cell,
                        embedding     = embeddings,
                        start_tokens  = start_tokens,
                        end_token     = end_token,
                        initial_state = dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size * beam_width*2).clone(cell_state=enc_state) ,
                        beam_width    = beam_width,
                        output_layer  = output_layer)


        inference_logits,_,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,
                                                                 impute_finished = False,
                                                                maximum_iterations=max_summary_length)

        return inference_logits

    def lstm_cell(lstm_size, keep_prob):
        cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)
        return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)

# Decoding layer 

    def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,
                       max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):
        '''Create the decoding cell and attention for the training and inference decoding layers'''
        output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))

        with tf.variable_scope(""decode""):
            dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])
            attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,
                                                         enc_output,
                                                         text_length,
                                                         normalize=False,
                                                         )
            dec_cell = tf.contrib.seq2seq.AttentionWrapper(cell =dec_cell,
                                                           attention_mechanism = attn_mech, 
                                                           attention_layer_size=rnn_size)

            training_logits = training_decoding_layer(dec_embed_input,summary_length,dec_cell,
                                                      output_layer,
                                                      vocab_size,
                                                      max_summary_length,
                                                      batch_size,enc_state)


        beam_width = 2
        enc_output = tf.contrib.seq2seq.tile_batch(enc_output, multiplier=beam_width)
        lengths = tf.contrib.seq2seq.tile_batch(text_length, multiplier=beam_width)
        enc_state = tf.contrib.seq2seq.tile_batch(enc_state, multiplier=beam_width)

        with tf.variable_scope(""decode"", reuse=True):
            dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])
            attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,
                                                         enc_output,
                                                         lengths,
                                                         normalize=False,
                                                         )
            dec_cell = tf.contrib.seq2seq.AttentionWrapper(cell =dec_cell,
                                                           attention_mechanism = attn_mech, 
                                                           attention_layer_size=rnn_size)


            inference_logits = inference_decoding_layer(embeddings,
                                                        vocab_to_int['<GO>'],
                                                        vocab_to_int['<EOS>'],
                                                        dec_cell,
                                                        output_layer,
                                                        max_summary_length,
                                                        batch_size,lengths,enc_state,beam_width)
        return training_logits, inference_logits


    def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, 
                      vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):
        '''Use the previous functions to create the training and inference logits'''

        # Use Numberbatch's embeddings and the newly created ones as our embeddings
        #embeddings = word_embedding_matrix
        embeddings_s = tf.get_variable(""word_embeddings"",[vocab_size, 300])
        embeddings_t = tf.get_variable(""word_embeddings2"",[vocab_size, 300])
        enc_embed_input = tf.nn.embedding_lookup(embeddings_s, input_data)
        enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)
        dec_input = process_encoding_input(target_data, vocab_to_int, batch_size) #shape=(batch_size, senquence length) each seq start with index of<GO>
        dec_embed_input = tf.nn.embedding_lookup(embeddings_t, dec_input)
        training_logits, inference_logits  = decoding_layer(dec_embed_input, 
                                                            embeddings_t,
                                                            enc_output,
                                                            enc_state, 
                                                            vocab_size, 
                                                            text_length, 
                                                            summary_length, 
                                                            max_summary_length,
                                                            rnn_size, 
                                                            vocab_to_int, 
                                                            keep_prob, 
                                                            batch_size,
                                                            num_layers)
        return training_logits, inference_logits

# Set the Hyperparameters
    epochs = 100
    batch_size = 64
    rnn_size = 256
    num_layers = 2
    learning_rate = 0.005
    keep_probability = 0.95


# Build the graph
    train_graph = tf.Graph()
    # Set the graph to default to ensure that it is ready for training
    with train_graph.as_default():

        # Load the model inputs    
        input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()

        # Create the training and inference logits
        training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),
                                                          targets, 
                                                          keep_prob,   
                                                          text_length,
                                                          summary_length,
                                                          max_summary_length,
                                                          len(vocab_to_int)+1,
                                                          rnn_size, 
                                                          num_layers, 
                                                          vocab_to_int,
                                                          batch_size)

        # Create tensors for the training logits and inference logits
        training_logits = tf.identity(training_logits.rnn_output, 'logits')
        #inference_logits = inference_logits.predicted_ids[:,:,0]
        inference_logits = tf.identity(inference_logits.predicted_ids, name='predictions')

        # Create the weights for sequence_loss, the sould be all True across since each batch is padded
        masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')

        with tf.name_scope(""optimization""):
            # Loss function
            cost = tf.contrib.seq2seq.sequence_loss(
                training_logits,
                targets,
                masks)

            # Optimizer
            optimizer = tf.train.AdamOptimizer(learning_rate)

            # Gradient Clipping
            gradients = optimizer.compute_gradients(cost)
            capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]
            train_op = optimizer.apply_gradients(capped_gradients)
    print(""Graph is built."")
    graph_location = ""./graph""
    print(graph_location)
    train_writer = tf.summary.FileWriter(graph_location)
    train_writer.add_graph(train_graph)


# Train the Model
    learning_rate_decay = 0.95
    min_learning_rate = 0.0005
    display_step = 20 # Check training loss after every 20 batches
    stop_early = 0 
    stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training
    per_epoch = 3 # Make 3 update checks per epoch
    #update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1
    update_check = 2

    update_loss = 0 
    batch_loss = 0
    summary_update_loss = [] # Record the update losses for saving improvements in the model

    checkpoint = ""./best_model.ckpt"" 
    with tf.Session(graph=train_graph) as sess:
        sess.run(tf.global_variables_initializer())

        # If we want to continue training a previous session
        #loader = tf.train.import_meta_graph(""./"" + checkpoint + '.meta')
        #loader.restore(sess, checkpoint)

        for epoch_i in range(1, epochs+1):
            update_loss = 0
            batch_loss = 0
            for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(
                    get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):
                start_time = time.time()
                _, loss = sess.run(
                    [train_op, cost],
                    {input_data: texts_batch,
                     targets: summaries_batch,
                     lr: learning_rate,
                     summary_length: summaries_lengths,
                     text_length: texts_lengths,
                     keep_prob: keep_probability})

                batch_loss += loss
                update_loss += loss
                end_time = time.time()
                batch_time = end_time - start_time

                if batch_i % display_step == 0 and batch_i > 0:
                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'
                          .format(epoch_i,
                                  epochs, 
                                  batch_i, 
                                  len(sorted_texts_short) // batch_size, 
                                  batch_loss / display_step, 
                                  batch_time*display_step))
                    batch_loss = 0

                if batch_i % update_check == 0 and batch_i > 0:
                    print(""Average loss for this update:"", round(update_loss/update_check,3))
                    summary_update_loss.append(update_loss)

                    # If the update loss is at a new minimum, save the model
                    if update_loss <= min(summary_update_loss):
                        print('New Record!') 
                        stop_early = 0
                        saver = tf.train.Saver() 
                        saver.save(sess, checkpoint)

                    else:
                        print(""No Improvement."")
                        stop_early += 1
                        if stop_early == stop:
                            break
                    update_loss = 0


            # Reduce learning rate, but not below its minimum value
            learning_rate *= learning_rate_decay
            if learning_rate < min_learning_rate:
                learning_rate = min_learning_rate

            if stop_early == stop:
                print(""Stopping Training."")
                break


    def text_to_seq(text):
        '''Prepare the text for the model'''

        text = clean_text(text)
        return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]
# Test The Model   

    input_sentences=[""The coffee tasted great and was at such a good price! I highly recommend this to everyone!"",
                   ""love individual oatmeal cups found years ago sam quit selling sound big lots quit selling found target expensive buy individually trilled get entire case time go anywhere need water microwave spoon know quaker flavor packets""]
    generagte_summary_length =  [3,2]

    # input_sentences = test_text[0:10]
    # generagte_summary_length = summary_l[0:10]
    texts = [text_to_seq(input_sentence) for input_sentence in input_sentences]
    checkpoint = ""./best_model.ckpt""
    if type(generagte_summary_length) is list:
        if len(input_sentences)!=len(generagte_summary_length):
            raise Exception(""[Error] makeSummaries parameter generagte_summary_length must be same length as input_sentences or an integer"")
        generagte_summary_length_list = generagte_summary_length
    else:
        generagte_summary_length_list = [generagte_summary_length] * len(texts)
    loaded_graph = tf.Graph()
    with tf.Session(graph=loaded_graph) as sess:
        # Load saved model
        loader = tf.train.import_meta_graph(checkpoint + '.meta')
        loader.restore(sess, checkpoint)
        input_data = loaded_graph.get_tensor_by_name('input:0')
        logits = loaded_graph.get_tensor_by_name('predictions:0')
        text_length = loaded_graph.get_tensor_by_name('text_length:0')
        summary_length = loaded_graph.get_tensor_by_name('summary_length:0')
        keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')
        #Multiply by batch_size to match the model's input parameters
        for i, text in enumerate(texts):
            generagte_summary_length = generagte_summary_length_list[i]
            answer_logits = sess.run(logits, {input_data: [text]*batch_size, 
                                              summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)], 
                                              text_length: [len(text)]*batch_size,
                                              keep_prob: 1.0})[0] 
            # Remove the padding from the summaries
            pad = vocab_to_int[""<PAD>""] 
            print('- Review:\n\r {}'.format(input_sentences[i]))
            print('- Summary:\n\r {}\n\r\n\r'.format("" "".join([int_to_vocab[i[0]] for i in answer_logits if i[0] != pad])))
tensorflow
",0,,3,2017-12-02T19:01:51Z,2017-12-20T03:01:52Z,NONE,2017-12-03T06:58:25Z
15059,Running tensorflow-gpu in virtual env in remote Ubuntu system,stat:awaiting response,"I was attempting to install tensorflow gpu version for my gpu system.
 - I don't have sudo access in the computer. 
 - Cuda version 7.5 (V7.5.17) is installed
After following all the steps and finding a suitable version of tensorflow suitable to cuda 7.5 I have installed everything. But after installing when I am trying to import tensorflow, these errors are coming. 
![image](https://user-images.githubusercontent.com/31855851/33518214-456db3ce-d75f-11e7-8d7e-a4f62f1c6dcc.png)

I don't know how to resolve it. Also, does this error affect the running of my code in gpu or can I ignore it and start coding?
The solutions I found on forum is related to reinstalling cuda but I cannot do that as I don't have sudo access. Can anyone please provide me some other solution to resolve this issue?
I really want to setup my environment soon for the project. 
Thank you!

Additional info

OS Platform and Distribution Ubuntu v 16.04
TensorFlow installed via creating virtualenv
TensorFlow version 0.12.0rc0
Bazel version - Not used
CUDA/cuDNN version - 7.5 
GPU model and memory - NA
Exact command to reproduce - NA
",0,,2,2017-12-02T17:50:25Z,2017-12-03T20:39:38Z,NONE,2017-12-03T00:56:30Z
15058,Go bindings: Two variables interfere unless op.VarHandleOpSharedName is used,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch linux
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**:  NA
- **GPU model and memory**:

### Describe the problem
When creating and assigning values to two variables in the Go bindings, they conflict, one value gets assigned to both variables. The variables assign OPs race, so it is somewhat non deterministic which gets assigned to both.

Reproduce:
Use `op.VarHandleOp()` to create two variable handles, and then use `op.AssignVariableOp()` to create OPs to assign the constant 1 to the first variable, and 2 to the second. Pull on the two assign OPs.
Evaluate the variable reader outputs for the two variables.
Expected result: First variable has a value of 1, and second variable has a value of 2.
Observed result: Both variables have the same value, usually 2, but occasionally 1.

Is this behavior correct?

If I use the optional parameter `op.VarHandleOpSharedName` in op.VarHandleOp, giving the two variables different names, it works as expected, so this is easy to work around.


### Source code / logs
```
package main

import (
	""fmt""

	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func makeVariable(s *op.Scope, i int32, name string) (init *tf.Operation, output tf.Output) {
	constant := op.Const(s, i)
	variable := op.VarHandleOp(s, tf.Int32, tf.ScalarShape())
	//variable := op.VarHandleOp(s, tf.Int32, tf.ScalarShape(), op.VarHandleOpSharedName(name))
	init = op.AssignVariableOp(s, variable, constant)
	output = op.ReadVariableOp(s, variable, tf.Int32)
	return
}

func main() {
	s := op.NewScope()
	init1, output1 := makeVariable(s.SubScope(""var1""), 1, ""variable_1"")
	init2, output2 := makeVariable(s.SubScope(""var2""), 2, ""variable_2"")

	graph, err := s.Finalize()
	if err != nil {
		panic(err)
	}
	sess, err := tf.NewSession(graph, nil)
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{init1, init2})
	if err != nil {
		panic(err)
	}
	results, err := sess.Run(nil, []tf.Output{output1, output2}, nil)
	if err != nil {
		panic(err)
	}
	fmt.Println(results[0].Value(), results[1].Value())
}
```
Prints:
```
[isaac@d6-arch tfes]$ go run shape_bug_demo.go 
2017-12-02 09:17:06.516455: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2017-12-02 09:17:06.594468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-12-02 09:17:06.594676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:01:00.0
totalMemory: 5.93GiB freeMemory: 5.15GiB
2017-12-02 09:17:06.594688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
2 2
```",0,,3,2017-12-02T17:34:38Z,2017-12-03T01:13:01Z,NONE,2017-12-03T00:56:26Z
15056,tf.losses.mean_squared_error is actually sum of squares,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.0-rc1-5211-gab0fcac 1.5.0-dev20171124
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

To truly get mean squared error one has to explictly use `reduction=Reduction.MEAN`. This indicates bad naming. Maybe `squared_error` is a better name, also similar to other names in `tf.losses`",0,,3,2017-12-02T12:43:06Z,2017-12-28T05:47:20Z,CONTRIBUTOR,2017-12-03T00:56:15Z
15055,update Android libs,"awaiting testing (then merge),cla: yes","For android sample:
- improve .gitignore
- upgrade build tools to current most recent stable 3.0.1
- add corresponding gradle",0,,9,2017-12-02T11:18:26Z,2017-12-10T20:12:40Z,CONTRIBUTOR,2017-12-04T02:38:44Z
15050,upgraded tensorflow to use GPU - she's a no worky anymore,type:build/install,"Script was running fine (if a bit slow).  Decided it was  time to upgrade to the gpu version of tensorflow.  From what I can tell, for some reason it is looking for libcublas.so.8.0.  Since I just upgraded to the latest cuda libs, I'm running libcublas.so.9.0.176  (found in /usr/local/cuda-9.0/lib64/) and, yes, that is in my path (first item, by the way).  


Using TensorFlow backend.
```
Traceback (most recent call last):
  File ""/home/mitch/MitchEng/data_Analytics/conv_radar.py"", line 3, in <module>
    from keras.models import Sequential
  File ""/home/mitch/anaconda2/lib/python2.7/site-packages/keras/__init__.py"", line 3, in <module>
    from . import utils
  File ""/home/mitch/anaconda2/lib/python2.7/site-packages/keras/utils/__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""/home/mitch/anaconda2/lib/python2.7/site-packages/keras/utils/conv_utils.py"", line 3, in <module>
    from .. import backend as K
  File ""/home/mitch/anaconda2/lib/python2.7/site-packages/keras/backend/__init__.py"", line 83, in <module>
    from .tensorflow_backend import *
  File ""/home/mitch/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line 1, in <module>
    import tensorflow as tf
  File ""/home/mitch/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/mitch/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/mitch/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/mitch/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/mitch/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/mitch/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.
```
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,8,2017-12-01T21:53:25Z,2017-12-09T01:40:31Z,NONE,2017-12-02T06:59:09Z
15046,stop gradients for weights in tf.losses,,"In the case that the weights given to tf.losses.* depend in some way on the model parameters, 
the derivative of that loss also calculated with respect to the weights. 
(Stupid) minimal example:
```python
import tensorflow as tf
x = tf.constant(0)
w = tf.get_variable(name=""W"", shape=(), initializer=tf.zeros_initializer())
L = tf.losses.mean_squared_error(x, x, weights=w)
tf.train.AdamOptimizer().compute_gradients(L)
```
results in 
```
[(<tf.Tensor 'gradients/mean_squared_error/Mul_grad/tuple/control_dependency_1:0' shape=() dtype=float32>,
  <tf.Variable 'W:0' shape=() dtype=float32_ref>)]
```

I would expect the weights to be considered constant for the calculation of a loss. In case you agree with me, I can make a PR that adds `stop_gradient` around the weights parameter.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**:  N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A
",0,,8,2017-12-01T19:25:12Z,2017-12-06T16:56:41Z,CONTRIBUTOR,2017-12-02T01:37:02Z
15042,Only install enum34 on Python <3.4 versions (Take 2),"awaiting testing (then merge),cla: yes","Python 3.6 sometimes has issues with enum34 because the standard library
relies on enum features not in enum34 (see
https://bitbucket.org/stoneleaf/enum34/issues/19/enum34-isnt-compatible-with-python-36
for more details).

We'll avoid the new versioning syntax in setuptools to allow old versions of setuptools to still work (see #14779)

Do you mind taking a look @gunan @yifeif?",0,,2,2017-12-01T16:21:39Z,2017-12-01T19:20:53Z,CONTRIBUTOR,2017-12-01T18:25:09Z
15040,raw video files to tfrecords (code integration),type:feature,"Since I had to address the issue of converting large (and many) raw RGB video files (e.g. .avi, .mp4 etc.) into tfrecords for threaded/QueueRunner training during a research project in the past, I was wondering if my resulting code [1] could be of any help to the TensorFlow community. If not, I would make a feature request and possibly participate or support the implementaton. 

When I had to address this at an early stage of my project, I couldn't find any useful implementations. This seems to have remained unchanged as of now.

[1] https://github.com/ferreirafabio/video2tfrecords

Thanks in advance.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04, macOS Sierra 10.13.1
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: I use the CPU version
- **GPU model and memory**: -
- **Exact command to reproduce**: -
",1,,13,2017-12-01T13:48:34Z,2017-12-20T07:16:14Z,NONE,2017-12-01T17:44:09Z
15038,Why tensorflow don't support tf.float64 on some ops?,type:feature,"Reproducing experiment of some paper using tensorflow may don't have good performance. I have tried my best to keep the structure and hyper-parameters unchanged. Does anyone encounter similar problem? Is it the code wrong?  I wonder that `caffe` can use `float64` but tensorflow only can use `float32`,  so there are more inaccuracy on `gradients`, why tf don't support `tf.float64`?",0,,4,2017-12-01T12:10:48Z,2017-12-05T04:35:31Z,NONE,2017-12-01T17:45:42Z
15034,Optimize graph & graph transform tools do not support NCHW,type:bug/performance,"I tried optimizing graph using both [Graph transform tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md) and [Optimize graph for inference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py). Both cases produced the same error because the fused batchnorm used not NCHW, but NHWC. I've got the error like this:

```
InvalidArgumentError (see above for traceback): Must provide as many biases as the channel dimension of the input tensor: [256] vs. 19 in [1,256,19,19]
	 [[Node: prefix/convblock/BatchNorm/FusedBatchNorm = BiasAdd[T=DT_FLOAT, data_format=""NHWC"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](prefix/convblock/Conv2D, prefix/convblock/Conv2D_bn_offset)]
```

Although NCHW is faster than NHWC in GPU environment, why the tools do not support NCHW?

",1,,5,2017-12-01T10:24:45Z,2018-01-31T23:02:26Z,NONE,2017-12-01T17:47:50Z
15033,cuDNN on windows will speed up image retraining?,stat:awaiting response,"If  I'll do 4 steps from http://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installwindows

it will speed up tensorflow image retraining? (https://www.tensorflow.org/versions/r0.12/how_tos/image_retraining/)

![screenshot_7](https://user-images.githubusercontent.com/8851301/33477760-8eea082a-d68f-11e7-89d3-f8386ada70b2.png)
",0,,3,2017-12-01T10:01:46Z,2017-12-20T06:00:35Z,NONE,2017-12-01T18:57:05Z
15032,Error while implementing the feature requested in  #10767,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
RHEL 6.8 
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
'v1.3.0-rc1-5326-gcae852a', '1.4.0'
- **Python version**: 
2.7.14
- **Bazel version (if compiling from source)**:
Build label: 0.5.4- (@non-git)
- **GCC/Compiler version (if compiling from source)**:
4.8.5
- **CUDA/cuDNN version**:
Not installed
- **GPU model and memory**:
Using tensorflow for CPU
- **Exact command to reproduce**:
I want to implement _in_top_k_ operation with options to specify what to do when a tie occurs for [CIFAR-10](https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10) data-set. I came across this feature request in #10767, modified the 6-files as shown [here](https://github.com/nolanliou/tensorflow/commit/681724f94e025bbd8377c5406eec4047d58fc31b), and rebuilt from source. When I run _eval_CIFAR10.py_ which contains the _in_top_k_ op, I get the following error.

  File ""eval_CIFAR10.py"", line 146, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 129, in run
    _sys.exit(main(argv))
  File ""eval_CIFAR10.py"", line 141, in main
    evaluate()
  File ""eval_CIFAR10.py"", line 130, in evaluate
    eval_once(saver, summary_writer, top_k_op, summary_op)
  File ""eval_CIFAR10.py"", line 63, in eval_once
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1686, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Multiple OpKernel registrations match NodeDef 'in_top_k/InTopKV2 = InTopKV2[T=DT_INT32, handle_ties=""SAMPLE""](softmax_linear/softmax_linear, Reshape_2, in_top_k/InTopKV2/k)': 'op: ""InTopKV2"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT32 } } } host_memory_arg: ""predictions"" host_memory_arg: ""targets"" host_memory_arg: ""k"" host_memory_arg: ""precision""' and 'op: ""InTopKV2"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT32 } } } host_memory_arg: ""predictions"" host_memory_arg: ""targets"" host_memory_arg: ""k"" host_memory_arg: ""precision""'
	 [[Node: in_top_k/InTopKV2 = InTopKV2[T=DT_INT32, handle_ties=""SAMPLE""](softmax_linear/softmax_linear, Reshape_2, in_top_k/InTopKV2/k)]]

Caused by op u'in_top_k/InTopKV2', defined at:
  File ""eval_CIFAR10.py"", line 146, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 129, in run
    _sys.exit(main(argv))
  File ""eval_CIFAR10.py"", line 141, in main
    evaluate()
  File ""eval_CIFAR10.py"", line 116, in evaluate
    top_k_op = tf.nn.in_top_k(logits, labels, 1, handle_ties=""SAMPLE"")
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 2523, in in_top_k
    return gen_nn_ops._in_top_kv2(predictions, targets, k, handle_ties, name=name)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 2536, in _in_top_kv2
    handle_ties=handle_ties, name=name)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3101, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1583, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Multiple OpKernel registrations match NodeDef 'in_top_k/InTopKV2 = InTopKV2[T=DT_INT32, handle_ties=""SAMPLE""](softmax_linear/softmax_linear, Reshape_2, in_top_k/InTopKV2/k)': 'op: ""InTopKV2"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT32 } } } host_memory_arg: ""predictions"" host_memory_arg: ""targets"" host_memory_arg: ""k"" host_memory_arg: ""precision""' and 'op: ""InTopKV2"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT32 } } } host_memory_arg: ""predictions"" host_memory_arg: ""targets"" host_memory_arg: ""k"" host_memory_arg: ""precision""'
	 [[Node: in_top_k/InTopKV2 = InTopKV2[T=DT_INT32, handle_ties=""SAMPLE""](softmax_linear/softmax_linear, Reshape_2, in_top_k/InTopKV2/k)]]

If you can kindly look into the matter I shall be much helped. Thank you. ",0,,5,2017-12-01T09:34:33Z,2017-12-07T17:35:10Z,NONE,2017-12-06T16:49:39Z
15031,Fix error message of WhereOpCPU,"awaiting testing (then merge),cla: yes",,0,,4,2017-12-01T08:52:25Z,2017-12-31T22:21:05Z,CONTRIBUTOR,2017-12-29T01:21:48Z
15029,build tensorflow Image Recognition with c++,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**: 0.6.1
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:8.0
- **GPU model and memory**:5.4.0
- **Exact command to reproduce**:



I follow this [tutorial](https://tensorflow.google.cn/tutorials/image_recognition#usage_with_the_c_api) try to build a a c++ tensorflow program.

Here is the steps:

1. build tensorflow from source. I am sure this step is right, because I can install python tensorflow with the whl file built by this step.
2. in the tensorflow source code directory, I run this command:

`bazel build tensorflow/examples/label_image/...`

the build failed with following logs(partial, because whole log is too big)



> WARNING: /home/scott/github/tensorflow/tensorflow/tensorflow/core/BUILD:1781:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/scott/github/tensorflow/tensorflow/tensorflow/tensorflow.bzl:1044:30
> INFO: Analysed 2 targets (0 packages loaded).
> INFO: Found 2 targets...
> INFO: From ProtoCompile tensorflow/core/example/example.pb.cc:
> bazel-out/local_linux-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.
> INFO: From ProtoCompile tensorflow/core/grappler/costs/op_performance_data.pb.cc:
> bazel-out/local_linux-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.
> bazel-out/local_linux-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.
> INFO: From ProtoCompile tensorflow/contrib/cloud/kernels/bigquery_table_partition.pb.cc:
> bazel-out/local_linux-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.
> INFO: From Executing genrule //tensorflow/cc:array_ops_genrule:
> 2017-12-01 15:02:52.546440: W tensorflow/core/framework/op_gen_lib.cc:372] Squeeze can't find input squeeze_dims to rename
> ERROR: /home/scott/github/tensorflow/tensorflow/tensorflow/examples/label_image/BUILD:14:1: Linking of rule '//tensorflow/examples/label_image:label_image' failed (Exit 1)
> /usr/bin/ld: warning: libcudnn.so.6, needed by bazel-out/local_linux-py3-opt/bin/_solib_local/_U_S_Stensorflow_Sexamples_Slabel_Uimage_Clabel_Uimage___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> bazel-out/local_linux-py3-opt/bin/_solib_local/_U_S_Stensorflow_Sexamples_Slabel_Uimage_Clabel_Uimage___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnGetConvolutionBackwardDataAlgorithm'
",0,,2,2017-12-01T08:17:23Z,2017-12-01T10:00:30Z,CONTRIBUTOR,2017-12-01T10:00:30Z
15027,Tutorial wrong code  cifar10_multi_gpu_train.py,,"when I run 
`python cifar10_multi_gpu_train.py --num_gpus=2`
reported a error

```
usage: cifar10_multi_gpu_train.py [-h] [--batch_size BATCH_SIZE]
                                  [--data_dir DATA_DIR] [--use_fp16 USE_FP16]
cifar10_multi_gpu_train.py: error: unrecognized arguments: --num_gpus=2
```

So what is `[--use_fp16 USE_FP16]` ,it didn;a appear at anywhere in this code file",0,,1,2017-12-01T07:23:04Z,2017-12-01T19:21:03Z,CONTRIBUTOR,2017-12-01T14:17:05Z
15026,tensorflow 1.4 tf.keras gives different result compared with using keras directly,"stat:awaiting response,type:bug/performance","I have tensorflow 1.4, when running the following code, the accuracy is different (78% vs. 34.90%) when I import Sequential, Dense, and model_from_json directly from keras (uncomment first 3 lines) compared with import from tensorflow.python.keras. Why is the big discrepancy? 
 
(the data pima-indians-diabetes.csv is available at http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data)

#from keras.models import Sequential
#from keras.layers import Dense
#from keras.models import model_from_json

from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.models import Sequential, model_from_json

import numpy
import os
# fix random seed for reproducibility
numpy.random.seed(7)
# load pima indians dataset
dataset = numpy.loadtxt(""pima-indians-diabetes.csv"", delimiter="","")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = Sequential()
model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))
model.add(Dense(8, kernel_initializer='uniform', activation='relu'))
model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))
# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# Fit the model
model.fit(X, Y, epochs=150, batch_size=10, verbose=0)
# evaluate the model
scores = model.evaluate(X, Y, verbose=0)
print(""%s: %.2f%%"" % (model.metrics_names[1], scores[1]*100))

# serialize model to JSON
model_json = model.to_json()
with open(""model.json"", ""w"") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights(""model.h5"")
print(""Saved model to disk"")",1,,9,2017-12-01T06:39:42Z,2018-01-24T20:13:33Z,NONE,2017-12-02T11:59:57Z
15024,Branch 177545934,cla: yes,,0,,1,2017-12-01T05:18:22Z,2017-12-02T09:38:23Z,MEMBER,2017-12-02T08:55:28Z
15023,Fix a BUILD file bug in `tensorflow/contrib/cloud/BUILD`,"awaiting testing (then merge),cla: yes","In `tensorflow/contrib/cloud`, invoking `bigquery_reader_ops_test` will fail.

The error is caused by the the fact that `bigquery_reader_ops_test` depends on `:bigquery_reader_ops_op_lib` and `:bigquery_reader_ops`.

However, bigquery_reader_ops_test is in python, `:bigquery_reader_ops_op_lib` and `:bigquery_reader_ops` are cc libraries. So they shouldn't be the dependencies of bigquery_reader_ops_test.

This fix removes the above two dependencies so that `bigquery_reader_ops_test` could run successfully.

Below is the full error message before this PR.

```
ubuntu@ubuntu:~/tensorflow$ bazel test -s --config=opt //tensorflow/contrib/cloud:bigquery_reader_ops_test
..........
WARNING: /home/ubuntu/tensorflow/tensorflow/core/BUILD:1815:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/ubuntu/tensorflow/tensorflow/tensorflow.bzl:1127:30
ERROR: /home/ubuntu/tensorflow/tensorflow/contrib/cloud/BUILD:58:1: in deps attribute of py_test rule //tensorflow/contrib/cloud:bigquery_reader_ops_test: '//tensorflow/contrib/cloud:bigquery_reader_ops_op_lib' does not have mandatory providers: 'py'. Since this rule was created by the macro 'tf_py_test', the error might have been caused by the macro implementation in /home/ubuntu/tensorflow/tensorflow/tensorflow.bzl:1368:12
ERROR: /home/ubuntu/tensorflow/tensorflow/contrib/cloud/BUILD:58:1: in deps attribute of py_test rule //tensorflow/contrib/cloud:bigquery_reader_ops_test: '//tensorflow/contrib/cloud/kernels:bigquery_reader_ops' does not have mandatory providers: 'py'. Since this rule was created by the macro 'tf_py_test', the error might have been caused by the macro implementation in /home/ubuntu/tensorflow/tensorflow/tensorflow.bzl:1368:12
ERROR: Analysis of target '//tensorflow/contrib/cloud:bigquery_reader_ops_test' failed; build aborted: Analysis of target '//tensorflow/contrib/cloud:bigquery_reader_ops_test' failed; build aborted
INFO: Elapsed time: 10.083s
FAILED: Build did NOT complete successfully (105 packages loaded)
ERROR: Couldn't start the build. Unable to run tests
ubuntu@ubuntu:~/tensorflow$
```

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,4,2017-12-01T04:22:32Z,2017-12-04T06:25:41Z,MEMBER,2017-12-04T02:46:21Z
15022,update label_image.py,"awaiting testing (then merge),cla: yes","1. add build rule for label_image.py
2. remove extraneous semicolons",0,,12,2017-12-01T04:19:25Z,2018-01-10T04:10:02Z,CONTRIBUTOR,2017-12-01T05:39:01Z
15021,add link to decode_bmp,"awaiting testing (then merge),cla: yes",add link to decode_bmp to image api guide,0,,2,2017-12-01T02:43:37Z,2017-12-08T15:04:13Z,CONTRIBUTOR,2017-12-04T02:48:42Z
15019,Tensorflow: how to save/restore tf.data.Dataset?,"stat:awaiting response,type:bug/performance","I made a model with `tf.data.Dataset()` as a data IO function

then i exported the graph and tried to restore it with meta_graph file But it failed and following error messages occurred.

I think that `tf.data.Dataset()` made a C++ object instead of python queue used before.

And the graph_def only has a C++ object handler reference, so the graph_def alone without real C++ object can't load complete graph.

How can I load a executable graph with `tf.data.Dataset()`? Or is it impossible for now?

```
File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Function
_make_dataset_5150cb86 is not defined.
         [[Node: batch_processing/OneShotIterator = OneShotIterator[container="""", dataset_factory=_make_dataset_5150cb86[], output_shapes=[[?,1], [?,299,299,3]], output_types=[DT_INT32, DT_FLOAT], shared_name="""",
_device=""/job:workers/replica:0/task:0/device:CPU:0""]()]]
```

In short, all the tensorflow graphs without` tf.data.Dataset` work, when i add following codes. 
```
graph = tf.train.export_meta_graph()
tf.reset_default_graph()
tf.train.import_meta_graph(graph)

```
But the graphs with` tf.data.Dataset `make a error message above


",1,,6,2017-12-01T01:40:47Z,2017-12-07T05:13:24Z,NONE,2017-12-01T13:03:40Z
15018,Introduce tf_http_archive,"awaiting testing (then merge),cla: yes",I decided to give https://github.com/tensorflow/tensorflow/pull/14813 another try.,1,,8,2017-12-01T00:14:13Z,2017-12-02T00:02:58Z,MEMBER,2017-12-01T18:03:36Z
15016,no such package 'tensorflow/examples/image_retraining': BUILD file not found on package path. for Virtual Environment on mac. ,type:support,**Template ignored**,0,,2,2017-11-30T23:53:32Z,2017-12-01T19:30:46Z,NONE,2017-12-01T19:30:44Z
15013,GPU visual studio dependecies ,stat:awaiting response,"I build tensorflow 1.4.0 for windows with GPU usage. But when I run my program in visual studio 2015 I get these error:
```
1>tf_core_kernels.lib(cudnn_rnn_ops.cc.obj) : error LNK2001: unresolved external symbol ""public: virtual __cdecl perftools::gputools::ScratchAllocator::~ScratchAllocator(void)"" (??1ScratchAllocator@gputools@perftools@@UEAA@XZ)
1>tf_core_kernels.lib(fft_ops.obj) : error LNK2001: unresolved external symbol ""public: virtual __cdecl perftools::gputools::ScratchAllocator::~ScratchAllocator(void)"" (??1ScratchAllocator@gputools@perftools@@UEAA@XZ)
1>tf_core_kernels.lib(conv_ops_3d.obj) : error LNK2001: unresolved external symbol ""public: virtual __cdecl perftools::gputools::ScratchAllocator::~ScratchAllocator(void)"" (??1ScratchAllocator@gputools@perftools@@UEAA@XZ)
1>tf_core_kernels.lib(conv_ops.obj) : error LNK2001: unresolved external symbol ""public: virtual __cdecl perftools::gputools::ScratchAllocator::~ScratchAllocator(void)"" (??1ScratchAllocator@gputools@perftools@@UEAA@XZ)
1>tf_core_kernels.lib(cudnn_rnn_ops.cc.obj) : error LNK2001: unresolved external symbol ""public: class perftools::gputools::Stream & __cdecl perftools::gputools::Stream::ThenRnnForward(class perftools::gputools::dnn::RnnDescriptor const &,class perftools::gputools::dnn::RnnSequenceTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::dnn::RnnSequenceTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> *,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> *,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> *,bool,class perftools::gputools::ScratchAllocator *,class perftools::gputools::ScratchAllocator *)"" (?ThenRnnForward@Stream@gputools@perftools@@QEAAAEAV123@AEBVRnnDescriptor@dnn@23@AEBVRnnSequenceTensorDescriptor@523@AEBV?$DeviceMemory@M@23@AEBVRnnStateTensorDescriptor@523@23221PEAV723@3434_NPEAVScratchAllocator@23@6@Z)
1>tf_core_kernels.lib(cudnn_rnn_ops.cc.obj) : error LNK2001: unresolved external symbol ""public: class perftools::gputools::Stream & __cdecl perftools::gputools::Stream::ThenRnnForward(class perftools::gputools::dnn::RnnDescriptor const &,class perftools::gputools::dnn::RnnSequenceTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::dnn::RnnSequenceTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> *,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> *,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> *,bool,class perftools::gputools::ScratchAllocator *,class perftools::gputools::ScratchAllocator *)"" (?ThenRnnForward@Stream@gputools@perftools@@QEAAAEAV123@AEBVRnnDescriptor@dnn@23@AEBVRnnSequenceTensorDescriptor@523@AEBV?$DeviceMemory@N@23@AEBVRnnStateTensorDescriptor@523@23221PEAV723@3434_NPEAVScratchAllocator@23@6@Z)
1>tf_core_kernels.lib(cudnn_rnn_ops.cc.obj) : error LNK2001: unresolved external symbol ""public: class perftools::gputools::Stream & __cdecl perftools::gputools::Stream::ThenRnnBackward(class perftools::gputools::dnn::RnnDescriptor const &,class perftools::gputools::dnn::RnnSequenceTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::dnn::RnnSequenceTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::DeviceMemory<float> *,class perftools::gputools::DeviceMemory<float> *,class perftools::gputools::DeviceMemory<float> *,class perftools::gputools::DeviceMemory<float> *,class perftools::gputools::DeviceMemory<unsigned char> *,class perftools::gputools::ScratchAllocator *)"" (?ThenRnnBackward@Stream@gputools@perftools@@QEAAAEAV123@AEBVRnnDescriptor@dnn@23@AEBVRnnSequenceTensorDescriptor@523@AEBV?$DeviceMemory@M@23@AEBVRnnStateTensorDescriptor@523@2322123232222PEAV723@444PEAV?$DeviceMemory@E@23@PEAVScratchAllocator@23@@Z)
1>tf_core_kernels.lib(cudnn_rnn_ops.cc.obj) : error LNK2001: unresolved external symbol ""public: class perftools::gputools::Stream & __cdecl perftools::gputools::Stream::ThenRnnBackward(class perftools::gputools::dnn::RnnDescriptor const &,class perftools::gputools::dnn::RnnSequenceTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::dnn::RnnSequenceTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::DeviceMemory<double> *,class perftools::gputools::DeviceMemory<double> *,class perftools::gputools::DeviceMemory<double> *,class perftools::gputools::DeviceMemory<double> *,class perftools::gputools::DeviceMemory<unsigned char> *,class perftools::gputools::ScratchAllocator *)"" (?ThenRnnBackward@Stream@gputools@perftools@@QEAAAEAV123@AEBVRnnDescriptor@dnn@23@AEBVRnnSequenceTensorDescriptor@523@AEBV?$DeviceMemory@N@23@AEBVRnnStateTensorDescriptor@523@2322123232222PEAV723@444PEAV?$DeviceMemory@E@23@PEAVScratchAllocator@23@@Z)
1>tf_core_kernels.lib(cudnn_rnn_ops.cc.obj) : error LNK2001: unresolved external symbol ""public: class perftools::gputools::port::StatusOr<class std::unique_ptr<class perftools::gputools::dnn::RnnDescriptor,struct std::default_delete<class perftools::gputools::dnn::RnnDescriptor> > > __cdecl perftools::gputools::StreamExecutor::createRnnDescriptor(int,int,int,enum perftools::gputools::dnn::RnnInputMode,enum perftools::gputools::dnn::RnnDirectionMode,enum perftools::gputools::dnn::RnnMode,enum perftools::gputools::dnn::DataType,float,unsigned __int64,class perftools::gputools::ScratchAllocator *)"" (?createRnnDescriptor@StreamExecutor@gputools@perftools@@QEAA?AV?$StatusOr@V?$unique_ptr@VRnnDescriptor@dnn@gputools@perftools@@U?$default_delete@VRnnDescriptor@dnn@gputools@perftools@@@std@@@std@@@port@23@HHHW4RnnInputMode@dnn@23@W4RnnDirectionMode@723@W4RnnMode@723@W4DataType@723@M_KPEAVScratchAllocator@23@@Z)
1>tf_core_kernels.lib(cudnn_rnn_ops.cc.obj) : error LNK2001: unresolved external symbol ""public: class perftools::gputools::port::StatusOr<class std::unique_ptr<class perftools::gputools::dnn::RnnSequenceTensorDescriptor,struct std::default_delete<class perftools::gputools::dnn::RnnSequenceTensorDescriptor> > > __cdecl perftools::gputools::StreamExecutor::createRnnSequenceTensorDescriptor(int,int,int,enum perftools::gputools::dnn::DataType)"" (?createRnnSequenceTensorDescriptor@StreamExecutor@gputools@perftools@@QEAA?AV?$StatusOr@V?$unique_ptr@VRnnSequenceTensorDescriptor@dnn@gputools@perftools@@U?$default_delete@VRnnSequenceTensorDescriptor@dnn@gputools@perftools@@@std@@@std@@@port@23@HHHW4DataType@dnn@23@@Z)
1>tf_core_kernels.lib(cudnn_rnn_ops.cc.obj) : error LNK2001: unresolved external symbol ""public: class perftools::gputools::port::StatusOr<class std::unique_ptr<class perftools::gputools::dnn::RnnStateTensorDescriptor,struct std::default_delete<class perftools::gputools::dnn::RnnStateTensorDescriptor> > > __cdecl perftools::gputools::StreamExecutor::createRnnStateTensorDescriptor(int,int,int,enum perftools::gputools::dnn::DataType)"" (?createRnnStateTensorDescriptor@StreamExecutor@gputools@perftools@@QEAA?AV?$StatusOr@V?$unique_ptr@VRnnStateTensorDescriptor@dnn@gputools@perftools@@U?$default_delete@VRnnStateTensorDescriptor@dnn@gputools@perftools@@@std@@@std@@@port@23@HHHW4DataType@dnn@23@@Z)
1>tf_core_gpu_kernels.lib(tf_core_gpu_kernels_generated_avgpooling_op_gpu.cu.cc.obj) : error LNK2001: unresolved external symbol cudaMemcpyAsync
1>tf_core_gpu_kernels.lib(tf_core_gpu_kernels_generated_argmax_op_gpu.cu.cc.obj) : error LNK2001: unresolved external symbol cudaMemcpyAsync
1>tf_core_gpu_kernels.lib(tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj) : error LNK2001: unresolved external symbol cudaMemcpyAsync
1>tf_core_gpu_kernels.lib(tf_core_gpu_kernels_generated_concat_lib_gpu_impl.cu.cc.obj) : error LNK2001: unresolved external symbol cudaMemcpyAsync
```

My system:
Windows 10
Cuda 8.0
Cudnn 6
cmake cmake-3.9.4-win64-x64
Python 3.5.2
VS2015",0,,2,2017-11-30T23:08:54Z,2017-12-01T19:49:37Z,NONE,2017-12-01T01:53:08Z
15008,Document iOS demo app in TF Lite Readme,cla: yes,"See the rendered markdown here: https://github.com/miaout17/tensorflow/blob/ios-example-readme/tensorflow/contrib/lite/README.md#ios-demo-app

I think the demo app is already well-explained for the Android, so it just require a few lines to describe how to build the iOS version. ",0,,2,2017-11-30T20:18:51Z,2017-12-02T09:39:56Z,CONTRIBUTOR,2017-11-30T20:22:13Z
15005,Make build rule for //tensorflow/contrib/lite/tools:benchmark_model,"awaiting testing (then merge),cla: yes,comp:lite","Resolve issue https://github.com/tensorflow/tensorflow/issues/14581
Note that this benchmark_model.cc is not completed yet. It doesn't
load and models.

1. With proper Android SDK and NDK settings in WORKSPACE, we can
   build armeabi-v7a or arm64-v8a binary, e.g.,

   > bazel build --cxxopt='--std=c++11' \
     --crosstool_top=//external:android/crosstool \
     --cpu=arm64-v8a \
     --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
    //tensorflow/contrib/lite/tools:benchmark_model

2. It's also possible to build this for Linux or OS X, e.g.,

   >  bazel --config opt --cxxopt='--std=c++11' \
     //tensorflow/contrib/lite/tools:benchmark_model",0,,3,2017-11-30T15:05:13Z,2018-01-19T01:43:44Z,CONTRIBUTOR,2017-12-29T01:14:12Z
15004,compile error with cmake for windows 10 and GPU enabled,,"Hi,
I am trying to compile the tensorflow.dll with cmake for windows 10 and GPU enabled. However, the it failed because of the following error:
```
"" C:\Users\mcuevas\bin\tensorflow\tensorflow\tensorflow/stream_executor/stream.h(1921): error C2065: 'tf_shared_lock
': undeclared identifier (compiling source file C:\Users\mcuevas\bin\tensorflow\tensorflow\tensorflow\core\grappler\
devices.cc) [C:\Users\mcuevas\bin\tensorflow\tensorflow\tensorflow\contrib\cmake\build_GPU\tf_core_cpu.vcxproj]
  C:\Users\mcuevas\bin\tensorflow\tensorflow\tensorflow/stream_executor/stream.h(1921): error C2146: syntax error: m
issing ';' before identifier 'lock' (compiling source file C:\Users\mcuevas\bin\tensorflow\tensorflow\tensorflow\cor
e\grappler\devices.cc) [C:\Users\mcuevas\bin\tensorflow\tensorflow\tensorflow\contrib\cmake\build_GPU\tf_core_cpu.vc
xproj]
  C:\Users\mcuevas\bin\tensorflow\tensorflow\tensorflow/stream_executor/stream.h(1921): error C2065: 'lock': undecla
red identifier (compiling source file C:\Users\mcuevas\bin\tensorflow\tensorflow\tensorflow\core\grappler\devices.cc
) [C:\Users\mcuevas\bin\tensorflow\tensorflow\tensorflow\contrib\cmake\build_GPU\tf_core_cpu.vcxproj]
  C:\Users\mcuevas\bin\tensorflow\tensorflow\tensorflow/stream_executor/stream.h(1921): error C2065: 'tf_shared_lock
': undeclared identifier (compiling source file C:\Users\mcuevas\bin\tensorflow\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_cudamalloc_allocator.cc) [C:\Users\mcuevas\bin\tensorflow\tensorflow\tensorflow\contrib\cmake\build_GP
U\tf_core_cpu.vcxproj]
  C:\Users\mcuevas\bin\tensorflow\tensorflow\tensorflow/stream_executor/stream.h(1921): error C2146: syntax error: m
issing ';' before identifier 'lock' (compiling source file C:\Users\mcuevas\bin\tensorflow\tensorflow\tensorflow\cor
e\common_runtime\gpu\gpu_cudamalloc_allocator.cc) [C:\Users\mcuevas\bin\tensorflow\tensorflow\tensorflow\contrib\cma
ke\build_GPU\tf_core_cpu.vcxproj]""
```
These are the commands I use:
cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:\Users\mcuevas\bin\swigwin-3.0.12\swigwin-3.0.12\swig.exe -DPYTHON_EXECUTABLE=C:\Users\mcuevas\AppData\Local\Continuum\anaconda3\envs\acsis_cpu\python.exe -DPYTHON_LIBRARIES=C:\Users\mcuevas\AppData\Local\Continuum\anaconda3\pkgs\python-3.5.2-0\libs\python35.lib -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0""

MSBuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj

Thanks in advance.

My system:
Windows 10
Cuda 8.0
Cudnn 6
cmake cmake-3.9.4-win64-x64
Python 3.5.2
VS2015",0,,2,2017-11-30T14:15:26Z,2017-11-30T23:01:17Z,NONE,2017-11-30T15:26:07Z
15003,msvcp140.dll missing on win 7,type:build/install,"Hi everyone :)

I installed Tensorflow on Windows 7 ultimate with Python 3.5.4, via the `pip3 install --upgrade tensorflow` command. It even stated it successfully installed version 1.4

But if I try to invoke `import tensorflow as tf` in the shell I get the following error:
![grafik](https://user-images.githubusercontent.com/33151777/33433308-5b8fee18-d5db-11e7-8ddf-7037bae45e11.png)
 
Of course I installed the recommended Visual C++ 2015 Redistributablle 64bit Update 3, but there is neither the .dll file in my system folder, nor does tensorflow work. I get the same error as before.

Any help is appreciated, Thank you :)
",0,,1,2017-11-30T13:36:25Z,2017-11-30T19:29:44Z,NONE,2017-11-30T19:29:44Z
15001,How can i read csv file by file name and line number in tensorflow?,,"I read some csv files using queue like [this](https://www.tensorflow.org/api_guides/python/reading_data#creating_threads_to_prefetch_using_queuerunner_objects).  Then, i get `key` and `value` when reading from file queues. The `key` contains file name and line number.

```
    filename_queue = tf.train.string_input_producer([""file0.csv"", ""file1.csv""]) 
    reader = tf.TextLineReader()
    key, value = reader.read(filename_queue)
```


I train model using Dynamic negative sampling like [IRGAN code](https://github.com/xf4fresh/irgan/blob/master/ltr-gan/ltr-gan-pointwise/ltr_dns_nn.py#L41). I need sample from large scale negative samples, so i want to save the example id such as filename:line rather than feature itself. 

However, i can not find a suitable function on document.
Thanks for your attention!",0,,1,2017-11-30T12:39:11Z,2017-11-30T19:13:31Z,NONE,2017-11-30T19:13:31Z
15000,"WIP: Support ""causal"" padding in tf.layers.convolutional.Conv1D","awaiting review,cla: yes","Fix #14933.

Because we don't see causal padding in other use cases expect of NTC, we choose to modify code at Conv1D, instead of `tf.nn.convolution`.
For simplicity, we hack the Conv1D's `__call__` method, instead of `build`, `call` and `_compute_output_shape`.

### How to test

+ [x] add test case.
+ [ ]  pass all tests.",0,,5,2017-11-30T11:51:15Z,2017-12-01T07:48:15Z,CONTRIBUTOR,2017-12-01T07:43:03Z
14999,fix clip weights tests,"awaiting testing (then merge),cla: yes",using learning rate 1; variable value after the first update becomes zero resulting in failed tests. ,1,,6,2017-11-30T10:43:59Z,2017-12-26T20:36:15Z,CONTRIBUTOR,2017-11-30T10:46:40Z
14991,[Go] Adds Operations() method to Graph,"awaiting testing (then merge),cla: yes","There is currently no way to list all of the operations in a graph
from the go api. This patch ads an Operations() method to retrieve the
list using the existing TF_GraphNextOperation c api. The graph_test
was modified to include testing this new method.

Signed-off-by: Vishvananda Ishaya Abrams <vishvananda@gmail.com>",0,,3,2017-11-30T06:35:22Z,2017-12-10T18:52:57Z,CONTRIBUTOR,2017-12-04T02:50:33Z
14990,fcn network to tensorflow lite ?,stat:awaiting response,"Input:
(u'input_image', (<tf.Tensor 'input_image:0' shape=<unknown> dtype=float32>,))
(u'image_width', (<tf.Tensor 'image_width:0' shape=<unknown> dtype=int32>,))
(u'image_height', (<tf.Tensor 'image_height:0' shape=<unknown> dtype=int32>,))

Output:
(u'Squeeze', (<tf.Tensor 'Squeeze:0' shape=(?, ?, 2) dtype=float32>,))
(u'Squeeze_1', (<tf.Tensor 'Squeeze_1:0' shape=(?, ?, 4) dtype=float32>,))
(u'Squeeze_2', (<tf.Tensor 'Squeeze_2:0' shape=(?, ?, 10) dtype=float32>,))

how to do?
 bazel-bin/tensorflow/contrib/lite/toco/toco \
  --input_file=PNet.pb \
  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \
  --output_file=PNet.float.lite --inference_type=FLOAT \
  --input_type=FLOAT --input_arrays=input_image,image_width,image_height \
  --output_arrays=Squeeze,Squeeze_1 --input_shapes=?


tensorflow/contrib/lite/toco/tooling_util.cc:1547] Check failed: array.final_data_type == array.data_type",0,,2,2017-11-30T06:26:31Z,2017-12-01T08:21:06Z,NONE,2017-12-01T07:06:25Z
14988,tf.image.crop_and_resize,,"- [ ] 

- [ ] #-

 **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 14.04LTS
- **TensorFlow installed from (source or binary)**: yes
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 2.7.6
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**:N/A
- **Exact command to reproduce**:N/A

Anyone know how the 
tf.image.crop_and_resize(images, boxes, batch_inds,
                                         [pooled_height, pooled_width],
                                         method='bilinear',
                                         name='Crop')
work? i cannot find the "" tf.image.crop_and_resize"" in  tensorflow/python/ops/gen_image_ops.py. 
Thank you",0,,4,2017-11-30T05:52:26Z,2017-12-01T19:35:01Z,NONE,2017-12-01T07:06:29Z
14985,tf.nn.fractional_max_pool output have same batch size when feed with different input batch size,"stat:awaiting tensorflower,type:bug/performance","
### Describe the problem
tf.nn.fractional_max_pool output have same batch size when feed with different input batch size.
Attached is test code I write. 2 different input is feed in with different batch size , outputs get same batch size.
[pool_test.py.txt](https://github.com/tensorflow/tensorflow/files/1516498/pool_test.py.txt)

###code result
shape of input_a (3, 32, 32, 3)
shape of output_a (3, 21, 21, 3)
shape of input_b **(4, 32, 32, 3)**
shape of output_b **(3, 21, 21, 3)**

### System information

== cat /etc/issue ===============================================
Linux c-1080u 4.10.0-40-generic #44~16.04.1-Ubuntu SMP Thu Nov 9 15:37:44 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux c-1080u 4.10.0-40-generic #44~16.04.1-Ubuntu SMP Thu Nov 9 15:37:44 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
numpydoc (0.7.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow'

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Thu Nov 30 11:55:40 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |
|  0%   51C    P8    21W / 280W |    860MiB / 11169MiB |      9%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1060      G   /usr/lib/xorg/Xorg                           542MiB |
|    0      1540      G   compiz                                       315MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61

== cat /etc/issue ===============================================
Linux c-1080u 4.10.0-40-generic #44~16.04.1-Ubuntu SMP Thu Nov 9 15:37:44 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux c-1080u 4.10.0-40-generic #44~16.04.1-Ubuntu SMP Thu Nov 9 15:37:44 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.5.0.post1)
tensorflow-gpu (1.4.0)
tensorflow-tensorboard (0.4.0rc3)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.4.0
tf.GIT_VERSION = v1.4.0-rc1-11-g130a514
tf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Thu Nov 30 11:56:18 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |
|  0%   51C    P0    80W / 280W |    860MiB / 11169MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1060      G   /usr/lib/xorg/Xorg                           542MiB |
|    0      1540      G   compiz                                       315MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61




",0,,4,2017-11-30T04:20:21Z,2018-01-05T14:05:19Z,NONE,2017-11-30T19:51:44Z
14984,Add `AWS_REGION` env for S3 in TensorFlow,"awaiting testing (then merge),cla: yes","This fix tries to address the issue raised in #14951 where the region can only be specified with non-common `S3_REGION` environment variables.

This fix adds the support of `AWS_REGION` which takes precedence over `S3_REGION`.

This fix fixes #14951.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,2,2017-11-30T04:01:00Z,2017-12-10T20:14:32Z,MEMBER,2017-12-04T02:52:07Z
14983,Switch over to max_pool_v2 in Python,"awaiting testing (then merge),cla: yes","This fix is a follow up to #11875 so that MaxPool in Python use v2 version. As 11875 has been merged several months ago, this fix conforms to the deprecation policy.

This fix is realted to #11875 and #4746.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,8,2017-11-29T23:33:29Z,2018-01-26T20:08:48Z,MEMBER,2017-12-19T01:58:30Z
14982,Added ability to skip rescan bottleneks,"awaiting review,cla: yes",,0,,2,2017-11-29T21:43:54Z,2018-01-19T08:12:46Z,CONTRIBUTOR,2017-12-26T19:50:01Z
14981,Bump the GRPC version TF depends on.,cla: yes,To help fix #14039,0,,5,2017-11-29T21:27:52Z,2017-12-04T06:20:57Z,OWNER,2017-12-01T19:08:41Z
14978,Header mismatch when running 'Simple Audio Recognition',type:build/install,"tensorflow: 1.4, ubuntu 17.10, python3.6 anaconda, cuda 8.0, cudnn 6.0

Going with [Simple Audio Recognition](https://www.tensorflow.org/versions/master/tutorials/audio_recognition), I met an error when running 
```bash
python tensorflow/examples/speech_commands/train.py --data_dir=tensorflow/examples/speech_commands/train/audio
```

error:
```
Traceback (most recent call last):                                                                                                                            
  File ""/home/jihao/.conda/envs/tf_cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1323, in _do_call                               
    return fn(*args)                                                                                                                                          
  File ""/home/jihao/.conda/envs/tf_cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1302, in _run_fn                                
    status, run_metadata)                                                                                                                                     
  File ""/home/jihao/.conda/envs/tf_cpu/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__                         
    c_api.TF_GetCode(self.status.status))                                                                                                                     
tensorflow.python.framework.errors_impl.InvalidArgumentError: Header mismatch: Expected RIFF but found \udc87\udc9b\udc8f\udc8c                               
         [[Node: DecodeWav = DecodeWav[desired_channels=1, desired_samples=-1, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile)]]
```
",1,,4,2017-11-29T14:59:15Z,2017-12-14T16:39:50Z,NONE,2017-11-29T16:58:43Z
14977,Wrong Code in example in Programmer's guide,"awaiting testing (then merge),cla: yes","In Programmer's guide Variable section
the `assignment` variable is a `tf.Tensor` and should use `.eval()` instead of  `.run()`

Otherwise, this code would produce an error:
```
AttributeError: 'Tensor' object has no attribute 'run'
```
",1,,2,2017-11-29T14:30:01Z,2017-11-30T04:34:43Z,CONTRIBUTOR,2017-11-30T04:34:43Z
14976,Add missing <stdio.h> include,"awaiting testing (then merge),cla: yes,comp:lite","Add missing <stdio.h> include in `tensorflow/contrib/lite/kernels/op_macros.h`. Otherwise, it fails to find `stderr`.

To reproduce it:

```shell
bazel build //tensorflow/contrib/lite/java:tflite_runtime
```",0,,2,2017-11-29T13:57:54Z,2017-12-26T02:37:46Z,CONTRIBUTOR,2017-12-26T02:37:46Z
14975,master branch: compilation of llvm_gpu_backend failed,,"### System information
- **OS Platform and Distribution**: slackware64-current
- **Python version**: 3.6.3
- **Bazel version**: 0.8.0
- **GCC/Compiler version**: 5.4.0
- **CUDA/cuDNN version**: 9 / 7
- **build environment**:
export PYTHON_BIN_PATH=/usr/bin/python3
export USE_DEFAULT_PYTHON_LIB_PATH=1
export CC_OPT_FLAGS=""-march=native""
export TF_NEED_JEMALLOC=1
export TF_NEED_S3=1
export TF_NEED_GCP=0
export TF_NEED_HDFS=0
export TF_NEED_GDR=0
export TF_ENABLE_XLA=1
export TF_NEED_VERBS=0
export TF_NEED_OPENCL=0
export TF_NEED_MKL=1
export TF_NEED_MPI=0
export TF_NEED_CUDA=1
export GCC_HOST_COMPILER_PATH=/opt/gcc54/bin/gcc-5.4.0
export TF_CUDA_CLANG=0
export TF_NEED_OPENCL_SYCL=0
export CLANG_CUDA_COMPILER_PATH=/usr/bin/clang
export CUDA_TOOLKIT_PATH=/usr/share/cuda
export TF_CUDA_VERSION=$($CUDA_TOOLKIT_PATH/bin/nvcc --version | sed -n 's/^.*release \(.*\),.*/\1/p')
export CUDNN_INSTALL_PATH=/usr/share/cuda
export TF_CUDNN_VERSION=$(sed -n 's/^#define CUDNN_MAJOR\s*\(.*\).*/\1/p' $CUDNN_INSTALL_PATH/include/cudnn.h)
export TF_CUDA_COMPUTE_CAPABILITIES=3.0
sed -i 's|/lib""|/lib64""|g' third_party/mkl/build_defs.bzl
sed -i 's|""lib""|""lib64""|g' third_party/mkl/build_defs.bzl
sed -i 's|/lib|/lib64|g' tensorflow/c/generate-pc.sh
- **build command**:
./configure
bazel build --config=opt --config=cuda --config=mkl //tensorflow:libtensorflow.so //tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package tmp`

### Building from master fails
ERROR: /tmp/SBo-VCS/tensorflow_cuda-VCS/tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/BUILD:16:1: C++ compilation of rule '//tensorflow/compiler/xla/service/gpu/llvm_gpu_backend:llvm_gpu_backend' failed (Exit 1)
tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:37:41: fatal error: llvm/CodeGen/CommandFlags.def: No such file or directory
compilation terminated.
INFO: Elapsed time: 1828.121s, Critical Path: 131.49s
FAILED: Build did NOT complete successfully",0,,10,2017-11-29T13:19:14Z,2017-11-30T02:21:44Z,NONE,2017-11-30T02:21:44Z
14973,Tensor as index to other tensor not supported?,,"I have 2 tensors (one is constant and one is placeholder), lets say:
A = [0.1,0.2,0.3,0.4,0.5,0.6,0.7]
B = [[1,2,4],[5,0],[3]]
I want to build a tensor C like this:
C= [[0.2,0.3,0.5],[0.6,0.1],[0.4]] which is a tensor in the same size of B and every element in C is equal to element in A indexed by B elements.

is there any way to do that?

Thanks!",0,,1,2017-11-29T11:22:18Z,2017-11-30T02:13:50Z,NONE,2017-11-30T02:13:50Z
14971,Fix Wrong Rendering of Code Example,"awaiting testing (then merge),cla: yes",Code example was being rendered as plain text. This commit fixes it.,0,,7,2017-11-29T09:34:03Z,2017-11-29T17:53:57Z,NONE,2017-11-29T16:17:36Z
14970,AttributeError: module 'tensorflow' has no attribute 'session',,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 (64bit)
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Why in tensorflow 1.4 when running validating command in sess = tf.session() Line gives this error? is Session attribute changed in 1.4 version of tensorflow compare to previous versions Like 0.12?

### Source code / logs
>>> import tensorflow as tf
>>> hello = tf.constant ('hello, tensorflow!')
>>> sess = tf.session ()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'session'",0,,1,2017-11-29T09:25:19Z,2017-11-29T14:51:59Z,NONE,2017-11-29T14:51:59Z
14968,Question about tf.contrib.image.rotate,,"Good Morning :) -
is it possible to parse the result from `tf.contrib.image.rotate` to a tensor to use it in `t.cast` ?

",0,,1,2017-11-29T08:52:56Z,2017-11-29T17:09:53Z,NONE,2017-11-29T17:09:53Z
14966,why the size of filter is 3*3 in inceptionv1.py?,,"https://github.com/tensorflow/tensorflow/blob/b5df90f91cde6eb12af9cbe818bd2cf4a9bcc687/tensorflow/contrib/slim/python/slim/nets/inception_v1.py#L103
according to the paper,  the size of filter in the scope of ""InceptionV1/Mixed_3b/Branch_2""  should be 5*5.
why is 3*3 in the script?",0,,1,2017-11-29T07:57:48Z,2017-11-29T17:09:33Z,NONE,2017-11-29T17:09:33Z
14965,compile tensorflow with the source code-----ERROR,stat:awaiting response,"when i compile tensorflow with the source code,an ERROR appear. If you know how to solve this problem,please help me. Thank you.

ERROR: /home/hy003/tensorflow/tensorflow/python/BUILD:4508:1 C++ compilation of rule '//tensorflow/python:framework/fast_tensor_util.so' failed(Exit 1). bazel-out/local-opt/genfiles/tensorflow/python/framework/fast_tensor_util.cpp: In function 'PyObject* __pyx_f_5numpy_PyDataType_SHAPE(PyArray_Descr*)': bazel-out/local/genfiles/tensorflow/python/framework/fastz_tensor_util.cpp:5500:48: error 'PyDataType_HASSUBARRAY' was not declared in this scope  __pyx_t_1 = (PyDataType_HASSUBARRAY(__pyx_v_d != 0))  ",0,,2,2017-11-29T07:03:11Z,2017-12-20T17:12:50Z,NONE,2017-11-29T17:28:08Z
14962,Tensorflow Conv model crashes on GPU with zero size batch,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, I have created two CNN models
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: I installed tensorflow via pip install (Python 3)
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: 8.0/6
- **GPU model and memory**: NVIDIA Tesla k80 totalMemory: 11.17GiB freeMemory: 11.09GiB
- **Exact command to reproduce**: See below:

### Below is the log:

> keep_dims is deprecated, use keepdims instead
> _________________________________________________________________
> Layer (type)                 Output Shape              Param #   
> =================================================================
> input_1 (InputLayer)         (None, 7, 264)            0         
> _________________________________________________________________
> reshape_1 (Reshape)          (None, 7, 264, 1)         0         
> _________________________________________________________________
> conv2d_1 (Conv2D)            (None, 7, 264, 64)        16960     
> _________________________________________________________________
> max_pooling2d_1 (MaxPooling2 (None, 7, 132, 64)        0         
> _________________________________________________________________
> flatten_1 (Flatten)          (None, 59136)             0         
> _________________________________________________________________
> dense_1 (Dense)              (None, 1024)              60556288  
> _________________________________________________________________
> dropout_1 (Dropout)          (None, 1024)              0         
> _________________________________________________________________
> dense_2 (Dense)              (None, 512)               524800    
> _________________________________________________________________
> dropout_2 (Dropout)          (None, 512)               0         
> _________________________________________________________________
> dense_3 (Dense)              (None, 88)                45144     
> =================================================================
> Total params: 61,143,192
> Trainable params: 61,143,192
> Non-trainable params: 0
> _________________________________________________________________
> /home/hpnhxxwn/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:2057: UserWarning: Using a generator with `use_multiprocessing=True` and multiple worker
> s may duplicate your data. Please consider using the`keras.utils.Sequence class.
>   UserWarning('Using a generator with `use_multiprocessing=True`'
> ld: learning rate is now 0.01
> 2017-11-29 04:58:20.964015: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX A
> VX2 FMA
> 2017-11-29 04:58:21.094051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUM
> A node, so returning NUMA node zero
> 2017-11-29 04:58:21.094719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1062] Found device 0 with properties: 
> name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
> pciBusID: 0000:00:04.0
> totalMemory: 11.17GiB freeMemory: 11.09GiB
> 2017-11-29 04:58:21.094744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1152] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0
> , compute capability: 3.7)
> Epoch 1/1000
> 5180/6944 [=====================>........] - ETA: 3:43 - loss: 0.1382 - acc: 0.9637 - mean_absolute_error: 0.0715 - sparse_categorical_accuracy: 7.1640e-05switching to  ['AkPnStgb']
> 5212/6944 [=====================>........] - ETA: 3:39 - loss: 0.1384 - acc: 0.9636 - mean_absolute_error: 0.0715 - sparse_categorical_accuracy: 7.1200e-052017-11-29 05:09:22.210897: F
>  tensorflow/stream_executor/cuda/cuda_dnn.cc:444] could not convert BatchDescriptor {count: 0 feature_map_count: 64 spatial: 7 264  value_min: 0.000000 value_max: 0.000000 layout: Batc
> hDepthYX} to cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM




### Below is the model. 
```
def baseline_model():
    inputs = Input(shape=input_shape)
    reshape = Reshape(input_shape_channels)(inputs)
    #normal convnet layer (have to do one initially to get 64 channels)
    conv1 = Conv2D(50,(5,25),activation='tanh')(reshape)
    do1 = Dropout(0.5)(conv1)
    pool1 = MaxPooling2D(pool_size=(1,3))(do1)
    conv2 = Conv2D(50,(3,5),activation='tanh')(pool1)
    do2 = Dropout(0.5)(conv2)
    pool2 = MaxPooling2D(pool_size=(1,3))(do2)
    flattened = Flatten()(pool2)
    fc1 = Dense(1000, activation='sigmoid')(flattened)
    do3 = Dropout(0.5)(fc1)
    fc2 = Dense(200, activation='sigmoid')(do3)
    do4 = Dropout(0.5)(fc2)
    outputs = Dense(note_range, activation='sigmoid')(do4)
    model = Model(inputs=inputs, outputs=outputs)
    return model
````

I have found other github issue created for the same issue, not so far seems like there is no fix yet. I heard the workaround is to use tf.cond, can someone show me how to use it in such case. ",0,,2,2017-11-29T06:05:40Z,2017-11-29T17:08:55Z,NONE,2017-11-29T12:43:45Z
14960,Method of stabilizing prediction box,,"I found that when I used other methods to detect objects, the prediction box was not stable. However, in the demo you provided, I found that the detection box is very stable. Why, can you give me the code to stabilize the detection box?",0,,1,2017-11-29T03:12:18Z,2017-11-29T17:07:03Z,NONE,2017-11-29T17:07:03Z
14959,[BUG]Out-of-Bounds Read in DecodeBmpOp class(tensorflow/core/kernels/decode_bmp_op.cc),stat:contributions welcome,"------------------------

### System information
- **The following is output of tf_env_collect.sh**:
== cat /etc/issue ===============================================
Linux ubuntu 4.4.0-31-generic #50~14.04.1-Ubuntu SMP Wed Jul 13 01:07:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""14.04.5 LTS, Trusty Tahr""
VERSION_ID=""14.04""

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux ubuntu 4.4.0-31-generic #50~14.04.1-Ubuntu SMP Wed Jul 13 01:07:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.4.0)
tensorflow (1.4.0)
tensorflow-tensorboard (0.4.0rc2)

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.4.0
tf.GIT_VERSION = v1.4.0-rc1-11-g130a514
tf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tensorflow/tools/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
                                                              

### Describe the problem
The DecodeBmpOp class is the decoder of bmp file. The class is in 
 tensorflow/core/kernels/decode_bmp_op.cc. When dealling with a bmp file, the decoder doesn't invalidate the meta info of bmp file, such as header_size, width, height. It causes a Out-of-Bound Read in DecodeBmpOp::Decode func or DecodeBmpOp ::Compute func. If given an evil bmp file, the program using this API will crash.

### Source code / logs

- **Here is the crash call stack of program**:
The bmp file  in the attachment causes a crash.

Program received signal SIGSEGV, Segmentation fault.
0x0000000004200bc0 in tensorflow::DecodeBmpOp::Decode (this=this@entry=0x602400032a40, input=input@entry=0x601c000214ce ""33"", output=0x7fffcefcf800 """", width=width@entry=2336, height=height@entry=61727, channels=channels@entry=3, top_down=top_down@entry=false) at tensorflow/core/kernels/decode_bmp_op.cc:122
Program received signal SIGSEGV (fault address 0x601c19caaa10)
pwndbg> bt
#0  0x0000000004200bc0 in tensorflow::DecodeBmpOp::Decode (this=this@entry=0x602400032a40, input=input@entry=0x601c000214ce ""33"", output=0x7fffcefcf800 """", width=width@entry=2336, height=height@entry=61727, channels=channels@entry=3, top_down=top_down@entry=false) at tensorflow/core/kernels/decode_bmp_op.cc:122
#1  0x0000000004202d3b in tensorflow::DecodeBmpOp::Compute (this=0x602400032a40, context=<optimized out>) at tensorflow/core/kernels/decode_bmp_op.cc:88
#2  0x00007ffff3ed8880 in tensorflow::ThreadPoolDevice::Compute (this=<optimized out>, op_kernel=0x602400032a40, context=0x7fffffff8320) at tensorflow/core/common_runtime/threadpool_device.cc:59
#3  0x00007ffff3d47110 in tensorflow::(anonymous namespace)::ExecutorState::Process (this=<optimized out>, tagged_node=..., scheduled_usec=<optimized out>) at tensorflow/core/common_runtime/executor.cc:1652
#4  0x00007ffff3d4cc0c in operator() (__closure=<optimized out>) at tensorflow/core/common_runtime/executor.cc:2055
#5  std::_Function_handler<void(), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(const TaggedNodeSeq&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::__lambda3>::_M_invoke(const std::_Any_data &) (__functor=...) at /usr/include/c++/4.8/functional:2071
#6  0x00007ffff3db2351 in operator() (this=0x7fffffff8790) at /usr/include/c++/4.8/functional:2471
#7  operator() (__closure=<optimized out>, c=...) at tensorflow/core/common_runtime/graph_runner.cc:146
#8  std::_Function_handler<void(std::function<void()>), tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, const NamedTensorList&, const std::vector<std::basic_string<char> >&, std::vector<tensorflow::Tensor>*)::__lambda2>::_M_invoke(const std::_Any_data &, std::function<void()>) (__functor=..., __args#0=...) at /usr/include/c++/4.8/functional:2071
#9  0x00007ffff3ce0258 in operator() (__args#0=..., this=<optimized out>) at /usr/include/c++/4.8/functional:2471
#10 tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady (this=0x6026000fd1a0, ready=..., inline_ready=0x0) at tensorflow/core/common_runtime/executor.cc:2055
#11 0x00007ffff3d00a0c in ScheduleReady (inline_ready=0x0, ready=..., this=<optimized out>) at tensorflow/core/common_runtime/executor.cc:2046
#12 RunAsync (done=<error reading variable: access outside bounds of object referenced via synthetic pointer>, this=<optimized out>) at tensorflow/core/common_runtime/executor.cc:1439
#13 tensorflow::(anonymous namespace)::ExecutorImpl::RunAsync (this=this@entry=0x602400032f40, args=..., done=...) at tensorflow/core/common_runtime/executor.cc:2564
#14 0x00007ffff3db999f in Run (args=..., this=0x602400032f40) at ./tensorflow/core/common_runtime/executor.h:117
#15 tensorflow::GraphRunner::Run (this=this@entry=0x6004002d08f0, graph=graph@entry=0x603a00003140, function_library=function_library@entry=0x602400033800, inputs=std::vector of length 0, capacity 0, output_names=std::vector of length 1, capacity 1 = {...}, outputs=outputs@entry=0x7fffffffa810) at tensorflow/core/common_runtime/graph_runner.cc:174
#16 0x00007ffff3c4f36d in tensorflow::ConstantFold (opts=..., function_library=function_library@entry=0x602400033800, env=env@entry=0x60060000e140, partition_device=partition_device@entry=0x6024000395c0, graph=graph@entry=0x603a00003480, was_mutated=was_mutated@entry=0x7fffffffb260) at tensorflow/core/common_runtime/constant_folding.cc:603
#17 0x00007ffff3db02bd in tensorflow::GraphOptimizer::Optimize (this=this@entry=0x7fffffffbe90, runtime=runtime@entry=0x602400033800, env=0x60060000e140, device=0x6024000395c0, graph=graph@entry=0x60060038d2f0, shape_map=shape_map@entry=0x0) at tensorflow/core/common_runtime/graph_optimizer.cc:66
#18 0x000000000ad04984 in tensorflow::DirectSession::GetOrCreateExecutors (this=this@entry=0x604000007080, inputs=..., outputs=..., target_nodes=..., executors_and_keys=executors_and_keys@entry=0x7fffffffc4f0, run_state_args=run_state_args@entry=0x7fffffffc730) at tensorflow/core/common_runtime/direct_session.cc:1208
#19 0x000000000ad0d0f7 in tensorflow::DirectSession::Run (this=<optimized out>, run_options=..., inputs=std::vector of length 0, capacity 0, output_names=std::vector of length 1, capacity 1 = {...}, target_nodes=std::vector of length 0, capacity 0, outputs=0x0, run_metadata=0x0) at tensorflow/core/common_runtime/direct_session.cc:472
#20 0x000000000ad6fa95 in tensorflow::ClientSession::Run (this=this@entry=0x7fffffffdc30, run_options=..., inputs=std::unordered_map with 0 elements, fetch_outputs=std::vector of length 1, capacity 1 = {...}, run_outputs=std::vector of length 0, capacity 0, outputs=outputs@entry=0x0, run_metadata=run_metadata@entry=0x0) at tensorflow/cc/client/client_session.cc:127
#21 0x000000000ad74b6d in Run (outputs=0x0, run_outputs=std::vector of length 0, capacity 0, fetch_outputs=std::vector of length 1, capacity 1 = {...}, inputs=std::unordered_map with 0 elements, this=0x7fffffffdc30) at tensorflow/cc/client/client_session.cc:90
#22 tensorflow::ClientSession::Run (this=this@entry=0x7fffffffdc30, fetch_outputs=std::vector of length 1, capacity 1 = {...}, outputs=outputs@entry=0x0) at tensorflow/cc/client/client_session.cc:76
#23 0x000000000048042a in main (argc=1, argc@entry=2, argv=argv@entry=0x7fffffffe2e8) at tensorflow/examples/decode_image/main.cc:99
#24 0x00007ffff03c1f45 in __libc_start_main (main=0x47e4b0 <main(int, char**)>, argc=2, argv=0x7fffffffe2e8, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffe2d8) at libc-start.c:287
#25 0x0000000000737eca in _start ()

- **source code of c++ program**:
#include <fstream>
#include <utility>
#include <vector>

#include ""tensorflow/cc/ops/const_op.h""
#include ""tensorflow/cc/ops/image_ops.h""
#include ""tensorflow/cc/ops/standard_ops.h""
#include ""tensorflow/core/framework/graph.pb.h""
#include ""tensorflow/core/framework/tensor.h""
#include ""tensorflow/core/graph/default_device.h""
#include ""tensorflow/core/graph/graph_def_builder.h""
#include ""tensorflow/core/lib/core/errors.h""
#include ""tensorflow/core/lib/core/stringpiece.h""
#include ""tensorflow/core/lib/core/threadpool.h""
#include ""tensorflow/core/lib/io/path.h""
#include ""tensorflow/core/lib/strings/stringprintf.h""
#include ""tensorflow/core/platform/env.h""
#include ""tensorflow/core/platform/init_main.h""
#include ""tensorflow/core/platform/logging.h""
#include ""tensorflow/core/platform/types.h""
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/core/util/command_line_flags.h""
#include ""tensorflow/cc/client/client_session.h""

// These are all common classes it's handy to reference with no namespace.
using tensorflow::Flag;
using tensorflow::Tensor;
using tensorflow::Status;
using tensorflow::string;
using tensorflow::int32;

int main(int argc, char* argv[]) {
  string image = ""tensorflow/examples/label_image/data/grace_hopper.jpg"";
  std::vector<Flag> flag_list = {
      Flag(""image"", &image, ""image to be processed""),
  };
  string usage = tensorflow::Flags::Usage(argv[0], flag_list);
  const bool parse_result = tensorflow::Flags::Parse(&argc, argv, flag_list);
  if (!parse_result) {
    LOG(ERROR) << usage;
    return -1;
  }

  // We need to call this to set up global state for TensorFlow.
  tensorflow::port::InitMain(argv[0], &argc, &argv);
  if (argc > 1) {
    LOG(ERROR) << ""Unknown argument "" << argv[1] << ""\n"" << usage;
    return -1;
  }

        using namespace ::tensorflow::ops;

  auto root = tensorflow::Scope::NewRootScope();
  tensorflow::Output file_reader = ReadFile(root.WithOpName(""input_image""), image);
        tensorflow::Output gif_reader = DecodeGif(root.WithOpName(""gif_reader""), file_reader);
        tensorflow::Output bmp_reader = DecodeBmp(root.WithOpName(""bmp_reader""), file_reader);
        tensorflow::Output jpeg_reader = DecodeJpeg(root.WithOpName(""jpeg_reader""), file_reader);
        tensorflow::Output png_reader = DecodePng(root.WithOpName(""png_reader""), file_reader);

        std::vector<tensorflow::Tensor> outputs;
        tensorflow::ClientSession session(root);
  session.Run({gif_reader}, nullptr);
  session.Run({bmp_reader}, nullptr);
  session.Run({jpeg_reader}, nullptr);
  session.Run({png_reader}, nullptr);

  return 0;
}

- **source code of python program**:
import argparse
import tensorflow as tf

if __name__ == ""__main__"":
        file_name = ""tensorflow/examples/label_image/data/grace_hopper.jpg""
        parser = argparse.ArgumentParser()
        parser.add_argument(""--image"", help=""image to be processed"")
        args = parser.parse_args()
        if args.image:
            file_name = args.image
        file_reader = tf.read_file(file_name, ""file_reader"")
        image_reader = tf.image.decode_bmp(file_reader, name='bmp_reader')
        sess = tf.Session()
        sess.run(image_reader)

[evil.zip](https://github.com/tensorflow/tensorflow/files/1512398/evil.zip)

",0,,2,2017-11-29T03:00:53Z,2017-11-30T19:14:36Z,NONE,2017-11-29T08:46:53Z
14958,Make tf_upgrade.py dependency free,"awaiting testing (then merge),cla: yes,stat:awaiting response","Nothing else references the ast_edits, so it will make tf_upgrade.py much
easier to use if it's just absorbed. This change fixes #11217 where a whole
bunch of folks encountered difficulties for this very reason.",1,,8,2017-11-28T23:27:52Z,2017-12-28T23:39:14Z,MEMBER,2017-12-10T20:38:02Z
14957,tf.profiler reports 0B memory usage,stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0-dev20171103
- **Python version**: 3.5
- **Exact command to reproduce**:

``` python
import tensorflow as tf
import numpy as np

graph = tf.Graph()
with graph.as_default(), tf.device(""/cpu:0""):
    a = tf.constant(np.ones((1000, 1000)))
    b = tf.constant(np.ones((1000, 1000)))
    c = a * b

with tf.Session(graph=graph) as sess:
    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
    run_metadata = tf.RunMetadata()
    sess.run(c, options=run_options, run_metadata=run_metadata)

    options = tf.profiler.ProfileOptionBuilder.time_and_memory()
    options[""min_bytes""] = 0
    options[""select""] = (""bytes"", ""peak_bytes"", ""output_bytes"",
                         ""residual_bytes"")
    tf.profiler.profile(graph, run_meta=run_metadata, cmd=""scope"",
                        options=options)
```

### Describe the problem
The above script gives output
```
==================Model Analysis Report======================
node name | requested bytes | peak bytes | residual bytes | output bytes
_TFProfRoot (--/0B, --/0B, --/0B, --/8.00MB)
  mul (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)
  _retval_mul_0_0 (0B/0B, 0B/0B, 0B/0B, 0B/0B)
```
`requested bytes` and `peak bytes` are both reported as 0, whereas I would have thought they would be 8MB (based on the description of these measures [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/options.md)).  

I think the memory is not being recorded in the `RunMetadata` the way the profiler expects.  For example, if we look at 
``` python
    mul_stats = run_metadata.step_stats.dev_stats[0].node_stats[1]

    print(""total_bytes"", [x.total_bytes for x in mul_stats.memory])   # --> [0]
    print(""persistent_mem"", mul_stats.memory_stats.host_persistent_memory_size)  # --> 8000000
```
`""total_bytes""` is what the profiler reports as ""requested bytes"" above.  It seems like that data isn't being updated in the `run_metadata`.  The correct value is in `memory_stats.host_persistent_memory_size`, but that value isn't available in the profiler output.  And according to the [profiler proto](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/core/profiler/tfprof_log.proto) that value is supposed to represent the bytes allocated to persistent objects (like Variables), even though none of the values in the example are persistent.  So I'm not sure if this is an issue with `tf.profiler`, or with how memory information is stored in `RunMetadata`.",1,,9,2017-11-28T23:25:49Z,2017-12-22T19:04:56Z,CONTRIBUTOR,2017-11-29T02:38:40Z
14956,traceback error,,"Traceback (most recent call last):
  File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec cod in run_globals
  File ""/usr/lib/python2.7/py_compile.py"", line 181, in <module>
    sys.exit(main())
  File ""/usr/lib/python2.7/py_compile.py"", line 173, in main
    compile(filename, doraise=True)
  File ""/usr/lib/python2.7/py_compile.py"", line 106, in compile
    with open(file, 'U') as f:
IOError: [Errno 2] No such file or directory: ''
[Finished in 0.7s with exit code 1]
[shell_cmd: python -m py_compile """"]
[dir: /opt/sublime_text]
[path: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin]c",0,,1,2017-11-28T23:09:04Z,2017-11-29T17:06:45Z,NONE,2017-11-29T17:06:45Z
14955,add support for quantized ops on windows,"awaiting testing (then merge),cla: yes","Quantized ops support on windows. Added some simple python unit test to make sure it works.
Gotcha: I need to use the latest version of gemmlowp in gemmlowp.cmake which is not on mirror.bazel.build so I use github.com directly for now.
This should provide the same functionality as tf on linux. Performance could be better because msvc can't make use of the inline asm in gemmlowp (compiling gemmlowp with clang would show much better results).",1,,6,2017-11-28T21:45:23Z,2017-12-10T18:39:03Z,CONTRIBUTOR,2017-11-29T00:30:57Z
14954,enable get the 'mathematical' gradient of output w.r.t neural network parameters,"stat:awaiting response,type:support","In some situation, it is necessary to get the 'mathematical' gradient of output w.r.t neural network parameters. 

For example, suppose I have a neural network `out = f(s)`, where `s` is a batch of input with shape`[None, dim_s]`, while out is a scaler, `f` is simply a MLP. With `tf.gradient(out, tf.trainable_variables())` I can get gradient of out w.r.t neural network parameters of f, which is a list of gradient. Now, I have two different batch of `s`: `s1` and `s2`, then we can get two different the above gradients `G1` and `G2`. It seems that it is impossible to compute cosine between `G1` and `G2` using current tensorflow? Do I need to flatten both gradients first? Do `G1` and `G2` are the usual gradient in math? ",0,,8,2017-11-28T20:44:24Z,2017-11-29T17:16:54Z,NONE,2017-11-28T22:14:17Z
14953,Tensor roll op implementation,cla: yes,"Closes #10761
Added a tf.manip.roll op that works similarly to numpy's np.roll. This was a feature requested in #10761 and was marked as contributions welcome.

### Usage:

Rolls the elements of a tensor by the offsets of `shift` along the dimensions
of `axis`. Elements that roll passed the last position will wrap around
to the first.
For example:
```
# 't' is [0, 1, 2, 3, 4]
roll(t, shift=2, axis=0) ==> [3, 4, 0, 1, 2]
# shifting along multiple dimensions
# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
roll(t, shift=[1, -2], axis=[0, 1]) ==> [[7, 8, 9, 5, 6], [2, 3, 4, 0, 1]]
# shifting along the same axis multiple times
# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
roll(t, shift=[2, -3], axis=[1, 1]) ==> [[1, 2, 3, 4, 0], [6, 7, 8, 9, 5]]
```
shift: `shift[i]` specifies the number of places by which elements are shifted
  along the dimension specified by `axis[i]`. Negative shifts will roll the
  elements in the opposite direction.
axis: `axis[i]` specifies the dimension that the shift `shift[i]` should occur.
  if the same axis is referenced more than once, the total shift for that axis
  will be the sum of all the shifts that belong to that axis.
output: Has the same shape and size as the input. The elements are shifted by
  the offsets of `shift` along the dimensions of `axis`.

### Unit tests:
```
[==========] Running 24 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 24 tests from RollOpTest
[ RUN      ] RollOpTest.ScalarIndices
[       OK ] RollOpTest.ScalarIndices (5 ms)
[ RUN      ] RollOpTest.ScalarIndices_NoMemcpy
[       OK ] RollOpTest.ScalarIndices_NoMemcpy (0 ms)
[ RUN      ] RollOpTest.ScalarIndices_Complex
[       OK ] RollOpTest.ScalarIndices_Complex (0 ms)
[ RUN      ] RollOpTest.Simple_TwoD32
[       OK ] RollOpTest.Simple_TwoD32 (0 ms)
[ RUN      ] RollOpTest.Simple_TwoD32_NoMemcpy
[       OK ] RollOpTest.Simple_TwoD32_NoMemcpy (0 ms)
[ RUN      ] RollOpTest.Simple_ThreeD32
[       OK ] RollOpTest.Simple_ThreeD32 (1 ms)
[ RUN      ] RollOpTest.Simple_ThreeD32_NoMemcpy
[       OK ] RollOpTest.Simple_ThreeD32_NoMemcpy (0 ms)
[ RUN      ] RollOpTest.Simple_TwoD64
[       OK ] RollOpTest.Simple_TwoD64 (0 ms)
[ RUN      ] RollOpTest.Simple_TwoD64_NoMemcpy
[       OK ] RollOpTest.Simple_TwoD64_NoMemcpy (0 ms)
[ RUN      ] RollOpTest.Simple_ThreeD64
[       OK ] RollOpTest.Simple_ThreeD64 (0 ms)
[ RUN      ] RollOpTest.Simple_ThreeD64_NoMemcpy
[       OK ] RollOpTest.Simple_ThreeD64_NoMemcpy (0 ms)
[ RUN      ] RollOpTest.ZeroShift_ThreeD32
[       OK ] RollOpTest.ZeroShift_ThreeD32 (0 ms)
[ RUN      ] RollOpTest.ZeroShift_ThreeD32_NoMemcpy
[       OK ] RollOpTest.ZeroShift_ThreeD32_NoMemcpy (0 ms)
[ RUN      ] RollOpTest.ZeroSize_ThreeD32
[       OK ] RollOpTest.ZeroSize_ThreeD32 (0 ms)
[ RUN      ] RollOpTest.ZeroSize_ThreeD32_NoMemcpy
[       OK ] RollOpTest.ZeroSize_ThreeD32_NoMemcpy (0 ms)
[ RUN      ] RollOpTest.OneSize_ThreeD32
[       OK ] RollOpTest.OneSize_ThreeD32 (1 ms)
[ RUN      ] RollOpTest.OneSize_ThreeD32_NoMemcpy
[       OK ] RollOpTest.OneSize_ThreeD32_NoMemcpy (0 ms)
[ RUN      ] RollOpTest.DuplicateShifts_TwoD32
[       OK ] RollOpTest.DuplicateShifts_TwoD32 (0 ms)
[ RUN      ] RollOpTest.DuplicateShifts_TwoD32_NoMemcpy
[       OK ] RollOpTest.DuplicateShifts_TwoD32_NoMemcpy (0 ms)
[ RUN      ] RollOpTest.Error_InputMustBeVectorOrHigher
[       OK ] RollOpTest.Error_InputMustBeVectorOrHigher (0 ms)
[ RUN      ] RollOpTest.Error_AxisMustBeScalarOrVector
[       OK ] RollOpTest.Error_AxisMustBeScalarOrVector (0 ms)
[ RUN      ] RollOpTest.Error_ShiftMustBeScalarOrVector
[       OK ] RollOpTest.Error_ShiftMustBeScalarOrVector (0 ms)
[ RUN      ] RollOpTest.Error_ShiftAndAxisMustBeSameSize
[       OK ] RollOpTest.Error_ShiftAndAxisMustBeSameSize (0 ms)
[ RUN      ] RollOpTest.Error_AxisOutOfRange
[       OK ] RollOpTest.Error_AxisOutOfRange (0 ms)
[----------] 24 tests from RollOpTest (7 ms total)

[----------] Global test environment tear-down
[==========] 24 tests from 1 test case ran. (8 ms total)
[  PASSED  ] 24 tests.
```

```
//tensorflow/python/kernel_tests:manip_ops_test                          PASSED in 0.9s
```

### Benchmarks:
```
Running main() from test_main.cc
Benchmark                     Time(ns) Iterations
-------------------------------------------------
BM_cpu_roll_outer/256/256        46739      16065	 5608.7MB/s 1402.2M items/s
BM_cpu_roll_outer/512/512       124247       5135	 8439.5MB/s 2109.9M items/s
BM_cpu_roll_outer/1024/1024     638245       1149	 6571.6MB/s 1642.9M items/s
BM_cpu_roll_outer/2048/2048    5261260        100	 3188.8MB/s 797.2M items/s
BM_cpu_roll_all/256/256         135631       5353	 1932.8MB/s 483.2M items/s
BM_cpu_roll_all/512/512         286927       2263	 3654.5MB/s 913.6M items/s
BM_cpu_roll_all/1024/1024       981417        727	 4273.7MB/s 1068.4M items/s
BM_cpu_roll_all/2048/2048      4549074        149	 3688.1MB/s 922.0M items/s
```

I had made a pull request for this before but accidentally closed it",1,,7,2017-11-28T19:57:27Z,2018-01-30T00:03:32Z,CONTRIBUTOR,2017-12-23T03:33:22Z
14951,Document S3 `S3_REGION` constant,"stat:awaiting tensorflower,type:docs","This gave me a lot of grief recently as the AWS Region for S3 is controlled by an undocumented constant `S3_REGION` which defaults to `us-east-1` unless defined on the system-level. Extra frustrating as it departs from the common naming practice: `AWS_REGION`:

https://github.com/tensorflow/tensorflow/blob/79422ab39b5fe0e1491abb8deabc7ecb5fd9f3a2/tensorflow/core/platform/s3/s3_file_system.cc#L52

If somebody could suggest where to document this, I'd be happy to contribute. ",1,,4,2017-11-28T19:16:21Z,2017-12-10T20:14:32Z,CONTRIBUTOR,2017-11-28T23:06:22Z
14949,Add user friendly error checking on download_dependencies.sh,cla: yes,"- Both on tflite and tensorflow makefiles.
- Check it is run on root.",0,,1,2017-11-28T17:09:26Z,2017-11-30T19:15:19Z,MEMBER,2017-11-28T17:11:10Z
14945,FreeBSD compatibility,"awaiting testing (then merge),cla: yes","After 1.2.0 tensorflow broke on FreeBSD, provided changes makes it build again.

Also, when nsync has merged this PR:
https://github.com/google/nsync/pull/3

The nsync dependency needs to be bumped.",0,,6,2017-11-28T14:37:02Z,2017-12-19T22:00:49Z,CONTRIBUTOR,2017-12-13T10:37:18Z
14940,The keys in end_points of slim are not unified for different layers.,type:bug/performance,"Look at the code:

```python
  with tf.name_scope('xx'):
    end_points_collection = 'dd'
    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
                      outputs_collections=end_points_collection):
      y = slim.conv2d(np.zeros([1,20,20,3],dtype=np.float32), 10, [2, 2])
      x = slim.max_pool2d(np.zeros([1,20,20,3],dtype=np.float32), [2, 2])
      end_points = slim.utils.convert_collection_to_dict(end_points_collection)
  print(end_points)
```

It output
```
OrderedDict([('Conv', <tf.Tensor 'xx/Conv/Relu:0' shape=(1, 20, 20, 10) dtype=fl
oat32>), ('xx/MaxPool2D', <tf.Tensor 'xx/MaxPool2D/MaxPool:0' shape=(1, 10, 10,
3) dtype=float32>)])
```

For `max_pool2d` layer, the key has prefix `xx`, but for `conv2d` layer, it don't have prefix `xx`. Because in `conv2d` it uses `variable_scope` but in `max_pool2d` it uses `name_scope`. So the behavior looks inconsistent, and may cause the code make mistake. Do we need to unify the behavior ?


For example, if we use multi-gpu to train the network, and we have many clones of network, which name_scope's prefix  are `clone_1`,  `clone_2` and so on. If we want to use the key of `end_points` to get the output of layer on different gpu. We should deal with `max_pool2d` and `conv2d`  differently.",1,,7,2017-11-28T13:45:35Z,2018-01-24T09:04:05Z,NONE,2017-11-28T18:14:08Z
14935,How to scale the weights at runtime in tensorflow?,,"I have asked this questione in the stackoverflow.
https://stackoverflow.com/questions/47527384/how-to-scale-the-weights-at-runtime-in-tensorflow

Thanks for your help! ",0,,1,2017-11-28T12:22:27Z,2017-11-28T21:09:59Z,NONE,2017-11-28T21:09:59Z
14931,Missing gradient for tf.argmax,,LookupError: No gradient defined for operation 'Argmax' (op type: Argmax),0,,3,2017-11-28T07:42:36Z,2017-11-29T04:46:45Z,NONE,2017-11-28T20:59:24Z
14927,Fix a typo in comment,"awaiting testing (then merge),cla: yes",its cached -> it's cached,0,,3,2017-11-28T01:01:44Z,2017-12-10T20:41:38Z,CONTRIBUTOR,2017-11-28T02:52:55Z
14923,_LayerRNNCell __call__() method incompatible with tuple of tensors,stat:awaiting tensorflower,"I'm trying to build a custom LSTM cell that should accept a tuple of tensors in the call method. However, as part of the `dynamic_rnn` loop, `_LayerRNNCell`'s `__call__()` method requires that `inputs` be a `2-D` tensor with shape `[batch_size, input_size]`, which is incompatible with a tuple of tensors. Is there a way around this, or can the `__call__()1 method be expanded to be more flexible?

The error that I receive:
`ValueError: Layer action_conditioned_lstm_cell_1 expects 1 inputs, but it received 2 input tensors.`

Linux Ubuntu 16.04
TensorFlow versions: ('v1.3.0-rc1-5211-gab0fcac', '1.5.0-dev20171127')",0,,4,2017-11-27T20:59:18Z,2017-11-28T05:52:14Z,NONE,2017-11-28T03:10:43Z
14922,Branch 176732156,cla: yes,,0,,2,2017-11-27T20:06:44Z,2017-11-28T00:36:37Z,MEMBER,2017-11-27T23:19:38Z
14921,Possible sparse gradients bug in 1.4,,"
### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Slackware 14.2
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0
- **Python version**: 3.6

### Problem
I get the following warning when I run my code:
```
/usr/lib64/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
```
When I was using the exact same code with TF 1.3, I did not get the warning. I was wondering if this is a possible bug in 1.4 or simply the warning got added in the update?

I located the problem to this part of my code:

```
act_t_ph = tf.placeholder(tf.int32, [None])
# ...
with tf.variable_scope(""action_value""):
    x = tf.layers.dense(x, 512, activation=tf.nn.relu)
    x = tf.layers.dense(x, 6, activation=None)
z_net_locs = x
action_mask = tf.one_hot(act_t_ph, 6, on_value=True, off_value=False, dtype=tf.bool)
z_locations = tf.boolean_mask(z_net_locs, action_mask)
# ...
```
Later, gradients are computed with respect to the variables in the dense layers and these gradients are backpropagated through  `z_locations`. 

I also tried changing my code to:

```
self.act_t_ph = tf.placeholder(tf.int32, [None])
# ...
with tf.variable_scope(""action_value""):
    x = tf.layers.dense(x, 512, activation=tf.nn.relu)
    x = tf.layers.dense(x, 6, activation=None)
z_net_locs = x
bsize = tf.shape(self.act_t_ph)[0]
b_ind = tf.range(0, bsize, 1, tf.int32)
ind   = tf.concat([act_t_ph, b_ind], axis=-1)
z_locations = tf.gather_nd(z_net_locs, ind)
# ...
```
and I no longer get the warning in TF 1.4. As far as I can tell, one of the operations in the original code cannot handle sparse gradients. 
",0,,3,2017-11-27T19:41:18Z,2017-11-30T02:00:13Z,NONE,2017-11-28T03:09:35Z
14919,Branch 176791620,cla: yes,,0,,1,2017-11-27T19:31:23Z,2017-11-28T00:36:53Z,MEMBER,2017-11-27T22:10:27Z
14915,Very different CPU usage/allocation behavior when using slightly different CPUs,,"I am running the same exact model on two largely similar systems, let's call them system A and B. However, TF's behavior is very different. On system A, CPU utilization is around 60% (on a 12-core system), while on system B CPU utilization is only around 8%. Moreover, on system A the same model runs about 10x slower than system B, even though it's using far fewer CPU resources.

The systems are similar in that they're both running:

Ubuntu 14.04
TensorFlow 1.4.0 (compiled from source)
Python 2.7
gcc 4.8.4

What's different:

System A:
Bazel 0.6.0
2x E5-2643 v3

System B:
Bazel 0.7.0
2x E5-2643 v4

Why would they behave so differently?",0,,4,2017-11-27T16:57:43Z,2017-11-28T03:00:36Z,NONE,2017-11-28T03:00:36Z
14914,"""and"", ""or"", etc, should be overloaded if possible",type:feature,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS X 10.13.1
- **TensorFlow installed from (source or binary)**: Binary (anaconda)
- **TensorFlow version (use command below)**: 1.1.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**:  N/A
- **GPU model and memory**: Intel HD Graphics 630 1536 MB (not used)
- **Exact command to reproduce**: N/A

### Describe the problem
It would make code much cleaner if more of Python's binary boolean operations were overloaded for 
TF. I understand this can't be done for ""=="" because of hashmap key problems but ""and"", ""or"", ""^"", ""not"", really as many operations as possible, would be great.


### Source code / logs

Currently:
```
def cube_isect(x_m,x_M,y_m,y_M):
  return tf.logical_or(tf.logical_and(x_m >= y_m, x_m <= y_M), 
                                  tf.logical_and(y_m >= x_m, y_m <= x_M))
```
Proposed:
```
def cube_isect(x_m,x_M,y_m,y_M):
  return (x_m >= y_m and x_m <= y_M) or (y_m >= x_m and y_m <= x_M)
```
",1,,7,2017-11-27T16:49:40Z,2017-11-29T18:29:47Z,NONE,2017-11-28T19:55:13Z
14913,[BUG] argparse (Argument Parser) is not working in nightly build,"stat:awaiting tensorflower,type:bug/performance","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip install tf-nightly-gpu
- **TensorFlow version (use command below)**: v1.3.0-rc1-5211-gab0fcac 1.5.0-dev20171125
- **Python version**: 3.5.3
- **Bazel version (if compiling from source)**:n/a
- **GCC/Compiler version (if compiling from source)**:n/a
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**:Titan Xp (12Gb)
- **Exact command to reproduce**: python test.py --myarg buzz

### Describe the problem
Nightly build is not handing correctly arguments passed to the script. The arguments are parsed correctly in the official 1.4 version.

```
(nightly) daniyar@sleepy-prism:~/tmp/argsparse$ python test.py --myarg buzz
buzz
FATAL Flags parsing error: Unknown command line flag 'myarg'
Pass --helpshort or --helpfull to see help on flags.
```

### Source code
```
import numpy as np
import argparse
import tensorflow as tf

parser = argparse.ArgumentParser(description='argsparse test for tensorflow nightly build')
parser.add_argument('--myarg', type=str, help='fizz or buzz', default='fizz')
args = parser.parse_args()

def main(argv):
    print(args.myarg)

if __name__ == '__main__':
    main([]) # test before tf.app.run()
    tf.app.run()
```",1,,12,2017-11-27T16:32:09Z,2017-12-29T22:45:26Z,CONTRIBUTOR,2017-11-27T16:54:44Z
14912,golang: ~15x speedup for decodeTensor(),"awaiting testing (then merge),cla: yes","Make decodeTensor() faster by running binary.Read() for the whole slice in last dimension.

Similar to https://github.com/tensorflow/tensorflow/pull/14427

before:

$ go test -bench=.
goos: linux
goarch: amd64
pkg: github.com/tensorflow/tensorflow/tensorflow/go
BenchmarkNewTensor/[150528]-8                200           7459717 ns/op
BenchmarkDecodeTensor/[150528]-8             100          11205557 ns/op
PASS
ok      github.com/tensorflow/tensorflow/tensorflow/go  3.447s


after:

$ go test -bench=.
goos: linux
goarch: amd64
pkg: github.com/tensorflow/tensorflow/tensorflow/go
BenchmarkNewTensor/[150528]-8                200           7009254 ns/op
BenchmarkDecodeTensor/[150528]-8            2000            747224 ns/op
PASS
ok      github.com/tensorflow/tensorflow/tensorflow/go  3.793s
",0,,2,2017-11-27T16:12:41Z,2017-11-29T16:02:57Z,CONTRIBUTOR,2017-11-28T18:32:21Z
14911,add extra document to parameter:num_epochs,"awaiting testing (then merge),cla: yes",Add extra documents to DatasetDataProvider. I forget to call  `tf.local_variables_initializer` when I set num_epochs to 1. I trace source code to find out that I need to do that.  I think put some extra documentation will help others to avoid this mistake.,0,,2,2017-11-27T15:18:38Z,2017-12-10T21:04:48Z,CONTRIBUTOR,2017-12-10T20:42:50Z
14910,Replace sys.maxint with sys.maxsize in learning.py,"awaiting testing (then merge),cla: yes",use sys.maxsize because sys.maxint doesn't exist in Python 3,0,,8,2017-11-27T14:53:44Z,2017-12-28T00:00:45Z,CONTRIBUTOR,2017-11-27T14:55:30Z
14909,Crossentropy loss function with weights by sample and by category and outcome,,"In my loss function I would like to weight each sample differently and in each sample, each category should be weighted differently as well depending on the outcome. Meaning if in a cross entropy the one_hot is correctly specified, a different weight needs to be applied than when the output is incorrect. So I would need two weights per category. A tensor with rank 3. One dimension for the samples, a second dimension for the amount of classes, and a third dimension that differentiates between correct and incorrect match.

I have seen that with sparse_softmax_cross_entropy it is possible to pass in a weight, that serves as a coefficient for positive examples. This is a good start, but I would need to pass in a tensor instead, to treat each sample differently. weighted_cross_entropy_with_logits seems to work in a very similar way but doesn't offer that functionality.

Is this a feature that could be added?",0,,1,2017-11-27T11:31:51Z,2017-11-28T02:49:56Z,NONE,2017-11-28T02:49:56Z
14908,TensorBoard Modifications for Word2Vec Example,"awaiting testing (then merge),cla: yes",TensorBoard modifications are added to Word2Vec example for visualizing the loss graphic and embeddings with proper words in TensorBoard.,0,,11,2017-11-27T11:13:14Z,2018-01-20T23:39:55Z,CONTRIBUTOR,2017-11-27T11:36:08Z
14906,Functionality Available?: Dataset Input perform slicing along time axis,stat:awaiting response,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7
- **TensorFlow installed from (source or binary)**: from pip
- **TensorFlow version (use command below)**: 1.3 
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0, 6.1
- **GPU model and memory**: k2200
- **Exact command to reproduce**:

### Describe the problem

Currently, as many suggested, when we want to train LSTM to predict the values of next time step, we would better slice all the samples along time axis to tuples of (lookback numbers of values, target predicted values) as (x, y), then store all these in a variable in RAM or as data file in disk. Then we build dataset to point to either form. However, this method makes LSTM stateless. Currently, our scheme to stateful LSTM is as follows:
for an input signal [1, 2, 3, 4, 5, 6, ..., 9, 10, 11]
we initialize LSTM and set state to 0.
feed ([1, 2, 3], 4) to train (`tf.nn.dynamic_rnn`), then feed ([2, 3, 4], 5) ....... ([8, 9, 10], 11), until end of this sequence.
In this batch, we have a number (Batch size, like 32) of signals like this and they will be processed in parallel in GPU by TF.
In the next batch of new signal samples, we reset LSTM with initial state being 0. And then repeat this process.

In this way, we think that given [8, 9, 10] to the model to predict next value (as 11), it is helpful for the model to choose whether to utilize the information of its current state (whether it is at beginning of a series of signal, zero initial; or it is at middle of a signal sequence).

Currently, before version 1.4, we built two generators, one (batch generator) is to generate a batch of signals. The other (time slicer) is to generate (x, y) [shape of (batch size, max time step, number of features) ] along time axis by using the data yield by the batch generator.

In version 1.4, we find Dataset, Estimator and Experiment pipeline powerful. Is there a way to implement the same idea using such pipeline?
Thanks!
",0,,4,2017-11-27T08:49:04Z,2017-11-28T02:48:36Z,NONE,2017-11-27T17:15:35Z
14905,Running label_wav.py from Simple Audio Recognition on Windows 7/64 is generating errors: CRITICAL:tensorflow:Audio file does not exist CRITICAL:tensorflow:Labels file does not exist CRITICAL:tensorflow:Graph file does not exist,stat:awaiting response,"Windows 7/64, GPU Nvidia M2000M, Python 3.5.4, tensorflow 1.5.0-dev20171120.

I was following the Simple Audio Tutorial. After retraining and freezing the model I was trying to run the script label_wav.py. The script produces an error: CRITICAL:tensorflow:Audio file does not exist CRITICAL:tensorflow:Labels file does not exist CRITICAL:tensorflow:Graph file does not exist:

C:\Users\bbb738>python tensorflow/tensorflow/examples/speech_commands/label_wav.py \--graph=/tmp/my_
frozen_graph.pb \--labels=/tmp/speech_commands_train/conv_labels.txt \--wav=/tmp/speech_dataset/left
/a5d485dc_nohash_0.wav
CRITICAL:tensorflow:Audio file does not exist
CRITICAL:tensorflow:Labels file does not exist
CRITICAL:tensorflow:Graph file does not exist
Traceback (most recent call last):
  File ""tensorflow/tensorflow/examples/speech_commands/label_wav.py"", line 135, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""C:\Users\bbb738\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\p
latform\app.py"", line 129, in run
    _sys.exit(main(argv))
  File ""tensorflow/tensorflow/examples/speech_commands/label_wav.py"", line 107, in main
    FLAGS.output_name, FLAGS.how_many_labels)
  File ""tensorflow/tensorflow/examples/speech_commands/label_wav.py"", line 93, in label_wav
    labels_list = load_labels(labels)
  File ""tensorflow/tensorflow/examples/speech_commands/label_wav.py"", line 58, in load_labels
    return [line.rstrip() for line in tf.gfile.GFile(filename)]
  File ""tensorflow/tensorflow/examples/speech_commands/label_wav.py"", line 58, in <listcomp>
    return [line.rstrip() for line in tf.gfile.GFile(filename)]
  File ""C:\Users\bbb738\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\l
ib\io\file_io.py"", line 214, in __next__
    return self.next()
  File ""C:\Users\bbb738\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\l
ib\io\file_io.py"", line 208, in next
    retval = self.readline()
  File ""C:\Users\bbb738\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\l
ib\io\file_io.py"", line 177, in readline
    self._preread_check()
  File ""C:\Users\bbb738\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\l
ib\io\file_io.py"", line 79, in _preread_check
    compat.as_bytes(self.__name), 1024 * 512, status)
  File ""C:\Users\bbb738\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\f
ramework\errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: NewRandomAccessFile failed to Create/Open:  :
 The system cannot find the path specified.
; No such process",0,,12,2017-11-27T08:43:50Z,2017-11-28T15:43:41Z,NONE,2017-11-27T16:11:31Z
14904,Fix typos,"awaiting testing (then merge),cla: yes","memory_intialized -> memory_initialized
set_memory_intialized -> set_memory_initialized

instrucion -> instruction

elment -> element

tensroflow -> tensorflow

interpretted -> interpreted",0,,7,2017-11-27T08:41:27Z,2017-12-10T20:15:54Z,CONTRIBUTOR,2017-11-30T19:23:20Z
14903,Non-deterministic result using  a pre-trained word embedding  in TensorFlow,,"I use pre-trained word embedding to initialize embedding in tensorflow, but got a non-deterministic result. However, if I fix the seed and use random initialization, the result is almost the same.What's the problem here? Thanks.
The code is here:

```
with tf.variable_scope(""embedding""):
    if init_embedding is None:
        self.embedding = tf.get_variable(name='embedding', shape=[vocab_size, word_dim],
                                              dtype=np.float32)
    else:
        self.embedding = tf.get_variable(name=""embedding"", shape=init_embedding.shape,
                                         initializer=tf.constant_initializer(init_embedding), `trainable=True)`
```

init_embedding is the pre-trained word embedding
",0,,1,2017-11-27T07:52:04Z,2017-11-28T02:46:48Z,NONE,2017-11-28T02:46:48Z
14902,tensorflow for python 3.6 will cause Jupyter notebook not executing properly,stat:awaiting tensorflower,"Details refer to this issue I originally posted. -->https://github.com/jupyter/notebook/issues/3084
",1,,6,2017-11-27T06:42:36Z,2018-01-18T22:58:51Z,NONE,2017-11-29T18:40:20Z
14899,Update tf_core_framework.cmake,"awaiting testing (then merge),cla: yes","To resolve this [issue](https://github.com/tensorflow/tensorflow/issues/14896)

Replace gpu_tracer.cc with device_tracer.cc",0,,5,2017-11-27T04:03:49Z,2017-12-10T20:40:38Z,CONTRIBUTOR,2017-11-27T04:07:22Z
14897,A bug in tensorflow r1.4 when applying  MultiRNNCell,,"
### System information
- **TensorFlow installed from source :
- **TensorFlow version: r1.4
- **Python version: 3.5.4
- **Bazel version: 0.5.4
- **GCC/Compiler version 5.4.0
- **CUDA/cuDNN version: 9.0 &5.0
- **GPU model and memory*: GeForce GTX 1080

### Describe the problem
when applying the MultiRNNCell as below, an error occurs. The code went well in tensorflow r1.3
# Source code
input_list is a list of tensor with shape[None, 8]
n_hidden = 32
lstm = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)
stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm]*2)
outputs, states = tf.nn.static_rnn(stacked_lstm, input_list, dtype=tf.float32)
# error
ValueError: Dimensions must be equal, but are 64 and 40 for 'rnn/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [?,64], [40,128].

However when only applying one single lstm, i works well.

opinions:
when calculating, the basiclstmcell will be called, where a class named Linear will be initialized, as an example, in my case, the variable self.weight in this class will be initialized as [40,128]([32+8,32*4]). 
*** code from rnn_cell_impl.py***
if self._linear is None:
    self._linear = _Linear([inputs, h], 4 * self._num_units, True)

But, when MultiRNNCell is the case, for example,  a 2 layers lstm. in the second layer, the weight should be [64,128]('h' in last layer (32)+'o' in  last layer(32)). Disappointingly, the weight will only be initialized once and stay with the shape [40,128] due to the sentence ""if self._linear is None:"". So that the reason why such error occurs.

i try to comment out this sentence, but since share variable mechanism is related. it dosen't work, and induces other problem.

ValueError: Trying to share variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel, but specified shape (64, 128) and found shape (40, 128).

Any idea how to solve this problem efficiently?



",0,,6,2017-11-27T03:16:15Z,2017-11-28T02:45:31Z,NONE,2017-11-27T03:30:39Z
14896,[bug report] CMakeList config error,type:build/install,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
When using cmake config to build tensorflow under windows, after enabling gpu, following error appears:

```
CMake Error at tf_core_framework.cmake:215 (list):
  list sub-command REMOVE_ITEM requires two or more arguments.
Call Stack (most recent call first):
  CMakeLists.txt:385 (include)
```
after checking tf_core_framwork.cmake, it is requesting to remove ""${tensorflow_source_dir}/tensorflow/core/platform/default/gpu_tracer.cc"" from core resources. However, this file is missing from the latest tensorflow. Commenting this line help to finish cmake config but I don't think this is a good practice.

Would the development team consider to update the cmake file?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,3,2017-11-27T02:42:54Z,2017-11-28T17:53:59Z,CONTRIBUTOR,2017-11-27T03:35:18Z
14895,Add `tf.unravel_index` as an equivalent of `np.unravel_index`,"awaiting testing (then merge),cla: yes","This fix tries to address the issue raised in #2075 where there was no implementation of  `tf.unravel_index`.

The `tf.unravel_index` could be quite useful in many places.

This fix adds the `tf.unravel_index` in CPU kernel. Note `order` in `np.unravel_index` has not been added yet.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,7,2017-11-27T01:14:24Z,2018-01-29T17:58:27Z,MEMBER,2017-12-01T17:31:55Z
14894,"'No gradients provided for any variable, check your graph for ops that do not support gradients'",type:bug/performance,"I write the code like following
```
import tensorflow as tf
input1=tf.Variable([1.0,2.0,3.0,4.0,5.0,6.0],name='input1')
input2=tf.Variable([2.0,3.0,4.0,6.0,8.0,9.0],name='input2')
values_range = tf.constant([0., 10.], dtype = tf.float32)
source_hist = tf.histogram_fixed_width(tf.to_float(input1), values_range, 11)
template_hist = tf.histogram_fixed_width(tf.to_float(input2), values_range, 11)
source_hist=tf.cast(source_hist,tf.float32)
template_hist=tf.cast(template_hist,tf.float32)
loss=2*tf.nn.l2_loss(source_hist-template_hist)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    writer=tf.summary.FileWriter('./',sess.graph)
    train_step=tf.train.AdamOptimizer(0.001).minimize(loss)
    for i in range(0,10000,1):
        sess.run(train_step)
        print('input1_value',input1.eval())
        print('input2_value',input2.eval())
    writer.close()
```
Tensorflow throws an error and shows 
''ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [""<tf.Variable 'input1:0' shape=(6,) dtype=float32_ref>"", ""<tf.Variable 'input2:0' shape=(6,) dtype=float32_ref>""] and loss Tensor(""mul:0"", shape=(), dtype=float32).''

  I know the op  tf.histogram_fixed_width doesn't support backpropagation and is not differentiable. While 
the op tf.floor has the same attribute as  tf.histogram_fixed_width. And the code below can run  without any error which surprises me a lot.

```
import tensorflow as tf
cst=tf.constant([1.2,1.4,2.8,4.6,6.8], dtype=tf.float32)
input=tf.Variable(cst)
new=tf.floor(input)
loss=2*tf.nn.l2_loss(input-new)
train_step=tf.train.AdamOptimizer(0.001).minimize(loss)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(0,10000,1):
        sess.run(train_step)
        print('input_value',input.eval())
        print('new_value',new.eval())
```
I could not figure it out for days, please help",0,,4,2017-11-27T01:06:56Z,2017-11-29T18:49:20Z,NONE,2017-11-28T17:51:37Z
14893,"[XLA] FIX XLA/tfcompile on OSX. #if Guard AVX, SSE and NEON instructions","awaiting testing (then merge),cla: yes","This fixes XLA / tfcompile on OSX. 

On OSX you currently run into linker errors because unsupported
instructions are registered. Add ifdefs to register only the
supported instructions.

Also include PR#14137 changes for missing __sincos/__sincosf in
XLA on macOS since it was closed without a merge. 

TEST=Build tensorflow/compiler/aot/tests:tfcompile builds
successfully on OSX (10.13.2)",0,,10,2017-11-27T00:06:18Z,2017-12-20T02:17:23Z,CONTRIBUTOR,2017-12-05T20:56:38Z
14892,R1.4,cla: no,,0,,3,2017-11-26T23:41:45Z,2017-12-20T00:39:16Z,NONE,2017-12-20T00:39:16Z
14891,`panic: runtime error: cgo argument has Go pointer to Go pointer` when using FIFOQueueV2 with non scalar shapes,stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: NA, using go bindings
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: NA, using CPU
- **Exact command to reproduce**:

### Describe the problem
When using `op.FIFOQueueV2()` from the go bindings and passing it only scalar shapes in `op.FIFOQueueV2Shapes`, the OP works as expected. However when using multi dimensional shapes, it panics with `panic: runtime error: cgo argument has Go pointer to Go pointer`.

### Source code / logs
For a working example with scalar shapes, replace the `dataShapes` and `data` lines with the commented versions below them.
```
package main

import (
	""fmt""

	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func main() {
	s := op.NewScope()
	dataType := []tf.DataType{tf.Int32}

	dataShapes := []tf.Shape{tf.MakeShape(2)} // Panics
	//dataShapes := []tf.Shape{tf.ScalarShape()} // Works

	data := op.Const(s, []int32{3, 4}) // Panics
	//data := op.Const(s, int32(3)) // Works

	queue := op.FIFOQueueV2(s, dataType, op.FIFOQueueV2Shapes(dataShapes))
	enqueue := op.QueueEnqueueV2(s, queue, []tf.Output{data})
	components := op.QueueDequeueV2(s, queue, dataType)
	graph, err := s.Finalize()
	if err != nil {
		panic(err)
	}
	sess, err := tf.NewSession(graph, nil)
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{enqueue})
	if err != nil {
		panic(err)
	}
	results, err := sess.Run(nil, components, nil)
	if err != nil {
		panic(err)
	}
	fmt.Println(results[0].Value())
}
```
```
[isaac@d6-arch tfes]$ go run queue_shape_error.go 
2017-11-26 14:51:13.523481: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
panic: runtime error: cgo argument has Go pointer to Go pointer

goroutine 1 [running]:
github.com/tensorflow/tensorflow/tensorflow/go.setAttr.func21(0xc42000e040, 0x1, 0x1, 0xe4a9c0, 0xc714a0, 0xc42000e040, 0xc42001614c, 0x1)
	/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:308 +0x100
github.com/tensorflow/tensorflow/tensorflow/go.setAttr(0xe4a9c0, 0xc42000e038, 0x4dc10a, 0x6, 0x4b6e00, 0xc42000c100, 0x0, 0x0)
	/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:309 +0x9b0
github.com/tensorflow/tensorflow/tensorflow/go.(*Graph).AddOperation(0xc42000e028, 0x4dcaa4, 0xb, 0x4dcaa4, 0xb, 0x0, 0x0, 0x0, 0xc42007c1e0, 0xc42008e1b8, ...)
	/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:176 +0x4a0
github.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).AddOperation(0xc42007c180, 0x4dcaa4, 0xb, 0x4dcaa4, 0xb, 0x0, 0x0, 0x0, 0xc42007c1e0, 0x7f05e202e000)
	/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:83 +0xa0
github.com/tensorflow/tensorflow/tensorflow/go/op.FIFOQueueV2(0xc42007c180, 0xc4200160e8, 0x1, 0x1, 0xc420057f40, 0x1, 0x1, 0x0, 0x7f05e20322f8)
	/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/wrappers.go:5136 +0x1ea
main.main()
	/home/isaac/go/src/github.com/is8ac/tfes/queue_shape_error.go:20 +0x238
exit status 2
```",1,,2,2017-11-26T23:18:56Z,2017-12-02T09:39:46Z,NONE,2017-11-28T02:44:05Z
14890,Tensorflow installation for python version 3.6,"stat:community support,type:build/install","I am trying to install tensorflow framework on my Windows 10 system and the framework is showing me the similar error of 
![image](https://user-images.githubusercontent.com/31855851/33243068-c272b944-d2ac-11e7-97fc-327b8b18bafa.png)
I have checked my system architecture which is 32 bit. I have tried it installing using the conda - 3.5 python version fix but then again same error arises. Please help me resolve this issue as I am on a fix for last few days.",0,,5,2017-11-26T18:23:25Z,2017-12-02T17:52:03Z,NONE,2017-11-28T17:46:14Z
14889,Add collection parameter into built-in runners,"awaiting review,cla: yes","To use built-in queue runners in fine-grained manner within a single graph,
each thread shouled be managed in different collection, not default collection.

Threads in different user-defined collections can be started and stopped,
	separately.

Signed-off-by: Taeksang Kim <voidbag@gmail.com>",0,,5,2017-11-26T17:37:01Z,2017-11-30T20:28:06Z,NONE,2017-11-30T20:28:06Z
14888,Is it possible to extend normal operators like add/minus with convolution-like operating?,"stat:awaiting response,type:feature","This is a feature request, and so far I haven't got any solution from tensorflow source code or websites like stackoverflow. 
It may be confusing to state the problem like my title, so, I am going to give it a demo:
1. The input matrix A has shape of [5,5], and the operation matrix B has shape of [3,3];
2. In convolution manner, tf.nn.conv2d(A, B, padding='VALID') will compute like this:
     create a sliding window C with the same size of the filter matrix B on matrix A, and apply 
     computation DOT(C, B) for all possible position of C on A. All obtained product of B and C 
     form the matrix of convolution result,
3. Here, if we enables the users to replace the DOT(C, B) with other element-wise operators 
   like ADD(C, B) or user defined ones, it will enable tons of more creative layer designs to 
   explore the power of AI. 
   A more flexiable interface will help users to avoid building his own operators by hacking the ops lib.
Sorry for interruption. If this request get passed, I hope I can help implementing it.",0,,1,2017-11-26T13:53:35Z,2017-11-28T17:24:20Z,NONE,2017-11-28T17:24:05Z
14887,feature request - decode_compressed,type:feature,"It will be great if you could add **tf.decode_compressed** to be used in **situations that tfrecords cannot be used**.

Currently, only **tf.decode_raw()** can be used in such situations, which becomes a big issue with massive amount/size of files.



",1,,12,2017-11-26T13:31:12Z,2017-12-20T00:00:50Z,NONE,2017-11-28T02:42:40Z
14885,tensorflow-terminate called after throwing an instance of 'std::system_error'   ,,"Hi
I am training a resNet50 with tensorflow, using a shared server with these properties:

ubuntu 16.04
3 gtx 1080 gpus
tensorflow 1.3
python 2.7
CUDA 8.0.4
CUDNN 6
but always after two epochs, and during the third epoch, I encounter this error:
`terminate called after throwing an instance of 'std::system_error' what():
Resource temporarily unavailable
Aborted (core dumped)`
with adding some print in my code, I have found where is the problem:
this is convert tfrecord to dataset:
`filenames = [""balanced_t.tfrecords""]
dataset = tf.contrib.data.TFRecordDataset(filenames)
    def parser(record):
    keys_to_features = {
        # ""label"": tf.FixedLenFeature((), tf.string, default_value=""""),
        ""mhot_label_raw"": tf.FixedLenFeature((), tf.string, default_value=""""),
        ""mel_spec_raw"": tf.FixedLenFeature((), tf.string, default_value=""""),
    }
    parsed = tf.parse_single_example(record, keys_to_features)

    mel_spec1d = tf.decode_raw(parsed['mel_spec_raw'], tf.float64)
    # label = tf.cast(parsed[""label""], tf.string)
    mhot_label = tf.decode_raw(parsed['mhot_label_raw'], tf.float64)
    mel_spec = tf.reshape(mel_spec1d, [96, 64])
    # aa=mel_spec
    return {""mel_data"": mel_spec}, mhot_label
    dataset = dataset.map(parser)
    dataset = dataset.batch(batch_size)
    dataset = dataset.repeat(3)
    iterator = dataset.make_one_shot_iterator()`
and this is my input pipline:
`while True:
            try:
               (features, labels) = sess.run(iterator.get_next())
            except tf.errors.OutOfRangeError:
               print(""end of training dataset"")
`
due to my prints output,the error is for this line:
`(features, labels) = sess.run(iterator.get_next())`
but I dont see any problem,can you help me now?

reproduce:replace any tfrecord with mine

",0,,2,2017-11-26T04:58:59Z,2017-11-27T16:10:44Z,NONE,2017-11-27T16:10:44Z
14884,Object detection works on Linux but not Mac,,"------------------------

### System information
- **OS Platform and Distribution **: Linux 16.04 and Mac OS 10.12.6
- **TensorFlow installed from (source or binary)**: Mac  Binary, Linux  Source
- **TensorFlow version (use command below)**: Mac ('v1.3.0-rc2-20-g0787eee', '1.3.0')
Linux  ('v1.3.0-rc1-4003-g1f582aa', '1.4.0-rc0')
- **Python version**: Mac  2.7.14, Linux  2.7.12
- **Bazel version (if compiling from source)**: Linux  0.7.0
- **GCC/Compiler version (if compiling from source)**: Linux GCC 5.4.0
- **CUDA/cuDNN version**: Linux CUDA 9, cuDNN 7
- **GPU model and memory**: Linux TitanXp

### Describe the problem

I trained a model using the tensor flow object detection api with faster_rcnn_resnet101. I then exported the model using the provided export_inference_graph.py. The model works on Linux, but does not work on Mac. Both platforms are using tensor flow 1.3.0. I've provided the crash log.

### Source code / logs
2017-11-25 20:39:12.847344: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'T' not in Op<name=Where; signature=input:bool -> index:int64>; NodeDef: ClipToWindow/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/cpu:0""](ClipToWindow/Greater). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: ClipToWindow/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/cpu:0""](ClipToWindow/Greater)]]
Traceback (most recent call last):
  File ""/Documents/detect.py"", line 13, in <module>
    model.detect(image)
  File ""/Documents/object_detector.py"", line 71, in detect
    feed_dict={self.image_tensor: image_np_expanded})
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'T' not in Op<name=Where; signature=input:bool -> index:int64>; NodeDef: ClipToWindow/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/cpu:0""](ClipToWindow/Greater). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: ClipToWindow/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/cpu:0""](ClipToWindow/Greater)]]

Caused by op u'ClipToWindow/Where', defined at:
  File ""/Documents/detect.py"", line 7, in <module>
    limbs = det.object_detector(""/Documents/graph.pbtxt"",""/Documents/graph.pb"", 2)
  File ""/Documents/object_detector.py"", line 49, in __init__
    tf.import_graph_def(od_graph_def, name='')
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 313, in import_graph_def
    op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): NodeDef mentions attr 'T' not in Op<name=Where; signature=input:bool -> index:int64>; NodeDef: ClipToWindow/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/cpu:0""](ClipToWindow/Greater). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: ClipToWindow/Where = Where[T=DT_BOOL, _device=""/job:localhost/replica:0/task:0/cpu:0""](ClipToWindow/Greater)]]
",1,,11,2017-11-26T01:47:11Z,2018-01-06T19:53:23Z,NONE,2017-11-28T22:22:00Z
14882,What if we don't install tensorflow under a new environment?,,"How come we need to install tensorflow as a [separate](https://user-images.githubusercontent.com/33768560/33234120-05bba17a-d1ef-11e7-9f8a-0390d005aa6a.png) environment?

If we do it this way, many common libraries are not available when tensorflow is activated. 
![image](https://i.stack.imgur.com/zOslW.png)

Most of the common libraries such as matplotlib, panda, etc. are not within tensorflow environment. So we have to install again to use them.
![image](https://user-images.githubusercontent.com/33768560/33233620-c2242610-d1e6-11e7-9a23-44a20991daff.png)

So why not just install under root so we don't have to re-install all those libraries under the new environment?

Thanks.

",0,,4,2017-11-25T20:14:15Z,2017-11-26T05:28:15Z,NONE,2017-11-26T00:42:58Z
14878,Some PATH typo : no `train_dir` in tutorial,"awaiting testing (then merge),cla: yes","This file uses `train_dir`. But in code file, there is no `train_dir`  anymore, it should be replaced with `log_dir` `input_data_dir` and `checkpoint_file` respectively",0,,4,2017-11-25T14:15:10Z,2017-12-10T20:17:01Z,CONTRIBUTOR,2017-11-25T14:19:10Z
14877,[CMake] Extract list of python modules,cla: yes,"Progressing #10296
@drpngx FYI",1,,22,2017-11-25T14:06:50Z,2017-12-03T14:30:15Z,CONTRIBUTOR,2017-11-27T17:39:36Z
14875,tf.data cannot be loaded with r1.4,"stat:awaiting tensorflower,type:support","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 8 (jessie)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: b'v1.4.0-14-gb5df90f' 1.4.1
- **Python version**: Python 3.6.3 |Anaconda, Inc.| (default, Nov 20 2017, 20:41:42)
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: gcc (Debian 4.9.2-10) 4.9.2
- **CUDA/cuDNN version**: CUDA 8, cnDNN 5.1
- **GPU model and memory**: TITAN X (Pascal), 12G
- **Exact command to reproduce**: `from tensorflow.data import TFRecordDataset`

### Describe the problem
After checking out r1.4 and compiling TF, tf.data cannot be loaded. Training works fine. But 'data' is listed when executing `print(dir(tf))`.

### Source code / logs
(tensorflow140) [13:08 user@server ~] > `python`
Python 3.6.3 |Anaconda, Inc.| (default, Nov 20 2017, 20:41:42)
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
\>\>\> `import tensorflow as tf`
\>\>\> `tf.__version__`
'1.4.1'
\>\>\> `from tensorflow.data import TFRecordDataset`
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow.data'
\>\>\> `print(dir(tf))`
['AUTO_REUSE', 'AggregationMethod', 'Assert', 'AttrValue', 'COMPILER_VERSION', 'ConditionalAccumulator', 'ConditionalAccumulatorBase', 'ConfigProto', 'DType', 'DeviceSpec', 'Dimension', 'Event', 'FIFOQueue', 'FixedLenFeature', 'FixedLenSequenceFeature', 'FixedLengthRecordReader', 'GIT_VERSION', 'GPUOptions', 'GRAPH_DEF_VERSION', 'GRAPH_DEF_VERSION_MIN_CONSUMER', 'GRAPH_DEF_VERSION_MIN_PRODUCER', 'Graph', 'GraphDef', 'GraphKeys', 'GraphOptions', 'HistogramProto', 'IdentityReader', 'IndexedSlices', 'InteractiveSession', 'LMDBReader', 'LogMessage', 'MetaGraphDef', 'NameAttrList', 'NoGradient', 'NodeDef', 'NotDifferentiable', 'OpError', 'Operation', 'OptimizerOptions', 'PaddingFIFOQueue', 'Print', 'PriorityQueue', 'QUANTIZED_DTYPES', 'QueueBase', 'RandomShuffleQueue', 'ReaderBase', 'RegisterGradient', 'RunMetadata', 'RunOptions', 'Session', 'SessionLog', 'SparseConditionalAccumulator', 'SparseFeature', 'SparseTensor', 'SparseTensorValue', 'Summary', 'SummaryMetadata', 'TFRecordReader', 'Tensor', 'TensorArray', 'TensorInfo', 'TensorShape', 'TextLineReader', 'VERSION', 'VarLenFeature', 'Variable', 'VariableScope', 'WholeFileReader', '\_\_builtins\_\_', '\_\_cached\_\_', '\_\_compiler_version\_\_', '\_\_doc\_\_', '\_\_file\_\_', '\_\_git_version\_\_', '\_\_loader\_\_', '\_\_name\_\_', '\_\_package\_\_', '\_\_path\_\_', '\_\_spec\_\_', '\_\_version\_\_', 'abs', 'accumulate_n', 'acos', 'acosh', 'add', 'add_check_numerics_ops', 'add_n', 'add_to_collection', 'all_variables', 'angle', 'app', 'arg_max', 'arg_min', 'argmax', 'argmin', 'as_dtype', 'as_string', 'asin', 'asinh', 'assert_equal', 'assert_greater', 'assert_greater_equal', 'assert_integer', 'assert_less', 'assert_less_equal', 'assert_negative', 'assert_non_negative', 'assert_non_positive', 'assert_none_equal', 'assert_positive', 'assert_proper_iterable', 'assert_rank', 'assert_rank_at_least', 'assert_rank_in', 'assert_same_float_dtype', 'assert_scalar', 'assert_type', 'assert_variables_initialized', 'assign', 'assign_add', 'assign_sub', 'atan', 'atan2', 'atanh', 'batch_to_space', 'batch_to_space_nd', 'betainc', 'bfloat16', 'bincount', 'bitcast', 'bitwise', 'bool', 'boolean_mask', 'broadcast_dynamic_shape', 'broadcast_static_shape', 'case', 'cast', 'ceil', 'check_numerics', 'cholesky', 'cholesky_solve', 'clip_by_average_norm', 'clip_by_global_norm', 'clip_by_norm', 'clip_by_value', 'colocate_with', 'compat', 'complex', 'complex128', 'complex64', 'concat', 'cond', 'confusion_matrix', 'conj', 'constant', 'constant_initializer', 'container', 'contrib', 'control_dependencies', 'convert_to_tensor', 'convert_to_tensor_or_indexed_slices', 'convert_to_tensor_or_sparse_tensor', 'cos', 'cosh', 'count_nonzero', 'count_up_to', 'create_partitioned_variables', 'cross', 'cumprod', 'cumsum', '**data**', 'decode_base64', 'decode_csv', 'decode_json_example', 'decode_raw', 'delete_session_tensor', 'depth_to_space', 'dequantize', 'deserialize_many_sparse', 'device', 'diag', 'diag_part', 'digamma', 'distributions', 'div', 'divide', 'double', 'dynamic_partition', 'dynamic_stitch', 'edit_distance', 'einsum', 'encode_base64', 'equal', 'erf', 'erfc', 'errors', 'estimator', 'exp', 'expand_dims', 'expm1', 'extract_image_patches', 'eye', 'fake_quant_with_min_max_args', 'fake_quant_with_min_max_args_gradient', 'fake_quant_with_min_max_vars', 'fake_quant_with_min_max_vars_gradient', 'fake_quant_with_min_max_vars_per_channel', 'fake_quant_with_min_max_vars_per_channel_gradient', 'feature_column', 'fft', 'fft2d', 'fft3d', 'fill', 'fixed_size_partitioner', 'flags', 'float16', 'float32', 'float64', 'floor', 'floor_div', 'floordiv', 'floormod', 'foldl', 'foldr', 'gather', 'gather_nd', 'get_collection', 'get_collection_ref', 'get_default_graph', 'get_default_session', 'get_local_variable', 'get_seed', 'get_session_handle', 'get_session_tensor', 'get_variable', 'get_variable_scope', 'gfile', 'global_norm', 'global_variables', 'global_variables_initializer', 'glorot_normal_initializer', 'glorot_uniform_initializer', 'gradients', 'graph_util', 'greater', 'greater_equal', 'group', 'half', 'hessians', 'histogram_fixed_width', 'identity', 'identity_n', 'ifft', 'ifft2d', 'ifft3d', 'igamma', 'igammac', 'imag', 'image', 'import_graph_def', 'initialize_all_tables', 'initialize_all_variables', 'initialize_local_variables', 'initialize_variables', 'initializers', 'int16', 'int32', 'int64', 'int8', 'invert_permutation', 'is_finite', 'is_inf', 'is_nan', 'is_non_decreasing', 'is_numeric_tensor', 'is_strictly_increasing', 'is_variable_initialized', 'keras', 'layers', 'lbeta', 'less', 'less_equal', 'lgamma', 'lin_space', 'linalg', 'linspace', 'load_file_system_library', 'load_op_library', 'local_variables', 'local_variables_initializer', 'log', 'log1p', 'log_sigmoid', 'logging', 'logical_and', 'logical_not', 'logical_or', 'logical_xor', 'losses', 'make_ndarray', 'make_template', 'make_tensor_proto', 'map_fn', 'matching_files', 'matmul', 'matrix_band_part', 'matrix_determinant', 'matrix_diag', 'matrix_diag_part', 'matrix_inverse', 'matrix_set_diag', 'matrix_solve', 'matrix_solve_ls', 'matrix_transpose', 'matrix_triangular_solve', 'maximum', 'meshgrid', 'metrics', 'min_max_variable_partitioner', 'minimum', 'mod', 'model_variables', 'moving_average_variables', 'multinomial', 'multiply', 'name_scope', 'negative', 'newaxis', 'nn', 'no_op', 'no_regularizer', 'norm', 'not_equal', 'one_hot', 'ones', 'ones_initializer', 'ones_like', 'op_scope', 'orthogonal_initializer', 'pad', 'parallel_stack', 'parse_example', 'parse_single_example', 'parse_single_sequence_example', 'parse_tensor', 'placeholder', 'placeholder_with_default', 'polygamma', 'pow', 'profiler', 'py_func', 'python_io', 'pywrap_tensorflow', 'qint16', 'qint32', 'qint8', 'qr', 'quantize_v2', 'quantized_concat', 'quint16', 'quint8', 'random_crop', 'random_gamma', 'random_normal', 'random_normal_initializer', 'random_poisson', 'random_shuffle', 'random_uniform', 'random_uniform_initializer', 'range', 'rank', 'read_file', 'real', 'realdiv', 'reciprocal', 'reduce_all', 'reduce_any', 'reduce_join', 'reduce_logsumexp', 'reduce_max', 'reduce_mean', 'reduce_min', 'reduce_prod', 'reduce_sum', 'register_tensor_conversion_function', 'report_uninitialized_variables', 'required_space_to_batch_paddings', 'reset_default_graph', 'reshape', 'resource', 'resource_loader', 'reverse', 'reverse_sequence', 'reverse_v2', 'rint', 'round', 'rsqrt', 'saturate_cast', 'saved_model', 'scalar_mul', 'scan', 'scatter_add', 'scatter_div', 'scatter_mul', 'scatter_nd', 'scatter_nd_add', 'scatter_nd_sub', 'scatter_nd_update', 'scatter_sub', 'scatter_update', 'segment_max', 'segment_mean', 'segment_min', 'segment_prod', 'segment_sum', 'self_adjoint_eig', 'self_adjoint_eigvals', 'sequence_mask', 'serialize_many_sparse', 'serialize_sparse', 'serialize_tensor', 'set_random_seed', 'setdiff1d', 'sets', 'shape', 'shape_n', 'sigmoid', 'sign', 'sin', 'sinh', 'size', 'slice', 'space_to_batch', 'space_to_batch_nd', 'space_to_depth', 'sparse_add', 'sparse_concat', 'sparse_fill_empty_rows', 'sparse_mask', 'sparse_matmul', 'sparse_maximum', 'sparse_merge', 'sparse_minimum', 'sparse_placeholder', 'sparse_reduce_max', 'sparse_reduce_max_sparse', 'sparse_reduce_sum', 'sparse_reduce_sum_sparse', 'sparse_reorder', 'sparse_reset_shape', 'sparse_reshape', 'sparse_retain', 'sparse_segment_mean', 'sparse_segment_sqrt_n', 'sparse_segment_sum', 'sparse_slice', 'sparse_softmax', 'sparse_split', 'sparse_tensor_dense_matmul', 'sparse_tensor_to_dense', 'sparse_to_dense', 'sparse_to_indicator', 'sparse_transpose', 'spectral', 'split', 'sqrt', 'square', 'squared_difference', 'squeeze', 'stack', 'stop_gradient', 'strided_slice', 'string', 'string_join', 'string_split', 'string_to_hash_bucket', 'string_to_hash_bucket_fast', 'string_to_hash_bucket_strong', 'string_to_number', 'substr', 'subtract', 'summary', 'svd', 'sysconfig', 'tables_initializer', 'tan', 'tanh', 'tensordot', 'test', 'tile', 'to_bfloat16', 'to_double', 'to_float', 'to_int32', 'to_int64', 'trace', 'train', 'trainable_variables', 'transpose', 'truediv', 'truncated_normal', 'truncated_normal_initializer', 'truncatediv', 'truncatemod', 'tuple', 'uint16', 'uint8', 'uniform_unit_scaling_initializer', 'unique', 'unique_with_counts', 'unsorted_segment_max', 'unsorted_segment_sum', 'unstack', 'user_ops', 'variable_axis_size_partitioner', 'variable_op_scope', 'variable_scope', 'variables_initializer', 'variance_scaling_initializer', 'variant', 'verify_tensor_all_finite', 'where', 'while_loop', 'write_file', 'zeros', 'zeros_initializer', 'zeros_like', 'zeta']",0,,4,2017-11-25T12:37:58Z,2017-11-30T04:02:24Z,NONE,2017-11-28T02:35:30Z
14874,tf.Print converts Variable to mutable Tensor,cla: yes,"The PR is proposed to resolve #14788.

tf.Print converts Variable to mutable Tensor, instead of constant.

```python
v = tf.Variable([99])
# <tf.Variable 'Variable:0' shape=(1,) dtype=int32_ref>

p = tf.Print(v, [v])
# Tensor(""PrintRef:0"", shape=(1,), dtype=int32_ref)
```

### How to test

+ [x] add test case
+ [ ] pass all tests",0,,5,2017-11-25T11:16:49Z,2017-12-01T02:00:23Z,CONTRIBUTOR,2017-11-30T20:01:16Z
14873,[Feature Request] Support for Flutter,"stat:awaiting tensorflower,type:feature","I recently moved from native development to Flutter seeing Google backing its development.
Since both TensorFlow and Flutter is by Google will there be a support for Flutter in Future?",1,,2,2017-11-25T04:18:23Z,2017-11-28T19:07:24Z,NONE,2017-11-25T06:24:10Z
14870,fix overpadding in MixtureSameFamily,"awaiting testing (then merge),cla: yes",This fixes pad_mix_dims when mixture distribution does not have scalar batch size -- previous version would add too many dimensions,0,,3,2017-11-24T20:34:39Z,2017-12-04T19:26:54Z,CONTRIBUTOR,2017-12-01T17:09:57Z
14869,fix overpadding in MixtureSameFamily,cla: no,This fixes pad_mix_dims when mixture distribution does not have scalar batch size -- previous version would add too many dimensions,0,,5,2017-11-24T20:03:42Z,2017-11-24T20:30:41Z,CONTRIBUTOR,2017-11-24T20:20:23Z
14868,"consuming Dataset becomes slower and slower, if make_one_shot_iterator each epoch ","stat:awaiting tensorflower,type:bug/performance","### Problem

I run make_one_shot_iterator() each epoch because  I want re-shuffle the dataset each epoch. I Know that the dataset.shuffle().repeat().batch() pipeline can do almost the same thing, but when data_num can not be divided exactly by batch_size, the pipeline merges two epochs at their boundary to construct a complete batch,  which I HATE.

So, I choose to run dataset.shuffle().batch() and make_one_shot_iterator() before each epoch. But I find that the speed of consuming dataset becomes slower and slower, significantly. I tried different settings to find out that it is make_one_shot_iterator() which makes consuming slow. 

By the way, I build tensorflow 1.4 from source on OS X with support of GPU, some hacky workaround. 

### Code

```
num_data = 1000
num_epoch = 50
batch_size = 32
dataset = tf.data.Dataset.range(num_data)

mode=3 
# model = 1,2,or 3
# 1: re-shuffle, re-batch and re-make-iterator each epoch
# 2: re-batch and re-make-iterator
# 3: only re-make-iterator

with tf.Session() as sess:
    for epoch in xrange(num_epoch):
        t1 = time.time()
        if mode==1: 
            _dataset = dataset.shuffle(num_data).batch(batch_size)
            iterator = _dataset.make_one_shot_iterator()
        elif mode==2:
            _dataset = dataset.batch(batch_size)
            iterator = _dataset.make_one_shot_iterator()
        elif mode==3: 
            iterator = dataset.make_one_shot_iterator()
        t2 = time.time()
        for i in xrange(num_data/batch_size):
            a = sess.run(iterator.get_next())
        t3 =time.time()
        print 'epoch %d make_iterator_time %.4f comsuming_time %.4f'%(epoch,t2-t1,t3-t2)
```

and the outputs:

```
epoch 0 make_iterator_time 0.0181 comsuming_time 0.1366
epoch 1 make_iterator_time 0.0036 comsuming_time 0.1444
epoch 2 make_iterator_time 0.0040 comsuming_time 0.1559
epoch 3 make_iterator_time 0.0036 comsuming_time 0.1695
epoch 4 make_iterator_time 0.0036 comsuming_time 0.1899
epoch 5 make_iterator_time 0.0036 comsuming_time 0.1955
epoch 6 make_iterator_time 0.0036 comsuming_time 0.2082
epoch 7 make_iterator_time 0.0037 comsuming_time 0.2191
epoch 8 make_iterator_time 0.0036 comsuming_time 0.2334
epoch 9 make_iterator_time 0.0040 comsuming_time 0.2461
epoch 10 make_iterator_time 0.0036 comsuming_time 0.2621
epoch 11 make_iterator_time 0.0036 comsuming_time 0.2720
epoch 12 make_iterator_time 0.0036 comsuming_time 0.2886
epoch 13 make_iterator_time 0.0036 comsuming_time 0.3006
epoch 14 make_iterator_time 0.0037 comsuming_time 0.3134
epoch 15 make_iterator_time 0.0039 comsuming_time 0.3260
epoch 16 make_iterator_time 0.0445 comsuming_time 0.3438
epoch 17 make_iterator_time 0.0037 comsuming_time 0.3576
epoch 18 make_iterator_time 0.0037 comsuming_time 0.3678
epoch 19 make_iterator_time 0.0040 comsuming_time 0.3827
epoch 20 make_iterator_time 0.0037 comsuming_time 0.3937
epoch 21 make_iterator_time 0.0038 comsuming_time 0.4172
epoch 22 make_iterator_time 0.0036 comsuming_time 0.4222
...
 ```

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS X 10.12.5
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: clang-802.0.42
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: GTX1080 8G
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""",1,,5,2017-11-24T19:27:25Z,2018-01-05T18:02:00Z,NONE,2017-11-27T22:17:26Z
14867,change bazel-mirror to mirror.bazel,"awaiting testing (then merge),cla: yes","hi i'm back , using only one email.",0,,15,2017-11-24T14:36:41Z,2017-11-30T19:59:33Z,NONE,2017-11-26T04:40:33Z
14866,"If I import cv2, "" tf.global_variables_initializer() "" will be very slow.",type:bug/performance,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04.3
- **TensorFlow installed from (source or binary)**:
Binary.
- **TensorFlow version (use command below)**:
1.3.0
- **Python version**: 
2.7.12
- **CUDA/cuDNN version**:
CUDA Version 8.0.61
- **GPU model and memory**:
NVIDIA GTX 1080Ti  12G

### Describe the problem
If I import cv2, "" tf.global_variables_initializer() "" will be very slow, about 143s. You can run my test code below, when "" import cv2 "" is commented out, the time is about 5s. The version of opencv is 2.4.13.4.

### Source code / logs
```Python
import tensorflow as tf
import time
import cv2

weight = tf.Variable(tf.truncated_normal([5,5,1,32], stddev=0.1))

ot = time.time()
init_op = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init_op)
nt = time.time()
print('time: {:.3f}'.format(nt-ot))
```",0,,3,2017-11-24T13:40:56Z,2017-11-28T22:16:37Z,NONE,2017-11-24T22:38:57Z
14862,C++ gradient for Select,"awaiting testing (then merge),cla: yes","Fix #14845 

migrate python implementation to c++ side, source: https://github.com/tensorflow/tensorflow/blob/27767d8e9c1325979cf32ff5b81c10df9006fd57/tensorflow/python/ops/math_grad.py#L919

### How to test

+ [x] add test case
+ [ ] pass all tests",0,,6,2017-11-24T11:24:50Z,2017-12-20T19:40:05Z,CONTRIBUTOR,2017-11-24T11:28:12Z
14861,Cant install tensorflow on my laptop,"stat:awaiting response,type:build/install","pip3 install --upgrade tensorflow-gpu

Collecting tensorflow-gpu

Could not find a version that satisfies the requirement tensorflow-gpu (from versions: )
No matching distribution found for tensorflow-gpu

this is what it shows please help",0,,15,2017-11-24T11:23:41Z,2018-01-18T22:06:39Z,NONE,2017-11-24T21:49:32Z
14858,off-by-one bug in graph_editor get_forward_walk_ops,stat:awaiting response,"This [line](https://github.com/tensorflow/tensorflow/blob/c9de294d0a0980b1636f76757c175afbf4f58ea8/tensorflow/contrib/graph_editor/select.py#L414) converts Tensor to its corresponding op by replacing it with its consuming op

`seed_ops = util.get_consuming_ops(ts)
`

Instead it should replace it with its producing op

`seed_ops = [t.op for t in ts]
`

This would makes `get_forward_walk_ops(tensor` give same result as `get_forward_walk_ops(tensor.op` which was probably the intention of this function

I know `contrib` isn't really supported, but wanted to file this bug in order to reference it in work-arounds in my code",0,,8,2017-11-24T09:38:31Z,2017-12-01T17:51:48Z,CONTRIBUTOR,2017-12-01T07:06:38Z
14856,Enable GCS filesystem for Windows,"awaiting testing (then merge),cla: yes",,0,,9,2017-11-24T06:45:14Z,2017-12-09T02:03:44Z,CONTRIBUTOR,2017-12-04T03:12:44Z
14855,tensorboard ImportError: cannot import name 'run_main',,"I install the tensorflow on Mac from source.
but when I run the tensorboard,and got a error like this

> Pro-2:Desktop xxh$ tensorboard
Traceback (most recent call last):
  File ""/Users/xxh/anaconda3/bin/tensorboard"", line 7, in <module>
    from tensorboard.main import run_main
ImportError: cannot import name 'run_main'`

How to fix it?",0,,5,2017-11-24T06:06:31Z,2017-11-28T02:05:15Z,NONE,2017-11-24T08:13:51Z
14854,Add batch support for various image_ops,"awaiting testing (then merge),cla: yes","Working on #8926
I used #7369 as a guide for my work here.

I have added batch support for:

- `flip_left_right`
- `flip_up_down`
- `random_flip_left_right`
- `random_flip_up_down`
- `transpose_image`
- `rot90`

I have corrected existing tests in `image_ops_test.py` and introduced a number of new tests based on existing tests for 3D inputs. 

This is my first contribution to this repository and I have tried to follow the [`contributing`](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md) guidelines. However, running `pylint` on `image_ops_impl.py` and `image_ops_test.py` revealed a number of pre-existing style violations. I've tried to fix the ones relevant to my work but may have missed some.",0,,9,2017-11-24T05:11:51Z,2017-12-07T00:22:12Z,CONTRIBUTOR,2017-11-26T22:50:09Z
14845,No gradient defined for op: Select,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: -

### Describe the problem
It seems there is no gradient defined for the Select operation in the C++ API.

I am actually getting this issue while using the C++ API through the C# bindings provided by the TensorFlowSharp project, and for this reason I didn't fill the ""exact command to reproduce"" field above. However, seeing that [```@ops.RegisterGradient(""Select"")``` is placed in math.grad.py](https://github.com/tensorflow/tensorflow/blob/27767d8e9c1325979cf32ff5b81c10df9006fd57/tensorflow/python/ops/math_grad.py#L919), and given that there is no analogous ```REGISTER_GRADIENT_OP(""Select"", SelectGrad)``` instruction in [math_grad.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/gradients/math_grad.cc), tells me that the gradient for the ```Select``` op is indeed still missing from the C++ API.

Hope its not a false alarm given that I didn't test libtensorflow.dll directly.


",0,,3,2017-11-23T21:32:11Z,2017-12-20T19:40:05Z,NONE,2017-11-24T11:32:51Z
14841,Estimators cause Out of range warning on FIFOQueue and fail to run all training steps,stat:awaiting tensorflower,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 4.13.12-1-ARCH
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.6.3
- **CUDA/cuDNN version**: 9.0.176-4/7.0.3-1
- **GPU model and memory**: 1080/1070

### Describe the problem
Trying to use estimators with a trivially small network fails to train for more than one step due to FIFOQueue closing with insufficient elements.

### Source code / logs
In the following example I try to train a single neuron for 1000 steps at a time.  set_size changes the training set's size.  With set_size=1000 I would expect training to complete 1000 steps however only 8 steps are completed and an Out of range warning is printed.  Setting set_size to 10 leads to only a single step being completed, I would expect at least 10 steps to complete, possible all 1000 if the input_fn is called repeatedly to fill a queue(not sure what default behaviour is supposed to be).  Setting set_size=1000000 allows the entire 1k training steps to complete.

code:

```
import numpy as np
import tensorflow as tf
import models

set_size = 1000

params = {""learning_rate"":0.00001}

def model_fn(features, labels, mode, params):
    """"""Build model for Estimator here""""""
    ####Build graph
    input_layer = tf.reshape(features[""x""],[-1,1])
    hidden_layer = tf.layers.dense(input_layer,1,activation=tf.nn.relu)
    output_layer = hidden_layer
    
    ####Prediction mode
    if mode == tf.estimator.ModeKeys.PREDICT:
        predictions = {""y"":output_layer}
        return tf.estimator.EstimatorSpec(mode=mode,predictions=predictions)
    
    loss = tf.losses.mean_squared_error(labels,output_layer)
    
    ####Training mode
    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=params[""learning_rate""])
        train_op = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(
            mode=mode,
            loss=loss,
            train_op=train_op)
    
    ####Eval mode
    elif mode == tf.estimator.ModeKeys.EVAL:
        eval_metric_ops = {""rmse"":tf.metrics.root_mean_squared_error(labels,output_layer)}
        return tf.estimator.EstimatorSpec(
            mode=mode,
            loss=loss,
            eval_metric_ops=eval_metric_ops)
        

input_fn = tf.estimator.inputs.numpy_input_fn(
    x={'x':np.array([[float(x)] for x in range(set_size)])},
    y=np.array([[float(x*2)] for x in range(set_size)]),
    shuffle=False
)


def test():
    nn = tf.estimator.Estimator(model_fn=model_fn, params=params)
    for x in range(5):
        print('START LOOP:',x)
        a = nn.train(input_fn=input_fn,steps=1000)
        print('--------')
        b = nn.evaluate(input_fn=input_fn)
        print(""----STATS----"",b)
    print('Done loop')
    c = nn.predict(input_fn=input_fn)
    #print('Predictions:',[x for x in c])

if __name__ == ""__main__"":
    test()
```
warning:
> 2017-11-23 11:23:52.370395: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: FIFOQueue '_2_enqueue_input/fifo_queue' is closed and has insufficient elements (requested 128, current size 0)
> 	 [[Node: fifo_queue_DequeueUpTo = QueueDequeueUpToV2[component_types=[DT_INT64, DT_DOUBLE, DT_DOUBLE], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](enqueue_input/fifo_queue, fifo_queue_DequeueUpTo/n)]]

Edit: This may be due to something in tf.estimator.inputs.numpy_input_fn as creating the input_fn manually does not cause the warning and early termination of training.",0,,2,2017-11-23T16:25:39Z,2017-11-28T18:07:39Z,NONE,2017-11-28T01:59:10Z
14839,Typo fix,"awaiting testing (then merge),cla: yes",Typo fix : uniqified -> uniquified,0,,2,2017-11-23T15:09:54Z,2017-11-29T21:49:41Z,CONTRIBUTOR,2017-11-29T21:49:36Z
14838,Typo fixing,"awaiting testing (then merge),cla: yes",typo fixed : libaries -> libraries,1,,2,2017-11-23T14:45:39Z,2017-11-29T21:50:00Z,CONTRIBUTOR,2017-11-29T21:50:00Z
14837,Fix missing __sincos in XLA on macOS,"awaiting review,cla: yes","Building XLA on macOS failed due to missing `__sincos` and `__sincosf`. 

```
tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:285:3: error: use of undeclared identifier 'sincosf'; did you mean '__sincosf'?
tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:285:24: error: use of undeclared identifier 'sincos'; did you mean '__sincos'?
```

https://github.com/tensorflow/tensorflow/pull/14288 tried to fix the issue but still needs https://github.com/tensorflow/tensorflow/pull/14137 too.",0,,4,2017-11-23T14:44:56Z,2017-12-26T19:39:29Z,CONTRIBUTOR,2017-12-04T03:00:50Z
14836,Update datasets.md,cla: yes,"There is omission of tf.data in front of Iterator.from_structure(~). I found it, When I read and tested 

So For some other people who read this article. I ask for pull request.  ",0,,5,2017-11-23T13:04:32Z,2017-11-29T21:45:15Z,CONTRIBUTOR,2017-11-23T13:16:59Z
14834,[AUC] result of tf.metrics.auc doesnot match with sklearn's,,"My tensorflow version is ('v1.3.0-rc1-4263-gc81acfb', '1.4.0-rc1'), and the system is Rehat with gcc version 4.8.5 20150623 (Red Hat 4.8.5-16). I run the program use CPU only.

I wrote a NN  use tensorflow for binary classification. I create the an `auc_op` in the following way:
 
```python
net = input_layer(features,) # get dense input layer from features
for layer_id in xrange(1, num_layer):
    net  = tf.add(tf.matmul(net, self._weights[layer_id]), self._bias[layer_id])
    if layer_id < num_layer - 1: # output layer without activation function to get `wx + b`
        net =tf.nn.relu(net)
logits = net
labels = tf.expand_dims(tf.cast(tf.convert_to_tensor(labels), dtype = tf.float32), axis = -1)
auc_op = tf.metrics.auc(labels = labels, predictions = tf.sigmoid(logits), num_thresholds = 102400)
```

I run the `auc_op` like this:
```python
for step in xrange(1, self._max_steps + 1):
    auc = self._sess.run(auc_op)
```

I also keep all the logits and labels in each step and concatenate them, then call sklearn like this:
```python
from sklearn.metrics import roc_auc_score
roc_auc_score(labels, sigmoid(logits))
```

Are there anything wrong in the way I use tf.metrics.auc? When I run the `auc_op`, it returns a tuple with two values and I don't which one is the correct auc. But both of them are not equal with sklearn's. 
I once wrote an program to calculate auc and it was exactly the same with sklearn's even in 1M data, thus I tend to think sklearn's result is the ground truth. ",0,,5,2017-11-23T12:16:19Z,2017-11-26T02:18:37Z,NONE,2017-11-23T13:30:41Z
14831,fixed bug that Dropout support_masking gets reset to False,"awaiting testing (then merge),cla: yes",fix #14819.,0,,4,2017-11-23T10:18:50Z,2017-12-11T02:44:45Z,CONTRIBUTOR,2017-11-23T11:43:30Z
14830,Feature request: Tensorflow lite on memory constrained bare-metal systems,comp:lite,"I'm interested in running Tensorflow Lite on devices with limited memory resources and possibly no operating systems abstractions available. 

This means removing any dependencies on file systems, threads, synchronization primitives, etc. and keeping the binary size as small as possible. I don't know if you discuss your roadmap openly here, but I'm wondering whether this is something that is planned for TFLite? If not, I may go ahead and try to implement this myself.",0,,5,2017-11-23T10:11:35Z,2017-11-27T08:51:53Z,CONTRIBUTOR,2017-11-23T12:22:54Z
14829,Visualizing Embeddings,type:docs,"https://www.tensorflow.org/programmers_guide/embedding#projections
n the visual for data exploration there are 2 options for distance. One of them should be ""Euclidean"" as against ""Euclidian""",1,,9,2017-11-23T10:06:38Z,2017-12-20T19:10:14Z,NONE,2017-11-23T11:35:11Z
14828,Eager: Can't take gradient of element-wise tf functions,stat:community support,"Maybe I'm missing something, but taking the gradient of functions like `tf.sin` and `tf.log` in eager mode is failing on a recent master (80e7c9f45c):

```python
In [1]: import tensorflow as tf

In [2]: import tensorflow.contrib.eager as tfe

In [3]: tfe.enable_eager_execution()

In [4]: g=tfe.gradients_function(tf.sin)

In [5]: g([1.0])
```

```
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-5-e4ceb6f55b16> in <module>()
----> 1 g([1.0])

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)
    509     """"""Computes the gradient of the decorated function.""""""
    510
--> 511     _, grad = val_and_grad_function(f, params=params)(*args, **kwds)
    512     return grad
    513

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)
    608       raise ValueError(""Functions to be differentiated cannot ""
    609                        ""receive keyword arguments."")
--> 610     val, vjp = make_vjp(f, params)(*args, **kwds)
    611     return val, vjp(dy=dy)
    612

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)
    660       args = _ensure_unique_tensor_objects(parameter_positions, args)
    661       for i in parameter_positions:
--> 662         sources.append(args[i])
    663         tape.watch(args[i])
    664       result = f(*args)

IndexError: list index out of range
```",0,,3,2017-11-23T09:33:17Z,2017-11-28T10:30:43Z,CONTRIBUTOR,2017-11-24T03:40:20Z
14826,bug about tensorflow can not call opencv imread properly,"stat:awaiting response,type:build/install","
------------------------

### System information
**- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7(but actually i am talking about c++ code)
- **Bazel version (if compiling from source)**: 0.5.4/0.7.0 all tried
- **GCC/Compiler version (if compiling from source)**:  4.8.4
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: CPU mode
- **Exact command to reproduce**: bazel run -c opt //tensorflow/cc/face:face**

### Describe the problem
i add opencv as a third party lib to tensorflow, and modify the workspace and BUILD file to include it to the project. it works well when i use tensorflow 1.2.1 or version before it. recently i update my tensorflow to the newest version, it recommand i must update my bazel at least 0.5.4(i use 0.5.2 with jdk7 before).
and when i update bazel, and move my own code to the new project, compiling seems ok. but when i run the binary, it seems not right. i can not load a jpeg file when i use cv::imread, it doesn't crash, but return a cv::Mat with size 0. in the new project, i can load a bmp file properly, so i guess it is because the project does not link the libjpeg.
but i never need to link the libjpeg manually, because it is included in the opencv library. so i guess there is a bug in the new version of tensorflow.
i have tried the linkopt with -ljpeg, but it does not work.

### Source code / logs
WORKSPACE File:
new_local_repository(
  name = ""opencv"",
  path = ""/usr/local"",
  build_file = ""opencv.BUILD"",
)
BUILD file of opencv:
cc_library(
    name = ""opencv"",
    srcs = glob([""lib/*.so*""]),
    hdrs = glob([""include/**/*.hpp""]),
    includes = [""include""],
    visibility = [""//visibility:public""], 
    linkstatic = 1,
)
BUILD file of my code
tf_cc_binary(
    name = ""face"",
    srcs = [""face.cc""],
    includes = ["".""],
    deps = [
        ""//tensorflow/cc:cc_ops"",
        ""//tensorflow/cc:client_session"",
        ""//tensorflow/core:tensorflow"",
        ""@opencv//:opencv"",
    ],
    copts = [""-fopenmp""],
    linkopts = [""-lgomp"", ""-ljpeg""],
)
my code:
        cv::Mat img = cv::imread(""pic.jpg"");
        std::cout<<line<<"" ""<<img.channels()<<"" ""<<img.cols<<"" ""<<img.rows<<endl;
the log will be: pic.jpg 1 0 0
but if i read a bmp file:
        cv::Mat img = cv::imread(""pic.bmp"");
        std::cout<<line<<"" ""<<img.channels()<<"" ""<<img.cols<<"" ""<<img.rows<<endl;
the log will be: pic.bmp 3 500 355
",0,,7,2017-11-23T08:16:13Z,2018-01-11T22:18:57Z,NONE,2017-11-28T16:57:20Z
14825,how to extract parameters of sim.batch_norm,,"using slim.batch_norm for normalize and here are the batch_norm_params:


![image](https://user-images.githubusercontent.com/31264567/33162735-1b13082c-d066-11e7-918c-62bec95e328c.png)


in this way, i think all the trainable variables (beta, gamma, moving_mean, moving_variance) was stored. and when i print elements in tf.trainable_variables, here is the result. 



![image](https://user-images.githubusercontent.com/31264567/33162835-8c4c091c-d066-11e7-96c9-da7f9b85865e.png)


missing gamma, 
i extracted the output tensor of the first layer, and manually calculate correspond feature map through these parameters.  its not the same, but can be transformed into the same through linear transformation.
so, i'm sure there's something wrong with batch_norm params. where can i find the correct ones. ",0,,1,2017-11-23T08:06:02Z,2017-11-27T18:48:58Z,NONE,2017-11-27T18:48:56Z
14824,'output' does not exist in model 'file:///android_asset/retrained_graph.pb',type:support,"I was retrain a inception model with food images,i got the final test prediction and retrained_graph.pb ,retrained_labels.txt file.i check the prediction using command prompt in windows and its work.but i was put the retrained_graph.ph and retrained_labels.txt files into android studio asset folder for deploying mobile,i got the exception like:
output' does not exist in model 'file:///android_asset/retrained_graph.pb

Can anyone help me solve this issue.",0,,1,2017-11-23T06:43:18Z,2017-11-28T04:41:23Z,NONE,2017-11-28T04:41:22Z
14823,TFlite readme.md add mobilenet frozen_graph.pb link,"awaiting testing (then merge),cla: yes",add mobilenet frozen_graph.pb link,0,,6,2017-11-23T06:40:14Z,2017-12-10T20:24:21Z,CONTRIBUTOR,2017-11-23T06:51:40Z
14821,[DONOTMERGE] Debug windows GPU build.,cla: yes,,0,,1,2017-11-23T04:44:58Z,2017-12-04T06:26:33Z,OWNER,2017-12-04T03:40:39Z
14819,Keras Dropout support_masking gets reset to False,"stat:community support,type:bug/performance","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.6
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0
- **Python version**: 3.6.1
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: see below

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The Keras Dropout layer constructor (tensorflow/python/keras/_impl/keras/layers/core.py) sets support_masking=True and then calls its super constructor, which sets it back to False. Other layers defined in that module appear to set support_masking=True after the super constructor call.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
from tensorflow.contrib.keras.api.keras.models import Sequential
from tensorflow.contrib.keras.api.keras.layers import Dropout, InputLayer, LSTM, Masking 

if __name__ == '__main__':

    test1 = True

    def model1():
        model = Sequential()
        model.add(InputLayer([8, 64]))
        model.add(Masking())
        model.add(Dropout(0.5))

    def model2():
        model = Sequential()
        model.add(InputLayer([8, 64]))
        model.add(Masking())
        model.add(LSTM(128, return_sequences=True))
        model.add(Dropout(0.5))

    if test1:
        model1()
    else:
        model2()
```

```
Traceback (most recent call last):
  File ""expose_dropout_bug.py"", line 16, in <module>
    model.add(Dropout(0.5))
  File ""/.venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/models.py"", line 501, in add
    output_tensor = layer(self.outputs[0])
  File ""/.venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py"", line 252, in __call__
    output = super(Layer, self).__call__(inputs, **kwargs)
  File ""/.venv/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 594, in __call__
    output_mask = self.compute_mask(inputs, previous_mask)
  File ""/.venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py"", line 308, in compute_mask
    'but was passed an input_mask: ' + str(mask))
TypeError: Layer dropout_1 does not support masking, but was passed an input_mask: Tensor(""masking/Any_1:0"", shape=(?, 8), dtype=bool)
```",0,,6,2017-11-23T04:24:01Z,2017-12-11T02:44:45Z,NONE,2017-11-23T07:29:21Z
14818,Error when setting model_dir for tf.keras.estimator.estimator_from_model(),"stat:awaiting tensorflower,type:bug/performance","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: tf.VERSION = 1.4.0 tf.GIT_VERSION = v1.4.0-rc1-11-g130a514
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0.61/6.0.21
- **GPU model and memory**: NVIDIA Tesla M60 8 GB
- **Exact command to reproduce**: See Below

### Describe the problem
When trying to use an estimator that is derived from ```tf.keras.estimator.estimator_from_model()``` and training with ```tf.estimator.train_and_evaluate()```, setting ```model_dir``` either in the ```RunConfig``` or in ```tf.keras.estimator.model_to_estimator``` causes a ```NotFoundError``` to be thrown. If the model_dir is not set, then a tmp directory is used as expected and the training is completed successfully.

I also tested this with the canned estimator ```tf.estimator.DNNRegressor```, and the settings were applied as expected when the RunConfig was passed to the estimator or the model_dir passed to the estimator directly.

Below is code to demonstrate this issue. 

### Source code / logs
Minimal example, NotFoundError is thrown:
```python
import os
import numpy as np
import tensorflow as tf

tf.logging.set_verbosity(tf.logging.INFO)

inputs = tf.keras.layers.Input(shape=(10,))
outputs = tf.keras.layers.Dense(10)(inputs)
model = tf.keras.models.Model(inputs, outputs)
model.compile(optimizer='sgd', loss='mse')

# Both of these result in a NotFoundError
run_config = tf.estimator.RunConfig(model_dir='min_out')
est_keras = tf.keras.estimator.model_to_estimator(keras_model=model, config=run_config)
#est_keras = tf.keras.estimator.model_to_estimator(keras_model=model, model_dir='min_out')

input_name = model.input_names[0]
data = np.random.rand(1000,10).astype(np.float32)
train_input_fn = tf.estimator.inputs.numpy_input_fn({input_name:data}, data, batch_size=10, num_epochs=None, shuffle=False)

train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=1000)
eval_spec = tf.estimator.EvalSpec(input_fn=train_input_fn, steps=10)
tf.estimator.train_and_evaluate(est_keras, train_spec, eval_spec)
```

Error generated:
```python
$ python minimal_modeldir.py
INFO:tensorflow:Using the Keras model from memory.
2017-11-22 21:40:44.540162: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2017-11-22 21:40:46.525335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-11-22 21:40:46.525573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:
name: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775
pciBusID: 0000:00:1e.0
totalMemory: 7.43GiB freeMemory: 7.35GiB
2017-11-22 21:40:46.525595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0, compute capability: 5.2)
INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa7fe724890>, '_model_dir': 'min_out', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.
INFO:tensorflow:Create CheckpointSaverHook.
2017-11-22 21:40:47.048052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0, compute capability: 5.2)
INFO:tensorflow:Restoring parameters from min_out/model.ckpt-1
2017-11-22 21:40:47.076405: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key dense_1/kernel not found in checkpoint
2017-11-22 21:40:47.079173: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key SGD/iterations not found in checkpoint
2017-11-22 21:40:47.079637: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key dense_1/kernel not found in checkpoint
         [[Node: save/RestoreV2_5 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2_5/tensor_names, save/RestoreV2_5/shape_and_slices)]]
2017-11-22 21:40:47.080045: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key SGD/lr not found in checkpoint
2017-11-22 21:40:47.080601: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key SGD/decay not found in checkpoint
2017-11-22 21:40:47.081693: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key dense_1/bias not found in checkpoint
2017-11-22 21:40:47.081871: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key training/SGD/Variable not found in checkpoint
2017-11-22 21:40:47.082097: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key SGD/momentum not found in checkpoint
2017-11-22 21:40:47.082403: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key training/SGD/Variable_1 not found in checkpoint
Traceback (most recent call last):
  File ""minimal_modeldir.py"", line 23, in <module>
    tf.estimator.train_and_evaluate(est_keras, train_spec, eval_spec)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 430, in train_and_evaluate
    executor.run_local()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 609, in run_local
    hooks=train_hooks)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 780, in _train_model
    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 368, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 673, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 493, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 851, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 856, in _create_session
    return self._sess_creator.create_session()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 554, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 428, in create_session
    init_fn=self._scaffold.init_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 273, in prepare_session
    config=config)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 205, in _restore_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1666, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key dense_1/kernel not found in checkpoint
         [[Node: save/RestoreV2_5 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2_5/tensor_names, save/RestoreV2_5/shape_and_slices)]]
         [[Node: save/RestoreV2_2/_9 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_30_save/RestoreV2_2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

Caused by op u'save/RestoreV2_5', defined at:
  File ""minimal_modeldir.py"", line 23, in <module>
    tf.estimator.train_and_evaluate(est_keras, train_spec, eval_spec)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 430, in train_and_evaluate
    executor.run_local()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 609, in run_local
    hooks=train_hooks)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 780, in _train_model
    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 368, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 673, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 493, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 851, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 856, in _create_session
    return self._sess_creator.create_session()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 554, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 419, in create_session
    self._scaffold.finalize()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 212, in finalize
    self._saver.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1227, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1263, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 745, in _build_internal
    restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 470, in _AddShardedRestoreOps
    name=""restore_shard""))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 427, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 267, in restore_op
    [spec.tensor.dtype])[0])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1021, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Key dense_1/kernel not found in checkpoint
         [[Node: save/RestoreV2_5 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2_5/tensor_names, save/RestoreV2_5/shape_and_slices)]]
         [[Node: save/RestoreV2_2/_9 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_30_save/RestoreV2_2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
```
",1,,3,2017-11-23T02:59:12Z,2017-11-30T23:55:36Z,NONE,2017-11-27T19:43:58Z
14815,cant downlod tensorflow it shows could not find a versone,"stat:awaiting response,type:build/install","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,5,2017-11-23T02:16:11Z,2018-01-09T23:25:34Z,NONE,2017-11-27T18:59:44Z
14813,Introduce tf_http_archive,cla: yes,"This new repository rule consolidates patched_http_archive,
temp_workaround_http_archive, http_archive, and new_http_archive.

The following behaviors have been introduced:

- A delete attribute that can rm -rf certain repo content after extraction
- Helpful error messages when mirroring requirements aren't followed

cc: @gunan",1,,2,2017-11-23T00:16:15Z,2017-11-29T22:37:17Z,MEMBER,2017-11-29T22:05:33Z
14812,segmentation fault due to pytorch and tensorflow conflictions,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 8 (jessie)
- **TensorFlow installed from (source or binary)**: from Anaconda, with command:
`pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.0-cp35-cp35m-linux_x86_64.whl`

- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0
- **Python version**: 3.5.4
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: cuda 8.0
- **GPU model and memory**: TITAN Xp, 12G
- **Exact command to reproduce**:
This fails:
```python
>>> import torch
>>> import tensorflow as tf
Segmentation fault (core dumped)
```


### Describe the problem
I am using pytorch version 0.2.0_4, for python 3.5, with cuda support. I installed it from the following command:

`conda install pytorch torchvision cuda80 -c soumith`

When I use tensorflow alone, it works fine; i.e., doing an import like 
```python
>>> import tensorflow as tf
```
has no problem. Also, Importing tensorflow before torch seems fine as well.


However if I import pytorch before tensorflow, it fails and reported a segmentation error (as shown above).

 ",0,,2,2017-11-22T22:10:45Z,2017-11-27T16:42:37Z,NONE,2017-11-22T23:10:46Z
14811,VERBS and OpenMPI not building anymore without CUDA ?,stat:awaiting response,"Hello,

When activating MPI or VERBS without CUDA, build fails with the following error:

```
ERROR: /build/python-tensorflow-cuda-1.4.0/tensorflow/contrib/verbs/BUILD:133:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma' failed (Exit 1): gcc failed: error executing command 
  (cd /tmp/tmp.plGX4Xhgql/.cache/bazel/_bazel_pbuilder/436710022b7d9d872ccd97b57710586f/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu/libfakeroot:/usr/lib64/libfakeroot:/usr/lib32/libfakeroot \
    PATH=/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3.4 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages/ \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL=0 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -mavx -msse4.1 -msse4.2 '-std=c++0x' -mavx -msse4.1 -msse4.2 -MD -MF bazel-out/local-py3-opt/bin/tensorflow/contrib/verbs/_objs/rdma/tensorflow/contrib/verbs/rdma.pic.d '-frandom-seed=bazel-out/local-py3-opt/bin/tensorflow/contrib/verbs/_objs/rdma/tensorflow/contrib/verbs/rdma.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DSNAPPY -DTENSORFLOW_USE_VERBS -iquote . -iquote bazel-out/local-py3-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local-py3-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/local-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local-py3-opt/genfiles/external/local_config_sycl -iquote external/nsync -iquote bazel-out/local-py3-opt/genfiles/external/nsync -iquote external/jemalloc -iquote bazel-out/local-py3-opt/genfiles/external/jemalloc -iquote external/gif_archive -iquote bazel-out/local-py3-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local-py3-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/local-py3-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/local-py3-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local-py3-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/local-py3-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/local-py3-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local-py3-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local-py3-opt/genfiles/external/zlib_archive -iquote external/curl -iquote bazel-out/local-py3-opt/genfiles/external/curl -iquote external/boringssl -iquote bazel-out/local-py3-opt/genfiles/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/local-py3-opt/genfiles/external/jsoncpp_git -iquote external/grpc -iquote bazel-out/local-py3-opt/genfiles/external/grpc -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/local-py3-opt/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/local-py3-opt/genfiles/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/local-py3-opt/genfiles/external/jemalloc/include -isystem external/gif_archive/lib -isystem bazel-out/local-py3-opt/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/local-py3-opt/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/local-py3-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local-py3-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local-py3-opt/genfiles/external/zlib_archive -isystem external/curl/include -isystem bazel-out/local-py3-opt/genfiles/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/local-py3-opt/genfiles/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/local-py3-opt/genfiles/external/jsoncpp_git/include -isystem external/grpc/include -isystem bazel-out/local-py3-opt/genfiles/external/grpc/include -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/contrib/verbs/rdma.cc -o bazel-out/local-py3-opt/bin/tensorflow/contrib/verbs/_objs/rdma/tensorflow/contrib/verbs/rdma.pic.o).
In file included from ./tensorflow/core/platform/stream_executor.h:26:0,
                 from ./tensorflow/core/common_runtime/gpu/gpu_util.h:23,
                 from tensorflow/contrib/verbs/rdma.cc:23:
./tensorflow/stream_executor/dso_loader.h:32:30: fatal error: cuda/cuda_config.h: No such file or directory
 #include ""cuda/cuda_config.h""
```

Is that expected ?

Thanks",0,,11,2017-11-22T22:05:58Z,2018-01-18T22:08:41Z,NONE,2017-11-28T01:38:10Z
14809,Batch Normalization layer is unusable,stat:awaiting tensorflower,"Despite the numerous submitted issues, `tf.layers.batch_normalization` still feels completely unusable. The major problems are:

1. It does not allow for input tensors with varying shapes. It is complete nonsense to have a fixed batch size. It should be allowed for the batch dimension to be vary.

2. One needs to manually update the running mean and variance. This is very uncomfortable and a very common pitfall for many beginners, while it would take just a couple of lines to do the update internally based on the value of the `training` parameter.

I have recently seen too many custom implementations of a batch normalization layer because of the above problems and it will definitely be very useful if these problems are fixed ASAP.

I am using `tensorflow-gpu`, version `1.4`",0,,9,2017-11-22T20:21:21Z,2018-01-04T00:51:46Z,NONE,2017-11-23T04:09:41Z
14808,Remove useless statements in Dockerfiles,cla: yes,"'CMD [""/bin/bash""]' is not useful since it's already provided by the base ubuntu image.
'RUN [""/bin/bash""]' looks like a typo and just creates an extra empty layer.

Signed-off-by: Felix Abecassis <fabecassis@nvidia.com>",0,,2,2017-11-22T20:16:26Z,2017-11-22T20:22:46Z,CONTRIBUTOR,2017-11-22T20:16:33Z
14807,seg fault training tf.nn.conv3d with minibatch size >2,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code! You can find it here: https://github.com/NERSC/CosmoFlow/tree/master/SegFault
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: SUSE Linux 12.2
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: ('v1.3.0-rc1-3112-g65b6a75', '1.4.0-rc0') Note this is NOT compiled with the Intel MKL options. 
- **Python version**: 2.7.13 
- **Bazel version (if compiling from source)**: 0.6.0
- **GCC/Compiler version (if compiling from source)**: 4.8.5
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A, Running on x86_64 Intel Haswell node
- **Exact command to reproduce**: See README in https://github.com/NERSC/CosmoFlow/tree/master/SegFault


### Describe the problem

A seg fault when training a tf.nn.conv3d with minibatch size more than 2 on a single Intel Haswell. The seg fault occurs at [line 187](https://github.com/NERSC/CosmoFlow/blob/master/SegFault/CosmoNet.py#L187).  

### Source code / logs

GDB log: https://github.com/NERSC/CosmoFlow/blob/master/SegFault/gdbTrace.log
It looks like some kind of cyclic dependency in Eigen::TensorEvaluator. ",0,,9,2017-11-22T20:05:42Z,2018-01-30T19:26:40Z,NONE,2017-11-23T01:28:43Z
14806,Go TensorFlow 1.4.0: DataType 21 is not supported,stat:contributions welcome,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch linux
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: NA (using go)
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0.176-4 / 7.0.3-1
- **GPU model and memory**: GTX 1060 6GB
- **Exact command to reproduce**:

### Describe the problem
Calling the Value() method on the evaluated output tensor of dataset related operations from the go package fails with the error `DataType 21 is not supported`. It looks like `op.TextLineDataset()` produces a tensor of type `tf.Half` can't be converted to a go type?
I may be using the datasets wrong. If so, the error and/or documentation should be improved.

### Source code / logs

```
package main

import (
	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func main() {
	s := op.NewScope()
	textLineHandle := op.TextLineDataset(s,
		op.Const(s.SubScope(""filename""), ""dataset.txt""),
		op.Const(s.SubScope(""compression_type""), """"),
		op.Const(s.SubScope(""buffer_size""), int64(1)),
	)
	graph, err := s.Finalize()
	if err != nil {
		panic(err)
	}
	sess, err := tf.NewSession(graph, nil)
	if err != nil {
		panic(err)
	}
	results, err := sess.Run(nil, []tf.Output{textLineHandle}, []*tf.Operation{})
	if err != nil {
		panic(err)
	}
	_ = results[0].Value()
}
```
Produces:
```
[isaac@d6-arch tfes]$ go run dataset_demo.go 
2017-11-22 11:06:36.945842: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2017-11-22 11:06:37.042484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-11-22 11:06:37.042786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:01:00.0
totalMemory: 5.93GiB freeMemory: 4.58GiB
2017-11-22 11:06:37.042801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
panic: BUG: Please report at https://github.com/tensorflow/tensorflow/issues with the note: Go TensorFlow 1.4.0: DataType 21 is not supported

goroutine 1 [running]:
github.com/tensorflow/tensorflow/tensorflow/go.typeOf(0xc400000015, 0x0, 0x0, 0x0, 0x49d72d, 0x4c6e40)
	/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/tensor.go:273 +0x14d
github.com/tensorflow/tensorflow/tensorflow/go.(*Tensor).Value(0xc42000c0e0, 0x0, 0xc420057f60)
	/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/tensor.go:175 +0x8d
main.main()
	/home/isaac/go/src/github.com/is8ac/tfes/dataset_demo.go:27 +0x2b0
exit status 2
```",1,,5,2017-11-22T19:42:17Z,2017-12-02T09:39:46Z,NONE,2017-11-27T17:00:43Z
14805,"Installation says to use cuDNN v6.1, but NVIDIA only offers 6.0 and 7.0.4",stat:awaiting tensorflower,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

NVIDIA only offers the following from their website

https://developer.nvidia.com/rdp/cudnn-download

Download cuDNN v7.0.4 (Nov 13, 2017), for CUDA 9.0
Download cuDNN v7.0.4 (Nov 13, 2017), for CUDA 8.0
Download cuDNN v6.0 (April 27, 2017), for CUDA 8.0
Download cuDNN v6.0 (April 27, 2017), for CUDA 7.5
Download cuDNN v5.1 (Jan 20, 2017), for CUDA 8.0
Download cuDNN v5.1 (Jan 20, 2017), for CUDA 7.5

installation instructions say to use 
https://www.tensorflow.org/install/install_windows

cuDNN v6.1. For details, ....

If you have a different version of one of the preceding packages, please change to the specified versions. In particular, the cuDNN version must match exactly: TensorFlow will not load if it cannot find cuDNN64_6.dll.



### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,10,2017-11-22T19:39:36Z,2018-01-08T23:12:59Z,NONE,2017-11-27T23:50:24Z
14804,Branch 176676125,cla: yes,,0,,1,2017-11-22T19:38:34Z,2017-11-22T22:49:05Z,MEMBER,2017-11-22T21:00:20Z
14801,"On the way to latest CMake, VS2017, CUDA 9, cudNN 7, Win10",stat:community support,"As many of us (#14126,#14691,#12052), I am trying to get TF1.4 build successfully on windows using the latest version of everything. As far as I can judge **I could do it** but with some hacks. As it is too long for me to complete, I would like to share what I did for help finalizing. It is too early for a PR.

I am using CMake 3.9.6 (though 3.10 came out). I have low cmake skill level.
I am not trying the python bindings.
VS2017 is the community edition.

Without GPU it is easy. The only issue is the heap overflow (C1002 or C1006 #11096). The trick is to reduce parallel build by `msbuild /m:4 /p:CL_MPCount=2 ...` such that 4*2 is approximately the number of core you really have (at least it worked for me). Using `/Zm2000` did not work for me, despite a lot of available memory (32G).

With GPU it is more tricky: the `tf_core_gpu_kernels.vcxproj` does not compile at all. AFAIU, the CMake strategy changed from v3.6, to allow parallel computing. CUDA is now treated as another language. Without modifications nvcc simply returns with code error 1 (or nothing happen I am not sure). Here are my modifications (from v1.4).

From `tensorflow/tensorflow/contrib/cmake/`
1/ adapt `cmakelists.txt` a little: 
- Change `CUDA 8.0` to `CUDA 9.0` l.223.
- Add `enable_language(""CUDA"")` l.224.
- **The `set(CUDA_NVCC_FLAGS ...)` directives do not work anymore**. See below.
- Add capabilities 6.0 and 6.1 in l.232, as well l.246. Might not be needed (it is only for performance).
- Change `64_80` to `64_90` and `64_6` to `64_7` l.247 and 248, similarly in l.272-276.

2/ in `tf_core_kernels.cmake`:
- Add `set_source_files_properties(${tf_core_gpu_kernels_srcs} PROPERTIES LANGUAGE CUDA)` to recognize '.cu.cc' extensions as cuda files in l.209.
- Rename `cuda_add_library(...)` as `add_library(...)` l.210.

3/ edit **(this is the trick)** `tf_core_gpu_kernels.vcxproj`, in the release section:
- Encompass cl.exe flags, ie `/bigobj /nologo ... -Ob2`  with the `-Xcompiler=""/bigobj ... -Ob2""` directive l.147. These former flags are for the c++ compiler not for nvcc and result in the crash.
- Add just before `--expt-relaxed-constexpr`, still in the `AdditionalOptions`.
- Switch `PerformDeviceLink`from `false` to `true` l.164.

Then everything compile (msbuild on  tf_tutorials_example_trainer.vcxproj) (and this tuto works). The remaining point before PR is to avoid third step, i.e. give the right directives to nvcc, by understanding how the CUDA_NVCC_FLAGS works, and add the linking. Hope this solution will work without missing symbols (#6396).

Otherwise it is a nightmare: both CUDA 8 and CMake 3.6 are not aware of VS2017. CMake compilation is not incremental (#14194) and takes about 4-5H (could use precompiled headers especially in tf_core_kernels)...
",1,,8,2017-11-22T16:30:20Z,2017-11-30T00:49:50Z,NONE,2017-11-28T02:19:40Z
14800,Potential memory leak from deleting array and closing file handler,"stat:contributions welcome,type:bug/performance","Here are couple of minor memory leak for review.
1. https://github.com/tensorflow/tensorflow/blob/6c95675492aa8d25619f5e4ce1674582c051a7fe/tensorflow/c/c_api.cc#L569-L593 ""delete []base;"" looks missing.
   

2. https://github.com/tensorflow/tensorflow/blob/6c95675492aa8d25619f5e4ce1674582c051a7fe/tensorflow/core/lib/io/snappy/snappy_outputbuffer.cc#L164-L173  ""delete []compressed_length_array;"" looks missing when macro TF_RETURN_IF_ERROR() fails.

3. https://github.com/tensorflow/tensorflow/blob/6c95675492aa8d25619f5e4ce1674582c051a7fe/tensorflow/core/platform/profile_utils/android_armv7a_cpu_utils_helper.cc#L113-L123 Two potential problems:
    a. There is no ""fclose()"" being called after fscanf() fails
    b. ""fclose()"" could be called instead of ""pclose()""

4. https://github.com/tensorflow/tensorflow/blob/6c95675492aa8d25619f5e4ce1674582c051a7fe/tensorflow/tools/proto_text/gen_proto_text_functions.cc#L132-L137 When ""fwrite() fails"", ""fclose()"" could be called before ""return -1"".

PS: I don't have handy working environment setup yet, currently browsing code may be better fit for me.",0,,8,2017-11-22T16:13:41Z,2017-12-11T01:44:54Z,NONE,2017-11-22T16:42:05Z
14798,Provide a list of supported XLA operations like TensorFlow Lite,"stat:awaiting tensorflower,type:docs",TensorFlow Lite provides a list of currently supported ops [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md) and I wonder if XLA could also have such a list. It's rough to develop and train a model with the full TensorFlow Python API only to get stuck during AOT compilation because of missing ops kernels in the tf2xla bridge.,1,,2,2017-11-22T14:54:06Z,2017-12-11T18:33:14Z,CONTRIBUTOR,2017-12-11T18:33:14Z
14797,XLA AOT tfcompile failure due to undeclared inclusions in cc_binary rule,,"This happens on a freshly cloned TensorFlow master with Bazel 0.7 on Ubuntu 17.04:

```sh
ERROR: tensorflow/BUILD:13:1: undeclared inclusion(s) in rule '//:model':
this rule is missing dependency declarations for the following files included by 'graph.cc':
  'tensorflow/compiler/tf2xla/xla_compiled_cpu_function.h'
  'tensorflow/compiler/tf2xla/xla_local_runtime_context.h'
  'tensorflow/core/platform/macros.h'
  '/tensorflow/core/platform/types.h'
  '/tensorflow/core/platform/platform.h'
  '/tensorflow/core/platform/default/integral_types.h'
  '/tensorflow/compiler/xla/executable_run_options.h'
```
graph.cc pretty much just does `#include ""graph.h""` as per the tfcompile tutorial and it's weird because these headers [seem to be included](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/tfcompile.bzl#L209-L230) in the tf_library rule but Bazel still complains that the subsequent cc_binary rule doesn't list them as dependencies.

This is my BUILD file, placed in the repo root (so I use TensorFlow's Bazel workspace after going through ./configure):
```sh
load(""@org_tensorflow//tensorflow/compiler/aot:tfcompile.bzl"", ""tf_library"")

tf_library(
  name = ""graph"",
  cpp_class = ""Graph"",
  graph = ""graph.pb"",
  config = ""graph.config.pb"",
)

cc_binary(
  name = ""model"",
  srcs = [""graph.cc""],
  deps = ["":graph"", ""//third_party/eigen3""],
  linkopts = [""-lpthread""]
)
```
I'm not comfortable with Bazel yet but building worked fine with earlier TensorFlow versions. Stuff started to become wonky somewhere around when @org_tensorflow was introduced throughout tfcompile.bzl, I think.",0,,1,2017-11-22T14:45:08Z,2017-11-22T15:42:51Z,CONTRIBUTOR,2017-11-22T15:42:51Z
14795,TypeError: __call__() got an unexpected keyword argument 'input_c',type:support,"I am using `tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops.CudnnGRU` as cudnn_cell.
But when I call cudnn_cell as follows
`  	    hiddens, output_h, output_c = cudnn_cell(
   	        inputs,
   	        input_h=init_state,
   	        input_c=init_state,
   	        params=cudnn_params,
 	        is_training=True)
`, an error occurs saying that input_c was an unexpected keyword.
But I have checked the source code and I'am certain that there is a keyword argument 'input_c'.",0,,2,2017-11-22T13:04:04Z,2017-11-27T19:28:53Z,NONE,2017-11-23T01:30:15Z
14794,Remove `non-fused` version of `adjust_saturation` as GPU kernel already exists,"awaiting testing (then merge),cla: yes","In the existing implementation for `adjust_saturation` the non-fused version was still in place. As the non-fused is for non-GPU support of `adjust_saturation` and GPU kernel already exists now (See commit 25c4f27#diff-b53c223158b7c4fd248ef581da6566c2), it makes sense to remove the non-fused version.

In addition, with the removal of non-fused implementation of `adjust_saturation`, now it is possible to provide batch support (in 4-D instead of previous 3-D). This resolves issue raised in #8926.

This fix removed non-fused version of `adjust_saturation` and added additional test cases for batch support.

Note: In PR #14187, non-fused version of `adjust_hue` has been removed so batch support for `adjust_hue` has been enabled as well. This PR also adds additional test cases for batch support of `adjust_hue`.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,3,2017-11-22T12:48:44Z,2017-12-10T22:13:39Z,MEMBER,2017-11-22T22:24:03Z
14793,[feature request] custom GraphKeys QUEUE_RUNNERS for input pipeline,stat:awaiting response,"i find no perfect answer about using input pipeline to train and eval in same Session
===========
[switch input pipeline at stackoverflow](eg: https://stackoverflow.com/questions/41162955/tensorflow-queues-switching-between-train-and-validation-data)
===========
if we define different input pipeline for train and eval,  the following code will start both train and eval input pipeline, that is not we want
```
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess, coord=coord)
```
if we can custom GraphKeys.QUEUE_RUNNERS  collection for different input pipeline , i think we can start input pipeline through parameter of collection.
`    tf.train.add_queue_runner(qr, collection=tf.GraphKeys.QUEUE_RUNNERS)`
`   eg: tf.train.add_queue_runner(qr, collection=tf.GraphKeys.TRAIN_QUEUE_RUNNERS)`
`   eg: tf.train.add_queue_runner(qr, collection=tf.GraphKeys.EVAL_QUEUE_RUNNERS)`
is it right ?  
@mrry  @all 
thanks ",0,,4,2017-11-22T11:55:21Z,2017-12-19T08:09:40Z,NONE,2017-11-28T02:59:30Z
14792,modified convolution document,"awaiting testing (then merge),cla: yes","fix #14027.

Document of _MaskedConv and MaskedConv2D could be revised too. Reasons are as follows.

https://github.com/tensorflow/tensorflow/blob/5fbda9d8da7b98f62e83a392f047adf307b48b02/tensorflow/contrib/model_pruning/python/layers/core_layers.py#L164-L171

https://github.com/tensorflow/tensorflow/blob/5fbda9d8da7b98f62e83a392f047adf307b48b02/tensorflow/contrib/model_pruning/python/layers/core_layers.py#L438-L447",1,,2,2017-11-22T11:27:22Z,2017-11-30T19:05:33Z,CONTRIBUTOR,2017-11-23T02:44:24Z
14791,TFLite: get closer to build with Bazel on Windows,"awaiting testing (then merge),cla: yes","Bazel cannot yet build TensorFlow Lite on Windows,
but this commit gets us closer.

In this commit:
- make the -Wno-implicit-fallthrough compiler flag
  in flatbuffers' BUILD file be conditional to
  non-Windows builds, because MSVC doesn't know
  this flag
- fix the Bazel build command in README.md by
  removing single quotes around --cxxflags,
  because it's not needed on Bash and is harmful
  on Windows (because cmd.exe doesn't remove the
  single quotes)
- fix non-ASCII quotes and apostrophes, as well as
  some formatting issues in README.md

See https://github.com/bazelbuild/bazel/issues/4148",0,,8,2017-11-22T10:57:01Z,2017-11-27T22:40:31Z,CONTRIBUTOR,2017-11-22T10:57:40Z
14789,Add templated functions for `safe_strto[f|d|i32|i64|u32|u64]`,"awaiting testing (then merge),cla: yes","While working on #14330 I noticed that there is no templated functions for `safe_strto[f|d|i32|i64|u32|u64]`. As a result, an additional wrapper has to be created in different places to apply the typename in a templated class or function.

Examples of the wrappers include the existing implementations in `tensorflow/core/kernels/string_to_number_op.cc` (`Convert`), `tensorflow/core/lib/strings/proto_text_util.h` (`ProtoParseNumeric`), and in #14330.

It might make sense to add a templated function for `safe_strto[f|d|i32|i64|u32|u64]` to avoid existing and future code duplications.

This fix adds
```
template<typename T>
bool SafeStringToNumeric(StringPiece s, T* value);
```
to address the above mentioned issue.

Note: If this PR is merged then #14330 will needs to be updated accordingly.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,2,2017-11-22T09:38:30Z,2017-12-27T02:46:18Z,MEMBER,2017-12-13T18:25:47Z
14788,tf.print makes a variable a constant?,,"```
In [1]: import tensorflow as tf

In [2]: # using print

In [3]: entcoeff =  tf.Variable([0], dtype=tf.float32, trainable=False)
   ...: entcoeff = tf.Print(entcoeff,[entcoeff,""printing""])

In [4]: tf.assign(entcoeff, [-1.])
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-4-dd57efca5923> in <module>()
----> 1 tf.assign(entcoeff, [-1.])

/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py in assign(ref, value, validate_shape, use_locking, name)
    270         ref, value, use_locking=use_locking, name=name,
    271         validate_shape=validate_shape)
--> 272   return ref.assign(value)

AttributeError: 'Tensor' object has no attribute 'assign'

In [5]: # not using print

In [6]: entcoeff =  tf.Variable([0], dtype=tf.float32, trainable=False)

In [7]: tf.assign(entcoeff, [-1.])
Out[7]: <tf.Tensor 'Assign:0' shape=(1,) dtype=float32_ref>

```",1,,11,2017-11-22T09:14:17Z,2017-12-07T15:29:39Z,NONE,2017-11-22T11:57:25Z
14785,Update layers pull request,"awaiting testing (then merge),cla: yes","I have created a new pull request by updating the unit test cases with reference to my previous pull request
https://github.com/tensorflow/tensorflow/pull/13829 .
I have already updated layers.py initially by checking for beta in the if condition.
The initial pull request was raised in accordance with the issue 
https://github.com/tensorflow/tensorflow/issues/11673 .

Please verify and get back.",0,,9,2017-11-22T06:53:26Z,2017-12-21T22:44:22Z,CONTRIBUTOR,2017-11-24T09:42:06Z
14781,Improve variance_scaling_initializer description,"awaiting testing (then merge),cla: yes","Added mention of the ""MRSA initialization"" alias. This name has been mentioned in [multiple publications](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=%22msra+initialization%22&btnG=). Mentioning it in the docs will make this initializer easier to find.",0,,5,2017-11-22T04:48:02Z,2017-12-10T20:56:13Z,CONTRIBUTOR,2017-11-22T16:53:27Z
14779,"""error in tensorflow setup command"" error when running building the TensorFlow pip package",stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: TensorFlow commit 70ba44b46bb9e5f5e55b2357676ffa7196b9bda7
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: 4.8.4
- **CUDA/cuDNN version**: CUDA 8.0, cuDNN 6.0
- **GPU model and memory**: GTX 1080 8 GB
- **Exact command to reproduce**:
```
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --config=monolithic
bazel-bin/tensorflow/tools/pip_package/build_pip_package ~/tensorflow_pkg/
```

### Describe the problem
When I try to build TensorFlow at commit 70ba44b46bb9e5f5e55b2357676ffa7196b9bda7 or later, I get the following error after running `bazel-bin/tensorflow/tools/pip_package/build_pip_package ~/tensorflow_pkg`:

```
reedwm@reedwm2:~/tensorflow_test$ bazel-bin/tensorflow/tools/pip_package/build_pip_package ~/tensorflow_ec2_pkg/
Tue Nov 21 18:22:34 PST 2017 : === Using tmpdir: /tmp/tmp.m24Ub0Z2z4
~/tensorflow_test/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/tensorflow_test
~/tensorflow_test
/tmp/tmp.m24Ub0Z2z4 ~/tensorflow_test
Tue Nov 21 18:22:35 PST 2017 : === Building wheel
error in tensorflow setup command: 'install_requires' must be a string or list of strings containing valid project/version requirement specifiers
```

This does not occur on the commit before 70ba44b46bb9e5f5e55b2357676ffa7196b9bda7. When running `./configure`, I choose the default option for everything except that I choose to use CUDA.

Note I use `--config=monolithic` to get around #13243.

/CC @alanhdu @gunan, any ideas what the issue could be?",1,,23,2017-11-22T03:47:05Z,2018-01-08T19:15:29Z,MEMBER,2017-11-27T16:34:58Z
14774,Load boundaries array into shared memory before hand for `bucketize`,"awaiting testing (then merge),cla: yes","This fix is a follow up to #13922. This fix loads boundaries array into shared memory before each thread, in order to improve performance for `bucketize` op.

The fix is based on feedback (https://github.com/tensorflow/tensorflow/pull/13922#discussion_r150058312).

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,6,2017-11-21T23:30:52Z,2017-12-16T00:06:14Z,MEMBER,2017-11-21T23:31:58Z
14773,Upgrade cuda to 9 and cudnn version to 7.,cla: yes,,1,,15,2017-11-21T22:44:58Z,2017-12-08T07:19:46Z,OWNER,2017-11-22T03:03:07Z
14770,Update Eigen hash for fix of fp16 predux bug,"cla: yes,stat:awaiting response","Attention: @benoitsteiner and @zheng-xq 

For Maxwell and earlier GPUs, Eigen was incorrectly casting fp16 values to
unsigned int during some reductions. This causes incorrect results in
Tensorflow's xent and sparse_xent ops when applied to fp16 data.",1,,9,2017-11-21T21:35:01Z,2017-12-06T22:28:41Z,CONTRIBUTOR,2017-11-28T20:53:22Z
14769,Type Serialization in as_graph_def function,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:  v1.3.0-rc2-20-g0787eee
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**: 0.7
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 8.1.0 (clang-802.0.42)
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Problem is that as_graph_def sometimes serialize the type information and sometimes doesn't.

### Source code / logs
```
import tensorflow as tf

# bool typed Op, no type serialized
x = tf.placeholder(tf.bool)
y = tf.placeholder(tf.bool)
op = tf.logical_or(x, y)
print op.graph.as_graph_def(add_shapes=True)

# float typed Op, type serialized
x = tf.placeholder(tf.float32)
y = tf.placeholder(tf.float32)
op = tf.add(x, y)
print op.graph.as_graph_def(add_shapes=True)
```

node for logical_or is, note no `T` in attr:
```
node {
  name: ""LogicalOr""
  op: ""LogicalOr""
  input: ""Placeholder""
  input: ""Placeholder_1""
  attr {
    key: ""_output_shapes""
    value {
      list {
        shape {
          unknown_rank: true
        }
      }
    }
  }
}
```

node for add is, note with `T` in attr:
```
node {
  name: ""Add""
  op: ""Add""
  input: ""Placeholder_2""
  input: ""Placeholder_3""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_output_shapes""
    value {
      list {
        shape {
          unknown_rank: true
        }
      }
    }
  }
}
```
If you print out `print tf.logical_or(x, y)`, the output is 
```
Tensor(""LogicalOr_1:0"", dtype=bool)
```
So problem might be with the serialization?
This seems like a bug to me.
",0,,1,2017-11-21T21:01:51Z,2017-11-30T22:49:05Z,CONTRIBUTOR,2017-11-30T22:49:05Z
14768,Feature request: Control order of 'feature_column.input_layer',,"On master (80e7c9f45c).

It seems that the mapping of features to columns in dense the input matrix is always sorted by the alphabetical order of the feature names. It would be nice is this was customizable, perhaps by respecting the order of the feature columns in the second argument to `input_layer`. Mainly useful  for debugging and introspecting the network to know which columns correspond to which features.

eg:

```python
sess.run(tf.feature_column.input_layer({'a': [1], 'b': [2]}, [tf.feature_column.numeric_column('a'), tf.feature_column.numeric_column('b')]))
```

gives `[1, 2`],

as does switching the order of the feature columns:

```python
sess.run(tf.feature_column.input_layer({'a': [1], 'b': [2]}, [tf.feature_column.numeric_column('b'), tf.feature_column.numeric_column('a')]))
```

I also tried giving the features as an `OrderedDict`, but `input_layer` doesn't seem to care about the ordering in that  situation either. 
",0,,7,2017-11-21T19:28:10Z,2017-11-27T17:23:12Z,CONTRIBUTOR,2017-11-22T09:05:20Z
14766,"Revert ""Fixed typo in usage docstring""",cla: yes,"Reverts tensorflow/tensorflow#14765

Sorry didn't realize this was sent to r1.2. @millskyle please fix the issue in master. Thanks.",0,,1,2017-11-21T19:15:27Z,2017-11-21T19:17:25Z,MEMBER,2017-11-21T19:25:47Z
14765,Fixed typo in usage docstring,cla: yes,Changed tf.SyncReplicasOptimizer to tf.train.SyncReplicasOptimizer in usage example.,0,,5,2017-11-21T18:43:41Z,2017-11-21T18:48:58Z,CONTRIBUTOR,2017-11-21T18:45:51Z
14763,MKL: Adding MKL-DNN graph pass implementation,"awaiting testing (then merge),cla: yes",,1,,5,2017-11-21T17:05:28Z,2017-12-06T20:11:06Z,CONTRIBUTOR,2017-11-29T21:48:47Z
14762,tf.TensorShape concatenation converts shape information to values,,"Tensorflow: 1.4.0

Example code:
```python
import tensorflow as tf
tf.InteractiveSession()
>>> tf.concat([tf.TensorShape([4,1]), tf.constant([1,1,1,1])], 0).eval()
array([4, 1, 1, 1, 1, 1])
```

Is it intended that this works and produces the output where the shape information gets converted into actual values?",0,,5,2017-11-21T14:47:25Z,2017-11-21T15:11:09Z,CONTRIBUTOR,2017-11-21T15:11:09Z
14761,tensorflow lite: error when convert frozen model to lite format,"comp:lite,stat:contributions welcome,type:feature","
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.3.0
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.7.0
- **GCC/Compiler version (if compiling from source)**:
gcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
- **CUDA/cuDNN version**:
cuda8.0/cudnn6.0


I tried to convert squeezenet frozen model to lite format with the following command:
""bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/frozen_model.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/squeezenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=prob --input_shapes=1,227,227,3""

the output is shown below:
2017-11-21 18:35:29.977505: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 170 operators, 231 arrays (0 quantized)
2017-11-21 18:35:29.981856: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 40 operators, 93 arrays (0 quantized)
2017-11-21 18:35:29.982061: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 40 operators, 93 arrays (0 quantized)
2017-11-21 18:35:29.982201: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 4071680 bytes, theoretical optimal value: 4071680 bytes.
2017-11-21 18:35:29.982317: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 0.781679 billion (note that a multiply-add is counted as 2 ops).
2017-11-21 18:35:29.982482: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze

Then I tried to convert mobilenet_v1_1.0_224.pb to lite format, the same error as above.
""bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet_v1_1.0_224.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=output --input_shapes=1,224,224,3""

output:
2017-11-21 22:07:39.747095: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 418 operators, 584 arrays (0 quantized)
2017-11-21 22:07:39.766175: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 31 operators, 88 arrays (0 quantized)
2017-11-21 22:07:39.766390: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 31 operators, 88 arrays (0 quantized)
2017-11-21 22:07:39.766592: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 6422528 bytes, theoretical optimal value: 4816896 bytes.
2017-11-21 22:07:39.766751: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 1.14264 billion (note that a multiply-add is counted as 2 ops).
2017-11-21 22:07:39.766952: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze

Although I installed tensorflow with ""pip install tensorflow-gpu"", in order to convert model to lite format, I git clone the tensorflow files and  configure, bazel to compile the files. I don't know whether this affect the converting of models, but the error is really strange!
",0,,7,2017-11-21T14:41:33Z,2017-12-20T01:41:25Z,NONE,2017-11-21T21:23:26Z
14758,Build Tensorflow Lite C++ API into a dynamic library for Android,"comp:lite,type:feature","Is there any way of building the Tensorflow Lite C++ API into a dynamic library for Android?
I have tried to build with bazel for armv7a but this only gives the corresponding static libraries:
`bazel build -c opt //tensorflow/contrib/lite:* --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=""-std=c++11"" --verbose_failures
`

",1,,3,2017-11-21T12:59:30Z,2017-12-09T01:41:23Z,NONE,2017-11-28T22:52:28Z
14757,change bazel-mirror to mirror.bazel,"cla: yes,stat:awaiting response","grep -v bazel-mirror:
do you mean to not use mirror to download ?

mirror links in workspace.bzl is marked as mirror.bazel , not bazel-mirror.
after this change , the download speed enhanced.

i already signed as contributor",1,,25,2017-11-21T12:54:20Z,2017-11-24T12:52:58Z,NONE,2017-11-21T19:17:50Z
14756,Downgrade to Bazel 0.4.2 for Tensorflow r1.0,type:build/install,"Hello, 

I need to checkout to Tensorflow r1.0 and as suggested the Bazel version should be 0.4.2

I have already installed Bazel and after upgrade bazel version is 0.7.0

Do you know the steps so I can downgrade to Bazel 0.4.2? 

I have tried with apt-get install bazel=0.4.2 but this does not work, 

and I have also tried to uninstall by executing the command rm -fr ~/.bazel ~/.bazelrc and deleting relevant data in ~/.cache/bazel/ folder, but this did not also work.

Any suggestions ?

Thank you in advance",0,,1,2017-11-21T12:52:55Z,2017-11-21T18:09:18Z,NONE,2017-11-21T18:09:18Z
14755,Problem with assigning values to matrix indices in tensorflow,,"Hi,

I am constructing a NN with tensorflow that uses a custom stddev function. I have for a batch and indices i and j a function `AcrossBatchSD(batch, i, j)`. Of course, `import tensorflow as tf`.

```
def AcrossBatchSD(batch, i, j):

    _, varR = tf.nn.moments(batch[:, i, j, 0], axes=[0])
    _, varG = tf.nn.moments(batch[:, i, j, 1], axes=[0])
    _, varB = tf.nn.moments(batch[:, i, j, 2], axes=[0]) 
 
    return tf.sqrt(varR), tf.sqrt(varG), tf.sqrt(varB)
```

This function seems to work, but then I would like to do the following:

```
def MinibatchStdDev(batch, window, mb_size):

    n = batch[0].shape[0].value 
    f1 = tf.Variable(tf.zeros([n, n]))
    f2 = tf.Variable(tf.zeros([n, n]))
    f3 = tf.Variable(tf.zeros([n, n]))

    for i in range(n):
        for j in range(n):

            sqrtR, sqrtG, sqrtB = AcrossBatchSD(batch, i, j)          
            f1[i, j].assign(sqrtR)
            f2[i, j].assign(sqrtG)
            f3[i, j].assign(sqrtB) 
                
    f = tf.divide(tf.add(tf.add(f1, f2), f3), 3)
    F = tf.reduce_mean(f)

    return tf.multiply(F, tf.ones(([mb_size, window, window]))))
```

My problem is that the function `assign` in tensorflow is not differentiable, so `tf.gradients` will outout a `NoneType`. Therefore, my network cannot be trained.

A test can be done with

```
mb_size = 3
window = 4
batch = tf.Variable(tf.random_normal([mb_size, 1024, 1024, 3]), tf.float32)
out = MinibatchStdDev(batch=batch, window=window, mb_size=mb_size)
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
sess.run(out)
```",0,,1,2017-11-21T12:42:01Z,2017-12-01T00:20:55Z,NONE,2017-12-01T00:20:55Z
14751,update how_tos/reading_data to use Dataset API,"awaiting testing (then merge),cla: yes","Since the Dataset API moved from .contrib.data into .data (core) update the MNIST example to use Dataset over queues.

This was the first page I've found when searching how to best get data into TF and I thought it should reflect the current best practice. Also, if I understand correctly, queues might be deprecated in favor of Datasets some time in the future.",0,,9,2017-11-21T11:43:50Z,2017-12-11T00:58:37Z,CONTRIBUTOR,2017-11-29T09:23:07Z
14748,[XLA] Use the specific float list rather than a hard coded list,"awaiting review,cla: yes,stat:awaiting response","This change makes the test use the list of supported types for the backend under test, rather than a hard coded list.

",0,,5,2017-11-21T09:06:15Z,2017-12-05T12:56:12Z,CONTRIBUTOR,2017-12-04T03:32:20Z
14745,execute command properly in bash.exe on windows,cla: yes,"On windows using bazel:

```
# doesn't work
C:\Windows\system32>C:\msys64\usr\bin\bash.exe -c ""patch --help""
/usr/bin/bash: patch: command not found

# works properly
C:\Windows\system32>C:\msys64\usr\bin\bash.exe -l -c ""patch --help""
```

After adding `-l`, it works. 

```
...

E:\>cd tensorflow_dev

E:\tensorflow_dev>cd tensorflow

E:\tensorflow_dev\tensorflow>cd test_bazel

E:\tensorflow_dev\tensorflow\test_bazel>bazel build :test_bazel
...............
____Loading package: tensorflow/test_bazel
____Loading package: @bazel_tools//tools/cpp
____Loading package: @bazel_tools//tools/jdk
____Loading package: @local_config_xcode//
____Loading package: @local_jdk//
DEBUG: C:/users/win7-vm/appdata/local/temp/_bazel_win7-vm/9apswmfr/external/baze
l_tools/tools/cpp/lib_cc_configure.bzl:37:3:
Auto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest
Visual C++ installed.

____Loading package: @local_config_cc//
DEBUG: C:/users/win7-vm/appdata/local/temp/_bazel_win7-vm/9apswmfr/external/baze
l_tools/tools/cpp/lib_cc_configure.bzl:37:3:
Auto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variabl
es,eg. VS140COMNTOOLS

DEBUG: C:/users/win7-vm/appdata/local/temp/_bazel_win7-vm/9apswmfr/external/baze
l_tools/tools/cpp/lib_cc_configure.bzl:37:3:
Auto-Configuration Warning: Visual C++ build tools found at C:\Program Files (x8
6)\Microsoft Visual Studio 14.0\VC\

____Loading complete.  Analyzing...
____Downloading https://mirror.bazel.build/github.com/google/protobuf/archive/b0
4e5cba356212e4e8c66c61bbe0c3a20537c5b9.tar.gz: 713,130 bytes
...
ERROR: E:/tensorflow_dev/tensorflow/test_bazel/BUILD:3:1: error loading package
'tensorflow': Encountered error while reading extension file 'protobuf.bzl': no
such package '@protobuf_archive//': Traceback (most recent call last):
        File ""E:/tensorflow_dev/tensorflow/workspace.bzl"", line 119
                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)
        File ""E:/tensorflow_dev/tensorflow/workspace.bzl"", line 111, in _apply_p
atch
                _execute_and_check_ret_code(repo_ctx, cmd)
        File ""E:/tensorflow_dev/tensorflow/workspace.bzl"", line 92, in _execute_
and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'C:\msys64\usr\bin\bash.exe -c patch -p
1 -d C:/users/win7-vm/appdata/local/temp/_bazel_win7-vm/9apswmfr/external/protob
uf_archive -i E:/tensorflow_dev/third_party/protobuf/add_noinlines.patch':
Stdout:
Stderr: /usr/bin/bash: patch: command not found
 and referenced by '//tensorflow/test_bazel:test_bazel'.
ERROR: E:/tensorflow_dev/tensorflow/test_bazel/BUILD:3:1: error loading package
'tensorflow': Encountered error while reading extension file 'protobuf.bzl': no
such package '@protobuf_archive//': Traceback (most recent call last):
        File ""E:/tensorflow_dev/tensorflow/workspace.bzl"", line 119
                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)
        File ""E:/tensorflow_dev/tensorflow/workspace.bzl"", line 111, in _apply_p
atch
                _execute_and_check_ret_code(repo_ctx, cmd)
        File ""E:/tensorflow_dev/tensorflow/workspace.bzl"", line 92, in _execute_
and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'C:\msys64\usr\bin\bash.exe -c patch -p
1 -d C:/users/win7-vm/appdata/local/temp/_bazel_win7-vm/9apswmfr/external/protob
uf_archive -i E:/tensorflow_dev/third_party/protobuf/add_noinlines.patch':
Stdout:
Stderr: /usr/bin/bash: patch: command not found
 and referenced by '//tensorflow/test_bazel:test_bazel'.
ERROR: Analysis of target '//tensorflow/test_bazel:test_bazel' failed; build abo
rted: error loading package 'tensorflow': Encountered error while reading extens
ion file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most
recent call last):
        File ""E:/tensorflow_dev/tensorflow/workspace.bzl"", line 119
                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)
        File ""E:/tensorflow_dev/tensorflow/workspace.bzl"", line 111, in _apply_p
atch
                _execute_and_check_ret_code(repo_ctx, cmd)
        File ""E:/tensorflow_dev/tensorflow/workspace.bzl"", line 92, in _execute_
and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'C:\msys64\usr\bin\bash.exe -c patch -p
1 -d C:/users/win7-vm/appdata/local/temp/_bazel_win7-vm/9apswmfr/external/protob
uf_archive -i E:/tensorflow_dev/third_party/protobuf/add_noinlines.patch':
Stdout:
Stderr: /usr/bin/bash: patch: command not found
.
____Elapsed time: 20.658s

E:\tensorflow_dev\tensorflow\test_bazel>
```",1,,5,2017-11-21T07:54:52Z,2017-11-22T20:46:40Z,CONTRIBUTOR,2017-11-21T07:56:13Z
14743,The API doc for tensorflow.keras.backend.set_learning_phase is wrong.,"stat:awaiting response,type:bug/performance","### System information
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
Python 2.7.13 :: Anaconda, Inc.

### Describe the problem
When I implemented Custom Estimator API with tf.keras, I countered following error: 

```python
TypeError: `pred` must be a Tensor, a Variable, or a Python bool.
```

This happens if I set {0, 1} to tensorflow.keras.backend.set_learning_phase, and doesn't happen if I set {False, True} instead of {0, 1}. However, [the API doc](https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_learning_phase) for tensorflow.keras.backend.set_learning_phase says the method is supposed to take {0, 1}. So, I think that the API doc should be modified.

### Source code / logs
- Source code (You can find more details [here](https://github.com/tensorflow/tensorflow/files/1490186/How.to.integrate.keras.into.Experiment.pdf))

```python
def inference(images, mode):
  if mode == tf.estimator.ModeKeys.TRAIN:
    tf.keras.backend.set_learning_phase(1) # this should be True
  else:
    tf.keras.backend.set_learning_phase(0) # this should be False
        
  model = tf.keras.models.Sequential()
  # Define input tensor in Keras world.
  model.add(tf.keras.layers.InputLayer(input_tensor=images))

  # The first convolutional layer.
  model.add(tf.keras.layers.Conv2D(
      filters=32, kernel_size=(3, 3), padding='same', activation='relu'))
  model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'))

  # The second convolutional layer.
  model.add(tf.keras.layers.Conv2D(
      filters=32, kernel_size=(3, 3), padding='same', activation='relu'))
  model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'))
  model.add(tf.keras.layers.Dropout(0.25))

  # The third convolutional layer
  model.add(tf.keras.layers.Conv2D(
      filters=64, kernel_size=(3, 3), padding='same', activation='relu'))

  # The fourth convolutional layer
  model.add(tf.keras.layers.Conv2D(
      filters=64, kernel_size=(3, 3), padding='same', activation='relu'))
  model.add(tf.keras.layers.Dropout(0.25))

  model.add(tf.keras.layers.Flatten())
  model.add(tf.keras.layers.Dense(512, activation='relu'))
  model.add(tf.keras.layers.Dropout(0.5))
  model.add(tf.keras.layers.Dense(NUM_CLASSES))
  logits = model.output
  return logits
```




- Error logs
```python
TypeError                                 Traceback (most recent call last)
<ipython-input-38-4dbe7b3f6667> in <module>()
     20   schedule='train_and_evaluate',
     21   run_config=run_config,
---> 22   hparams=hparams
     23 )
     24 

/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.pyc in run(experiment_fn, output_dir, schedule, run_config, hparams)
    216   schedule = schedule or _get_default_schedule(run_config)
    217 
--> 218   return _execute_schedule(experiment, schedule)
    219 
    220 

/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.pyc in _execute_schedule(experiment, schedule)
     44     logging.error('Allowed values for this experiment are: %s', valid_tasks)
     45     raise TypeError('Schedule references non-callable member %s' % schedule)
---> 46   return task()
     47 
     48 

/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.pyc in train_and_evaluate(self)
    623                   hooks=self._eval_hooks)
    624           ]
--> 625       self.train(delay_secs=0)
    626 
    627     # If the checkpoint_and_export flag and appropriate estimator configuration

/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.pyc in train(self, delay_secs)
    365     return self._call_train(input_fn=self._train_input_fn,
    366                             max_steps=self._train_steps,
--> 367                             hooks=self._train_monitors + extra_hooks)
    368 
    369   def evaluate(self, delay_secs=None, name=None):

/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.pyc in _call_train(self, _sentinel, input_fn, steps, hooks, max_steps)
    805                                    steps=steps,
    806                                    max_steps=max_steps,
--> 807                                    hooks=hooks)
    808     else:
    809       return self._estimator.fit(input_fn=input_fn,

/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    300 
    301     saving_listeners = _check_listeners_type(saving_listeners)
--> 302     loss = self._train_model(input_fn, hooks, saving_listeners)
    303     logging.info('Loss for final step: %s.', loss)
    304     return self

/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _train_model(self, input_fn, hooks, saving_listeners)
    709       with ops.control_dependencies([global_step_read_tensor]):
    710         estimator_spec = self._call_model_fn(
--> 711             features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
    712       # Check if the user created a loss summary, and add one if they didn't.
    713       # We assume here that the summary is called 'loss'. If it is not, we will

/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _call_model_fn(self, features, labels, mode, config)
    692     if 'config' in model_fn_args:
    693       kwargs['config'] = config
--> 694     model_fn_results = self._model_fn(features=features, **kwargs)
    695 
    696     if not isinstance(model_fn_results, model_fn_lib.EstimatorSpec):

<ipython-input-29-a5666390b8b0> in cifar10_model_fn(features, labels, mode, params)
     10 
     11   # Calculate logits through CNN
---> 12   logits = inference(images, mode)
     13 
     14   # Get predictions

<ipython-input-37-23187df0ff6c> in inference(images, mode)
     20 
     21     # NOTE: Dropout is not working with model_fn in TF1.4
---> 22     model.add(tf.keras.layers.Dropout(0.25))
     23 
     24     # The third convolutional layer

/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/models.pyc in add(self, layer)
    499           output_tensors=self.outputs)
    500     else:
--> 501       output_tensor = layer(self.outputs[0])
    502       if isinstance(output_tensor, list):
    503         raise TypeError('All layers in a Sequential model '

/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.pyc in __call__(self, inputs, **kwargs)
    250     """"""
    251     # Actually call the layer (optionally building it).
--> 252     output = super(Layer, self).__call__(inputs, **kwargs)
    253 
    254     # Update learning phase info.

/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.pyc in __call__(self, inputs, *args, **kwargs)
    573         if in_graph_mode:
    574           self._assert_input_compatibility(inputs)
--> 575         outputs = self.call(inputs, *args, **kwargs)
    576 
    577         if outputs is None:

/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/layers/core.pyc in call(self, inputs, training)
    116     if training is None:
    117       training = K.learning_phase()
--> 118     output = super(Dropout, self).call(inputs, training=training)
    119     if training is K.learning_phase():
    120       output._uses_learning_phase = True  # pylint: disable=protected-access

/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/core.pyc in call(self, inputs, training)
    298     return utils.smart_cond(training,
    299                             dropped_inputs,
--> 300                             lambda: array_ops.identity(inputs))
    301 
    302 

/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/utils.pyc in smart_cond(pred, fn1, fn2, name)
    201     raise TypeError('`fn2` must be callable.')
    202 
--> 203   pred_value = constant_value(pred)
    204   if pred_value is not None:
    205     if pred_value:

/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/utils.pyc in constant_value(pred)
    231     pred_value = tensor_util.constant_value(pred)
    232   else:
--> 233     raise TypeError('`pred` must be a Tensor, a Variable, or a Python bool.')
    234   return pred_value

TypeError: `pred` must be a Tensor, a Variable, or a Python bool.
```",1,,4,2017-11-21T06:13:07Z,2018-01-09T23:25:01Z,NONE,2017-11-25T06:56:17Z
14742,foldl and foldr gives different results on gpu vs cpu in tensorflow 1.4,type:bug/performance,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: - Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: - Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: - source
- **TensorFlow version (use command below)**: v1.4.0-3-g5addbae, 1.4.0
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**:  0.7.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: default 
- **GPU model and memory**: 1080ti (11GB)
- **Exact command to reproduce**:  
See code below

### Describe the problem
While testing foldl and foldr, I get the expected result when run on cpu, but get a zero result when running on gpu.  

### Source code / logs
import tensorflow as tf

with tf.device('/gpu:0'):
    els = tf.constant([1.0,2.0,3.0])
    f = tf.foldl(lambda a, x: a + x, els)

with tf.Session() as sess:
    print tf.GIT_VERSION,tf.VERSION
    print sess.run([els,f])

------------
Result: 
v1.4.0-3-g5addbae 1.4.0
[array([ 1.,  2.,  3.], dtype=float32), 0.0]

The last number should be 6.0, ie the sum of the input array.  I get this if I change the device to /cpu:0

",1,,7,2017-11-21T04:32:32Z,2018-01-24T22:58:56Z,NONE,2017-12-06T16:34:30Z
14741,Have tf-nightly depend on tb-nightly,cla: yes,"TensorBoard now has an automated nightly release process!

https://pypi.python.org/pypi/tb-nightly/",1,,2,2017-11-21T03:40:36Z,2017-11-22T01:03:51Z,MEMBER,2017-11-21T21:15:44Z
14740,Tensorflow lite doesn't support Gather Op with multiple dims?,comp:lite,"Hello,

Following up this SO question which didn't get too much attention:
https://stackoverflow.com/questions/47321911/cant-convert-model-to-tensorflows-lite-format
I'm filling this form as a question/feature request.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS, builing and trying to use TF Lite for iOS
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: Latest, made a pull from HEAD 3 days ago
- **Python version**:  2.7.14
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: clang 9.0.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 

`bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/Users/valentinradu/Playgrounds/char-rnn-tensorflow/remote_save/latest/graph_frz.pb' '--output_file=/Users/valentinradu/Playgrounds/char-rnn-tensorflow/remote_save/latest/graph.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--input_type=FLOAT' '--inference_type=FLOAT' '--input_shapes=1,128:1,50,50' '--input_arrays=state_in,data_in' '--output_arrays=state_out,data_out'`

### Describe the problem
I have a trained rnn that I try to use on mobile. Problem is, when I use toco to convert my .pb file to .tflite it fails with the following error message. Having a look over the source code that generated that exception, I think it's because of the toco's lack of support for multidimensional inputs. But I'm not sure. If so, will this be added later?

```
WARNING: Config values are not defined in any .rc file: opt.
INFO: Found 1 target...
Target //tensorflow/contrib/lite/toco:toco up-to-date:
  bazel-bin/tensorflow/contrib/lite/toco/toco
INFO: Elapsed time: 0.287s, Critical Path: 0.00s

INFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/Users/valentinradu/Playgrounds/char-rnn-tensorflow/remote_save/latest/graph_frz.pb' '--output_file=/Users/valentinradu/Playgrounds/char-rnn-tensorflow/remote_save/latest/graph.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--input_type=FLOAT' '--inference_type=FLOAT' '--input_shapes=1,128:1,50,50' '--input_arrays=state_in,data_in' '--output_arrays=state_out,data_out'
2017-11-16 06:48:00.156091: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: Fill
2017-11-16 06:48:00.156811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: Fill
2017-11-16 06:48:00.156821: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: Pack
2017-11-16 06:48:00.156829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: Pack
2017-11-16 06:48:00.156841: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: Unpack
2017-11-16 06:48:00.156856: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: StridedSlice
2017-11-16 06:48:00.156872: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: StridedSlice
2017-11-16 06:48:00.157260: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: Pack
2017-11-16 06:48:00.157277: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: Pack
2017-11-16 06:48:00.158053: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 40 operators, 64 arrays (0 quantized)
2017-11-16 06:48:00.158141: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:793] Check failed: indices_shape.dimensions_count() == 1 (2 vs. 1)
```

### Source code / logs
The repository I user to train the model can be found in full here:
https://github.com/valentinradu/char-rnn-tensorflow/blob/master/char_rnn/model.py
",0,,2,2017-11-21T02:48:21Z,2017-11-28T17:23:24Z,NONE,2017-11-28T17:23:24Z
14739,Eager: Warn with invalid policy,type:bug/performance,"If a user accidentally writes `tfe.enable_eager_execution(tfe.DEVICE_PLACEMENT_WARN)` instead of the correct `tfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_WARN)`, they won't get an error until later in their program.

For example, `tfe.num_gpus()` after  the incorrect enable call produces

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-8-71d6509178f5> in <module>()
----> 1 tfe.num_gpus()

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/context.py in num_gpus()
    458     The number of available GPU devices.
    459   """"""
--> 460   return context().num_gpus()

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/context.py in num_gpus(self)
    286   def num_gpus(self):
    287     """"""The number of GPUs available to execute operations.""""""
--> 288     self._initialize_handle_and_devices()
    289     return self._num_gpus
    290 

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/context.py in _initialize_handle_and_devices(self)
    121         with errors.raise_exception_on_not_ok_status() as status:
    122           if self._config is not None:
--> 123             config_str = self._config.SerializeToString()
    124             pywrap_tensorflow.TFE_ContextOptionsSetConfig(
    125                 opts, config_str, len(config_str), status)

AttributeError: 'int' object has no attribute 'SerializeToString'
```

I'd think it makes more sense to throw an error immediately after the incorrect `enable_eager_execution`. 

This is on `master` (ab00df9).",1,,2,2017-11-21T02:31:54Z,2017-12-02T09:39:46Z,CONTRIBUTOR,2017-11-25T06:31:40Z
14736,Does Tensorflow support Graphic for AMD (GPU also)  like NVIDIA (GPU),stat:community support,"Dear All

I have laptop lenovo G50-80 I7 5500 with AMD RADEON R5 M230-2GB for graphic (it is also gpu based on this link ( https://www.futuremark.com/hardware/gpu/AMD+Radeon+R5+M230/review ), does tensorflow support this amd for computing  the same like what tensorflow did with GPU from NVIDIA ?. if it has to be configured from the source, what is the setting of tensorflow that when i compile it it will run the gpu?

Thx",0,,3,2017-11-20T23:57:15Z,2017-12-05T18:27:25Z,NONE,2017-12-05T18:27:25Z
14734,Fixing download_dependencies.sh bugs for generating TFLite iOS exmaples,cla: yes,"This is the 2nd try for #14631

To verify this:
* `git clean -fdx` to clean all local files. 
* run `tensorflow/contrib/lite/download_dependencies.sh`
* Verify that you see ""download_dependencies.sh completed successfully"", so the script is completed. 
* Verify these files are downloaded to correct location. 
 * tensorflow/contrib/lite/examples/ios/camera/data/labels.txt
 * tensorflow/contrib/lite/examples/ios/camera/data/mobilenet_quant_v1_224.tflite
 * tensorflow/contrib/lite/examples/ios/simple/data/labels.txt
 * tensorflow/contrib/lite/examples/ios/simple/data/mobilenet_v1_1.0_224.tflite
* Run `tensorflow/contrib/lite/build_ios_universal_lib.sh` to verify the library can be built. ",0,,3,2017-11-20T23:28:17Z,2017-11-22T04:53:59Z,CONTRIBUTOR,2017-11-27T18:34:38Z
14733,tf.contrib.ffmpeg.decode_audio console flood,,"When we evaluate the tensor returned by `tensorflow.contrib.ffmpeg.decode_audio()`, the ffmpeg log shows up in the terminal, leading to a flood of messages when decoding a large number of files.

Asked [here](https://stackoverflow.com/questions/47361507/tf-contrib-ffmpeg-decode-audio-verbosity) as well. I could not find an easy way such as an environment variable for ffmpeg to turn off the output log, there is only a command-line argument `loglevel` but TF's `decode_audio()` does not support it.

Currently using `ffmpeg 3.4 (gcc 7.2.0)` and `tensorflow 1.4.0` on linux.

Issue filed as instructed [here](https://github.com/tensorflow/tensorflow/issues/11339#issuecomment-345836714). @rryan @fredbertsch ",0,,2,2017-11-20T22:36:59Z,2017-11-21T22:45:30Z,NONE,2017-11-21T21:54:26Z
14730,Only install enum34 on Python <3.4 versions,cla: yes,"Python 3.6 sometimes has issues with enum34 because the standard library
relies on enum features not in enum34 (see
https://bitbucket.org/stoneleaf/enum34/issues/19/enum34-isnt-compatible-with-python-36
for more details).

cc @macat",0,,9,2017-11-20T16:39:32Z,2017-11-21T22:30:30Z,CONTRIBUTOR,2017-11-20T19:31:01Z
14728,Edit mnist data read in to match tutorial,"awaiting review,cla: yes","The accompanying tutorial (https://www.tensorflow.org/get_started/mnist/beginners) and this file did not match. The proposed code change was copied from the tutorial and confirmed to run correctly.

Note that this removes the ability to pass a custom data directory as an argument. However, since this is aimed at absolute beginners I believe clarity is more important than flexibility.",0,,6,2017-11-20T16:11:34Z,2017-12-04T06:52:19Z,NONE,2017-12-04T06:52:19Z
14726,Make a copy of a model,stat:awaiting response,"Hi,  is there a canonical method in Tensorflow for this? For example, in Keras, we can use keras.models.clone_model for this purpose.  I though that model's copy would be such a nice feature, since copy.deepcopy does not work for me in Tensorflow. 
I want to copy weights from this model to another model of identical structure,  and I do not want to save a model then restore it to another instance for this situation. Specifically, the situation at every iteration we train model1 then make model2 as a copy of current model1, adding noise to model1 parameters and sample from model2 and then use these samples to update model1.
```
Class Model1(object):
    def method1(self):
        ....
    def method2(self):
        ....
```",0,,4,2017-11-20T15:30:29Z,2017-12-01T13:58:19Z,NONE,2017-12-01T07:06:56Z
14725,NameError: global name 'xrange' is not defined in Python 3 - and solution proposal,,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 Docker image
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/boosted_trees/examples/boston.py


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Running the example code under python 3.6 in Docker environment I got xrange error: NameError: global name 'xrange' is not defined in Python 3

I propose to change every xrange to range under the 3.x versions:
# patch for xrange
find /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees -type f -print0 | xargs -0 sed -i 's/ xrange(/ range(/g'


### Source code / logs
NameError: global name 'xrange' is not defined in Python 3
",0,,4,2017-11-20T14:46:53Z,2017-11-21T09:30:53Z,NONE,2017-11-20T17:42:47Z
14724,Using GPU mnist_deep.py throws OOM when allocating tensor with shape...,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, I am using the mnist_deep.py with tensorflow 1.4.0
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 & Tensorflow 1.4.0 binary installation, Linux Ubuntu 16.04 & Tensorflow 1.4.0 built form source
- **TensorFlow installed from (source or binary)**: Windows 10 installed with TF binary, Linux Ubuntu 16.04 TF built from source
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.6.2 on Windows 10, 3.5.2 on Linux
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: CUDA 8.0, CuDNN 6.0 
- **GPU model and memory**: For Windows 10: NVIDIA GeForce 940MX, For Linux: HW similar to NVIDIA Jetson TX2
- **Exact command to reproduce**: python mnist_deep.py

### Describe the problem
The mnist_deep.py sample given in Tensorflow examples/tutorials works fine when run on CPU. But when the same example is run using GPU, an OOM occurs when trying to allocate memory for tensor (specifically 10000) in both the cases. It does not matter if one increases/decreases the number of iterations to train the model, the OOM occurs even after a single iteration is executed.

The other examples like mnist.py, mnist_softmax.py, mnist_softmax_xla.py, etc. runs properly without any issues on the GPU. I have also tried to use the config_proto options but none of them seem to help.

### Source code / logs
************************************************
#### Windows 10:
tensor_name=""edge_75_Mean_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'conv1/Conv2D', defined at:
 File ""mnist_deep.py"", line 176, in <module>
 tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
 File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\platform\app.py"", line 48, in run
 _sys.exit(main(_sys.argv[:1] + flags_passthrough))
 File ""mnist_deep.py"", line 137, in main
 y_conv, keep_prob = deepnn(x)
 File ""mnist_deep.py"", line 63, in deepnn
 h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
 File ""mnist_deep.py"", line 105, in conv2d
 return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')
 File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 630, in conv2d
 data_format=data_format, name=name)
 File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
 op_def=op_def)
 File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 2956, in create_op
 op_def=op_def)
 File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1470, in __init__
 self._traceback = self._graph._extract_stack() # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10000,32,28,28]
 [[Node: conv1/Conv2D = Conv2D[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](reshape/Reshape, conv1/Variable/read)]]
 [[Node: Mean_1/_7 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_75_Mean_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
************************************************
####Linux:
tensor_name=""edge_75_Mean_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'conv1/Conv2D', defined at:
 File ""mnist_deep.py"", line 176, in <module>
 tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
 File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\platform\app.py"", line 48, in run
 _sys.exit(main(_sys.argv[:1] + flags_passthrough))
 File ""mnist_deep.py"", line 137, in main
 y_conv, keep_prob = deepnn(x)
 File ""mnist_deep.py"", line 63, in deepnn
 h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
 File ""mnist_deep.py"", line 105, in conv2d
 return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')
 File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 630, in conv2d
 data_format=data_format, name=name)
 File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
 op_def=op_def)
 File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 2956, in create_op
 op_def=op_def)
 File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1470, in __init__
 self._traceback = self._graph._extract_stack() # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10000,32,28,28]
 [[Node: conv1/Conv2D = Conv2D[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](reshape/Reshape, conv1/Variable/read)]]
 [[Node: Mean_1/_7 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_75_Mean_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
************************************************

Further detailed logs can be attached if needed. Your help and pointers to solve this will be much appreciated.",0,,7,2017-11-20T14:02:11Z,2017-11-22T08:14:13Z,NONE,2017-11-20T23:45:42Z
14723,Allow GANEstimator get_hooks_fn to be set manually,"awaiting testing (then merge),cla: yes",,2,,8,2017-11-20T13:48:25Z,2017-12-11T03:40:05Z,CONTRIBUTOR,2017-11-29T10:35:48Z
14722,Add a way to provide target nodes in Android,"awaiting testing (then merge),cla: yes",This is required when running some models as a step for initializing the graph etc.,1,,12,2017-11-20T13:24:53Z,2018-01-26T20:01:56Z,CONTRIBUTOR,2017-11-20T14:12:30Z
14721,Add back whitespace,"awaiting testing (then merge),cla: yes","When `tfcompile_flags` was changed so it could be not just a string but also a list of strings, the initial white space was erroneously removed (probably a misunderstanding of str.join) meaning `--out_object=` would consume the first flag.

E.g. this would no longer work:
```
tf_library(
  tfcompile_flags = ""--target_cpu='core-avx2'""
)
```

While this would work:
```
tf_library(
  tfcompile_flags = "" --target_cpu='core-avx2'""
)
```",0,,3,2017-11-20T13:12:25Z,2017-11-22T05:44:28Z,CONTRIBUTOR,2017-11-22T11:36:47Z
14719,Tensorflow Lite demo app with inception-v3/Mobilenet_v1 (float) model crashes,"comp:lite,stat:awaiting response,type:support","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 14.04
- **Python version**: 3.4.3
- **Bazel version (if compiling from source)**: 0.5.4


### Describe the problem
Device: Galaxy S8
I downloaded the ""Inception V3 Slim 2016"" from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md . I pushed the imagenet_2015_label_strings.txt and the ""inceptionv3_non_slim_2015.tflite"" to the asset folder

I edited the ImageClassifier.java of tflite demo app. The changes are the followings:
private static final String MODEL_PATH = ""/inceptionv3_non_slim_2015.tflite"";
static final int DIM_IMG_SIZE_X = 299;
static final int DIM_IMG_SIZE_Y = 299;

The app hangs when it starts! (I could run the app with the default mobilenet quantized graph).
Similar is the case with mobilenet_v1_224_Float graph as well (the app hangs or crashes). I assume, the float model graph is not yet supported by TF Lite. However, in the documentation its written that it does support float for most operations. I am thinking the error is due to image pre-processing output and input size of float model grpah. The error log is stated below:

The Error log:
11-21 14:31:43.034 2111-2416/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground
                                                                                    Process: android.example.com.tflitecamerademo, PID: 2111
                                                                                    java.lang.IllegalArgumentException: Failed to get input dimensions. 0-th input should have 1072812 bytes, but found 268203 bytes.
                                                                                        at org.tensorflow.lite.NativeInterpreterWrapper.getInputDims(Native Method)
                                                                                        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:82)
                                                                                        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:112)
                                                                                        at org.tensorflow.lite.Interpreter.run(Interpreter.java:93)
                                                                                        at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:112)
                                                                                        at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:663)
                                                                                        at com.example.android.tflitecamerademo.Camera2BasicFragment.-wrap0(Camera2BasicFragment.java)
                                                                                        at com.example.android.tflitecamerademo.Camera2BasicFragment$4.run(Camera2BasicFragment.java:558)
                                                                                        at android.os.Handler.handleCallback(Handler.java:751)
                                                                                        at android.os.Handler.dispatchMessage(Handler.java:95)
                                                                                        at android.os.Looper.loop(Looper.java:154)
                                                                                        at android.os.HandlerThread.run(HandlerThread.java:61)


### Additional Questions:
1) On the app the the tensorflow lite graph format is "".tflite"". However, on the documentation https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md the format is written as "".lite""


",0,,13,2017-11-20T12:23:26Z,2017-11-30T01:16:43Z,NONE,2017-11-29T20:45:38Z
14718,tensorflow/contrib/lite/download_dependencies.sh does not finish without error!,comp:lite,"for tensorflow/contrib/lite/download_dependencies.sh, I can not run successfully with commit 049a34d692095b7e137bca27d2445415314ceaf7.
And I rollback to 4b4b51cdd9e8c3c748b76dd8649bcd5556e84d76, everything is good.",1,,3,2017-11-20T11:12:19Z,2017-11-28T18:22:34Z,NONE,2017-11-20T17:04:45Z
14717,Coverage for NMT,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: 4.8.4
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: NVIDIA TITAN X (12GB)
- **Exact command to reproduce**:  NA

Modelling coverage is a very useful feature in NMT to reduce over-translations.

Ref.: 
https://www.aclweb.org/anthology/P/P16/P16-1008.pdf,
https://arxiv.org/pdf/1704.04368.pdf

Is this feature available right now or, if not, how can I hack the current attention mechanism (say, Bahadanau) to add this feature ? ",0,,1,2017-11-20T11:10:47Z,2017-12-01T00:29:23Z,NONE,2017-12-01T00:29:23Z
14715,where is mobile  ssd model?,,where is mobile  ssd model? where is it.where  is  mobile net model??,0,,1,2017-11-20T10:38:01Z,2017-12-01T00:32:24Z,NONE,2017-12-01T00:32:24Z
14714,"Update docs for using Docker with GPU, with nvidia-docker2","awaiting review,cla: yes","Update docs to reflect the current version of `nvidia-docker` (version 2).

For reference: https://github.com/NVIDIA/nvidia-docker/wiki/About-version-2.0",0,,6,2017-11-20T10:25:02Z,2017-11-22T07:04:24Z,NONE,2017-11-22T07:12:21Z
14711,//tensorflow/python:session_list_devices_test fails on X86,stat:awaiting tensorflower,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
     Ubuntu 16.04 
- **TensorFlow installed from (source or binary)**:
      Installed from source
- **TensorFlow version (use command below)**:
      TF-1.3.1
- **Python version**:
      Python 2.7.12
- **Bazel version (if compiling from source)**:
      Bazel - 0.5.4
- **CUDA/cuDNN version**:
      NA
- **GPU model and memory**:
      NA
- **Exact command to reproduce**:
     ` bazel test --config=opt  //tensorflow/python:session_list_devices_test `

### Describe the problem
Following 3 sub-tests are failing on Ubuntu:16.04 (x86) with the assertion errors
1) FAIL: testListDevices (__main__.SessionListDevicesWithCApiTest)
https://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/python/client/session_list_devices_test.py#L39
 `self.assertGreaterEqual(1, len(devices), devices)` .....# Getting AssertionError due to: ""1"" unexpectedly not greater than or equal to ""3""  
2) FAIL: testListDevicesGrpcSession (__main__.SessionListDevicesWithCApiTest)
https://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/python/client/session_list_devices_test.py#L47
 `self.assertGreaterEqual(1, len(devices), devices`) .....#  Getting AssertionError due to: ""1"" unexpectedly not greater than or equal to ""3""  
3) FAIL: testListDevicesClusterSpecPropagation (__main__.SessionListDevicesWithCApiTest)
https://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/python/client/session_list_devices_test.py#L66
 `self.assertGreaterEqual(2, len(devices), devices)` ..... #  Getting AssertionError due to: ""2"" unexpectedly not greater than or equal to ""6""

Is this is a known failure (can we ignore ) or I am missing something here ?. Please provide your comments on this.Thanks!
### Source code / logs
```

$  bazel test --config=opt  //tensorflow/python:session_list_devices_test

2017-11-20 08:35:46.791667: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:343] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.
2017-11-20 08:35:46.795931: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session 026f646fb0c59742 with config:
F.
======================================================================
FAIL: testListDevices (__main__.SessionListDevicesTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/0cb601a163d4c4c6c065cdaa9629611b/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/session_list_devices_test.runfiles/org_tensorflow/tensorflow/python/client/session_list_devices_test.py"", line 39, in testListDevices
    self.assertGreaterEqual(1, len(devices), devices)
AssertionError: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 42115744), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_EXEC:0, XLA_EXEC, 42116496), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 42116608)]

======================================================================
FAIL: testListDevicesClusterSpecPropagation (__main__.SessionListDevicesTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/0cb601a163d4c4c6c065cdaa9629611b/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/session_list_devices_test.runfiles/org_tensorflow/tensorflow/python/client/session_list_devices_test.py"", line 66, in testListDevicesClusterSpecPropagation
    self.assertGreaterEqual(2, len(devices), devices)
AssertionError: [_DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 42238272), _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_EXEC:0, XLA_EXEC, 42238864), _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 42238896), _DeviceAttributes(/job:worker/replica:0/task:1/device:CPU:0, CPU, 42238928), _DeviceAttributes(/job:worker/replica:0/task:1/device:XLA_EXEC:0, XLA_EXEC, 42238960), _DeviceAttributes(/job:worker/replica:0/task:1/device:XLA_CPU:0, XLA_CPU, 42238992)]

======================================================================
FAIL: testListDevicesGrpcSession (__main__.SessionListDevicesTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/0cb601a163d4c4c6c065cdaa9629611b/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/session_list_devices_test.runfiles/org_tensorflow/tensorflow/python/client/session_list_devices_test.py"", line 47, in testListDevicesGrpcSession
    self.assertGreaterEqual(1, len(devices), devices)
AssertionError: [_DeviceAttributes(/job:local/replica:0/task:0/device:CPU:0, CPU, 42226672), _DeviceAttributes(/job:local/replica:0/task:0/device:XLA_EXEC:0, XLA_EXEC, 41891296), _DeviceAttributes(/job:local/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 41891328)]

======================================================================
FAIL: testListDevices (__main__.SessionListDevicesWithCApiTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/0cb601a163d4c4c6c065cdaa9629611b/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/session_list_devices_test.runfiles/org_tensorflow/tensorflow/python/client/session_list_devices_test.py"", line 39, in testListDevices
    self.assertGreaterEqual(1, len(devices), devices)
AssertionError: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 42268048), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_EXEC:0, XLA_EXEC, 42268800), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 42268912)]

======================================================================
FAIL: testListDevicesClusterSpecPropagation (__main__.SessionListDevicesWithCApiTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/0cb601a163d4c4c6c065cdaa9629611b/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/session_list_devices_test.runfiles/org_tensorflow/tensorflow/python/client/session_list_devices_test.py"", line 66, in testListDevicesClusterSpecPropagation
    self.assertGreaterEqual(2, len(devices), devices)
AssertionError: [_DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 42313280), _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_EXEC:0, XLA_EXEC, 42313872), _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 42313904), _DeviceAttributes(/job:worker/replica:0/task:1/device:CPU:0, CPU, 42313936), _DeviceAttributes(/job:worker/replica:0/task:1/device:XLA_EXEC:0, XLA_EXEC, 42313968), _DeviceAttributes(/job:worker/replica:0/task:1/device:XLA_CPU:0, XLA_CPU, 42314000)]

======================================================================
FAIL: testListDevicesGrpcSession (__main__.SessionListDevicesWithCApiTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/0cb601a163d4c4c6c065cdaa9629611b/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/session_list_devices_test.runfiles/org_tensorflow/tensorflow/python/client/session_list_devices_test.py"", line 47, in testListDevicesGrpcSession
    self.assertGreaterEqual(1, len(devices), devices)
AssertionError: [_DeviceAttributes(/job:local/replica:0/task:0/device:CPU:0, CPU, 42260656), _DeviceAttributes(/job:local/replica:0/task:0/device:XLA_EXEC:0, XLA_EXEC, 42260688), _DeviceAttributes(/job:local/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 42331824)]

----------------------------------------------------------------------
Ran 8 tests in 1.857s

FAILED (failures=6)
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.

```",0,,2,2017-11-20T08:59:18Z,2017-12-07T05:13:24Z,CONTRIBUTOR,2017-12-01T00:46:18Z
14710,Change ndimage.imread to imageio.imread.,"awaiting testing (then merge),cla: yes","Scipy will not support imread from 1.0.0 as its document says:
https://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.misc.imread.html

Change to imageio.imread and add its correspond exception.",1,,2,2017-11-20T07:17:27Z,2017-11-22T07:13:19Z,CONTRIBUTOR,2017-11-21T19:18:29Z
14709,Include _solib_local for MKL-DNN libs,"awaiting testing (then merge),cla: yes","`_solib_local/libmklml_intel.so` is not getting included in the package.
`build_pip_package.sh` has been updated to copy `*solib*` instead of just `_solib_k8`, so just update `setup.py` to include it.
See https://github.com/tensorflow/tensorflow/issues/13711 for details.",1,,9,2017-11-20T07:13:14Z,2017-12-19T01:09:38Z,CONTRIBUTOR,2017-12-10T09:44:25Z
14706,failed to bazel build tensorflow lite ,comp:lite,"build tensorflow lite  demo with ""bazel --output_base=/data/wjx/bazel/tensorflow/output --output_user_root=/data/wjx/bazel/tensorflow build --cxxopt='--std=c++11'  //tensorflow/contrib/
lite/java/demo/app/src/main:TfLiteCameraDemo""

Environment:
OS: ubuntu 16.04
tf version: tensorflow 1.4 master
python:2.7.12
AndroidSDK: 27 BuildToolsVersion: 27.0.1
NDK: android-ndk-r14e

ERROR: /data/wjx/bazel/tensorflow/output/external/androidsdk/com.android.support/BUILD:4277:1: Merging Android resources for @androidsdk//com.android.support:support-compat-25.2.0 failed (Exit 1)
Nov 20, 2017 1:31:06 AM com.google.devtools.build.android.AndroidResourceMergingAction main
SEVERE: Unexpected
java.io.IOException: Mount point not found
	at sun.nio.fs.LinuxFileStore.findMountEntry(LinuxFileStore.java:91)
	at sun.nio.fs.UnixFileStore.<init>(UnixFileStore.java:65)
	at sun.nio.fs.LinuxFileStore.<init>(LinuxFileStore.java:44)
	at sun.nio.fs.LinuxFileSystemProvider.getFileStore(LinuxFileSystemProvider.java:51)
	at sun.nio.fs.LinuxFileSystemProvider.getFileStore(LinuxFileSystemProvider.java:39)
	at sun.nio.fs.UnixFileSystemProvider.getFileStore(UnixFileSystemProvider.java:368)
	at java.nio.file.Files.getFileStore(Files.java:1461)
	at com.google.devtools.build.android.ScopedTemporaryDirectory.makeWritable(ScopedTemporaryDirectory.java:59)
	at com.google.devtools.build.android.ScopedTemporaryDirectory.visitFile(ScopedTemporaryDirectory.java:83)
	at com.google.devtools.build.android.ScopedTemporaryDirectory.visitFile(ScopedTemporaryDirectory.java:36)
	at java.nio.file.Files.walkFileTree(Files.java:2670)
	at java.nio.file.Files.walkFileTree(Files.java:2742)
	at com.google.devtools.build.android.ScopedTemporaryDirectory.close(ScopedTemporaryDirectory.java:96)
	at com.google.devtools.build.android.AndroidResourceMergingAction.main(AndroidResourceMergingAction.java:289)
	at com.google.devtools.build.android.ResourceProcessorBusyBox$Tool$7.call(ResourceProcessorBusyBox.java:91)
	at com.google.devtools.build.android.ResourceProcessorBusyBox.main(ResourceProcessorBusyBox.java:172)

Exception in thread ""main"" java.io.IOException: Mount point not found
	at sun.nio.fs.LinuxFileStore.findMountEntry(LinuxFileStore.java:91)
	at sun.nio.fs.UnixFileStore.<init>(UnixFileStore.java:65)
	at sun.nio.fs.LinuxFileStore.<init>(LinuxFileStore.java:44)
	at sun.nio.fs.LinuxFileSystemProvider.getFileStore(LinuxFileSystemProvider.java:51)
	at sun.nio.fs.LinuxFileSystemProvider.getFileStore(LinuxFileSystemProvider.java:39)
	at sun.nio.fs.UnixFileSystemProvider.getFileStore(UnixFileSystemProvider.java:368)
	at java.nio.file.Files.getFileStore(Files.java:1461)
	at com.google.devtools.build.android.ScopedTemporaryDirectory.makeWritable(ScopedTemporaryDirectory.java:59)
	at com.google.devtools.build.android.ScopedTemporaryDirectory.visitFile(ScopedTemporaryDirectory.java:83)
	at com.google.devtools.build.android.ScopedTemporaryDirectory.visitFile(ScopedTemporaryDirectory.java:36)
	at java.nio.file.Files.walkFileTree(Files.java:2670)
	at java.nio.file.Files.walkFileTree(Files.java:2742)
	at com.google.devtools.build.android.ScopedTemporaryDirectory.close(ScopedTemporaryDirectory.java:96)
	at com.google.devtools.build.android.AndroidResourceMergingAction.main(AndroidResourceMergingAction.java:289)
	at com.google.devtools.build.android.ResourceProcessorBusyBox$Tool$7.call(ResourceProcessorBusyBox.java:91)
	at com.google.devtools.build.android.ResourceProcessorBusyBox.main(ResourceProcessorBusyBox.java:172)
Target //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 16.750s, Critical Path: 3.39s
FAILED: Build did NOT complete successfully


can anyone help me?",1,,4,2017-11-20T03:27:00Z,2017-11-29T01:26:20Z,NONE,2017-11-29T00:28:59Z
14705,Complie error with cuda9.0+cudnn7.0+tensorflow-r1.4 on Ubuntu 16.04LTS,,"```
ERROR: /home/wavy/Ten/tensorflow/tensorflow/contrib/factorization/BUILD:116:1: Linking of rule '//tensorflow/contrib/factorization:gen_gen_factorization_ops_py_wrappers_cc' failed (Exit 1)
```
This is the error output.

Thanks!
",0,,8,2017-11-20T03:24:12Z,2017-12-01T00:51:58Z,NONE,2017-12-01T00:51:58Z
14704,tf.data.Dataset.padded_batch() doesn't work with nested elements,,"### System information

TF 1.4 (pip install)
Python version 3.5.2 (Anaconda)

### Problem description

`tf.data.Dataset.padded_batch()` fails if a dataset element has some nested structure instead of being a tensor. Dataset API is supposed to work with Estimator's input_fn functionality which should return
features and labels as separate python objects and it is very inconvenient to merge everything into a single tensor, make a batch and then split.

### Source

    import tensorflow as tf
    print(tf.__version__)    

    dataset = tf.data.Dataset.range(100)
    dataset = dataset.map(lambda x: {'x': tf.fill([tf.cast(x, tf.int32)], x),
                                                           'y': tf.fill([tf.cast(x, tf.int32)], x)})
    dataset = dataset.padded_batch(4, padded_shapes=[None])

    iterator = dataset.make_one_shot_iterator()
    next_element = iterator.get_next()

    with tf.train.MonitoredSession() as sess:
        print(sess.run(next_element))
        print(sess.run(next_element))

### Actual

    ---------------------------------------------------------------------------
    TypeError                                 Traceback (most recent call last)
    <ipython-input-38-bb9f335976ed> in <module>()
          2 dataset = dataset.map(lambda x: {'x': tf.fill([tf.cast(x, tf.int32)], x),
          3                                  'y': tf.fill([tf.cast(x, tf.int32)], x)})
    ----> 4 dataset = dataset.padded_batch(4, padded_shapes=[None])
          5 
          6 iterator = dataset.make_one_shot_iterator()

    ~/anaconda3/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py in padded_batch(self, batch_size, padded_shapes, padding_values)
        693       A `Dataset`.
        694     """"""
    --> 695     return PaddedBatchDataset(self, batch_size, padded_shapes, padding_values)
        696 
        697   def map(self, map_func, num_parallel_calls=None):

    ~/anaconda3/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, batch_size, padded_shapes, padding_values)
       1290                       self._default_padding(input_dataset))
       1291     self._padded_shapes = nest.map_structure_up_to(
    -> 1292         input_dataset.output_shapes, _partial_shape_to_tensor, padded_shapes)
       1293     self._padding_values = nest.map_structure_up_to(
       1294         input_dataset.output_shapes, _padding_value_to_tensor, padding_values,

    ~/anaconda3/lib/python3.5/site-packages/tensorflow/python/data/util/nest.py in map_structure_up_to(shallow_tree, func, *inputs)
        510     raise ValueError(""Cannot map over no sequences"")
        511   for input_tree in inputs:
    --> 512     assert_shallow_structure(shallow_tree, input_tree)
        513 
        514   # Flatten each input separately, apply the function to corresponding elements,

    ~/anaconda3/lib/python3.5/site-packages/tensorflow/python/data/util/nest.py in assert_shallow_structure(shallow_tree, input_tree, check_types)
        354       raise TypeError(
        355           ""If shallow structure is a sequence, input must also be a sequence. ""
    --> 356           ""Input has type: %s."" % type(input_tree))
        357 
        358     if check_types and not isinstance(input_tree, type(shallow_tree)):

    TypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: <class 'list'>.

### Expected

It should produce dictionary, where x and y values are batch tensors with proper paddings.
",0,,1,2017-11-19T14:53:21Z,2017-11-20T17:24:50Z,NONE,2017-11-20T17:24:50Z
14701,"startup time (_make_train_function()) very slow on Tesla V100-SXM2-16GB GPU, compared to less powerful GPU",stat:awaiting tensorflower,"cross posted on keras: https://github.com/fchollet/keras/issues/8537

Running mnist_cnn.py (slightly modified - mainly adding logging) from tensorflow 1.4
running was done using a prebuilt docker image: tensorflow/tensorflow:1.4.0-gpu-py3
on a p2.xlarge aws machine (that has a Tesla K80 GPU) performance is good, the 1st batch (which is dominated by the call to _make_train_function) takes about 2 seconds: (see time stamp for begin batch and end batch)

```
2017-11-19 08:26:26,172 : INFO : fit

2017-11-19 08:26:26,637 : INFO : begin batch
2017-11-19 08:26:26.638409: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2017-11-19 08:26:26.760940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-11-19 08:26:26.761478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
totalMemory: 11.17GiB freeMemory: 11.11GiB
2017-11-19 08:26:26.761506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)

2017-11-19 08:26:28,135 : INFO : end batch
x_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
Train on 60000 samples, validate on 10000 samples
Epoch 1/1
60000/60000 [==============================] - 12s - loss: 0.3526 - acc: 0.8920 - val_loss: 0.0818 - val_acc: 0.9755
Test loss: 0.081773182778
Test accuracy: 0.9755
```

on a p3.2xlarge machine (with a Tesla V100-SXM2-16GB GPU) the same part takes about 10 minutes

```
2017-11-19 08:26:44,120 : INFO : fit

2017-11-19 08:26:44,715 : INFO : begin batch
2017-11-19 08:26:44.716680: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2017-11-19 08:26:46.108295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-11-19 08:26:46.108775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:1e.0
totalMemory: 15.77GiB freeMemory: 15.36GiB
2017-11-19 08:26:46.108815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)

2017-11-19 08:36:16,552 : INFO : end batch
x_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
Train on 60000 samples, validate on 10000 samples
Epoch 1/1
60000/60000 [==============================] - 576s - loss: 0.3418 - acc: 0.8949 - val_loss: 0.0769 - val_acc: 0.9772
Test loss: 0.0769035610346
Test accuracy: 0.9772
```

the code that was used:
```
#!/usr/bin/env python
'''Trains a simple convnet on the MNIST dataset.

Gets to 99.25% test accuracy after 12 epochs
(there is still a lot of margin for parameter tuning).
16 seconds per epoch on a GRID K520 GPU.
'''

from __future__ import print_function
import cProfile
import os
from tensorflow.contrib import keras
from tensorflow.contrib.keras import backend as K
import logging


logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='\n%(asctime)s : %(levelname)s : %(message)s')

class callback(keras.callbacks.Callback):
    def on_batch_begin(self, batch, logs=None):
      if batch <= 1:
            logger.info('begin batch')

class callback(keras.callbacks.Callback):
    def on_batch_end(self, batch, logs=None):
        if batch <= 1:
            logger.info('end batch')

batch_size = 128
num_classes = 10
epochs = 1

# input image dimensions
img_rows, img_cols = 28, 28

# the data, shuffled and split between train and test sets
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

if K.image_data_format() == 'channels_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = keras.models.Sequential()
model.add(keras.layers.Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=input_shape))
model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))
model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(keras.layers.Dropout(0.25))
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(128, activation='relu'))
model.add(keras.layers.Dropout(0.5))
model.add(keras.layers.Dense(num_classes, activation='softmax'))

model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
profiler = cProfile.Profile()
profiler.enable()
logger.info('fit')
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test), callbacks=[callback()])
profiler.dump_stats(os.path.expanduser('~/profiler.pstats'))
score = model.evaluate(x_test, y_test, verbose=0)

print('Test loss:', score[0])
print('Test accuracy:', score[1])

```
",0,,4,2017-11-19T09:02:52Z,2017-12-01T15:49:52Z,NONE,2017-12-01T02:04:42Z
14698,MemoryError from tensorflow.contrib.learning.datasets in Python3,,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0
- **Python version**: 3.5.3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: python -c ""import tensorflow as tf; tf.contrib.learn.datasets.load_dataset('dbpedia', size='full')""



### Describe the problem
The command results in an MemoryError, even though the dataset easily fits into my memory (64GB), and also works with Python 2. 

### Source code / logs
Here is the traceback:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/james/python3.5-ve/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py"", line 71, in load_dataset
    return DATASETS[name](size, test_with_fake_data)
  File ""/home/james/python3.5-ve/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py"", line 65, in load_dbpedia
    train_path, target_dtype=np.int32, features_dtype=np.str, target_column=0)
  File ""/home/james/python3.5-ve/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 72, in load_csv_without_header
    data = np.array(data)
MemoryError

",1,,15,2017-11-19T03:24:39Z,2017-12-06T02:09:33Z,NONE,2017-12-05T18:23:07Z
14697,"tf.train.Scaffold does't accept global_step, and therefore only one checkpoint (0) is saved",,"Disclaimer: Issue #10661 might be related. 

When I create a Scaffold object and attempt to use it in a MonitoredTrainingSession, there is no ability to specify the global step.  As a consequence (I think), only one checkpoint is created, labeled ""model.ckpt-0"". 

As I understand it, the code below (taken from a custom class I've written...hence the `self`s) should save a new checkpoint every 10 seconds, but only actually creates a single checkpoint at the start of training, and then never again.

    self.GLOBAL_STEP = tf.train.get_or_create_global_step()
    self.init_op = tf.global_variables_initializer()       
    self.Saver = tf.train.Saver()

    self.Scaffold = tf.train.Scaffold( init_op=self.init_op,
                                       saver=self.Saver
                                   #   global_step=self.GLOBAL_STEP  
                                           )

    self.sess = tf.train.MonitoredTrainingSession( master='',
                                                   is_chief=True,
                                                   scaffold = self.Scaffold,
                                                   checkpoint_dir='./chkpt/',
                                                   save_checkpoint_secs=10,
                                                       )

If I uncomment the `global_step=...` line above, I get an error (as expected) since `Scaffold.__init__()` doesn't take a global_step argument, however shouldn't it?",0,,1,2017-11-19T01:14:08Z,2017-12-01T02:16:01Z,CONTRIBUTOR,2017-12-01T02:16:01Z
14695,Update CONTRIBUTING.md,"awaiting testing (then merge),cla: yes",Add Objective-C Style guide to list.,1,,2,2017-11-18T23:00:54Z,2017-11-30T19:05:58Z,CONTRIBUTOR,2017-11-24T21:21:36Z
14693,Why did you call BasicLSTMCell a cell and not a layer?,"stat:awaiting tensorflower,type:feature","`BasicLSTMCell` is actually a layer (as for a layer in MLPs) of LSTM units. Each of these LSTM units contains a cell. Each cell of an LSTM unit contains a **scalar value** for the CEC and a **scalar** representing the previous state.

People are usually first introduced to MLPs or feed-forward (and fully connected) neural networks, before being introduced to RNNs and, in particular, LSTMs. 

Why would you call `BasicLSTMCell` a **cell**, if it can be thought more intuitively (at least for me) as a layer of LSTM units (as I describe them above) containing just one scalar-based cell? Wouldn't it be less ambiguous to call a `BasicLSTMCell` `BasicLSTMLayer`???

Moreover, the first parameter to `BasicLSTMCell`'s `__init__` method is `num_units`, i.e. the number of LSTM units, i.e. the number of LSTM cells and gates (if we have 3 gates for every LSTM unit, then the total number of gates in one layer of LSTMs is 3 * `num_units`). 

It almost seems that you created TF to make it as confusing as possible to make it seem hard. It also almost seems that the person who wrote the name of the class `BasicLSTMCell` is a different person of the person who wrote its `__init__` method. What's going on??? A little bit of consistency, for once, no???

A similar argument can be said for `MultiRNNCell`, which, a lot more intuitively, can be thought as a sequence of layers.

### Request

Change classes such as `BasicLSTMCell` and `MultiRNNCell` to have more descriptive names of what they actually are in future versions of TF. Then change the corresponding documentation to be more compliant with these changes.",0,,4,2017-11-18T17:15:30Z,2017-11-19T07:45:30Z,NONE,2017-11-19T07:45:30Z
14691,"compile failed for tf-gpu 1.4, cuda 9, cudnn 7, vc 2017, windows 10.","stat:awaiting tensorflower,type:build/install","I would greatly appreciate if anyone could help me out with compiling.
I followed #5600 and #13962 in compiling the wheel, however when building i got into 6 types of in total 90 errors. Respectively c2070, c2059, c2064, c2001, c1057, c2146.

My build command was:
`cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^
-DSWIG_EXECUTABLE=C:/swigwin-3.0.12/swig.exe ^
-DPYTHON_EXECUTABLE=python.exe ^
-DPYTHON_LIBRARIES=C:/ProgramData/Anaconda3/libs/python36_lib ^
-DPYTHON_INCLUDE_DIR=C:/ProgramData/Anaconda3/Include ^
-DNUMPY_INCLUDE_DIR=C:/ProgramData/Anaconda3/lib/site-packages/numpy/core/include ^
-Dtensorflow_ENABLE_GPU=ON ^
-DCUDNN_HOME=""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0"" ^
-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX `

Everything looks good until the compiler_arch_native_support failed.
It manage to configure the build so I continue with: 
MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj 

The last 9999 line of the failed log is here. 
[tf-compile-build-error-log.txt](https://github.com/tensorflow/tensorflow/files/1484750/tf-compile-build-error-log.txt)

Warmest Regards,
Colman",0,,19,2017-11-18T16:13:31Z,2018-01-03T21:43:21Z,NONE,2017-11-18T23:18:01Z
14689,Couldn't find field google.protobuf.EnumDescriptorProto.EnumReservedRange.start,,"Im trying to run the following code 

```
import tensorflow as tf

print(""Hello TensorFlow version"", tf.__Version__)
```


It is firing the following error 

> Users/anaconda/envs/cnn/bin/python /Users/Downloads/rude-carnie/version.py
> Traceback (most recent call last):
>   File ""/Users/Downloads/rude-carnie/version.py"", line 1, in <module>
>     import tensorflow as tf
>   File ""/Users/anaconda/envs/cnn/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
>     from tensorflow.python import *
>   File ""/Users/anaconda/envs/cnn/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 52, in <module>
>     from tensorflow.core.framework.graph_pb2 import *
>   File ""/Users/anaconda/envs/cnn/lib/python3.6/site-packages/tensorflow/core/framework/graph_pb2.py"", line 10, in <module>
>     from google.protobuf import descriptor_pb2
>   File ""/Users/anaconda/envs/cnn/lib/python3.6/site-packages/google/protobuf/descriptor_pb2.py"", line 735, in <module>
>     options=None, file=DESCRIPTOR),
>   File ""/Users/anaconda/envs/cnn/lib/python3.6/site-packages/google/protobuf/descriptor.py"", line 501, in __new__
>     return _message.default_pool.FindFieldByName(full_name)
> KeyError: ""Couldn't find field google.protobuf.EnumDescriptorProto.EnumReservedRange.start""",0,,22,2017-11-18T11:21:32Z,2017-12-01T02:19:14Z,NONE,2017-11-23T14:16:57Z
14688,how to build tensorflow lite into a static c++ library using android ndk,"comp:lite,stat:awaiting tensorflower","I want to write some c++ test binary using tensorflow lite.
from the README.md I can only see how to build the demo app.
Could you please tell me how to build tensorflow lite into a static library using android ndk?",1,,6,2017-11-18T09:16:11Z,2017-11-27T01:19:37Z,NONE,2017-11-21T11:14:27Z
14686,"i'm a newcomer , and met the following problem!!",,"i install tensorflow following the guide explanation in 
**www.tensorflow.org/install/install_windows**
### System information
- **OS Platform and Distribution:win10
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below): i use the following command: pip3 install --upgrade tensorflow-gpu
- **Python version:3.5.2 
- **CUDA/cuDNN version**:cuda 8.0 /cudnn 8.0
- **GPU model and memory:GTX1070 8g
i download the source successfully and when i enter the following short program inside the python interactive shell:   import tensorflow as tf
it goes wrong:
Traceback (most recent call last):
  File ""E:\SoftWare\Python\lib\site-packages\tensorflow\python\platform\self_check.py"", line 87, in preload_check
    ctypes.WinDLL(build_info.cudnn_dll_name)
  File ""E:\SoftWare\Python\lib\ctypes\__init__.py"", line 347, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] Could not find the specified module

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""E:\SoftWare\Python\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""E:\SoftWare\Python\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""E:\SoftWare\Python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""E:\SoftWare\Python\lib\site-packages\tensorflow\python\platform\self_check.py"", line 97, in preload_check
    % (build_info.cudnn_dll_name, build_info.cudnn_version_number))
ImportError: Could not find 'cudnn64_6.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 6 from this URL: https://developer.nvidia.com/cudnn


in my file sys ,the dll file's name is cudnn64_5.dll
and following is my path:
![image](https://user-images.githubusercontent.com/33776263/32978257-56054f12-cc79-11e7-838e-a10a1b138b07.png)
(i can find my msvcp140.dll)
please help ,thx",0,,2,2017-11-18T08:11:13Z,2017-11-18T23:18:27Z,NONE,2017-11-18T09:03:59Z
14684,Does SavedModelBuilder save checkpoints ?,,"Does SavedModelBuilder.save() create checkpoint files ? The documentation says this is a wrapper for Saver but doesn't mention checkpoints. It looks like this function doesn't create checkpoints.   

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.1
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A",0,,7,2017-11-18T06:38:03Z,2018-01-31T22:22:18Z,NONE,2017-12-01T07:06:07Z
14683,Cannot convert a string input to combination of tensors as defined in the Input function,,"While working on the tensor flow java api, I trained a model in python and saved it using the code below -

There are two problems I'm facing now while making predictions in Python and Java as below - 

1. Making predictions in Python
2.  Making prediction in Java with input as string 
3.  How to convert data to complex input types like CrossColumn tensors, bucketized tensors, embedding tensors etc in Java

**_1] Making predictions in Python -_** 

Gist for training code can be found at - https://gist.github.com/gaganmalhotra/8c40e7650f27cf3f894bad092fbe01ab

While loading the model using the Predictor and making predictions as : 

```
# Preparing the single test dataframe to be used for prediction
input_single_predict = df_test[2:3] #this is just a single row from the test dataframe I'm using to test for prediction

K_CATEGORICAL_COLUMNS = [""gender"", ""native_country"", ""education"", ""occupation"", ""workclass"", ""marital_status"", ""race""]

def predict_ip_fn(df):
  categorical_cols = {k: tf.SparseTensor(
      indices=[[i, 0] for i in range(df[k].size)],
      values=df[k].values,
      dense_shape=[df[k].size, 1])
                      for k in K_CATEGORICAL_COLUMNS}
  return categorical_cols

dict_predict = predict_ip_fn(input_single_predict)

# Loading the model from disk
from tensorflow.contrib import predictor
export_dir = ""/Users/Documents/SampleTF_projects/temp/1510957027""
predict_fn = predictor.from_saved_model(export_dir, signature_def_key=None)

predictions = predict_fn(dict_predict) . #<<<<<< ****** Error is caused here ******
print(predictions['probabilities'])

```
**But it leads to the error as below -** 
`
ValueError: Got unexpected keys in input_dict: set(['workclass', 'gender', 'marital_status', 'race', 'native_country', 'education', 'occupation'])
`

**Just to cross verify with the model features, you can find the feature columns used in the model as below -** 

```
LinearClassifier(params = {
	'gradient_clip_norm': None,
	'head': < tensorflow.contrib.learn.python.learn.estimators.head._BinaryLogisticHead object at 0x121e5a310 > ,
	'joint_weights': False,
	'optimizer': None,
	'feature_columns': [_SparseColumn(column_name = 'gender', is_integerized = False, bucket_size = None, lookup_config = _SparseIdLookupConfig(vocabulary_file = None, keys = ('Female', 'Male'), num_oov_buckets = 0, vocab_size = 2, default_value = -1), combiner = 'sum', dtype = tf.string), 
  _SparseColumn(column_name = 'native_country', is_integerized = False, bucket_size = 1000, lookup_config = None, combiner = 'sum', dtype = tf.string),
  _SparseColumn(column_name = 'education', is_integerized = False, bucket_size = 1000, lookup_config = None, combiner = 'sum', dtype = tf.string), 
  _SparseColumn(column_name = 'occupation', is_integerized = False, bucket_size = 1000, lookup_config = None, combiner = 'sum', dtype = tf.string), 
  _SparseColumn(column_name = 'workclass', is_integerized = False, bucket_size = 100, lookup_config = None, combiner = 'sum', dtype = tf.string), 
  _SparseColumn(column_name = 'marital_status', is_integerized = False, bucket_size = 1000, lookup_config = None, combiner = 'sum', dtype = tf.string), 
  _SparseColumn(column_name = 'race', is_integerized = False, bucket_size = 1000, lookup_config = None, combiner = 'sum', dtype = tf.string)]
})
```

**_2] Making predictions in Java_** 

Below is the java code for making the predictions from the loaded model:
```

public static void main(String[] args) throws UnsupportedEncodingException {
TensorFlow.loadLibrary(""/Users/gagandeep.malhotra/Documents/gcTensorFlowPredictIncome/census_keras/lib/python2.7/site-packages/tensorflow/contrib/layers/python/ops/_sparse_feature_cross_op.so"");
		
		  try (SavedModelBundle b = SavedModelBundle.load(""/Users/Documents/SampleTF_projects/temp/1510957027/"", ""serve"")) {

	          
	          /**
			 * 
			 * The given SavedModel SignatureDef contains the following input(s):
			 * inputs['inputs'] tensor_info: dtype: DT_STRING shape: (-1) name:
			 * input_example_tensor:0 The given SavedModel SignatureDef contains the
			 * following output(s): outputs['classes'] tensor_info: dtype: DT_STRING shape:
			 * (-1, -1) name:
			 * linear/binary_logistic_head/_classification_output_alternatives/classes_tensor:0
			 * outputs['scores'] tensor_info: dtype: DT_FLOAT shape: (-1, 2) name:
			 * linear/binary_logistic_head/predictions/probabilities:0 Method name is:
			 * tensorflow/serving/classify
			 * 
			 * 
			 */
			String[] inputs = new String[] { ""HS-grad"", ""Male"", ""Divorced"", ""United-States"", ""Handlers-cleaners"",
					""White"", ""Private"" };

			byte[][][] stringMatrix = new byte[7][1][];
			for (int i = 0; i < 7; ++i) {
				stringMatrix[i][0] = String.format(inputs[i]).getBytes(""UTF-8"");
			}

			Tensor<String> t = Tensors.create(stringMatrix);

			Session sess = b.session();

			final String xName = ""input_example_tensor:0"";
			final String scoresName = ""linear/binary_logistic_head/predictions/probabilities:0"";
			List<Tensor<?>> outputs = s.runner().feed(xName, t).fetch(scoresName).run();

			float[][] classes = new float[2][2];
			outputs.get(0).copyTo(classes);

	      }
		  
```

**_3] How to create a Complex Input data types in JAVA_**

In python, we can create different input tensors like CrossedColumn, Bucketized etc , Is there a way that we can convert similarly in Java as we dont have estimators or contrib libraries present in JAVA API.


If anyone you could help or guide in the right direction.. @eggie5 @asimshankar @ry",0,,1,2017-11-18T02:15:09Z,2017-12-01T02:21:33Z,NONE,2017-12-01T02:21:33Z
14682,MKL: Adding MKL-DNN Reshape op,"awaiting testing (then merge),cla: yes",,2,,5,2017-11-18T01:40:08Z,2017-12-06T19:27:54Z,CONTRIBUTOR,2017-11-29T21:49:22Z
14681,MKL: Adding MKL-DNN AddN op,"awaiting testing (then merge),cla: yes",,2,,5,2017-11-18T01:38:48Z,2017-12-06T20:30:45Z,CONTRIBUTOR,2017-11-29T21:49:39Z
14680,MKL: Adding MKL-DNN Identity op,"awaiting testing (then merge),cla: yes",,2,,6,2017-11-18T01:38:05Z,2017-12-06T20:31:13Z,CONTRIBUTOR,2017-11-29T21:50:02Z
14679,MKL: Adding MKL-DNN pooling ops,"awaiting testing (then merge),cla: yes",,2,,5,2017-11-18T01:36:07Z,2017-12-06T22:28:04Z,CONTRIBUTOR,2017-11-22T22:55:02Z
14678,Fix shape inference for bitwise ops with broadcasting,cla: yes,"This fix tries to address the issue raised in #14646 where shape inference for bitwise ops is incorrect with broadcasting.
As was specified in #14646, in the following
```
>>> import tensorflow as tf
>>> tf.bitwise.bitwise_and(tf.zeros([3,1], dtype=tf.int32), tf.zeros([1,3], dtype=tf.int32))
<tf.Tensor 'BitwiseAnd:0' shape=(3, 1) dtype=int32>
```
the result shape should be (3, 3), not (3, 1).

This fix fixes the issue by changing
`.SetShapeFn(shape_inference::UnchangedShape)`
to
`.SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)`

Additional test cases have been added.

This fix fixes #14646.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,3,2017-11-18T01:06:32Z,2017-12-27T22:44:18Z,MEMBER,2017-12-27T01:51:21Z
14673,Fixed bug in code within programmer's guide markdown docs for Variabl,"awaiting testing (then merge),cla: yes,stat:awaiting response","es. Had a call to {tensor}.run() method, but Tensor instances have no run() method, switched to eval() instead.",0,,2,2017-11-17T22:15:23Z,2017-12-20T21:57:56Z,CONTRIBUTOR,2017-12-20T00:22:36Z
14672,Compilation Flags,"type:build/install,type:docs","Can someone please explain the flags that can be enabled when compiling TensorFlow from source? I don't seem to understand the functionality of most of them and they are not documented.
",1,,5,2017-11-17T22:03:01Z,2018-01-04T20:02:05Z,NONE,2017-12-05T18:17:07Z
14671,Added mode to input_fn argument,"awaiting testing (then merge),cla: yes","And modified existing tests to include the mode argument. 

(deleted previous PR because I accidentally pulled a bunch of other commits into the branch)",1,,8,2017-11-17T21:34:05Z,2017-12-29T00:45:00Z,MEMBER,2017-12-20T00:23:13Z
14667,`mean_relative_error` supports complex label,cla: yes,"Fix #14658 


### How to test

+ [x] add test case
+ [ ] pass all tests",1,,7,2017-11-17T20:41:03Z,2017-12-28T18:53:33Z,CONTRIBUTOR,2017-12-27T01:49:46Z
14665,Use cub::ReduceByKey to count partition indices,cla: yes,"This implements a suggestion made by @ekelsen in the comments for #13905.
It replaces the previously custom-made counting method, and is likely more efficient.

In order to use cub::ReduceByKey properly, I defined a specialization of TransformOutputIterator that only allows writes in a bounded interval. This is needed in the case of wrong inputs.

I've also added tests for the GPU kernel, covering the case of wrong inputs.",1,,2,2017-11-17T19:25:18Z,2017-11-20T20:58:54Z,CONTRIBUTOR,2017-11-18T16:00:38Z
14664,TensorFlow Lite Android example doesn't compile with Bazel,comp:lite,"TensorFlow Lite Android example doesn't compile with Bazel, as explained in its README.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master (c0662f1620c2b97abb79b8ae6a8a30f7c7719475)
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: 7.2.0
- **CUDA/cuDNN version**: (not using cuda)
- **GPU model and memory**: (not using GPU)
- **Exact command to reproduce**: `bazel build --cxxopt='--std=c++11'   //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo`

### Describe the problem
When trying to compile the TF Lite Android demo with Bazel, it doesn't work, yielding:

```
WARNING: The major revision of the Android NDK referenced by android_ndk_repository rule 'androidndk' is 16. The major revisions supported by Bazel are [10, 11, 12, 13, 14]. Defaulting to revision 14.
INFO: Analysed target //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo (0 packages loaded).
INFO: Found 1 target...
ERROR: /home/santiago/repos/tensorflow/tensorflow/contrib/lite/kernels/internal/BUILD:273:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels/internal:tensor_utils' failed (Exit 1)
In file included from tensorflow/contrib/lite/kernels/internal/tensor_utils.cc:24:
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/neon_tensor_utils.h:21:
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/cpu_check.h:21:
external/androidndk/ndk/sources/android/cpufeatures/cpu-features.h:31:10: fatal error: 'sys/cdefs.h' file not found
#include <sys/cdefs.h>
         ^~~~~~~~~~~~~
1 error generated.
Target //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 14.636s, Critical Path: 7.92s
FAILED: Build did NOT complete successfully
```

I have already tried installing all APT packages that contain a file named `sys/cdefs.h`, including `g{cc,++}-{5,6,7}-multilib`. NDK 16 is installed, along with LLDB 3.0 and cmake.

### Source code / logs
tf_env.txt:

```
== cat /etc/issue ===============================================
Linux s.local 4.13.0-16-lowlatency #19-Ubuntu SMP PREEMPT Wed Oct 11 19:51:52 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""17.10 (Artful Aardvark)""
VERSION_ID=""17.10""
VERSION_CODENAME=artful

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 7.2.0-8ubuntu3) 7.2.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux s.local 4.13.0-16-lowlatency #19-Ubuntu SMP PREEMPT Wed Oct 11 19:51:52 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)

== check for virtualenv =========================================
True

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""tensorflow/python/pywrap_tensorflow.py"", line 25, in <module>
    from tensorflow.python.platform import self_check
ImportError: No module named platform

== env ==========================================================
LD_LIBRARY_PATH /home/santiago/torch/install/lib:/home/santiago/torch/install/lib:
DYLD_LIBRARY_PATH /home/santiago/torch/install/lib:/home/santiago/torch/install/lib:

== nvidia-smi ===================================================
/dev/fd/63: lnea 105: nvidia-smi: orden no encontrada

== cuda libs  ===================================================
```",1,,8,2017-11-17T19:24:00Z,2017-11-29T02:27:20Z,CONTRIBUTOR,2017-11-27T23:51:40Z
14662,"Distributed TF hangs because of ""CreateSession still waiting for response from worker....""",,"**UPDATE:** The first 2 posts are no longer appropriate to describe the issue. Please jump to my 3rd post.

Hi,

I followed the idea of this https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/benchmark_cnn.py#L1670 to implement a worker sync queue. Everything seemed to work fine except when I increased the size of the dataset or number of workers: all workers hang when they try to evaluate the sync op. Typical example of my code is as follow:

For each worker:
```
    def create_sync_queue_ops(self, op_prefix):
        """"""
        op_prefix: a string that denote where the sync is being used.
        """"""
        #Evenly distribute queues to all ps or workers
        device_name = ""/job:ps/task:{}/cpu:0"".format(self.sync_queue_counter % self.num_ps_nodes)
        self.sync_queue_counter += 1
        with tf.device(device_name):
            sync_queues = [tf.FIFOQueue(self.num_worker_nodes, [tf.bool], shapes=[[]], shared_name=""{0}_{1}"".format(op_prefix,i)) for i in xrange(self.num_worker_nodes)]
            token = tf.constant(False)
            queue_ops = []
            for i, q in enumerate(sync_queues):
                if i == self.worker_id:
                    queue_ops.append(tf.no_op())
                else:
                    queue_ops.append(q.enqueue(token))
            #Drain tokens off queue for this worker after enqueuing ops
            with tf.control_dependencies(queue_ops):
                wait_ops = sync_queues[self.worker_id].dequeue_many(len(sync_queues)-1)
            return wait_ops

(some graph definition...)
sess = tf.Session()
demo_sync_ops = self.create_sync_queue_ops(""demo"")
if self.is_chief_worker: #only execute by worker 0, other workers do nothing
    sess.run(tf.global_variables_initializer())
print ""finishing message""
sess.run(demo_sync_ops)
```

I could **occasionally** see all workers hang after printing the ""finishing message"". 
My observation so far is that this only happened when dataset is huge or number of worker is big. e.g. 10+TB dataset with 300-500 workers.

I haven't been able to see why this occurred, not sure if it is a TF issue or some network bottleneck that I was not aware of. Any help would be much appreciated!
  ",1,,14,2017-11-17T18:57:03Z,2018-01-24T16:25:15Z,CONTRIBUTOR,2017-11-29T23:44:07Z
14657,FusedBatchNorm & Conv2D backwards doesn't support zero batch size,stat:awaiting tensorflower,"Most ops in TF work well with tensors with zero elements. However, <del>convolution</del> fusedbatchnorm with cudnn gives the following error:
```
2017-11-17 08:00:20.835113: F tensorflow/stream_executor/cuda/cuda_dnn.cc:444] could not convert BatchDescriptor {count: 0 feature_map_count: 1024 spatial: 28 28  value_min: 0.000000 value_max: 0.000000 layout: BatchDepthYX} to cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM 
```

I would expect it checks and returns a 4D tensor with zero batch-size. Currently I have to work around it by `tf.cond`.",1,,10,2017-11-17T16:25:50Z,2017-12-29T00:38:36Z,CONTRIBUTOR,2017-11-18T00:39:21Z
14656,little_modify,"cla: no,stat:awaiting response",Modify the list '[' ']' to the tuple '(' ')',1,,7,2017-11-17T16:09:33Z,2018-01-17T18:22:50Z,NONE,2017-12-27T01:41:53Z
14655,Support dynamic partition in loss function,stat:awaiting response,"

### System information

Python 3.6.1, Ubuntu 16.04, TF 1.3

### Describe the problem

I am trying to implement a loss function in Tensorflow similar to the Theano loss function described here: https://github.com/Lasagne/Lasagne/issues/767

I have tried several code but they all lead to runtime error.  Seems that dynamically sized tensors aren't supported in loss function.  I have tried various optimizers, including adam, without success.

When tested standalone, the function works fine and produces results similar to a numpy based implementation.

### Source code / logs
  Here is one code I tried:

    import tensorflow as tf
    
    def pair_loss(y_true, y_pred):
        y_true = tf.cast(y_true, tf.int32)
        parts = tf.dynamic_partition(y_pred, y_true, 2)
        y_pos = parts[1]
        y_neg = parts[0]
        y_pos = tf.expand_dims(y_pos, 0)
        y_neg = tf.expand_dims(y_neg, -1)
        out = tf.sigmoid(y_neg - y_pos)
        return tf.reduce_mean(out, axis=-1)

Here is the theano code for reference:

    import theano
    
    def calc_auroc_loss(pred_vr, y_vr):
        pos_pred_vr = pred_vr[y_vr.nonzero()]
        neg_pred_vr = pred_vr[theano.tensor.eq(y_vr, 0).nonzero()]
        pred_diffs_vr = pos_pred_vr.dimshuffle(0, 'x') - neg_pred_vr.dimshuffle('x', 0)
        num_pairs_vr = theano.tensor.sum(theano.tensor.eq(y_vr, 1)) * theano.tensor.sum(theano.tensor.eq(y_vr, 0))
        auroc_vr = theano.tensor.sum(theano.tensor.nnet.sigmoid(pred_diffs_vr)) / num_pairs_vr
        return -auroc_vr",0,,12,2017-11-17T14:05:50Z,2017-11-20T14:34:45Z,NONE,2017-11-17T16:50:52Z
14646,tf.bitwise.bitwise_and and friends have bad shape functions,stat:community support,"The bitwise ops are componentwise and do normal broadcasting at the kernel level.  However, they claim unchanged shape during op registration:

    #define BINARY_BITWISE()                                                     \
      Input(""x: T"")                                                              \
          .Input(""y: T"")                                                         \
          .Output(""z: T"")                                                        \
          .SetIsCommutative()                                                    \
          .Attr(""T: {int8, int16, int32, int64, uint8, uint16, uint32, uint64}"") \
          .SetShapeFn(shape_inference::UnchangedShape)

To reproduce, do

    >>> import tensorflow as tf
    >>> tf.bitwise.bitwise_and(tf.zeros([3,1], dtype=tf.int32), tf.zeros([1,3], dtype=tf.int32))
    <tf.Tensor 'BitwiseAnd:0' shape=(3, 1) dtype=int32>

The result shape should be `(3, 3)`, not `(3, 1)`.",0,,1,2017-11-17T07:36:31Z,2017-12-27T22:44:18Z,CONTRIBUTOR,2017-11-18T01:10:08Z
14644,[Java] Add support for shape list attributes,"awaiting testing (then merge),cla: yes",This allows passing a list of shapes as an attribute to an operation. Some operation wrappers cannot be generated without it.,1,,2,2017-11-17T04:28:44Z,2017-11-17T20:43:23Z,CONTRIBUTOR,2017-11-17T18:40:13Z
14643,"Can google publish checkpoint file for some common network:VGG, AlexNet","stat:awaiting tensorflower,type:feature","In tensorflow/models repository, developer maintains some pre-built model and checkpoint files from some famous network, it is very convenient  for some developer don't have too much computation resources, train a network from scratch can take a very long time.  The author of these networks do publish weights file but It is not for tensorflow, mostly for caffe. ",0,,3,2017-11-17T03:06:14Z,2018-01-04T07:07:48Z,CONTRIBUTOR,2017-12-20T01:23:19Z
14642,Android tensorflow lite kernel_util.cc:34 input_product_scale < output_scale,"comp:lite,stat:awaiting response,type:support","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 7.0
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 3.0
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
1.down load Quantilized MobileNet model 0.75_224 from [here](https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_0.75_224_frozen.tgz
)

2.Transform the frozen .pb model to .tflite file:
 bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=tmp/mobilenet_v1_0.75_224/frozen_graph.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=tmp/mobilenet_v1_0.75_224/mobilenet_v1_0.75_224.tflite --inference_type=QUANTIZED_UINT8  --input_type=QUANTIZED_UINT8  --input_arrays=input --default_ranges_min=0 --default_ranges_max=6 --output_arrays=MobilenetV1/Predictions/Reshape_1 --input_shapes=1,224,224,3

3.Change the ""MODEL_PATH"" in Tensorflow lite Android demo to ""mobilenet_v1_0.75_224.tflite""

### logs
4.Then run the demo, make the error:
FATAL EXCEPTION: CameraBackground
                                                                                      Process: android.example.com.tflitecamerademo, PID: 13909
                                                                                      java.lang.NullPointerException: Can not allocate memory for the given inputs: tensorflow/contrib/lite/kernels/kernel_util.cc:34 input_product_scale < output_scale was not true.
                                                                                          at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
                                                                                          at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:95)
                                                                                          at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:112)
                                                                                          at org.tensorflow.lite.Interpreter.run(Interpreter.java:93)
                                                                                          at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:109)
                                                                                          at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:663)
                                                                                          at com.example.android.tflitecamerademo.Camera2BasicFragment.-wrap0(Camera2BasicFragment.java)
                                                                                          at com.example.android.tflitecamerademo.Camera2BasicFragment$4.run(Camera2BasicFragment.java:558)
                                                                                          at android.os.Handler.handleCallback(Handler.java:739)
                                                                                          at android.os.Handler.dispatchMessage(Handler.java:95)
                                                                                          at android.os.Looper.loop(Looper.java:135)
                                                                                          at android.os.HandlerThread.run(HandlerThread.java:61)

need your help?

Followed the same steps , transform other mobilenet models, only the  ""mobilenet_v1_1.0_128"" can run successfully.
",0,,9,2017-11-17T02:58:44Z,2018-01-24T23:41:17Z,NONE,2017-12-20T01:23:13Z
14641,Incorrect second derivative for softmax cross entropy,"comp:eager,stat:awaiting tensorflower","
### System information
-  I've written custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: tensorflow-gpu==1.2.1
- python version: 2.7.12
- **CUDA/cuDNN version**: CUDA version: 8.0 and CUDNN version is: 8.0
- **GPU model and memory**: GeForce GTX 750 2GB

Incorrect second derivative for softmax cross entropy

### Source code / logs
```
import tensorflow as tf
import tensorflow.contrib.eager as tfe
import numpy as np

tfe.enable_eager_execution()

logits = [0.5, 0.5]
y = [1, 0]

def loss_function(x):
    loss2 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=x)
    return loss2

grad_loss = tfe.gradients_function(loss_function)
print grad_loss(logits)[0] # prints correct gradient [-0.5 0.5]

gradgrad_loss = tfe.gradients_function(lambda x: grad_loss(x)[0])
print gradgrad_loss(logits)[0] # this should be [-0.25,  0.25], but it prints [0. 0.]
```",1,,14,2017-11-17T01:23:36Z,2017-11-17T19:19:33Z,NONE,2017-11-17T19:07:57Z
14638,Add LICENSES to gitignore for iOS example,"awaiting testing (then merge),cla: yes","Update gitignore file for ios to cover the license files that get
installed following the install instructions.",1,,2,2017-11-16T23:07:19Z,2017-12-20T02:16:29Z,CONTRIBUTOR,2017-12-20T00:25:14Z
14637,macOS doesn't have wget,"awaiting testing (then merge),cla: yes","Make building easier on macOS by using curl instead of wget
because wget isn't part of macOS, and wget installed from
brew has trouble resolving certificates appropriately.",1,,5,2017-11-16T22:25:54Z,2018-01-19T14:58:59Z,CONTRIBUTOR,2017-12-20T00:25:56Z
14636,Tensorflow Python ,type:support,"Hi This is my code for tensorflow_serve python client :

```
data = f.read()
data = base64.urlsafe_b64encode(data)
request = predict_pb2.PredictRequest()
request.model_spec.name = 'test'
request.model_spec.signature_name = 'serving_default'
data = tf.contrib.util.make_tensor_proto(data,shape=[1])

req=request.inputs['input'].CopyFrom(data)
```
could you please do me a favor and tell me how to convert(change) :
```

dtype: DT_STRING
tensor_shape {
dim {
size: 1
 }
}
string_val: ""_9j_4AAQSkZJRgABAQEAYABgAAD_4QAWRXhpZgAASUk
```

to :
```

inputs: {
dtype: DT_STRING
tensor_shape {
dim {
size: 1
 }
}
string_val: ""_9j_4AAQSkZJRgABAQEAYABgAAD_4QAWRXhpZgAASUk
```
",0,,2,2017-11-16T20:40:19Z,2017-11-21T15:41:43Z,NONE,2017-11-21T15:41:40Z
14633,[FEATURE REQUEST] Report uninitialized data iterators.,stat:awaiting tensorflower,"At https://github.com/GPflow/GPflow/issues/561 we found that, iterators are not variables :), therefore `report_uninitalized_variables` fails with:

```
...
~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py in is_variable_initialized(ref, name)
    182     A `Tensor` of type `bool`.
    183   """"""
--> 184   if ref.dtype._is_ref_dtype:
    185     return gen_state_ops.is_variable_initialized(ref=ref, name=name)
    186   # Handle resource variables.

AttributeError: 'Iterator' object has no attribute 'dtype'
```

```
import numpy as np
from tensorflow import data
Dataset = data.Dataset
pl = tf.placeholder(tf.float32)
data = Dataset.from_tensor_slices(pl)
iterator = data.make_initializable_iterator()
batch = iterator.get_next()

## In fact it fails for TensorFlow 1.3 and 1.4
sess.run(tf.report_uninitialized_variables([iterator]))
```

Frankly, it must be very easy to adapt `report_unitialized_variables` to report data's unitialized iterators. This would be helpful to avoid re-initializing datasets each time when we call `session.run` in GPflow. Right now, it provides initializing by demand feature.

Best,
Artem Artemev",0,,9,2017-11-16T19:20:00Z,2017-12-12T21:35:16Z,NONE,2017-11-17T01:30:21Z
14632,libtensorflow_cc.so linker issues with release 1.4 Undefined reference ,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
  Yes, code similar to the example label_image.cc

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
 Linux Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
From source

- **TensorFlow version (use command below)**:
1.4.0

- **Python version**: 
3.6.1

- **Bazel version (if compiling from source)**:
0.7

- **GCC/Compiler version (if compiling from source)**:
5.4.0

- **CUDA/cuDNN version**:
N/A

- **GPU model and memory**:
N/A

- **Exact command to reproduce**:
build libtensorflow_cc.so from command:
bazel build --config=opt --config=mkl --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 //tensorflow:libtensorflow_cc.so //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
After successfully build the library, try to compile my code with the library using following gcc command:

g++ -I/usr/local/include -L/usr/local/lib -ltensorflow -std=c++11 rtclassifier.cc

result in 'undefined reference to `tensorflow::GraphDef::GraphDef()' error

### Source code / logs

/tmp/ccxBMZph.o: In function `LoadGraph(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unique_ptr<tensorflow::Session, std::default_delete<tensorflow::Session> >*)':
tl_classifier.cc:(.text+0x90): undefined reference to `tensorflow::GraphDef::GraphDef()'
tl_classifier.cc:(.text+0xa4): undefined reference to `tensorflow::Env::Default()'
tl_classifier.cc:(.text+0xc4): undefined reference to `tensorflow::ReadBinaryProto(tensorflow::Env*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::MessageLite*)'
tl_classifier.cc:(.text+0x15e): undefined reference to `tensorflow::SessionOptions::SessionOptions()'
tl_classifier.cc:(.text+0x16d): undefined reference to `tensorflow::NewSession(tensorflow::SessionOptions const&)'
tl_classifier.cc:(.text+0x22a): undefined reference to `tensorflow::GraphDef::~GraphDef()'
tl_classifier.cc:(.text+0x2b7): undefined reference to `tensorflow::GraphDef::~GraphDef()'
/tmp/ccxBMZph.o: In function `ReadTensorFromImageFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, int, float, float, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*)':
tl_classifier.cc:(.text+0x332): undefined reference to `tensorflow::Scope::NewRootScope()'
tl_classifier.cc:(.text+0x3dd): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'
tl_classifier.cc:(.text+0x3fd): undefined reference to `tensorflow::ops::ReadFile::ReadFile(tensorflow::Scope const&, tensorflow::Input)'
tl_classifier.cc:(.text+0x40c): undefined reference to `tensorflow::Scope::~Scope()'
tl_classifier.cc:(.text+0x504): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'
tl_classifier.cc:(.text+0x528): undefined reference to `tensorflow::ops::DecodePng::DecodePng(tensorflow::Scope const&, tensorflow::Input, tensorflow::ops::DecodePng::Attrs const&)'
tl_classifier.cc:(.text+0x587): undefined reference to `tensorflow::Scope::~Scope()'
tl_classifier.cc:(.text+0x671): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'
tl_classifier.cc:(.text+0x691): undefined reference to `tensorflow::ops::DecodeGif::DecodeGif(tensorflow::Scope const&, tensorflow::Input)'
tl_classifier.cc:(.text+0x6f4): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'
tl_classifier.cc:(.text+0x714): undefined reference to `tensorflow::ops::Squeeze::Squeeze(tensorflow::Scope const&, tensorflow::Input)'
tl_classifier.cc:(.text+0x773): undefined reference to `tensorflow::Scope::~Scope()'
tl_classifier.cc:(.text+0x7be): undefined reference to `tensorflow::Scope::~Scope()'
tl_classifier.cc:(.text+0x867): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'
tl_classifier.cc:(.text+0x88b): undefined reference to `tensorflow::ops::DecodeJpeg::DecodeJpeg(tensorflow::Scope const&, tensorflow::Input, tensorflow::ops::DecodeJpeg::Attrs const&)'
tl_classifier.cc:(.text+0x8ea): undefined reference to `tensorflow::Scope::~Scope()'
tl_classifier.cc:(.text+0x97a): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'
tl_classifier.cc:(.text+0x99c): undefined reference to `tensorflow::ops::Cast::Cast(tensorflow::Scope const&, tensorflow::Input, tensorflow::DataType)'
tl_classifier.cc:(.text+0x9ab): undefined reference to `tensorflow::Scope::~Scope()'
tl_classifier.cc:(.text+0xa38): undefined reference to `tensorflow::ops::ExpandDims::ExpandDims(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)'
tl_classifier.cc:(.text+0xaea): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'
tl_classifier.cc:(.text+0xb0a): undefined reference to `tensorflow::ops::Const(tensorflow::Scope const&, tensorflow::Input::Initializer const&)'
tl_classifier.cc:(.text+0xb60): undefined reference to `tensorflow::ops::ResizeBilinear::ResizeBilinear(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)'
tl_classifier.cc:(.text+0xb9c): undefined reference to `tensorflow::Scope::~Scope()'
tl_classifier.cc:(.text+0xc9c): undefined reference to `tensorflow::ops::Subtract::Subtract(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)'
tl_classifier.cc:(.text+0xcd5): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'
tl_classifier.cc:(.text+0xcf9): undefined reference to `tensorflow::ops::Div::Div(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)'
tl_classifier.cc:(.text+0xd17): undefined reference to `tensorflow::Scope::~Scope()'
tl_classifier.cc:(.text+0xdbb): undefined reference to `tensorflow::GraphDef::GraphDef()'
tl_classifier.cc:(.text+0xddb): undefined reference to `tensorflow::Scope::ToGraphDef(tensorflow::GraphDef*) const'
tl_classifier.cc:(.text+0xe3e): undefined reference to `tensorflow::SessionOptions::SessionOptions()'
tl_classifier.cc:(.text+0xe4d): undefined reference to `tensorflow::NewSession(tensorflow::SessionOptions const&)'
tl_classifier.cc:(.text+0x109e): undefined reference to `tensorflow::GraphDef::~GraphDef()'
tl_classifier.cc:(.text+0x1116): undefined reference to `tensorflow::Scope::~Scope()'
tl_classifier.cc:(.text+0x1175): undefined reference to `tensorflow::Scope::~Scope()'
tl_classifier.cc:(.text+0x11b4): undefined reference to `tensorflow::Scope::~Scope()'
tl_classifier.cc:(.text+0x1216): undefined reference to `tensorflow::Scope::~Scope()'
tl_classifier.cc:(.text+0x1275): undefined reference to `tensorflow::Scope::~Scope()'
/tmp/ccxBMZph.o:tl_classifier.cc:(.text+0x12d7): more undefined references to `tensorflow::Scope::~Scope()' follow
/tmp/ccxBMZph.o: In function `ReadTensorFromImageFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, int, float, float, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*)':
tl_classifier.cc:(.text+0x15ee): undefined reference to `tensorflow::GraphDef::~GraphDef()'
tl_classifier.cc:(.text+0x167a): undefined reference to `tensorflow::Scope::~Scope()'
/tmp/ccxBMZph.o: In function `GetTopLabels(std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> > const&, int, tensorflow::Tensor*, tensorflow::Tensor*)':
tl_classifier.cc:(.text+0x18fe): undefined reference to `tensorflow::Scope::NewRootScope()'
tl_classifier.cc:(.text+0x1999): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'
tl_classifier.cc:(.text+0x19bd): undefined reference to `tensorflow::ops::TopK::TopK(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)'
tl_classifier.cc:(.text+0x19db): undefined reference to `tensorflow::Scope::~Scope()'
tl_classifier.cc:(.text+0x1a08): undefined reference to `tensorflow::GraphDef::GraphDef()'
tl_classifier.cc:(.text+0x1a28): undefined reference to `tensorflow::Scope::ToGraphDef(tensorflow::GraphDef*) const'
tl_classifier.cc:(.text+0x1a8b): undefined reference to `tensorflow::SessionOptions::SessionOptions()'
tl_classifier.cc:(.text+0x1a9a): undefined reference to `tensorflow::NewSession(tensorflow::SessionOptions const&)'
tl_classifier.cc:(.text+0x1d7e): undefined reference to `tensorflow::GraphDef::~GraphDef()'
tl_classifier.cc:(.text+0x1d9c): undefined reference to `tensorflow::Scope::~Scope()'
tl_classifier.cc:(.text+0x1de4): undefined reference to `tensorflow::Scope::~Scope()'
tl_classifier.cc:(.text+0x1f1a): undefined reference to `tensorflow::GraphDef::~GraphDef()'
tl_classifier.cc:(.text+0x1f3d): undefined reference to `tensorflow::Scope::~Scope()'
/tmp/ccxBMZph.o: In function `PrintTopLabels(std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)':
tl_classifier.cc:(.text+0x1ff4): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'
tl_classifier.cc:(.text+0x201c): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'
tl_classifier.cc:(.text+0x2081): undefined reference to `tensorflow::Tensor::Tensor()'
tl_classifier.cc:(.text+0x2090): undefined reference to `tensorflow::Tensor::Tensor()'
tl_classifier.cc:(.text+0x21ed): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'
tl_classifier.cc:(.text+0x225a): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'
tl_classifier.cc:(.text+0x2284): undefined reference to `tensorflow::Tensor::~Tensor()'
tl_classifier.cc:(.text+0x2293): undefined reference to `tensorflow::Tensor::~Tensor()'
tl_classifier.cc:(.text+0x22e2): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'
",0,,6,2017-11-16T19:06:47Z,2017-11-17T22:51:42Z,NONE,2017-11-16T19:30:42Z
14631,Fixing download_dependencies.sh bugs for generating TFLite iOS exmaples.,cla: yes,"Root cause: The script downloads files for building TFLite for iOS
example. It writes to `downloads/` directory and conflicts with the
visibility rule ""**/*"" in BUILD",1,,4,2017-11-16T18:59:28Z,2017-11-20T22:56:22Z,CONTRIBUTOR,2017-11-16T18:59:39Z
14624,Bug: using regularizer for shared variables in tf.cond branches ,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.1
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:


### Describe the problem
Setting the `regularizer` of a shared variable in `tf.cond` branches gives an unexpected behaviour - only one copy of the regularization op is added to `tf.GraphKeys.REGULARIZATION_LOSSES`. This is different from adding operations to a collection explicitly. And optimizing the regularization loss doesn't raise an error.


### Source code
```python
import tensorflow as tf
from tensorflow.contrib.layers.python.layers import regularizers


def regularised_model(is_training):
    scope_name = 'foo'
    with tf.variable_scope(scope_name) as scope:
        if is_training:
            scope.reuse_variables()
        y = tf.get_variable(
            name='y', shape=(),
            initializer=tf.constant_initializer(1.0),
            regularizer=regularizers.l2_regularizer(scale=0.1, scope=scope_name))
        reg = tf.nn.l2_loss(y) * 0.1
        tf.add_to_collection('test_collection', reg)
    return y

binary_flag = tf.placeholder(dtype=tf.bool, shape=())
y_cond = tf.cond(binary_flag,
                 lambda: regularised_model(False),
                 lambda: regularised_model(True))

reg_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
assert(len(reg_loss) == 1)
output = y_cond + reg_loss[0]
opt = tf.train.AdamOptimizer(0.1).minimize(output)

reg_test = tf.get_collection('test_collection')
assert(len(reg_test) == 2)
output_reg_true = y_cond + reg_test[0]
output_reg_false = y_cond + reg_test[1]


with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(output_reg_true, feed_dict={binary_flag: True})) # 1.05
    print(sess.run(output_reg_false, feed_dict={binary_flag: False})) # 1.05

    print(sess.run(output, feed_dict={binary_flag: True})) # 1.05
    
    print(sess.run([opt], feed_dict={binary_flag: True})) # [None]
    print(sess.run([opt], feed_dict={binary_flag: False})) # [None]

    print(sess.run(output, feed_dict={binary_flag: False})) # Error: Retval[0] does not have value

```
### Log
```
1.05
1.05
1.05
[None]
[None]
Traceback (most recent call last):
  File ""test_tf_cond.py"", line 43, in <module>
    print(sess.run(output, feed_dict={binary_flag: False})) # Error: Retval[0] does not have value
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value
```",0,,1,2017-11-16T16:05:16Z,2017-12-05T18:07:27Z,NONE,2017-12-05T18:07:27Z
